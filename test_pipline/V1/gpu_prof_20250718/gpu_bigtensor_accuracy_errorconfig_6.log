2025-07-17 14:35:45.005510 test begin: paddle.mode(Tensor([2, 10, 10],"float64"), 1, )
W0717 14:35:45.225003 110622 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 10, 10],"float64"), 1, ) 	 200 	 1278 	 9.513684272766113 	 0.04295063018798828 	 221.50278659768634 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-17 14:35:55.292249 test begin: paddle.moveaxis(Tensor([2, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([2, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 840 	 1317841 	 10.470689296722412 	 7.790558576583862 	 1.344022921308138 	 53.749157190322876 	 72.55010867118835 	 0.740855639981531 	 
2025-07-17 14:38:19.863244 test begin: paddle.moveaxis(x=Tensor([4, 2, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([4, 2, 3, 5, 7],"float64"), source=0, destination=2, ) 	 840 	 1317841 	 9.166221618652344 	 5.970916032791138 	 1.5351449540260145 	 53.68256425857544 	 73.36828541755676 	 0.7316862313607971 	 
2025-07-17 14:40:42.060471 test begin: paddle.moveaxis(x=Tensor([4, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([4, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 840 	 1317841 	 9.882874488830566 	 7.806450843811035 	 1.265988179079578 	 53.13740038871765 	 73.09173560142517 	 0.7269960133178389 	 
2025-07-17 14:43:05.986302 test begin: paddle.multigammaln(Tensor([10, 20],"float32"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([10, 20],"float32"), 2, ) 	 200 	 56625 	 10.110743522644043 	 2.508315086364746 	 4.03089052791105 	 6.562710285186768 	 6.200989723205566 	 1.0583327143129357 	 
2025-07-17 14:43:32.694729 test begin: paddle.multigammaln(Tensor([10, 20],"float64"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([10, 20],"float64"), 2, ) 	 200 	 56625 	 10.287306547164917 	 2.405731439590454 	 4.276165817127203 	 6.628762245178223 	 6.172931432723999 	 1.073843491932823 	 
2025-07-17 14:43:58.201631 test begin: paddle.multiplex(inputs=list[Tensor([4, 4],"float32"),Tensor([4, 4],"float32"),], index=Tensor([2, 1],"int32"), )
[Prof] paddle.multiplex 	 paddle.multiplex(inputs=list[Tensor([4, 4],"float32"),Tensor([4, 4],"float32"),], index=Tensor([2, 1],"int32"), ) 	 34 	 188272 	 6.598387718200684 	 14.414034366607666 	 0.4577752175675997 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 14:44:39.265687 test begin: paddle.multiplex(inputs=list[Tensor([4, 4],"float32"),Tensor([4, 4],"float32"),], index=Tensor([4, 1],"int32"), )
[Prof] paddle.multiplex 	 paddle.multiplex(inputs=list[Tensor([4, 4],"float32"),Tensor([4, 4],"float32"),], index=Tensor([4, 1],"int32"), ) 	 36 	 188272 	 7.890493631362915 	 24.043275594711304 	 0.3281788124201576 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 14:45:32.247038 test begin: paddle.multiplex(inputs=list[Tensor([7, 4],"float32"),Tensor([7, 4],"float32"),], index=Tensor([6, 1],"int32"), )
[Prof] paddle.multiplex 	 paddle.multiplex(inputs=list[Tensor([7, 4],"float32"),Tensor([7, 4],"float32"),], index=Tensor([6, 1],"int32"), ) 	 62 	 188272 	 9.136196613311768 	 34.085402965545654 	 0.2680383923448655 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 14:46:37.461776 test begin: paddle.multiply(Tensor([512, 872, 14, 14],"float32"), Tensor([512, 872, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 14, 14],"float32"), Tensor([512, 872, 1, 1],"float32"), ) 	 87953408 	 16804 	 8.526538848876953 	 8.79872465133667 	 0.9690653119349101 	 25.398173332214355 	 26.1700222492218 	 0.9705063713871929 	 
2025-07-17 14:47:53.802198 test begin: paddle.multiply(x=Tensor([128, 224, 56, 56],"float32"), y=Tensor([128, 224, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 56, 56],"float32"), y=Tensor([128, 224, 1, 1],"float32"), ) 	 89944064 	 16804 	 8.737137079238892 	 9.079216241836548 	 0.9623228312349943 	 21.744821786880493 	 26.625251293182373 	 0.8166992133684929 	 
2025-07-17 14:49:05.123431 test begin: paddle.multiply(x=Tensor([128, 256, 56, 56],"float32"), y=Tensor([128, 256, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 56, 56],"float32"), y=Tensor([128, 256, 1, 1],"float32"), ) 	 102793216 	 16804 	 9.973173379898071 	 10.365291833877563 	 0.962170051720309 	 24.807236433029175 	 30.295528411865234 	 0.8188415166679657 	 
2025-07-17 14:50:24.028469 test begin: paddle.mv(Tensor([3, 36],"float32"), Tensor([36],"float32"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([3, 36],"float32"), Tensor([36],"float32"), ) 	 144 	 554776 	 6.3121724128723145 	 8.467780351638794 	 0.7454341221369425 	 39.33312439918518 	 55.004862546920776 	 0.7150844957685852 	 
2025-07-17 14:52:14.452133 test begin: paddle.mv(Tensor([5, 100],"float64"), Tensor([100],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([5, 100],"float64"), Tensor([100],"float64"), ) 	 600 	 554776 	 9.503792524337769 	 8.440487384796143 	 1.1259767464917914 	 38.72374129295349 	 54.95204949378967 	 0.7046823849095892 	 
2025-07-17 14:54:06.108781 test begin: paddle.mv(Tensor([64, 32],"float64"), Tensor([32],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([64, 32],"float64"), Tensor([32],"float64"), ) 	 2080 	 554776 	 9.622572422027588 	 8.430163145065308 	 1.1414455754228519 	 44.234007596969604 	 54.68468475341797 	 0.8088920654188253 	 
2025-07-17 14:56:03.096220 test begin: paddle.nan_to_num(Tensor([148, 5, 3],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([148, 5, 3],"float32"), neginf=-1.1920928955078125e-07, ) 	 2220 	 67676 	 9.978965044021606 	 0.8663144111633301 	 11.518872265580063 	 5.280879974365234 	 7.078015565872192 	 0.7460961233015451 	 
2025-07-17 14:56:26.382700 test begin: paddle.nan_to_num(Tensor([1948, 2],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([1948, 2],"float32"), neginf=-1.1920928955078125e-07, ) 	 3896 	 67676 	 9.913041830062866 	 0.8252401351928711 	 12.012311819693595 	 5.20707631111145 	 7.067956447601318 	 0.7367159588085177 	 
2025-07-17 14:56:49.403235 test begin: paddle.nan_to_num(Tensor([400, 1],"float64"), neginf=-2.220446049250313e-16, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([400, 1],"float64"), neginf=-2.220446049250313e-16, ) 	 400 	 67676 	 9.840233564376831 	 0.830634355545044 	 11.846648887909152 	 5.323418855667114 	 7.0505781173706055 	 0.7550329585813303 	 
2025-07-17 14:57:12.455852 test begin: paddle.nanmean(Tensor([2, 3, 4, 5],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 5],"float32"), -1, False, ) 	 120 	 76935 	 9.941126585006714 	 4.333124399185181 	 2.2942167519760304 	 6.663082838058472 	 8.644890785217285 	 0.7707538479783118 	 
2025-07-17 14:57:42.388654 test begin: paddle.nanmean(Tensor([2, 3, 4, 5],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 5],"float32"), 2, True, ) 	 120 	 76935 	 9.72100019454956 	 4.150264024734497 	 2.342260669830864 	 6.662714719772339 	 8.380831003189087 	 0.7949945199034597 	 
2025-07-17 14:58:11.310713 test begin: paddle.nanmean(Tensor([2, 3, 4, 5],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 5],"float32"), None, False, ) 	 120 	 76935 	 9.880667448043823 	 4.252776861190796 	 2.3233449039405265 	 6.667963743209839 	 8.333030223846436 	 0.8001847544160208 	 
2025-07-17 14:58:40.453844 test begin: paddle.nanmedian(Tensor([120],"float32"), keepdim=False, )
[Prof] paddle.nanmedian 	 paddle.nanmedian(Tensor([120],"float32"), keepdim=False, ) 	 120 	 132170 	 10.606434106826782 	 25.51078224182129 	 0.4157627941897856 	 7.728546857833862 	 40.03056859970093 	 0.1930661274167263 	 combined
2025-07-17 15:00:04.953849 test begin: paddle.nanmedian(Tensor([120],"float32"), keepdim=False, mode="min", )
[Prof] paddle.nanmedian 	 paddle.nanmedian(Tensor([120],"float32"), keepdim=False, mode="min", ) 	 120 	 132170 	 10.597225666046143 	 28.680793285369873 	 0.3694885828507333 	 7.659470796585083 	 18.166313409805298 	 0.4216304444274792 	 combined
2025-07-17 15:01:10.101445 test begin: paddle.nanmedian(Tensor([2, 100],"float32"), axis=1, mode="min", )
[Prof] paddle.nanmedian 	 paddle.nanmedian(Tensor([2, 100],"float32"), axis=1, mode="min", ) 	 200 	 132170 	 9.537641763687134 	 2.5052127838134766 	 3.8071184313409008 	 None 	 None 	 None 	 combined
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-17 15:01:31.375531 test begin: paddle.nanquantile(Tensor([4, 7, 6],"float64"), q=0, axis=1, )
W0717 15:01:31.386291 25762 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 7, 6],"float64"), q=0, axis=1, ) 	 168 	 24523 	 10.751761674880981 	 5.369370698928833 	 2.002424916764624 	 4.796977758407593 	 5.5933027267456055 	 0.8576288452026367 	 
2025-07-17 15:01:58.008886 test begin: paddle.nanquantile(Tensor([4, 7, 6],"float64"), q=0.35, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 7, 6],"float64"), q=0.35, ) 	 168 	 24523 	 11.39442253112793 	 4.895364999771118 	 2.3275940673802 	 4.530986070632935 	 5.162904739379883 	 0.8776040425601871 	 
2025-07-17 15:02:24.002795 test begin: paddle.nanquantile(Tensor([4, 7, 6],"float64"), q=0.35, axis=2, keepdim=True, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 7, 6],"float64"), q=0.35, axis=2, keepdim=True, ) 	 168 	 24523 	 9.807337522506714 	 5.2085020542144775 	 1.882947807339554 	 4.022869348526001 	 5.0944743156433105 	 0.78965347536903 	 
2025-07-17 15:02:48.387748 test begin: paddle.nansum(Tensor([2, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, ) 	 120 	 175749 	 10.362424850463867 	 2.482117176055908 	 4.174833062043345 	 12.262548208236694 	 15.660869598388672 	 0.7830055752139315 	 
2025-07-17 15:03:29.163561 test begin: paddle.nansum(Tensor([2, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, ) 	 120 	 175749 	 10.211708545684814 	 2.325540781021118 	 4.391111361719905 	 12.196407556533813 	 16.303739547729492 	 0.7480742390927314 	 
2025-07-17 15:04:10.781857 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 720 	 175749 	 10.15563178062439 	 2.6057076454162598 	 3.8974563391596586 	 12.487562656402588 	 16.967225074768066 	 0.735981434876633 	 
2025-07-17 15:04:53.551377 test begin: paddle.neg(Tensor([32, 8],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([32, 8],"float32"), ) 	 256 	 1013704 	 9.843944072723389 	 10.3450448513031 	 0.9515612753948968 	 52.99831247329712 	 68.09633159637451 	 0.7782843984523651 	 
2025-07-17 15:07:14.842074 test begin: paddle.neg(Tensor([32, 8],"float64"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([32, 8],"float64"), ) 	 256 	 1013704 	 9.49930214881897 	 10.474105596542358 	 0.906932058423663 	 53.062440156936646 	 67.62886881828308 	 0.7846122681648567 	 
2025-07-17 15:09:35.753825 test begin: paddle.neg(Tensor([8, 16, 32],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([8, 16, 32],"float32"), ) 	 4096 	 1013704 	 9.516790866851807 	 10.418255090713501 	 0.9134726289563374 	 53.26708221435547 	 68.06903576850891 	 0.7825449797101234 	 
2025-07-17 15:11:57.640463 test begin: paddle.negative(Tensor([2, 3, 4, 5],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 5],"float16"), ) 	 120 	 828925 	 9.89697003364563 	 8.547886610031128 	 1.1578265465093236 	 43.0735023021698 	 56.35320329666138 	 0.7643487820100845 	 
2025-07-17 15:13:55.519268 test begin: paddle.negative(Tensor([2, 3, 4, 5],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 5],"float32"), ) 	 120 	 828925 	 9.909507513046265 	 8.599704265594482 	 1.1523079407151262 	 43.23701524734497 	 56.02192234992981 	 0.7717874259521753 	 
2025-07-17 15:15:53.294082 test begin: paddle.negative(Tensor([2, 3, 4, 5],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 5],"float64"), ) 	 120 	 828925 	 9.939716577529907 	 8.574227571487427 	 1.1592550459686015 	 42.88276410102844 	 56.25847029685974 	 0.7622454694332862 	 
2025-07-17 15:17:50.955347 test begin: paddle.nextafter(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), ) 	 240 	 572324 	 5.01924467086792 	 6.196612358093262 	 0.8099981700989239 	 None 	 None 	 None 	 
2025-07-17 15:18:02.199307 test begin: paddle.nextafter(Tensor([4, 3, 2],"float32"), Tensor([4, 3, 2],"float64"), )
W0717 15:18:02.201048 26388 dygraph_functions.cc:57914] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 2],"float32"), Tensor([4, 3, 2],"float64"), ) 	 48 	 572324 	 9.287227392196655 	 9.04686450958252 	 1.0265686395943634 	 None 	 None 	 None 	 
2025-07-17 15:18:20.538953 test begin: paddle.nextafter(Tensor([4, 3, 2],"float64"), Tensor([4, 3, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 2],"float64"), Tensor([4, 3, 2],"float32"), ) 	 48 	 572324 	 9.274214029312134 	 6.308104753494263 	 1.4702060906922714 	 None 	 None 	 None 	 
2025-07-17 15:18:36.478734 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 1536, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 1536, 49],"float32"), 1, None, ) 	 9332736 	 112277 	 7.796195983886719 	 8.370862245559692 	 0.9313492153119824 	 18.30387020111084 	 10.452040433883667 	 1.7512245878587427 	 
2025-07-17 15:19:22.131644 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 1536, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 1536, 49],"float32"), 1, None, ) 	 9633792 	 112277 	 8.02865481376648 	 8.627002239227295 	 0.9306424863621678 	 18.859631776809692 	 10.490156888961792 	 1.7978407736355815 	 
2025-07-17 15:20:08.500274 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 1024, 144],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 1024, 144],"float32"), 1, None, ) 	 9437184 	 112277 	 9.991492509841919 	 5.212522745132446 	 1.9168247312056654 	 18.107532739639282 	 10.433208465576172 	 1.735567040511473 	 
2025-07-17 15:20:52.677854 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 7, 7],"float32"), output_size=1, ) 	 204417024 	 7020 	 10.3543701171875 	 10.522958517074585 	 0.9839789922564521 	 24.320331811904907 	 4.809644937515259 	 5.056575303970191 	 
2025-07-17 15:21:47.073810 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 7, 7],"float32"), output_size=1, ) 	 205219840 	 7020 	 9.945013284683228 	 10.563862085342407 	 0.9414183188250965 	 24.462384939193726 	 4.829071521759033 	 5.065649748397821 	 
2025-07-17 15:22:41.742705 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 7, 7],"float32"), output_size=1, ) 	 205520896 	 7020 	 9.961524486541748 	 10.579386949539185 	 0.9415975173283222 	 24.478395700454712 	 4.836639404296875 	 5.0610338407093325 	 
2025-07-17 15:23:37.102064 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 860160 	 27182 	 18.205901861190796 	 0.5819652080535889 	 31.283488444406025 	 1.6201589107513428 	 2.0614683628082275 	 0.7859247029841814 	 
2025-07-17 15:23:59.590841 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 774144 	 27182 	 10.075098037719727 	 0.5840158462524414 	 17.251412101864023 	 1.6359074115753174 	 2.06467866897583 	 0.7923302720935255 	 
2025-07-17 15:24:15.061294 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 774144 	 27182 	 9.999027729034424 	 0.5942516326904297 	 16.82625201005267 	 1.6182899475097656 	 2.055809259414673 	 0.7871790342896514 	 
2025-07-17 15:24:29.345672 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
W0717 15:24:29.360152 26671 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, ) 	 2304 	 7409 	 20.111915349960327 	 98.13784432411194 	 0.2049353691073377 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 15:26:33.630489 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([8, 8],"float32"), Tensor([8],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, )
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([8, 8],"float32"), Tensor([8],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, ) 	 96 	 7409 	 9.773671865463257 	 12.875049114227295 	 0.7591172490878557 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 15:26:59.051491 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 32],"float32"), 16, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 32],"float32"), 16, False, None, ) 	 192 	 424201 	 10.50700306892395 	 9.200379133224487 	 1.1420184882360958 	 32.46276140213013 	 39.13625383377075 	 0.8294805512048765 	 
2025-07-17 15:28:31.481158 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 32],"float32"), output_size=16, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 32],"float32"), output_size=16, ) 	 192 	 424201 	 10.722308874130249 	 9.612667560577393 	 1.1154353155937293 	 32.42353415489197 	 39.144248485565186 	 0.8283090213584878 	 
2025-07-17 15:30:03.393418 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 32],"float64"), 8, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 32],"float64"), 8, False, None, ) 	 192 	 424201 	 10.932527303695679 	 9.390172719955444 	 1.1642519929865094 	 32.305445194244385 	 39.53857207298279 	 0.8170615047658514 	 
2025-07-17 15:31:36.213777 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 294 	 776020 	 10.28384804725647 	 13.230268001556396 	 0.7772970317794535 	 45.44111704826355 	 57.05020570755005 	 0.7965110113923718 	 
2025-07-17 15:33:42.228407 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, ) 	 294 	 776020 	 10.122054815292358 	 15.169012546539307 	 0.6672850183383643 	 45.76821327209473 	 56.63881015777588 	 0.8080715880965812 	 
2025-07-17 15:35:50.620805 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, ) 	 294 	 776020 	 10.068294048309326 	 15.330400228500366 	 0.6567535027292771 	 46.13783121109009 	 57.22465920448303 	 0.8062578589804098 	 
2025-07-17 15:37:59.388898 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 1470 	 759911 	 10.200710535049438 	 13.051504135131836 	 0.7815735588353625 	 44.56186032295227 	 56.24363827705383 	 0.7923004572258002 	 
2025-07-17 15:40:03.467809 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 1470 	 759911 	 9.91788911819458 	 14.98401427268982 	 0.661898002611432 	 44.60538840293884 	 56.04271721839905 	 0.795917660971205 	 
2025-07-17 15:42:09.024907 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 	 1470 	 759911 	 10.03456425666809 	 14.95629334449768 	 0.6709258788615389 	 44.694480419158936 	 55.90064454078674 	 0.7995342591541773 	 
2025-07-17 15:44:14.617778 test begin: paddle.nn.functional.affine_grid(Tensor([20, 2, 3],"float32"), Tensor([4],"int64"), align_corners=True, )
[Prof] paddle.nn.functional.affine_grid 	 paddle.nn.functional.affine_grid(Tensor([20, 2, 3],"float32"), Tensor([4],"int64"), align_corners=True, ) 	 124 	 421642 	 23.306684255599976 	 44.024938344955444 	 0.5293973173336778 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 15:45:45.726725 test begin: paddle.nn.functional.affine_grid(Tensor([20, 2, 3],"float32"), list[20,1,7,7,], align_corners=True, )
[Prof] paddle.nn.functional.affine_grid 	 paddle.nn.functional.affine_grid(Tensor([20, 2, 3],"float32"), list[20,1,7,7,], align_corners=True, ) 	 120 	 421642 	 10.053602933883667 	 33.29859113693237 	 0.30192277182360855 	 22.403268098831177 	 60.779279470443726 	 0.3686004226115519 	 
2025-07-17 15:47:52.269836 test begin: paddle.nn.functional.affine_grid(Tensor([20, 2, 3],"float32"), list[20,2,5,7,], align_corners=False, )
[Prof] paddle.nn.functional.affine_grid 	 paddle.nn.functional.affine_grid(Tensor([20, 2, 3],"float32"), list[20,2,5,7,], align_corners=False, ) 	 120 	 421642 	 9.9031240940094 	 50.4537672996521 	 0.19628116241931623 	 23.41811442375183 	 80.63209438323975 	 0.29043167739692916 	 
2025-07-17 15:50:36.772337 test begin: paddle.nn.functional.avg_pool2d(Tensor([128, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([128, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 102760448 	 22767 	 8.747940301895142 	 18.92523956298828 	 0.46223670103512543 	 15.989478588104248 	 71.86492419242859 	 0.2224935010755753 	 
2025-07-17 15:52:34.534620 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([16, 128, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 134217728 	 22767 	 11.297463178634644 	 24.348859786987305 	 0.46398325332146834 	 19.671886920928955 	 93.21731400489807 	 0.2110325440174698 	 
2025-07-17 15:55:06.168064 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 256, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([8, 256, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 117964800 	 22767 	 9.995198965072632 	 22.125449180603027 	 0.45175123377089393 	 17.299160957336426 	 82.17956066131592 	 0.2105044224881041 	 
2025-07-17 15:57:21.860661 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", ) 	 196608 	 192890 	 3.5605690479278564 	 2.538816213607788 	 1.4024524614438731 	 11.984838724136353 	 14.88946533203125 	 0.8049206910307073 	 
2025-07-17 15:57:55.188951 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 3211264 	 192890 	 4.376101970672607 	 28.721166849136353 	 0.15236504817714944 	 38.934287309646606 	 35.194488286972046 	 1.1062609290460668 	 
2025-07-17 15:59:43.709829 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 3211264 	 192890 	 10.00159740447998 	 44.370330810546875 	 0.22541182862000636 	 35.85647773742676 	 35.342737913131714 	 1.0145359373560066 	 
2025-07-17 16:01:49.347414 test begin: paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 60],"float32"), Tensor([40],"float32"), Tensor([40],"float32"), Tensor([40],"float32"), Tensor([40],"float32"), )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 60],"float32"), Tensor([40],"float32"), Tensor([40],"float32"), Tensor([40],"float32"), Tensor([40],"float32"), ) 	 3600160 	 459308 	 8.66747236251831 	 12.6127450466156 	 0.6871995216334027 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 16:03:29.827828 test begin: paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), data_format="NHWC", ) 	 3600240 	 459308 	 9.9595365524292 	 12.922938585281372 	 0.7706866736775062 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 40, 50, 60]) and output[0] has a shape of torch.Size([30, 60, 40, 50]).
2025-07-17 16:05:51.823735 test begin: paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", ) 	 3600240 	 459308 	 9.956783771514893 	 12.777329921722412 	 0.7792538685713687 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 40, 50, 60]) and output[0] has a shape of torch.Size([30, 60, 40, 50]).
2025-07-17 16:08:13.029487 test begin: paddle.nn.functional.bilinear(Tensor([1, 3],"float32"), Tensor([1, 3],"float32"), Tensor([6, 3, 3],"float32"), Tensor([1, 6],"float32"), None, )
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([1, 3],"float32"), Tensor([1, 3],"float32"), Tensor([6, 3, 3],"float32"), Tensor([1, 6],"float32"), None, ) 	 66 	 1129 	 0.0719451904296875 	 0.329850435256958 	 0.2181145838829687 	 0.25527405738830566 	 1.0063056945800781 	 0.2536744636974643 	 
2025-07-17 16:08:14.706714 test begin: paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, )
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, ) 	 21 	 1129 	 0.05249595642089844 	 0.25406885147094727 	 0.20662098528398842 	 0.2007606029510498 	 0.7707099914550781 	 0.26048786855872935 	 
2025-07-17 16:08:17.286733 test begin: paddle.nn.functional.bilinear(Tensor([5, 5],"float32"), Tensor([5, 4],"float32"), Tensor([1000, 5, 4],"float32"), Tensor([1, 1000],"float32"), None, )
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([5, 5],"float32"), Tensor([5, 4],"float32"), Tensor([1000, 5, 4],"float32"), Tensor([1, 1000],"float32"), None, ) 	 21045 	 1129 	 9.117101192474365 	 47.6941397190094 	 0.19115768197493188 	 24.836435317993164 	 124.08355355262756 	 0.2001589623032458 	 
2025-07-17 16:11:43.236352 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 10164, 80],"float32"), Tensor([16, 10164, 80],"float32"), weight=Tensor([16, 10164, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 10164, 80],"float32"), Tensor([16, 10164, 80],"float32"), weight=Tensor([16, 10164, 80],"float32"), reduction="sum", ) 	 39029760 	 29760 	 8.480686902999878 	 8.73222303390503 	 0.9711944908039454 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 16:12:15.760712 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 11109, 80],"float32"), Tensor([16, 11109, 80],"float32"), weight=Tensor([16, 11109, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 11109, 80],"float32"), Tensor([16, 11109, 80],"float32"), weight=Tensor([16, 11109, 80],"float32"), reduction="sum", ) 	 42658560 	 29760 	 9.180347919464111 	 9.432318925857544 	 0.9732864199807022 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 16:12:51.758188 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 12096, 80],"float32"), Tensor([16, 12096, 80],"float32"), weight=Tensor([16, 12096, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 12096, 80],"float32"), Tensor([16, 12096, 80],"float32"), weight=Tensor([16, 12096, 80],"float32"), reduction="sum", ) 	 46448640 	 29760 	 9.953257083892822 	 10.207175970077515 	 0.9751234928319978 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 16:13:29.965000 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 300, 80],"float32"), Tensor([16, 300, 80],"float32"), weight=Tensor([16, 300, 80],"float32"), reduction="none", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 300, 80],"float32"), Tensor([16, 300, 80],"float32"), weight=Tensor([16, 300, 80],"float32"), reduction="none", ) 	 1152000 	 127109 	 4.124067783355713 	 6.128762245178223 	 0.6729038618850496 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 16:13:50.587729 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([300, 1000],"float32"), Tensor([300, 1000],"float32"), weight=Tensor([300, 1000],"float32"), reduction="none", pos_weight=None, )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([300, 1000],"float32"), Tensor([300, 1000],"float32"), weight=Tensor([300, 1000],"float32"), reduction="none", pos_weight=None, ) 	 900000 	 127109 	 4.116039752960205 	 6.224954605102539 	 0.6612160271154949 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 16:14:11.235170 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 28, 28],"float32"), Tensor([512, 28, 28],"float32"), weight=Tensor([512, 1, 1],"float32"), reduction="mean", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 28, 28],"float32"), Tensor([512, 28, 28],"float32"), weight=Tensor([512, 1, 1],"float32"), reduction="mean", ) 	 803328 	 127109 	 6.995715618133545 	 8.609638690948486 	 0.8125446222834303 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 16:14:40.985544 test begin: paddle.nn.functional.celu(Tensor([2, 4, 4],"float64"), 0.2, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 4, 4],"float64"), 0.2, None, ) 	 32 	 1018291 	 8.838027238845825 	 15.527191638946533 	 0.5691967642543668 	 53.68430018424988 	 69.19420194625854 	 0.7758496907868837 	 
2025-07-17 16:17:08.255345 test begin: paddle.nn.functional.celu(Tensor([2, 4, 4],"float64"), 1.0, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 4, 4],"float64"), 1.0, None, ) 	 32 	 1018291 	 8.843206644058228 	 15.735319137573242 	 0.5619972856439986 	 54.07751750946045 	 69.09849214553833 	 0.7826150156151015 	 
2025-07-17 16:19:36.848983 test begin: paddle.nn.functional.celu(x=Tensor([2, 4, 4],"float64"), )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(x=Tensor([2, 4, 4],"float64"), ) 	 32 	 1018291 	 9.116511344909668 	 15.526405811309814 	 0.5871617330953038 	 54.02742552757263 	 68.91706228256226 	 0.7839484699167586 	 
2025-07-17 16:22:04.747095 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 4, 4, 9],"float64"), 3, "NHWC", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 4, 4, 9],"float64"), 3, "NHWC", ) 	 288 	 933276 	 9.822196245193481 	 17.037644863128662 	 0.5764996467586782 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 4, 4, 9]) and output[0] has a shape of torch.Size([2, 9, 4, 4]).
2025-07-17 16:23:20.050425 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 4, 4, 9],"float64"), 3, "NHWC", None, )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 4, 4, 9],"float64"), 3, "NHWC", None, ) 	 288 	 933276 	 9.844154357910156 	 17.061392784118652 	 0.5769842170841669 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 4, 4, 9]) and output[0] has a shape of torch.Size([2, 9, 4, 4]).
2025-07-17 16:24:35.781779 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 4],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 4],"float64"), 3, "NCHW", ) 	 288 	 933276 	 10.360563039779663 	 17.050048351287842 	 0.6076559330693686 	 48.66378569602966 	 70.15784287452698 	 0.6936328670062145 	 
2025-07-17 16:27:02.021543 test begin: paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 26112065 	 12666 	 2.9560203552246094 	 1.1325888633728027 	 2.609967703921883 	 5.779819488525391 	 2.405776023864746 	 2.402476137092941 	 
2025-07-17 16:27:14.940298 test begin: paddle.nn.functional.conv1d(Tensor([32, 32, 58081],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 32, 58081],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 59474977 	 12666 	 5.683267593383789 	 2.6668906211853027 	 2.1310463759694254 	 11.124912738800049 	 5.629480600357056 	 1.9761881296996457 	 
2025-07-17 16:27:41.918565 test begin: paddle.nn.functional.conv1d(Tensor([32, 32, 58081],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 32, 58081],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 59476000 	 12666 	 9.760026931762695 	 18.449134588241577 	 0.5290235639553076 	 18.271357774734497 	 163.95346331596375 	 0.11144234104724435 	 
2025-07-17 16:31:14.567601 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 1052416 	 64462 	 10.424972295761108 	 3.0339622497558594 	 3.4360916311994316 	 24.442481994628906 	 8.87455415725708 	 2.7542208387608187 	 
2025-07-17 16:32:01.392375 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 276608 	 64462 	 10.263958930969238 	 2.370007276535034 	 4.330771062431172 	 22.056462287902832 	 8.971808433532715 	 2.4584187737965264 	 
2025-07-17 16:32:45.932702 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 1056000 	 64462 	 10.221578359603882 	 3.254575490951538 	 3.1406794489856513 	 24.56067991256714 	 9.0774405002594 	 2.7056833819913537 	 
2025-07-17 16:33:33.081866 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 68161552 	 6667 	 9.824337244033813 	 9.894068241119385 	 0.9929522421529526 	 105.95879316329956 	 117.50519371032715 	 0.9017371046977576 	 
2025-07-17 16:37:41.158541 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 68690960 	 6667 	 9.891345262527466 	 9.980903387069702 	 0.9910270522547829 	 107.22493076324463 	 118.4125108718872 	 0.9055202864438316 	 
2025-07-17 16:41:49.098295 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 69222416 	 6667 	 9.962677478790283 	 10.039955139160156 	 0.9923029874836335 	 107.48988127708435 	 119.36811828613281 	 0.9004907074050075 	 
2025-07-17 16:45:58.403273 test begin: paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 320, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 320, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 104857857 	 8476 	 8.857876062393188 	 8.916553974151611 	 0.993419216445218 	 9.045504093170166 	 8.85167670249939 	 1.0218972514683062 	 
2025-07-17 16:46:37.871650 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 58982657 	 8476 	 5.040536642074585 	 5.0705530643463135 	 0.9940802468900701 	 5.640810012817383 	 5.5803587436676025 	 1.0108328643240685 	 
2025-07-17 16:47:00.304246 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 117965057 	 8476 	 9.954840898513794 	 10.010099411010742 	 0.9944797239040237 	 10.863385438919067 	 10.59950041770935 	 1.024895986679601 	 
2025-07-17 16:47:43.890888 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", ) 	 6554 	 107107 	 10.261818170547485 	 3.3075594902038574 	 3.1025347241494394 	 23.090824365615845 	 17.893096923828125 	 1.2904878604254324 	 
2025-07-17 16:48:38.796071 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 12612 	 107107 	 9.358850955963135 	 1.9108264446258545 	 4.897802718967303 	 22.353986024856567 	 9.2988920211792 	 2.4039408107915468 	 
2025-07-17 16:49:21.729722 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 12944 	 107107 	 10.784929990768433 	 4.57528829574585 	 2.3572132057331494 	 22.262019634246826 	 15.992161989212036 	 1.3920581625714084 	 
2025-07-17 16:50:15.353782 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 	 62268 	 79601 	 73.90541911125183 	 2.7263875007629395 	 27.107452293766197 	 180.73218059539795 	 179.54682970046997 	 1.0066019037869143 	 
2025-07-17 16:57:32.282479 test begin: paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 8522 	 79601 	 8.576448917388916 	 3.4873268604278564 	 2.459318916935902 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([4, 6, 8, 6, 8]) and output[0] has a shape of torch.Size([4, 6, 10, 10, 10]).
2025-07-17 16:58:06.660707 test begin: paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 4, 3, 3, 3],"float32"), Tensor([4],"float32"), output_size=tuple(10,17,10,), padding="valid", stride=tuple(1,2,1,), dilation=1, groups=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 4, 3, 3, 3],"float32"), Tensor([4],"float32"), output_size=tuple(10,17,10,), padding="valid", stride=tuple(1,2,1,), dilation=1, groups=1, data_format="NCDHW", ) 	 8628 	 79601 	 10.43910264968872 	 2.7809500694274902 	 3.7537900318497277 	 21.773696660995483 	 10.368311405181885 	 2.1000234088371807 	 
2025-07-17 16:58:52.459361 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([10, 3],"float32"), Tensor([10, 3],"float32"), Tensor([10],"int64"), margin=0.5, reduction="mean", name=None, )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([10, 3],"float32"), Tensor([10, 3],"float32"), Tensor([10],"int64"), margin=0.5, reduction="mean", name=None, ) 	 70 	 33723 	 10.023345708847046 	 6.7134621143341064 	 1.4930218623630738 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 16:59:20.763420 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([5, 3],"float64"), Tensor([5, 3],"float64"), Tensor([5],"int32"), margin=0.5, reduction="mean", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([5, 3],"float64"), Tensor([5, 3],"float64"), Tensor([5],"int32"), margin=0.5, reduction="mean", ) 	 35 	 33723 	 10.15692925453186 	 6.778171062469482 	 1.4984763826292986 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 16:59:49.200566 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([5, 3],"float64"), Tensor([5, 3],"float64"), Tensor([5],"int32"), margin=0.5, reduction="none", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([5, 3],"float64"), Tensor([5, 3],"float64"), Tensor([5],"int32"), margin=0.5, reduction="none", ) 	 35 	 33723 	 9.809200048446655 	 6.356232643127441 	 1.5432411932015533 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 17:00:15.982810 test begin: paddle.nn.functional.cosine_similarity(Tensor([10, 12, 10],"float32"), Tensor([10, 1, 10],"float32"), axis=2, eps=1e-06, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([10, 12, 10],"float32"), Tensor([10, 1, 10],"float32"), axis=2, eps=1e-06, ) 	 1300 	 78269 	 9.97959041595459 	 7.30229926109314 	 1.3666367344222845 	 20.582595825195312 	 29.68041729927063 	 0.693473936624911 	 
2025-07-17 17:01:23.562774 test begin: paddle.nn.functional.cosine_similarity(Tensor([210, 1024],"float32"), Tensor([210, 1024],"float32"), axis=-1, eps=1e-08, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([210, 1024],"float32"), Tensor([210, 1024],"float32"), axis=-1, eps=1e-08, ) 	 430080 	 78269 	 9.663557767868042 	 7.010628700256348 	 1.378415286422841 	 17.937774419784546 	 27.46353578567505 	 0.6531487627729591 	 
2025-07-17 17:02:25.659788 test begin: paddle.nn.functional.cosine_similarity(Tensor([32, 128],"float32"), Tensor([32, 128],"float32"), )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([32, 128],"float32"), Tensor([32, 128],"float32"), ) 	 8192 	 78269 	 9.679335355758667 	 6.768688440322876 	 1.4300163822131764 	 17.954153537750244 	 27.059027671813965 	 0.6635180596844649 	 
2025-07-17 17:03:27.129558 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 2048, 151936],"float32"), Tensor([1, 2048, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
[Prof] paddle.nn.functional.cross_entropy 	 paddle.nn.functional.cross_entropy(Tensor([1, 2048, 151936],"float32"), Tensor([1, 2048, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, ) 	 311166976 	 1988 	 7.5515806674957275 	 538.6394152641296 	 0.01401973278133146 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 2048, 1]) and output[0] has a shape of torch.Size([1, 2048]).
2025-07-17 17:12:44.945151 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 4096, 100352],"float32"), Tensor([1, 4096, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
[Prof] paddle.nn.functional.cross_entropy 	 paddle.nn.functional.cross_entropy(Tensor([1, 4096, 100352],"float32"), Tensor([1, 4096, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, ) 	 411045888 	 1988 	 9.824448108673096 	 378.6647961139679 	 0.02594497352142606 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 4096, 1]) and output[0] has a shape of torch.Size([1, 4096]).
2025-07-17 17:19:28.457822 test begin: paddle.nn.functional.cross_entropy(Tensor([8, 1024, 50304],"float32"), Tensor([8, 1024, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
[Prof] paddle.nn.functional.cross_entropy 	 paddle.nn.functional.cross_entropy(Tensor([8, 1024, 50304],"float32"), Tensor([8, 1024, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, ) 	 412098560 	 1988 	 9.942921161651611 	 200.23575973510742 	 0.04965607129718056 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([8, 1024, 1]) and output[0] has a shape of torch.Size([8, 1024]).
2025-07-17 17:23:14.712292 test begin: paddle.nn.functional.elu(Tensor([1, 21504, 2],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([1, 21504, 2],"float32"), ) 	 43008 	 1002721 	 9.297322511672974 	 15.58457350730896 	 0.596572149203355 	 54.35667037963867 	 69.16872882843018 	 0.7858561419347155 	 
2025-07-17 17:25:43.139493 test begin: paddle.nn.functional.elu(Tensor([10, 20, 1],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([10, 20, 1],"float32"), ) 	 200 	 1002721 	 9.321110248565674 	 15.419212341308594 	 0.6045127365938209 	 54.84789824485779 	 69.47062873840332 	 0.7895120461827331 	 
2025-07-17 17:28:12.205791 test begin: paddle.nn.functional.elu(Tensor([15, 20],"float32"), 1.0, )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([15, 20],"float32"), 1.0, ) 	 300 	 1002721 	 9.26708459854126 	 15.242337226867676 	 0.6079831761106929 	 54.634970903396606 	 68.65768218040466 	 0.7957590347987274 	 
2025-07-17 17:30:40.037779 test begin: paddle.nn.functional.embedding(Tensor([1, 1024],"int64"), weight=Tensor([151936, 4096],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0717 17:30:50.961907 31461 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

2025-07-17 17:30:51.125231 test begin: paddle.nn.functional.embedding(Tensor([1, 4097],"int64"), weight=Tensor([100352, 8192],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0717 17:31:13.487833 31477 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

2025-07-17 17:31:13.676631 test begin: paddle.nn.functional.embedding(Tensor([8, 1024],"int64"), weight=Tensor([50304, 4096],"float16"), padding_idx=None, sparse=False, name=None, )
[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([8, 1024],"int64"), weight=Tensor([50304, 4096],"float16"), padding_idx=None, sparse=False, name=None, ) 	 206053376 	 65077 	 9.944782733917236 	 28.777839183807373 	 0.34557086341329396 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 17:32:26.769518 test begin: paddle.nn.functional.fold(Tensor([3, 12, 12],"float32"), output_sizes=list[4,5,], kernel_sizes=2, )
[Prof] paddle.nn.functional.fold 	 paddle.nn.functional.fold(Tensor([3, 12, 12],"float32"), output_sizes=list[4,5,], kernel_sizes=2, ) 	 432 	 300156 	 9.107297658920288 	 6.178814888000488 	 1.4739554144285878 	 18.954132080078125 	 26.800066709518433 	 0.7072419738920385 	 
2025-07-17 17:33:27.821465 test begin: paddle.nn.functional.fold(Tensor([3, 12, 12],"float64"), output_sizes=list[4,5,], kernel_sizes=2, )
[Prof] paddle.nn.functional.fold 	 paddle.nn.functional.fold(Tensor([3, 12, 12],"float64"), output_sizes=list[4,5,], kernel_sizes=2, ) 	 432 	 300156 	 9.018630027770996 	 6.129166841506958 	 1.471428378600576 	 18.94665503501892 	 27.294984579086304 	 0.6941441926857157 	 
2025-07-17 17:34:29.218595 test begin: paddle.nn.functional.fold(Tensor([3, 12, 12],"float64"), output_sizes=list[4,5,], kernel_sizes=list[2,2,], strides=list[1,1,], paddings=list[0,0,0,0,], dilations=list[1,1,], name=None, )
[Prof] paddle.nn.functional.fold 	 paddle.nn.functional.fold(Tensor([3, 12, 12],"float64"), output_sizes=list[4,5,], kernel_sizes=list[2,2,], strides=list[1,1,], paddings=list[0,0,0,0,], dilations=list[1,1,], name=None, ) 	 432 	 300156 	 9.365062236785889 	 5.947832107543945 	 1.5745337237928576 	 19.22080969810486 	 27.081376791000366 	 0.7097427079295423 	 
2025-07-17 17:35:31.359649 test begin: paddle.nn.functional.gelu(Tensor([124, 96, 96, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([124, 96, 96, 512],"float32"), False, None, ) 	 585105408 	 1657 	 6.374086141586304 	 5.580667495727539 	 1.1421727143690592 	 8.487377882003784 	 8.4943106174469 	 0.999183837776208 	 
2025-07-17 17:36:22.698397 test begin: paddle.nn.functional.gelu(Tensor([128, 96, 96, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 96, 96, 512],"float32"), False, None, ) 	 603979776 	 1657 	 6.575838565826416 	 5.759165287017822 	 1.1418041049541536 	 8.760162830352783 	 8.767379760742188 	 0.9991768429580614 	 
2025-07-17 17:37:15.701251 test begin: paddle.nn.functional.gelu(Tensor([128, 96, 96, 768],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 96, 96, 768],"float32"), False, None, ) 	 905969664 	 1657 	 9.85783576965332 	 8.61998701095581 	 1.1436021605513131 	 13.142921924591064 	 13.142082691192627 	 1.0000638584780022 	 
2025-07-17 17:38:34.634567 test begin: paddle.nn.functional.glu(Tensor([30, 457, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([30, 457, 512],"float32"), -1, None, ) 	 7019520 	 93494 	 9.106557607650757 	 3.6294310092926025 	 2.509086847038781 	 15.163328647613525 	 6.595287084579468 	 2.29911578573541 	 
2025-07-17 17:39:09.373998 test begin: paddle.nn.functional.glu(Tensor([30, 477, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([30, 477, 512],"float32"), -1, None, ) 	 7326720 	 93494 	 9.52363634109497 	 3.772534132003784 	 2.524466580779876 	 15.814878702163696 	 6.593966245651245 	 2.398386360044486 	 
2025-07-17 17:39:45.767186 test begin: paddle.nn.functional.glu(Tensor([30, 498, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([30, 498, 512],"float32"), -1, None, ) 	 7649280 	 93494 	 9.993695497512817 	 3.911869764328003 	 2.5547106881329356 	 16.55853533744812 	 6.671437740325928 	 2.4820040270118997 	 
2025-07-17 17:40:23.103839 test begin: paddle.nn.functional.grid_sample(Tensor([100, 1, 768, 768],"float32"), Tensor([100, 1, 12544, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([100, 1, 768, 768],"float32"), Tensor([100, 1, 12544, 2],"float32"), align_corners=False, ) 	 61491200 	 170 	 0.011190652847290039 	 0.011130571365356445 	 1.0053978794045197 	 0.0624079704284668 	 0.059563636779785156 	 1.0477528539635268 	 
2025-07-17 17:40:24.257405 test begin: paddle.nn.functional.grid_sample(Tensor([200, 1, 544, 544],"float32"), Tensor([200, 1, 12544, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([200, 1, 544, 544],"float32"), Tensor([200, 1, 12544, 2],"float32"), align_corners=False, ) 	 64204800 	 170 	 0.015306234359741211 	 0.025933027267456055 	 0.5902216583464343 	 0.07945609092712402 	 0.07847428321838379 	 1.012511203269076 	 
2025-07-17 17:40:26.680542 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 	 630296320 	 170 	 9.959686279296875 	 7.804999351501465 	 1.2760649720465267 	 80.46468591690063 	 83.99319195747375 	 0.9579905709219907 	 
2025-07-17 17:43:53.567453 test begin: paddle.nn.functional.group_norm(Tensor([30, 256, 16, 168],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), )
[Prof] paddle.nn.functional.group_norm 	 paddle.nn.functional.group_norm(Tensor([30, 256, 16, 168],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), ) 	 20644352 	 46629 	 10.382988929748535 	 11.432055711746216 	 0.9082346335209174 	 17.206843852996826 	 15.654441118240356 	 1.0991669215803321 	 
2025-07-17 17:44:49.035776 test begin: paddle.nn.functional.group_norm(Tensor([30, 256, 24, 112],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), )
[Prof] paddle.nn.functional.group_norm 	 paddle.nn.functional.group_norm(Tensor([30, 256, 24, 112],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), ) 	 20644352 	 46629 	 10.382908344268799 	 11.429749250411987 	 0.9084108598353149 	 17.206335067749023 	 15.654290437698364 	 1.099145000294172 	 
2025-07-17 17:45:44.427738 test begin: paddle.nn.functional.group_norm(Tensor([30, 64, 32, 336],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([64],"float32"), bias=Tensor([64],"float32"), )
[Prof] paddle.nn.functional.group_norm 	 paddle.nn.functional.group_norm(Tensor([30, 64, 32, 336],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([64],"float32"), bias=Tensor([64],"float32"), ) 	 20643968 	 46629 	 9.96632981300354 	 11.331727266311646 	 0.8795066787949153 	 16.149474143981934 	 15.573036193847656 	 1.0370151294172172 	 
2025-07-17 17:46:39.954307 test begin: paddle.nn.functional.hardshrink(Tensor([3],"float32"), -1, None, )
[Prof] paddle.nn.functional.hardshrink 	 paddle.nn.functional.hardshrink(Tensor([3],"float32"), -1, None, ) 	 3 	 1008698 	 9.471632719039917 	 11.455243110656738 	 0.8268382108999955 	 54.59267449378967 	 66.54620409011841 	 0.8203724801471622 	 
2025-07-17 17:49:02.033006 test begin: paddle.nn.functional.hardshrink(Tensor([3],"float32"), 0.5, None, )
[Prof] paddle.nn.functional.hardshrink 	 paddle.nn.functional.hardshrink(Tensor([3],"float32"), 0.5, None, ) 	 3 	 1008698 	 9.073987483978271 	 11.527438163757324 	 0.7871642731953411 	 54.27941131591797 	 67.22691226005554 	 0.8074059850606788 	 
2025-07-17 17:51:24.147410 test begin: paddle.nn.functional.hardshrink(Tensor([3],"float64"), -1, None, )
[Prof] paddle.nn.functional.hardshrink 	 paddle.nn.functional.hardshrink(Tensor([3],"float64"), -1, None, ) 	 3 	 1008698 	 9.115217924118042 	 11.315023183822632 	 0.8055854394669111 	 53.51080322265625 	 67.88249635696411 	 0.788285730407829 	 
2025-07-17 17:53:46.793706 test begin: paddle.nn.functional.hardsigmoid(Tensor([1024, 576, 1, 1],"float32"), slope=0.2, offset=0.5, )
[Prof] paddle.nn.functional.hardsigmoid 	 paddle.nn.functional.hardsigmoid(Tensor([1024, 576, 1, 1],"float32"), slope=0.2, offset=0.5, ) 	 589824 	 881132 	 8.308352708816528 	 12.969484329223633 	 0.6406077911745219 	 47.41262936592102 	 103.39063334465027 	 0.4585776083590873 	 
2025-07-17 17:56:38.906266 test begin: paddle.nn.functional.hardsigmoid(Tensor([300, 4096],"float32"), )
[Prof] paddle.nn.functional.hardsigmoid 	 paddle.nn.functional.hardsigmoid(Tensor([300, 4096],"float32"), ) 	 1228800 	 881132 	 7.827710866928101 	 13.031335353851318 	 0.6006837100247502 	 47.424835205078125 	 100.99902582168579 	 0.46955735284820344 	 
2025-07-17 17:59:28.233295 test begin: paddle.nn.functional.hardsigmoid(Tensor([640, 960, 1, 1],"float32"), slope=0.2, offset=0.5, )
[Prof] paddle.nn.functional.hardsigmoid 	 paddle.nn.functional.hardsigmoid(Tensor([640, 960, 1, 1],"float32"), slope=0.2, offset=0.5, ) 	 614400 	 881132 	 8.458989381790161 	 12.811931610107422 	 0.6602430952032874 	 47.50720000267029 	 103.48281288146973 	 0.45908299822778803 	 
2025-07-17 18:02:20.518271 test begin: paddle.nn.functional.hardswish(Tensor([256, 80, 112, 112],"float32"), None, )
[Prof] paddle.nn.functional.hardswish 	 paddle.nn.functional.hardswish(Tensor([256, 80, 112, 112],"float32"), None, ) 	 256901120 	 4213 	 6.232445001602173 	 6.2456958293914795 	 0.9978784064816365 	 9.481906175613403 	 9.488680839538574 	 0.9992860267892095 	 
2025-07-17 18:03:00.880266 test begin: paddle.nn.functional.hardswish(Tensor([512, 48, 112, 112],"float32"), None, )
[Prof] paddle.nn.functional.hardswish 	 paddle.nn.functional.hardswish(Tensor([512, 48, 112, 112],"float32"), None, ) 	 308281344 	 4213 	 7.477994441986084 	 7.484630823135376 	 0.9991133321995284 	 11.375823020935059 	 11.372623443603516 	 1.0002813403035289 	 
2025-07-17 18:03:49.150054 test begin: paddle.nn.functional.hardswish(Tensor([512, 64, 112, 112],"float32"), None, )
[Prof] paddle.nn.functional.hardswish 	 paddle.nn.functional.hardswish(Tensor([512, 64, 112, 112],"float32"), None, ) 	 411041792 	 4213 	 9.968323469161987 	 9.977877855300903 	 0.9990424430648006 	 15.158703804016113 	 15.154147863388062 	 1.000300639842578 	 
2025-07-17 18:04:56.116537 test begin: paddle.nn.functional.hardtanh(Tensor([10, 20, 1],"float32"), -1.0, 1.0, )
[Prof] paddle.nn.functional.hardtanh 	 paddle.nn.functional.hardtanh(Tensor([10, 20, 1],"float32"), -1.0, 1.0, ) 	 200 	 985886 	 9.034444332122803 	 18.310487270355225 	 0.4934027259203318 	 52.61514759063721 	 67.8935718536377 	 0.7749650836468419 	 
2025-07-17 18:07:23.980950 test begin: paddle.nn.functional.hardtanh(Tensor([3, 3, 3],"float64"), -3.2, -3.2, None, )
[Prof] paddle.nn.functional.hardtanh 	 paddle.nn.functional.hardtanh(Tensor([3, 3, 3],"float64"), -3.2, -3.2, None, ) 	 27 	 985886 	 8.71423053741455 	 18.22497868537903 	 0.47814763944857364 	 51.974273681640625 	 67.2246720790863 	 0.7731428369110617 	 
2025-07-17 18:09:50.129981 test begin: paddle.nn.functional.hardtanh(Tensor([3, 3, 3],"float64"), -3.4, 0, None, )
[Prof] paddle.nn.functional.hardtanh 	 paddle.nn.functional.hardtanh(Tensor([3, 3, 3],"float64"), -3.4, 0, None, ) 	 27 	 985886 	 9.00705885887146 	 18.302802085876465 	 0.49211365651065225 	 52.235920906066895 	 67.61762142181396 	 0.7725193493602421 	 
2025-07-17 18:12:17.300331 test begin: paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 5],"float64"), Tensor([10, 10, 5],"float64"), )
[Prof] paddle.nn.functional.hinge_embedding_loss 	 paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 5],"float64"), Tensor([10, 10, 5],"float64"), ) 	 1000 	 60144 	 11.54031777381897 	 5.984500408172607 	 1.9283677812201627 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 18:12:43.506636 test begin: paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 5],"float64"), Tensor([10, 10, 5],"float64"), reduction="mean", margin=1.0, name=None, )
[Prof] paddle.nn.functional.hinge_embedding_loss 	 paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 5],"float64"), Tensor([10, 10, 5],"float64"), reduction="mean", margin=1.0, name=None, ) 	 1000 	 60144 	 10.834255933761597 	 5.949358224868774 	 1.82107977436517 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 18:13:08.935471 test begin: paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 5],"float64"), Tensor([10, 10, 5],"float64"), reduction="none", )
[Prof] paddle.nn.functional.hinge_embedding_loss 	 paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 5],"float64"), Tensor([10, 10, 5],"float64"), reduction="none", ) 	 1000 	 60144 	 10.476199865341187 	 5.193666934967041 	 2.017110453273159 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 18:13:32.138912 test begin: paddle.nn.functional.instance_norm(Tensor([2, 32, 128],"float32"), )
[Prof] paddle.nn.functional.instance_norm 	 paddle.nn.functional.instance_norm(Tensor([2, 32, 128],"float32"), ) 	 8192 	 307890 	 8.65876841545105 	 13.92726731300354 	 0.6217133785725916 	 21.08781599998474 	 28.01347303390503 	 0.7527740660524996 	 
2025-07-17 18:14:43.858208 test begin: paddle.nn.functional.instance_norm(Tensor([8, 32, 32, 64],"float32"), None, None, Tensor([32],"float32"), Tensor([32],"float32"), True, 0.9, 1e-05, )
[Prof] paddle.nn.functional.instance_norm 	 paddle.nn.functional.instance_norm(Tensor([8, 32, 32, 64],"float32"), None, None, Tensor([32],"float32"), Tensor([32],"float32"), True, 0.9, 1e-05, ) 	 524352 	 307890 	 8.970833539962769 	 23.286412954330444 	 0.3852389613443883 	 27.537543535232544 	 52.793373823165894 	 0.5216098449678733 	 
2025-07-17 18:16:37.127288 test begin: paddle.nn.functional.instance_norm(Tensor([8, 32, 32, 64],"float64"), None, None, Tensor([32],"float64"), Tensor([32],"float64"), True, 0.9, 1e-05, )
[Prof] paddle.nn.functional.instance_norm 	 paddle.nn.functional.instance_norm(Tensor([8, 32, 32, 64],"float64"), None, None, Tensor([32],"float64"), Tensor([32],"float64"), True, 0.9, 1e-05, ) 	 524352 	 307890 	 8.847448110580444 	 23.31325387954712 	 0.3795029280894321 	 27.797739505767822 	 51.88689827919006 	 0.5357371596235205 	 
2025-07-17 18:18:29.037143 test begin: paddle.nn.functional.interpolate(Tensor([4, 256, 336, 320],"float16"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
[Prof] paddle.nn.functional.interpolate 	 paddle.nn.functional.interpolate(Tensor([4, 256, 336, 320],"float16"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", ) 	 110100480 	 22291 	 6.095290660858154 	 16.725333213806152 	 0.3644346323591756 	 20.80480670928955 	 19.17933201789856 	 1.0847513714176313 	 
2025-07-17 18:19:34.514917 test begin: paddle.nn.functional.interpolate(Tensor([4, 256, 336, 336],"float16"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
[Prof] paddle.nn.functional.interpolate 	 paddle.nn.functional.interpolate(Tensor([4, 256, 336, 336],"float16"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", ) 	 115605504 	 22291 	 6.903956651687622 	 17.55167555809021 	 0.3933502889133189 	 26.128713130950928 	 20.315064191818237 	 1.2861742834892986 	 
2025-07-17 18:20:49.604424 test begin: paddle.nn.functional.interpolate(Tensor([4, 256, 336, 336],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
[Prof] paddle.nn.functional.interpolate 	 paddle.nn.functional.interpolate(Tensor([4, 256, 336, 336],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", ) 	 115605504 	 22291 	 9.910696983337402 	 18.770991563796997 	 0.527979406397041 	 27.8736789226532 	 27.28389596939087 	 1.0216165225788865 	 
2025-07-17 18:22:15.948884 test begin: paddle.nn.functional.l1_loss(Tensor([16, 511, 257],"float32"), Tensor([16, 511, 257],"float32"), )
[Prof] paddle.nn.functional.l1_loss 	 paddle.nn.functional.l1_loss(Tensor([16, 511, 257],"float32"), Tensor([16, 511, 257],"float32"), ) 	 4202464 	 183321 	 7.039067506790161 	 9.695399045944214 	 0.7260214327882408 	 15.544047117233276 	 22.334781408309937 	 0.6959569844480286 	 
2025-07-17 18:23:11.394443 test begin: paddle.nn.functional.l1_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 1],"float32"), "none", )
[Prof] paddle.nn.functional.l1_loss 	 paddle.nn.functional.l1_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 1],"float32"), "none", ) 	 14475840 	 183321 	 20.858460903167725 	 20.608322143554688 	 1.0121377547318313 	 25.329116344451904 	 29.081352949142456 	 0.8709744828154153 	 
2025-07-17 18:24:47.642371 test begin: paddle.nn.functional.l1_loss(Tensor([64, 3, 128, 128],"float32"), Tensor([64, 3, 128, 128],"float32"), "mean", name=None, )
[Prof] paddle.nn.functional.l1_loss 	 paddle.nn.functional.l1_loss(Tensor([64, 3, 128, 128],"float32"), Tensor([64, 3, 128, 128],"float32"), "mean", name=None, ) 	 6291456 	 183321 	 9.945497989654541 	 10.536024808883667 	 0.9439516487535972 	 15.525624990463257 	 22.0750253200531 	 0.7033117636499232 	 
2025-07-17 18:25:47.179696 test begin: paddle.nn.functional.label_smooth(label=Tensor([128, 40, 33712],"float32"), epsilon=0.1, )
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([128, 40, 33712],"float32"), epsilon=0.1, ) 	 172605440 	 10028 	 10.000503778457642 	 20.277302026748657 	 0.4931870998057606 	 9.998622179031372 	 10.015976667404175 	 0.9982673194088719 	 combined
2025-07-17 18:26:43.494288 test begin: paddle.nn.functional.label_smooth(label=Tensor([160, 32, 33712],"float32"), epsilon=0.1, )
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([160, 32, 33712],"float32"), epsilon=0.1, ) 	 172605440 	 10028 	 10.102102994918823 	 20.277292490005493 	 0.4981978239894732 	 9.998629331588745 	 10.176504850387573 	 0.9825209616254392 	 combined
2025-07-17 18:27:41.460369 test begin: paddle.nn.functional.label_smooth(label=Tensor([256, 20, 33712],"float32"), epsilon=0.1, )
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([256, 20, 33712],"float32"), epsilon=0.1, ) 	 172605440 	 10028 	 10.000510931015015 	 20.27732014656067 	 0.4931870118306165 	 9.998932123184204 	 10.089093685150146 	 0.9910634627073938 	 combined
2025-07-17 18:28:38.314023 test begin: paddle.nn.functional.layer_norm(Tensor([7, 220, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([7, 220, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 1579008 	 534338 	 8.355558633804321 	 11.985888242721558 	 0.6971163475413049 	 40.021552324295044 	 60.22784638404846 	 0.6645024640113129 	 
2025-07-17 18:30:39.227682 test begin: paddle.nn.functional.layer_norm(Tensor([7, 286, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([7, 286, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 2052096 	 534338 	 10.262735843658447 	 12.031081914901733 	 0.8530185328508979 	 44.91234731674194 	 60.98855638504028 	 0.7364061387712789 	 
2025-07-17 18:32:47.514636 test begin: paddle.nn.functional.layer_norm(Tensor([7, 435, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([7, 435, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 3120128 	 534338 	 9.565616607666016 	 12.013790130615234 	 0.7962197194779991 	 41.265454053878784 	 77.06196999549866 	 0.535484027417015 	 
2025-07-17 18:35:09.038896 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 32, 608, 1088],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 32, 608, 1088],"float32"), 0.1, ) 	 254017536 	 6453 	 9.460564613342285 	 9.48388147354126 	 0.9975414222263295 	 14.677218914031982 	 14.287492990493774 	 1.0272774183544666 	 
2025-07-17 18:36:08.571333 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 64, 304, 544],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 64, 304, 544],"float32"), 0.1, ) 	 127008768 	 6453 	 4.738749027252197 	 4.751282453536987 	 0.997362096148281 	 7.213137149810791 	 7.254751443862915 	 0.9942638566773604 	 
2025-07-17 18:36:37.832880 test begin: paddle.nn.functional.leaky_relu(Tensor([64, 64, 256, 256],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([64, 64, 256, 256],"float32"), 0.1, None, ) 	 268435456 	 6453 	 9.996036052703857 	 9.996774196624756 	 0.9999261617891552 	 15.16754937171936 	 15.096157789230347 	 1.0047291227003432 	 
2025-07-17 18:37:38.537442 test begin: paddle.nn.functional.linear(x=Tensor([1, 25088],"float32"), weight=Tensor([25088, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([1, 25088],"float32"), weight=Tensor([25088, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, ) 	 102789632 	 1592 	 0.4845449924468994 	 0.4769704341888428 	 1.0158805613830935 	 0.9569671154022217 	 0.9419598579406738 	 1.015931950109166 	 
2025-07-17 18:37:43.174456 test begin: paddle.nn.functional.linear(x=Tensor([2, 25088],"float32"), weight=Tensor([25088, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([2, 25088],"float32"), weight=Tensor([25088, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, ) 	 102814720 	 1592 	 0.714153528213501 	 0.7467107772827148 	 0.956399117222213 	 0.9936132431030273 	 0.9682133197784424 	 1.0262338090229923 	 
2025-07-17 18:37:48.285106 test begin: paddle.nn.functional.linear(x=Tensor([4096, 12544],"float32"), weight=Tensor([12544, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([4096, 12544],"float32"), weight=Tensor([12544, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, ) 	 64226304 	 1592 	 9.982271194458008 	 9.971629619598389 	 1.0010671851307738 	 19.458478689193726 	 19.443872213363647 	 1.0007512122929938 	 
2025-07-17 18:38:48.358475 test begin: paddle.nn.functional.local_response_norm(Tensor([3, 3, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([3, 3, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 14400 	 70993 	 8.62499737739563 	 9.272261381149292 	 0.930193511901038 	 13.43137240409851 	 20.66351008415222 	 0.6500043966102176 	 
2025-07-17 18:39:40.542263 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 40, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 40, 40],"float32"), size=5, data_format="NCDHW", ) 	 43200 	 70993 	 9.652618169784546 	 9.226240396499634 	 1.046213598926674 	 14.655730724334717 	 21.004563093185425 	 0.6977403271525091 	 
2025-07-17 18:40:36.243275 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 40, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 40, 3],"float32"), size=5, data_format="NDHWC", ) 	 43200 	 70993 	 9.64997410774231 	 9.497005224227905 	 1.0161070653224622 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3, 40, 40, 3]) and output[0] has a shape of torch.Size([3, 3, 3, 40, 40]).
2025-07-17 18:41:10.124908 test begin: paddle.nn.functional.log_loss(Tensor([102400, 1],"float32"), Tensor([102400, 1],"float32"), epsilon=1e-07, )
[Prof] paddle.nn.functional.log_loss 	 paddle.nn.functional.log_loss(Tensor([102400, 1],"float32"), Tensor([102400, 1],"float32"), epsilon=1e-07, ) 	 204800 	 939713 	 9.084303140640259 	 140.84495425224304 	 0.06449860549758094 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 18:44:31.079568 test begin: paddle.nn.functional.log_loss(Tensor([25600, 1],"float32"), Tensor([25600, 1],"float32"), epsilon=1e-07, )
[Prof] paddle.nn.functional.log_loss 	 paddle.nn.functional.log_loss(Tensor([25600, 1],"float32"), Tensor([25600, 1],"float32"), epsilon=1e-07, ) 	 51200 	 939713 	 9.060316801071167 	 139.31301879882812 	 0.06503567921498074 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 18:47:50.932717 test begin: paddle.nn.functional.log_loss(Tensor([400, 1],"float32"), Tensor([400, 1],"float32"), )
[Prof] paddle.nn.functional.log_loss 	 paddle.nn.functional.log_loss(Tensor([400, 1],"float32"), Tensor([400, 1],"float32"), ) 	 800 	 939713 	 8.789559841156006 	 139.35000681877136 	 0.06307541737394443 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 18:51:10.536999 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 10, 10],"float32"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 10, 10],"float32"), None, ) 	 1000 	 996038 	 8.813268184661865 	 12.719448804855347 	 0.6928970209226055 	 53.85751676559448 	 69.50111150741577 	 0.7749159056232918 	 
2025-07-17 18:53:35.796022 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 10, 10],"float64"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 10, 10],"float64"), None, ) 	 1000 	 996038 	 8.765076160430908 	 12.688671350479126 	 0.6907796662335287 	 53.32307505607605 	 69.80236673355103 	 0.7639150010431655 	 
2025-07-17 18:56:00.678125 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 10],"float32"), )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 10],"float32"), ) 	 1000 	 996038 	 9.025152444839478 	 12.708142280578613 	 0.7101866067892776 	 53.934712171554565 	 68.47150015830994 	 0.7876957865221953 	 
2025-07-17 18:58:25.640165 test begin: paddle.nn.functional.log_softmax(Tensor([128, 192612],"float32"), axis=-1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([128, 192612],"float32"), axis=-1, ) 	 24654336 	 58818 	 20.43531560897827 	 18.222084760665894 	 1.1214587066947381 	 40.012232542037964 	 18.770476579666138 	 2.1316577856836516 	 
2025-07-17 19:00:04.185563 test begin: paddle.nn.functional.log_softmax(Tensor([2944, 6629],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([2944, 6629],"float32"), axis=1, ) 	 19515776 	 58818 	 7.105072021484375 	 7.029966354370117 	 1.0106836453161072 	 13.848574876785278 	 11.993226528167725 	 1.1546996835472103 	 
2025-07-17 19:00:46.158342 test begin: paddle.nn.functional.log_softmax(Tensor([4224, 6629],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([4224, 6629],"float32"), axis=1, ) 	 28000896 	 58818 	 10.000069618225098 	 9.88510274887085 	 1.011630316070046 	 19.586791276931763 	 16.99688982963562 	 1.152375021151247 	 
2025-07-17 19:01:43.594166 test begin: paddle.nn.functional.lp_pool1d(Tensor([2, 3, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([2, 3, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, ) 	 192 	 302712 	 9.05348515510559 	 30.35171103477478 	 0.2982858246354799 	 23.957204341888428 	 93.02207803726196 	 0.2575432074554587 	 
2025-07-17 19:04:19.999873 test begin: paddle.nn.functional.lp_pool1d(Tensor([2, 3, 32],"float64"), 5.0, 5, 3, 0, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([2, 3, 32],"float64"), 5.0, 5, 3, 0, False, "NCL", None, ) 	 192 	 302712 	 8.912863731384277 	 30.062021255493164 	 0.2964825171147017 	 23.894541025161743 	 83.54533219337463 	 0.286006894674322 	 
2025-07-17 19:06:46.812656 test begin: paddle.nn.functional.lp_pool1d(Tensor([2, 3, 32],"float64"), norm_type=5, kernel_size=5, stride=3, padding=list[0,], )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([2, 3, 32],"float64"), norm_type=5, kernel_size=5, stride=3, padding=list[0,], ) 	 192 	 302712 	 10.031561851501465 	 30.219727993011475 	 0.33195407496127477 	 23.81401753425598 	 90.89102983474731 	 0.2620062461340048 	 
2025-07-17 19:09:21.778791 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 6144 	 548439 	 9.026679515838623 	 52.64822316169739 	 0.17145269058967424 	 33.35407757759094 	 145.0283601284027 	 0.22998313949120355 	 
2025-07-17 19:13:21.845929 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 6144 	 548439 	 9.378341436386108 	 52.559096336364746 	 0.17843422147837412 	 33.074047327041626 	 144.1934151649475 	 0.2293727996469683 	 
2025-07-17 19:17:21.651904 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, ) 	 6144 	 548439 	 9.76891016960144 	 52.50932741165161 	 0.18604142637396948 	 33.45613765716553 	 144.52316522598267 	 0.23149325303560897 	 
2025-07-17 19:21:21.919689 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), 0.0, "mean", )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), 0.0, "mean", ) 	 300 	 76657 	 3.040661334991455 	 4.8381006717681885 	 0.6284824440993245 	 8.139968872070312 	 15.397712707519531 	 0.5286479249671368 	 
2025-07-17 19:21:53.346303 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), 0.0, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), 0.0, "mean", None, ) 	 300 	 76657 	 3.0123977661132812 	 4.813348770141602 	 0.6258424041074965 	 8.395918607711792 	 15.289634704589844 	 0.549124865958465 	 
2025-07-17 19:22:24.867753 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([128],"float32"), Tensor([128],"float32"), Tensor([128],"float32"), 0.5, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([128],"float32"), Tensor([128],"float32"), Tensor([128],"float32"), 0.5, "mean", None, ) 	 384 	 76657 	 9.61916708946228 	 4.740075588226318 	 2.0293277840030526 	 8.949336290359497 	 15.115090608596802 	 0.5920795661833153 	 
2025-07-17 19:23:03.580769 test begin: paddle.nn.functional.max_pool2d(Tensor([10, 128, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([10, 128, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 294912000 	 6578 	 7.371719598770142 	 10.377560138702393 	 0.7103519035536902 	 25.13579034805298 	 50.712467432022095 	 0.4956530735118822 	 
2025-07-17 19:24:43.520094 test begin: paddle.nn.functional.max_pool2d(Tensor([1536, 24, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([1536, 24, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 462422016 	 6578 	 17.66631531715393 	 19.602293014526367 	 0.9012371820001991 	 47.3751118183136 	 83.0323166847229 	 0.5705623269334859 	 
2025-07-17 19:27:41.236473 test begin: paddle.nn.functional.max_pool2d(Tensor([8, 64, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([8, 64, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, ) 	 253755392 	 6578 	 9.933890581130981 	 10.868271827697754 	 0.914026695193112 	 26.225096225738525 	 45.960369348526 	 0.5706023819536514 	 
2025-07-17 19:29:20.685303 test begin: paddle.nn.functional.max_pool3d(Tensor([8, 64, 16, 112, 112],"float32"), kernel_size=tuple(3,3,3,), stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.max_pool3d 	 paddle.nn.functional.max_pool3d(Tensor([8, 64, 16, 112, 112],"float32"), kernel_size=tuple(3,3,3,), stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, ) 	 102760448 	 51230 	 41.52942728996277 	 40.663511514663696 	 1.0212946630294537 	 87.54375123977661 	 38.126081466674805 	 2.2961644069374594 	 
2025-07-17 19:32:50.566202 test begin: paddle.nn.functional.max_pool3d(x=Tensor([8, 32, 32, 56, 56],"float32"), kernel_size=list[1,1,1,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NCDHW", )
[Prof] paddle.nn.functional.max_pool3d 	 paddle.nn.functional.max_pool3d(x=Tensor([8, 32, 32, 56, 56],"float32"), kernel_size=list[1,1,1,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NCDHW", ) 	 25690112 	 51230 	 8.068771600723267 	 27.54236912727356 	 0.29295851651095783 	 24.887689352035522 	 23.6609046459198 	 1.0518485968509779 	 
2025-07-17 19:34:15.633009 test begin: paddle.nn.functional.max_pool3d(x=Tensor([8, 320, 4, 56, 56],"float32"), kernel_size=list[1,1,1,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NCDHW", )
[Prof] paddle.nn.functional.max_pool3d 	 paddle.nn.functional.max_pool3d(x=Tensor([8, 320, 4, 56, 56],"float32"), kernel_size=list[1,1,1,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NCDHW", ) 	 32112640 	 51230 	 9.995409965515137 	 34.3652024269104 	 0.29085846320195163 	 30.187531232833862 	 29.442507028579712 	 1.0253043738269625 	 
2025-07-17 19:36:00.742106 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
/root/paddlejob/workspace/env_run/PaddleAPITest/tester/api_config/config_analyzer.py:1871: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 48 	 270495 	 14.467130422592163 	 14.38417673110962 	 1.0057670100300655 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:36:52.977396 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 48 	 270495 	 9.974565982818604 	 10.79766058921814 	 0.9237710243252667 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:37:35.060700 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 48 	 270495 	 10.197025775909424 	 10.99289345741272 	 0.9276016196657826 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:38:17.265183 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 1673216 	 166991 	 3.8289718627929688 	 5.046310901641846 	 0.7587665400376324 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:38:36.901266 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 774144 	 166991 	 3.7492411136627197 	 5.168663024902344 	 0.7253792896923392 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:38:56.036034 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 3434496 	 166991 	 9.986665964126587 	 14.275535345077515 	 0.6995650756852482 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:39:30.536768 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 720 	 335202 	 9.784215211868286 	 11.121610641479492 	 0.8797480443504094 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:40:11.300463 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 720 	 335202 	 9.862841844558716 	 11.31134581565857 	 0.8719423846900115 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:40:52.424420 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 720 	 335202 	 9.763236045837402 	 11.721998453140259 	 0.8328985953092226 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:41:33.709651 test begin: paddle.nn.functional.maxout(Tensor([100, 4, 3, 3],"float32"), 2, 1, None, )
[Error] shape '[100, 0, 2, 2, 2, 3, 3]' is invalid for input of size 3600
2025-07-17 19:41:40.735734 test begin: paddle.nn.functional.maxout(Tensor([100, 4, 3, 3],"float64"), 2, 1, None, )
[Error] shape '[100, 0, 2, 2, 2, 3, 3]' is invalid for input of size 3600
2025-07-17 19:41:47.214845 test begin: paddle.nn.functional.maxout(x=Tensor([100, 4, 3, 3],"float32"), groups=2, )
[Error] shape '[100, 0, 2, 2, 2, 3, 3]' is invalid for input of size 3600
2025-07-17 19:41:53.770720 test begin: paddle.nn.functional.mish(Tensor([12, 128, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 128, 40, 40],"float32"), ) 	 2457600 	 604472 	 9.953883647918701 	 9.292232751846313 	 1.071204726973818 	 32.14019703865051 	 42.606043338775635 	 0.754357704213285 	 
2025-07-17 19:43:29.116153 test begin: paddle.nn.functional.mish(Tensor([12, 256, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 256, 40, 40],"float32"), ) 	 4915200 	 604472 	 21.32786202430725 	 22.014889001846313 	 0.9687926213263467 	 32.19951605796814 	 43.174500703811646 	 0.7457993846614517 	 
2025-07-17 19:45:28.017079 test begin: paddle.nn.functional.mish(Tensor([12, 512, 20, 20],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 512, 20, 20],"float32"), ) 	 2457600 	 604472 	 9.943511486053467 	 9.110595464706421 	 1.0914227862024697 	 32.10489344596863 	 43.55823516845703 	 0.7370568004375344 	 
2025-07-17 19:47:02.822251 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 1],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 1],"float32"), "mean", ) 	 14475840 	 100009 	 13.574646711349487 	 9.582681894302368 	 1.4165811680987392 	 15.956639051437378 	 18.434606075286865 	 0.8655806902664761 	 
2025-07-17 19:48:00.661469 test begin: paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 64, 128],"float32"), Tensor([64, 3, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 64, 128],"float32"), Tensor([64, 3, 3, 64, 128],"float32"), "none", ) 	 9437184 	 100009 	 6.860717058181763 	 4.573035478591919 	 1.50025450060454 	 9.231521129608154 	 15.192090511322021 	 0.6076531154634902 	 
2025-07-17 19:48:37.352242 test begin: paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 64, 128],"float32"), Tensor([64, 4, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 64, 128],"float32"), Tensor([64, 4, 3, 64, 128],"float32"), "none", ) 	 12582912 	 100009 	 9.958809852600098 	 5.926102161407471 	 1.680499185021617 	 12.048123359680176 	 19.79651427268982 	 0.6085982205615411 	 
2025-07-17 19:49:25.399827 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5],"float64"), Tensor([5, 5],"float64"), reduction="mean", weight=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5],"float64"), Tensor([5, 5],"float64"), reduction="mean", weight=None, ) 	 50 	 85118 	 13.194109201431274 	 11.569802522659302 	 1.1403919103710534 	 18.144788026809692 	 22.630438089370728 	 0.8017868657757935 	 
2025-07-17 19:50:30.969927 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5],"float64"), Tensor([5, 5],"float64"), reduction="mean", weight=Tensor([5, 5],"float64"), )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5],"float64"), Tensor([5, 5],"float64"), reduction="mean", weight=Tensor([5, 5],"float64"), ) 	 75 	 85118 	 9.654947996139526 	 12.053256511688232 	 0.8010240209172493 	 20.13568687438965 	 25.659791469573975 	 0.7847174790280538 	 
2025-07-17 19:51:38.759846 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5],"float64"), Tensor([5, 5],"float64"), weight=Tensor([5, 5],"float64"), reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5],"float64"), Tensor([5, 5],"float64"), weight=Tensor([5, 5],"float64"), reduction="mean", name=None, ) 	 75 	 85118 	 11.222644329071045 	 12.090294599533081 	 0.9282358040725042 	 20.07419180870056 	 25.592222213745117 	 0.7843864296363868 	 
