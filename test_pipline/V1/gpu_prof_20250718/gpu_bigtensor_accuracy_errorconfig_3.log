2025-07-17 14:35:39.198135 test begin: paddle.acosh(Tensor([],"float32"), )
W0717 14:35:40.150449 110404 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.acosh 	 paddle.acosh(Tensor([],"float32"), ) 	 1 	 1133027 	 10.088554620742798 	 13.570338010787964 	 0.743426922212456 	 58.11753606796265 	 99.45664596557617 	 0.5843504524381229 	 
2025-07-17 14:38:44.864678 test begin: paddle.add(x=Tensor([8, 256, 320, 352],"float32"), y=Tensor([8, 256, 320, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 320, 352],"float32"), y=Tensor([8, 256, 320, 352],"float32"), ) 	 461373440 	 4475 	 9.05301809310913 	 8.98831295967102 	 1.0071988073544424 	 9.768258571624756 	 0.2483654022216797 	 39.33019045424874 	 
2025-07-17 14:39:25.532702 test begin: paddle.add(x=Tensor([8, 256, 336, 336],"float32"), y=Tensor([8, 256, 336, 336],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 336, 336],"float32"), y=Tensor([8, 256, 336, 336],"float32"), ) 	 462422016 	 4475 	 9.08087706565857 	 9.009803295135498 	 1.0078884930330771 	 9.787803411483765 	 0.2708280086517334 	 36.14029235827754 	 
2025-07-17 14:40:06.437511 test begin: paddle.add(x=Tensor([8, 256, 352, 352],"float32"), y=Tensor([8, 256, 352, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 352, 352],"float32"), y=Tensor([8, 256, 352, 352],"float32"), ) 	 507510784 	 4475 	 9.964438199996948 	 9.890900611877441 	 1.0074348728194882 	 10.743228912353516 	 0.27552294731140137 	 38.99213846682364 	 
2025-07-17 14:40:52.245611 test begin: paddle.add_n(list[Tensor([64, 128, 64, 64],"float16"),Tensor([64, 128, 64, 64],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 64, 64],"float16"),Tensor([64, 128, 64, 64],"float16"),], ) 	 67108864 	 109137 	 19.90157699584961 	 55.55753302574158 	 0.35821563543198676 	 None 	 None 	 None 	 combined
2025-07-17 14:42:09.994365 test begin: paddle.add_n(list[Tensor([64, 128, 64, 64],"float32"),Tensor([64, 128, 64, 64],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 64, 64],"float32"),Tensor([64, 128, 64, 64],"float32"),], ) 	 67108864 	 109137 	 34.1476309299469 	 76.54732131958008 	 0.4460983133215435 	 None 	 None 	 None 	 combined
2025-07-17 14:44:02.032206 test begin: paddle.add_n(list[Tensor([64, 256, 32, 32],"float16"),Tensor([64, 256, 32, 32],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 256, 32, 32],"float16"),Tensor([64, 256, 32, 32],"float16"),], ) 	 33554432 	 109137 	 9.992468118667603 	 28.845569849014282 	 0.34641257465083736 	 None 	 None 	 None 	 combined
2025-07-17 14:44:41.563371 test begin: paddle.addmm(Tensor([10, 10],"float32"), x=Tensor([10, 10],"float32"), y=Tensor([10, 10],"float32"), )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([10, 10],"float32"), x=Tensor([10, 10],"float32"), y=Tensor([10, 10],"float32"), ) 	 300 	 540141 	 12.792141437530518 	 13.066147089004517 	 0.9790293458655014 	 51.57792520523071 	 60.25765013694763 	 0.8559564650796954 	 
2025-07-17 14:46:59.388640 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 	 7900 	 540141 	 14.224378824234009 	 18.70048451423645 	 0.7606422610818004 	 62.07582187652588 	 69.33061361312866 	 0.8953594760161044 	 
2025-07-17 14:49:43.730378 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), )
[Prof] paddle.addmm 	 paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), ) 	 55 	 540141 	 9.628384590148926 	 13.389043092727661 	 0.7191241766469958 	 55.88640332221985 	 73.60828447341919 	 0.7592406713730855 	 
2025-07-17 14:52:16.565411 test begin: paddle.all(Tensor([448],"bool"), )
[Prof] paddle.all 	 paddle.all(Tensor([448],"bool"), ) 	 448 	 568466 	 6.7271668910980225 	 7.519845485687256 	 0.8945884465182214 	 None 	 None 	 None 	 
2025-07-17 14:52:30.851667 test begin: paddle.all(Tensor([5, 6, 10],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([5, 6, 10],"bool"), None, False, None, ) 	 300 	 568466 	 7.1137566566467285 	 7.687291860580444 	 0.9253917745890801 	 None 	 None 	 None 	 
2025-07-17 14:52:45.995492 test begin: paddle.all(Tensor([5, 6, 10],"float64"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([5, 6, 10],"float64"), None, False, None, ) 	 300 	 568466 	 9.532799005508423 	 7.706556797027588 	 1.2369725230838777 	 None 	 None 	 None 	 
2025-07-17 14:53:03.245489 test begin: paddle.allclose(Tensor([1124, 32],"float32"), Tensor([1124, 32],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([1124, 32],"float32"), Tensor([1124, 32],"float32"), ) 	 71936 	 834722 	 15.174280881881714 	 119.91895532608032 	 0.12653780080572105 	 None 	 None 	 None 	 
2025-07-17 14:55:19.184039 test begin: paddle.allclose(Tensor([13, 32, 64],"float32"), Tensor([13, 32, 64],"float32"), rtol=0.0001, atol=0.0001, )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([13, 32, 64],"float32"), Tensor([13, 32, 64],"float32"), rtol=0.0001, atol=0.0001, ) 	 53248 	 834722 	 9.889882564544678 	 116.77793717384338 	 0.08468964946539467 	 None 	 None 	 None 	 
2025-07-17 14:57:26.280295 test begin: paddle.allclose(Tensor([30522, 8],"float32"), Tensor([30522, 8],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([30522, 8],"float32"), Tensor([30522, 8],"float32"), ) 	 488352 	 834722 	 10.007627725601196 	 118.36532521247864 	 0.08454864385018514 	 None 	 None 	 None 	 
2025-07-17 14:59:36.675841 test begin: paddle.amax(Tensor([10, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 1000 	 423649 	 5.125899791717529 	 9.278879642486572 	 0.5524265848052191 	 38.27137899398804 	 58.330206871032715 	 0.6561159482702595 	 
2025-07-17 15:01:27.751104 test begin: paddle.amax(Tensor([10, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 1000 	 423649 	 5.5055320262908936 	 6.1842427253723145 	 0.8902516073153387 	 34.55302953720093 	 59.068252086639404 	 0.5849678688057276 	 
2025-07-17 15:03:13.070268 test begin: paddle.amax(Tensor([10, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 10],"float32"), axis=list[0,1,], keepdim=False, ) 	 1000 	 423649 	 10.011679887771606 	 6.238300800323486 	 1.6048728986028458 	 33.753385066986084 	 59.347954511642456 	 0.568737125731344 	 
2025-07-17 15:05:02.429621 test begin: paddle.amin(Tensor([10, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 1000 	 423561 	 5.073125839233398 	 6.073398590087891 	 0.8353026339343197 	 34.326714277267456 	 58.70046591758728 	 0.5847775437670386 	 
2025-07-17 15:06:46.621063 test begin: paddle.amin(Tensor([10, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 1000 	 423561 	 5.524179935455322 	 5.978434085845947 	 0.9240178709227421 	 34.76905298233032 	 49.439332246780396 	 0.7032670427015034 	 
2025-07-17 15:08:22.340826 test begin: paddle.amin(Tensor([10, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 10],"float32"), axis=list[0,1,], keepdim=False, ) 	 1000 	 423561 	 10.010490894317627 	 6.045313835144043 	 1.6559092161803548 	 33.87713623046875 	 48.77405905723572 	 0.6945728300101982 	 
2025-07-17 15:10:01.055088 test begin: paddle.angle(Tensor([2, 3],"complex128"), )
[Prof] paddle.angle 	 paddle.angle(Tensor([2, 3],"complex128"), ) 	 6 	 1205464 	 9.777496099472046 	 29.703130960464478 	 0.32917392151305896 	 61.31875014305115 	 196.7886860370636 	 0.31159692855260107 	 
2025-07-17 15:14:58.735256 test begin: paddle.angle(Tensor([],"complex64"), )
[Prof] paddle.angle 	 paddle.angle(Tensor([],"complex64"), ) 	 1 	 1205464 	 10.096203088760376 	 28.388667345046997 	 0.35564202313716115 	 61.14083290100098 	 186.9313259124756 	 0.32707644158918636 	 
2025-07-17 15:19:46.126033 test begin: paddle.any(Tensor([1, 300, 4096],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([1, 300, 4096],"bool"), ) 	 1228800 	 565025 	 10.0193452835083 	 9.638778686523438 	 1.0394828649315246 	 None 	 None 	 None 	 
2025-07-17 15:20:05.810751 test begin: paddle.any(Tensor([1124, 32],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([1124, 32],"bool"), ) 	 35968 	 565025 	 9.60681700706482 	 7.464434385299683 	 1.2870120509042593 	 None 	 None 	 None 	 
2025-07-17 15:20:22.890053 test begin: paddle.any(Tensor([512, 32],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([512, 32],"bool"), ) 	 16384 	 565025 	 9.520378112792969 	 7.423277854919434 	 1.2825032686178894 	 None 	 None 	 None 	 
2025-07-17 15:20:39.840304 test begin: paddle.arange(-100, 100, )
2025-07-17 15:20:39.949510 test begin: paddle.arange(-14, 14, 1, dtype=Dtype(float32), )
2025-07-17 15:20:39.950104 test begin: paddle.arange(-19, 20, dtype="float32", name=None, )
2025-07-17 15:20:39.950539 test begin: paddle.argmax(Tensor([29151, 100, 32],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 100, 32],"float32"), axis=1, ) 	 93283200 	 2422 	 3.3733811378479004 	 0.7595615386962891 	 4.441221633783576 	 None 	 None 	 None 	 
2025-07-17 15:20:45.930260 test begin: paddle.argmax(Tensor([29151, 100, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 100, 64],"float32"), axis=1, ) 	 186566400 	 2422 	 6.719997882843018 	 2.0887279510498047 	 3.2172681365543627 	 None 	 None 	 None 	 
2025-07-17 15:20:58.474725 test begin: paddle.argmax(Tensor([80239, 20, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([80239, 20, 64],"float32"), axis=1, ) 	 102705920 	 2422 	 9.99960446357727 	 0.8297157287597656 	 12.051843923128203 	 None 	 None 	 None 	 
2025-07-17 15:21:11.006574 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 3, 3, 3, 3],"float64"), axis=0, ) 	 729 	 909237 	 9.517242431640625 	 12.79163932800293 	 0.7440205424496195 	 None 	 None 	 None 	 
2025-07-17 15:21:33.326941 test begin: paddle.argmin(Tensor([4, 4, 4, 4, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 4, 4, 4, 4],"float64"), axis=0, ) 	 1024 	 909237 	 9.396902799606323 	 12.276129245758057 	 0.7654613772377288 	 None 	 None 	 None 	 
2025-07-17 15:21:55.007022 test begin: paddle.argmin(Tensor([5, 5, 5, 5],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([5, 5, 5, 5],"float64"), axis=0, ) 	 625 	 909237 	 9.483623743057251 	 12.100215435028076 	 0.7837566028455795 	 None 	 None 	 None 	 
2025-07-17 15:22:16.595575 test begin: paddle.argsort(Tensor([1500],"float32"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([1500],"float32"), stable=True, ) 	 1500 	 118625 	 9.929850578308105 	 4.581917762756348 	 2.1671821914880023 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 15:22:39.075047 test begin: paddle.argsort(Tensor([1500],"float64"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([1500],"float64"), stable=True, ) 	 1500 	 118625 	 11.398257970809937 	 6.884952068328857 	 1.6555319278463134 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 15:23:05.206062 test begin: paddle.argsort(Tensor([1500],"int32"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([1500],"int32"), stable=True, ) 	 1500 	 118625 	 9.886484861373901 	 4.601808309555054 	 2.148391283671227 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 15:23:27.594844 test begin: paddle.as_complex(Tensor([1, 8192, 1, 64, 2],"bfloat16"), )
[Error] (NotFound) The kernel with key (GPU, Undefined(AnyLayout), bfloat16) of kernel `as_complex` is not registered and fail to fallback to CPU one. Selected wrong DataType `bfloat16`. Paddle support following DataTypes: float64, float32.
  [Hint: Expected kernel_iter != iter->second.end(), but received kernel_iter == iter->second.end().] (at ../paddle/phi/core/kernel_factory.cc:380)

[Error] view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: BFloat16
2025-07-17 15:23:27.613337 test begin: paddle.as_complex(Tensor([1, 8192, 7, 64, 2],"bfloat16"), )
[Error] (NotFound) The kernel with key (GPU, Undefined(AnyLayout), bfloat16) of kernel `as_complex` is not registered and fail to fallback to CPU one. Selected wrong DataType `bfloat16`. Paddle support following DataTypes: float64, float32.
  [Hint: Expected kernel_iter != iter->second.end(), but received kernel_iter == iter->second.end().] (at ../paddle/phi/core/kernel_factory.cc:380)

[Error] view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: BFloat16
2025-07-17 15:23:27.726344 test begin: paddle.as_complex(Tensor([32, 15, 8, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([32, 15, 8, 8, 32, 2],"float32"), ) 	 1966080 	 3212023 	 10.518894910812378 	 14.10698938369751 	 0.7456512955888626 	 122.94173550605774 	 201.0109281539917 	 0.6116171724348926 	 
2025-07-17 15:29:16.437428 test begin: paddle.as_real(Tensor([1, 8192, 1, 64],"complex64"), )
[Prof] paddle.as_real 	 paddle.as_real(Tensor([1, 8192, 1, 64],"complex64"), ) 	 524288 	 3170745 	 9.820858716964722 	 12.260961771011353 	 0.8009859993352416 	 119.56189012527466 	 198.35450506210327 	 0.6027687149724215 	 
2025-07-17 15:34:56.464602 test begin: paddle.as_real(Tensor([1, 8192, 7, 64],"complex64"), )
[Prof] paddle.as_real 	 paddle.as_real(Tensor([1, 8192, 7, 64],"complex64"), ) 	 3670016 	 3170745 	 9.762006759643555 	 12.182832956314087 	 0.8012920143162701 	 119.33865857124329 	 198.43073153495789 	 0.6014121786887592 	 
2025-07-17 15:40:36.862589 test begin: paddle.as_real(Tensor([4, 32, 32, 33],"complex64"), )
[Prof] paddle.as_real 	 paddle.as_real(Tensor([4, 32, 32, 33],"complex64"), ) 	 135168 	 3170745 	 20.26346230506897 	 13.16490626335144 	 1.539202930861615 	 143.0901222229004 	 199.17390203475952 	 0.7184180294762138 	 
2025-07-17 15:46:52.568853 test begin: paddle.as_strided(Tensor([32, 32],"float16"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([32, 32],"float16"), shape=tuple(3,), stride=tuple(1,), ) 	 1024 	 576859 	 11.873763084411621 	 2.611205577850342 	 4.5472341148208715 	 33.91187524795532 	 49.61559820175171 	 0.6834922177106401 	 
2025-07-17 15:48:30.627589 test begin: paddle.as_strided(Tensor([32, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([32, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 	 1024 	 576859 	 11.712647438049316 	 2.638605833053589 	 4.438953060485953 	 34.17386221885681 	 56.08724498748779 	 0.6092982856704849 	 
2025-07-17 15:50:15.251426 test begin: paddle.as_strided(Tensor([32, 32],"float32"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([32, 32],"float32"), shape=tuple(3,), stride=tuple(1,), ) 	 1024 	 576859 	 9.883404731750488 	 2.580052614212036 	 3.8306989079635265 	 33.68468379974365 	 58.52678966522217 	 0.575542994796446 	 
2025-07-17 15:51:59.936610 test begin: paddle.asin(Tensor([8, 16, 32],"complex128"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([8, 16, 32],"complex128"), ) 	 4096 	 1101486 	 12.241279363632202 	 18.537289142608643 	 0.6603597359602689 	 61.691853523254395 	 131.03422570228577 	 0.4708071741761606 	 
2025-07-17 15:55:43.471736 test begin: paddle.asin(Tensor([8, 16, 32],"complex64"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([8, 16, 32],"complex64"), ) 	 4096 	 1101486 	 10.886395931243896 	 18.593621969223022 	 0.585490871507635 	 68.18730401992798 	 132.82294607162476 	 0.5133699111233234 	 
2025-07-17 15:59:33.992776 test begin: paddle.asin(Tensor([8, 16, 32],"float32"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([8, 16, 32],"float32"), ) 	 4096 	 1101486 	 9.662235260009766 	 11.460679054260254 	 0.8430770301012873 	 57.74798631668091 	 116.44655203819275 	 0.4959183874997039 	 
2025-07-17 16:02:50.319988 test begin: paddle.asinh(Tensor([8, 16, 32],"complex128"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([8, 16, 32],"complex128"), ) 	 4096 	 1084332 	 9.360667705535889 	 22.63781976699829 	 0.4134968738986955 	 56.66131114959717 	 121.26185607910156 	 0.4672640926148767 	 
2025-07-17 16:06:21.276940 test begin: paddle.asinh(Tensor([8, 16, 32],"complex64"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([8, 16, 32],"complex64"), ) 	 4096 	 1084332 	 9.132612943649292 	 14.756685256958008 	 0.6188796999206256 	 60.9456992149353 	 125.7169988155365 	 0.48478487228573136 	 
2025-07-17 16:09:51.838378 test begin: paddle.asinh(Tensor([8, 16, 32],"float32"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([8, 16, 32],"float32"), ) 	 4096 	 1084332 	 9.27930474281311 	 11.406886339187622 	 0.8134827039465325 	 64.09527802467346 	 130.65706062316895 	 0.49056115084000046 	 
2025-07-17 16:13:27.285001 test begin: paddle.assign(Tensor([234881024],"bfloat16"), Tensor([234881024],"bfloat16"), )
/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return func(*args, **kwargs)
[Prof] paddle.assign 	 paddle.assign(Tensor([234881024],"bfloat16"), Tensor([234881024],"bfloat16"), ) 	 469762048 	 3930 	 2.8055222034454346 	 2.798178195953369 	 1.0026245674784708 	 None 	 None 	 None 	 combined
2025-07-17 16:13:42.327299 test begin: paddle.assign(Tensor([469762048],"bfloat16"), Tensor([469762048],"bfloat16"), )
[Prof] paddle.assign 	 paddle.assign(Tensor([469762048],"bfloat16"), Tensor([469762048],"bfloat16"), ) 	 939524096 	 3930 	 5.562559604644775 	 5.568436622619629 	 0.9989445838440577 	 None 	 None 	 None 	 combined
2025-07-17 16:14:09.276945 test begin: paddle.assign(Tensor([822083584],"bfloat16"), Tensor([822083584],"bfloat16"), )
[Prof] paddle.assign 	 paddle.assign(Tensor([822083584],"bfloat16"), Tensor([822083584],"bfloat16"), ) 	 1644167168 	 3930 	 9.724170207977295 	 9.733410120010376 	 0.9990507014582602 	 None 	 None 	 None 	 combined
2025-07-17 16:14:55.047886 test begin: paddle.atan(Tensor([8, 16, 32],"complex128"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([8, 16, 32],"complex128"), ) 	 4096 	 1073676 	 9.496517419815063 	 16.56705355644226 	 0.5732170411269208 	 56.03487682342529 	 120.74991703033447 	 0.46405726978138095 	 
2025-07-17 16:18:17.923858 test begin: paddle.atan(Tensor([8, 16, 32],"complex64"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([8, 16, 32],"complex64"), ) 	 4096 	 1073676 	 9.397507905960083 	 11.487895488739014 	 0.8180356371775815 	 55.7226357460022 	 115.38586926460266 	 0.48292426188010207 	 
2025-07-17 16:21:30.485951 test begin: paddle.atan(Tensor([8, 16, 32],"float32"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([8, 16, 32],"float32"), ) 	 4096 	 1073676 	 9.694464445114136 	 11.054643392562866 	 0.8769585866185601 	 55.54571533203125 	 102.45756268501282 	 0.5421338735413458 	 
2025-07-17 16:24:29.664150 test begin: paddle.atan2(Tensor([100],"float64"), Tensor([100, 100],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([100],"float64"), Tensor([100, 100],"float64"), ) 	 10100 	 941002 	 31.157010316848755 	 10.967033863067627 	 2.840969646475927 	 90.0889368057251 	 173.88873958587646 	 0.5180837874854679 	 
2025-07-17 16:29:36.248386 test begin: paddle.atan2(Tensor([111, 222, 333],"float64"), Tensor([222, 333],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0708a2e560>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 16:39:44.058504 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), )
W0717 16:39:44.308400 28844 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), ) 	 7776 	 941002 	 16.503819227218628 	 11.194546461105347 	 1.4742731458179186 	 60.59483623504639 	 124.93824458122253 	 0.48499830006538625 	 
2025-07-17 16:43:18.004350 test begin: paddle.atanh(Tensor([8, 16, 32],"complex128"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([8, 16, 32],"complex128"), ) 	 4096 	 1089029 	 11.622278213500977 	 21.66177797317505 	 0.5365338998439312 	 59.13318109512329 	 133.7096631526947 	 0.4422506175009503 	 
2025-07-17 16:47:04.242246 test begin: paddle.atanh(Tensor([8, 16, 32],"complex64"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([8, 16, 32],"complex64"), ) 	 4096 	 1089029 	 14.708042621612549 	 15.511910676956177 	 0.9481773669224501 	 58.65182089805603 	 133.99096870422363 	 0.4377296579407984 	 
2025-07-17 16:50:47.117120 test begin: paddle.atanh(Tensor([8, 16, 32],"float32"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([8, 16, 32],"float32"), ) 	 4096 	 1089029 	 9.948131084442139 	 11.59541940689087 	 0.8579362880595947 	 58.417473554611206 	 116.98126268386841 	 0.4993746196130512 	 
2025-07-17 16:54:04.861766 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), ) 	 120 	 3717319 	 5.879567861557007 	 23.15227508544922 	 0.2539520561092593 	 None 	 None 	 None 	 
2025-07-17 16:54:33.901725 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 360 	 3717319 	 10.523399591445923 	 35.94913339614868 	 0.29273027183941297 	 None 	 None 	 None 	 
2025-07-17 16:55:20.380510 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 72 	 3717319 	 10.580432415008545 	 37.099609375 	 0.2851898603045209 	 None 	 None 	 None 	 
2025-07-17 16:56:08.066554 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), ) 	 120 	 2752932 	 5.211201190948486 	 21.951001405715942 	 0.2374015241779135 	 None 	 None 	 None 	 
2025-07-17 16:56:36.003378 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 360 	 2752932 	 10.370896339416504 	 20.407328128814697 	 0.508194716817094 	 None 	 None 	 None 	 
2025-07-17 16:57:06.787425 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 72 	 2752932 	 10.389386892318726 	 28.121776342391968 	 0.36944276797541126 	 None 	 None 	 None 	 
2025-07-17 16:57:45.306961 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), ) 	 120 	 2182286 	 4.884951114654541 	 15.06723403930664 	 0.32421021017599694 	 None 	 None 	 None 	 
2025-07-17 16:58:05.264132 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 360 	 2182286 	 10.052991390228271 	 15.667827367782593 	 0.6416327646614243 	 None 	 None 	 None 	 
2025-07-17 16:58:30.990174 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 72 	 2182286 	 9.949979305267334 	 22.336929321289062 	 0.44544973761385037 	 None 	 None 	 None 	 
2025-07-17 16:59:03.282703 test begin: paddle.audio.functional.get_window("bartlett", 1, )
Warning: The core code of paddle.audio.functional.get_window is too complex.
2025-07-17 16:59:03.286487 test begin: paddle.audio.functional.get_window("bartlett", 512, )
2025-07-17 16:59:03.286964 test begin: paddle.audio.functional.get_window("blackman", 1, )
2025-07-17 16:59:03.287396 test begin: paddle.bincount(Tensor([16],"int32"), weights=Tensor([16],"float32"), )
[Prof] paddle.bincount 	 paddle.bincount(Tensor([16],"int32"), weights=Tensor([16],"float32"), ) 	 32 	 166171 	 9.918245792388916 	 15.747860670089722 	 0.6298154397077484 	 None 	 None 	 None 	 
2025-07-17 16:59:29.650842 test begin: paddle.bincount(Tensor([20],"int64"), minlength=Tensor([1],"int32"), )
[Prof] paddle.bincount 	 paddle.bincount(Tensor([20],"int64"), minlength=Tensor([1],"int32"), ) 	 21 	 166171 	 13.669222831726074 	 13.522785663604736 	 1.0108289202952805 	 None 	 None 	 None 	 
2025-07-17 16:59:56.853812 test begin: paddle.bincount(x=Tensor([10],"int32"), weights=Tensor([10],"int32"), )
[Prof] paddle.bincount 	 paddle.bincount(x=Tensor([10],"int32"), weights=Tensor([10],"int32"), ) 	 20 	 166171 	 10.049229621887207 	 18.666746139526367 	 0.5383492948783514 	 None 	 None 	 None 	 
2025-07-17 17:00:25.592932 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 4320 	 1119130 	 10.026134729385376 	 12.799119710922241 	 0.7833456484377973 	 None 	 None 	 None 	 
2025-07-17 17:00:48.448515 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 4320 	 1119130 	 9.925568580627441 	 19.254624128341675 	 0.5154901240589572 	 None 	 None 	 None 	 
2025-07-17 17:01:17.634248 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 4320 	 1119130 	 9.88105297088623 	 15.6903657913208 	 0.629752875254953 	 None 	 None 	 None 	 
2025-07-17 17:01:43.501784 test begin: paddle.bitwise_invert(Tensor([2, 3, 4, 5],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([2, 3, 4, 5],"int32"), ) 	 120 	 1177797 	 10.146865367889404 	 12.706564903259277 	 0.7985529877777352 	 None 	 None 	 None 	 
2025-07-17 17:02:06.391885 test begin: paddle.bitwise_invert(Tensor([3, 4, 1],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([3, 4, 1],"int32"), ) 	 12 	 1177797 	 10.126377582550049 	 19.329091548919678 	 0.5238930943506251 	 None 	 None 	 None 	 
2025-07-17 17:02:36.216128 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([200, 300],"int16"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([200, 300],"int16"), ) 	 120000 	 1091406 	 10.176135540008545 	 18.439747095108032 	 0.5518587368645755 	 None 	 None 	 None 	 
2025-07-17 17:03:04.859628 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([200, 300],"int16"), False, )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([200, 300],"int16"), False, ) 	 120000 	 1091406 	 11.05620265007019 	 13.415112972259521 	 0.8241602342770269 	 None 	 None 	 None 	 
2025-07-17 17:03:29.338829 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([200, 300],"int32"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([200, 300],"int32"), ) 	 120000 	 1091406 	 10.164223909378052 	 12.221338987350464 	 0.8316784208259339 	 None 	 None 	 None 	 
2025-07-17 17:03:52.290747 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 2160 	 1173096 	 10.086492538452148 	 19.749836444854736 	 0.5107127123110885 	 None 	 None 	 None 	 
2025-07-17 17:04:22.132294 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 2160 	 1173096 	 9.87960934638977 	 13.618210554122925 	 0.7254704505503999 	 None 	 None 	 None 	 
2025-07-17 17:04:45.637190 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 2160 	 1173096 	 9.968664407730103 	 14.8577721118927 	 0.6709393799189329 	 None 	 None 	 None 	 
2025-07-17 17:05:10.469196 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 4320 	 1103012 	 9.950893640518188 	 14.910379648208618 	 0.6673802998513 	 None 	 None 	 None 	 
2025-07-17 17:05:35.977755 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 4320 	 1103012 	 11.119229793548584 	 12.737460851669312 	 0.8729549729757442 	 None 	 None 	 None 	 
2025-07-17 17:05:59.839938 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 4320 	 1103012 	 12.38499641418457 	 19.913771390914917 	 0.6219312339718265 	 None 	 None 	 None 	 
2025-07-17 17:06:32.144545 test begin: paddle.bitwise_right_shift(Tensor([200, 300],"int16"), Tensor([200, 300],"int16"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 300],"int16"), Tensor([200, 300],"int16"), ) 	 120000 	 1105786 	 10.286320686340332 	 20.24608564376831 	 0.5080646633294488 	 None 	 None 	 None 	 combined
2025-07-17 17:07:02.683702 test begin: paddle.bitwise_right_shift(Tensor([200, 300],"int32"), Tensor([200, 300],"int32"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 300],"int32"), Tensor([200, 300],"int32"), ) 	 120000 	 1105786 	 10.327718019485474 	 18.61902666091919 	 0.5546862469004925 	 None 	 None 	 None 	 combined
2025-07-17 17:07:31.638491 test begin: paddle.bitwise_right_shift(Tensor([200, 300],"int64"), Tensor([200, 300],"int64"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 300],"int64"), Tensor([200, 300],"int64"), ) 	 120000 	 1105786 	 10.084070444107056 	 13.035419464111328 	 0.7735900230805901 	 None 	 None 	 None 	 combined
2025-07-17 17:07:54.764821 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 4320 	 1110069 	 10.181091547012329 	 14.49166488647461 	 0.7025480941471788 	 None 	 None 	 None 	 
2025-07-17 17:08:19.452856 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 4320 	 1110069 	 10.140252590179443 	 17.04995632171631 	 0.5947377458828991 	 None 	 None 	 None 	 
2025-07-17 17:08:46.648380 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 4320 	 1110069 	 10.21583867073059 	 12.725952863693237 	 0.8027562871049188 	 None 	 None 	 None 	 
2025-07-17 17:09:09.600752 test begin: paddle.bmm(Tensor([112, 435, 435],"float32"), Tensor([112, 435, 64],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([112, 435, 435],"float32"), Tensor([112, 435, 64],"float32"), ) 	 24311280 	 6797 	 3.0781447887420654 	 3.077500104904175 	 1.0002094829621169 	 4.799131631851196 	 4.793294668197632 	 1.0012177352025302 	 
2025-07-17 17:09:25.905739 test begin: paddle.bmm(Tensor([26, 1024, 1024],"float32"), Tensor([26, 1024, 12],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([26, 1024, 1024],"float32"), Tensor([26, 1024, 12],"float32"), ) 	 27582464 	 6797 	 2.8526134490966797 	 2.8604307174682617 	 0.9972671009565647 	 3.488438844680786 	 3.4882798194885254 	 1.0000455884276749 	 
2025-07-17 17:09:41.393553 test begin: paddle.bmm(Tensor([4, 81, 7332],"float32"), Tensor([4, 7332, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 81, 7332],"float32"), Tensor([4, 7332, 512],"float32"), ) 	 17391504 	 6797 	 10.060310363769531 	 10.057073831558228 	 1.0003218164911098 	 3.379863739013672 	 3.375441074371338 	 1.0013102479186835 	 
2025-07-17 17:10:08.572359 test begin: paddle.broadcast_shape(list[0,0,0,], list[0,], )
2025-07-17 17:10:08.573781 test begin: paddle.broadcast_shape(list[0,1,3,], list[0,1,0,3,], )
2025-07-17 17:10:08.574288 test begin: paddle.broadcast_shape(list[0,1,3,], list[0,1,1,5,3,], )
2025-07-17 17:10:08.574725 test begin: paddle.broadcast_tensors(list[Tensor([16384, 1024],"float32"),Tensor([],"float32"),], )
[Prof] paddle.broadcast_tensors 	 paddle.broadcast_tensors(list[Tensor([16384, 1024],"float32"),Tensor([],"float32"),], ) 	 16777217 	 5789 	 1.3763713836669922 	 0.055130720138549805 	 24.9655977685039 	 0.9523410797119141 	 0.7148141860961914 	 1.3322918014721088 	 
2025-07-17 17:10:12.466548 test begin: paddle.broadcast_tensors(list[Tensor([200, 200],"float64"),Tensor([200, 200],"float64"),], )
[Prof] paddle.broadcast_tensors 	 paddle.broadcast_tensors(list[Tensor([200, 200],"float64"),Tensor([200, 200],"float64"),], ) 	 80000 	 5789 	 0.09467744827270508 	 0.04029440879821777 	 2.3496423225073517 	 0.40946340560913086 	 0.5053951740264893 	 0.8101846369978786 	 
2025-07-17 17:10:13.524577 test begin: paddle.broadcast_tensors(list[Tensor([32, 3, 1024, 768],"float32"),Tensor([],"float32"),], )
[Prof] paddle.broadcast_tensors 	 paddle.broadcast_tensors(list[Tensor([32, 3, 1024, 768],"float32"),Tensor([],"float32"),], ) 	 75497473 	 5789 	 9.924236059188843 	 0.08719253540039062 	 113.8197898893118 	 3.9330484867095947 	 1.3109869956970215 	 3.000066743315393 	 
2025-07-17 17:10:32.936764 test begin: paddle.broadcast_to(Tensor([111, 222, 333],"float64"), list[111,222,333,], )
[Prof] paddle.broadcast_to 	 paddle.broadcast_to(Tensor([111, 222, 333],"float64"), list[111,222,333,], ) 	 8205786 	 10161 	 1.0094192028045654 	 0.04637503623962402 	 21.766434803173084 	 1.09004545211792 	 0.7296216487884521 	 1.4939872657670685 	 
2025-07-17 17:10:38.006150 test begin: paddle.broadcast_to(Tensor([64, 1327104],"float32"), tuple(64,1327104,), )
[Prof] paddle.broadcast_to 	 paddle.broadcast_to(Tensor([64, 1327104],"float32"), tuple(64,1327104,), ) 	 84934656 	 10161 	 5.01051926612854 	 0.045516252517700195 	 110.08198146761022 	 5.232842445373535 	 0.746351957321167 	 7.011226264020851 	 
2025-07-17 17:10:53.571380 test begin: paddle.broadcast_to(Tensor([64, 1327104],"int64"), tuple(64,1327104,), )
[Prof] paddle.broadcast_to 	 paddle.broadcast_to(Tensor([64, 1327104],"int64"), tuple(64,1327104,), ) 	 84934656 	 10161 	 9.975868225097656 	 0.04551076889038086 	 219.19797156417968 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 17:11:16.896232 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([4],"float64"), )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([4],"float64"), ) 	 12 	 864209 	 9.62897801399231 	 9.248987674713135 	 1.0410845329936025 	 None 	 None 	 None 	 
2025-07-17 17:11:36.111432 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([4],"float64"), out_int32=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([4],"float64"), out_int32=True, ) 	 12 	 864209 	 9.98617172241211 	 9.55671763420105 	 1.0449374047292297 	 None 	 None 	 None 	 
2025-07-17 17:11:55.658821 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([4],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([4],"float64"), right=True, ) 	 12 	 864209 	 9.915894746780396 	 9.4856276512146 	 1.04535989724525 	 None 	 None 	 None 	 
2025-07-17 17:12:15.064713 test begin: paddle.cartesian_prod(list[Tensor([3],"complex128"),Tensor([5],"complex128"),Tensor([5],"complex128"),], )
[Prof] paddle.cartesian_prod 	 paddle.cartesian_prod(list[Tensor([3],"complex128"),Tensor([5],"complex128"),Tensor([5],"complex128"),], ) 	 13 	 154404 	 10.05043077468872 	 10.881504774093628 	 0.9236250852562686 	 16.88645839691162 	 33.21581983566284 	 0.5083860184833111 	 
2025-07-17 17:13:26.130261 test begin: paddle.cartesian_prod(list[Tensor([4],"int32"),Tensor([4],"int32"),Tensor([5],"int32"),Tensor([0],"int32"),], )
[Prof] paddle.cartesian_prod 	 paddle.cartesian_prod(list[Tensor([4],"int32"),Tensor([4],"int32"),Tensor([5],"int32"),Tensor([0],"int32"),], ) 	 13 	 154404 	 9.786921739578247 	 3.9972074031829834 	 2.4484398112004153 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 17:13:56.895696 test begin: paddle.cartesian_prod(list[Tensor([4],"int32"),Tensor([4],"int32"),Tensor([5],"int32"),], )
[Prof] paddle.cartesian_prod 	 paddle.cartesian_prod(list[Tensor([4],"int32"),Tensor([4],"int32"),Tensor([5],"int32"),], ) 	 13 	 154404 	 10.001416683197021 	 11.039808511734009 	 0.9059411377078416 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 17:14:35.101726 test begin: paddle.cast(Tensor([128256, 3072],"bfloat16"), Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([128256, 3072],"bfloat16"), Dtype(float16), ) 	 394002432 	 5170 	 5.912493944168091 	 10.211559295654297 	 0.5790001088946576 	 5.958003520965576 	 10.065996170043945 	 0.5918940778754106 	 combined
2025-07-17 17:15:23.745332 test begin: paddle.cast(Tensor([2, 1, 32768, 32768],"float16"), dtype=Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1, 32768, 32768],"float16"), dtype=Dtype(float16), ) 	 2147483648 	 5170 	 33.44822955131531 	 0.010272979736328125 	 3255.942327330115 	 33.46450757980347 	 0.2477569580078125 	 135.06990015088994 	 combined
2025-07-17 17:17:59.938421 test begin: paddle.cast(Tensor([8, 1024, 50304],"float16"), dtype="float32", )
[Prof] paddle.cast 	 paddle.cast(Tensor([8, 1024, 50304],"float16"), dtype="float32", ) 	 412090368 	 5170 	 10.00901174545288 	 11.424572944641113 	 0.8760950447734481 	 9.425873517990112 	 9.432447671890259 	 0.9993030277899406 	 combined
2025-07-17 17:18:55.275267 test begin: paddle.cdist(Tensor([6380, 4],"float32"), Tensor([1, 4],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([6380, 4],"float32"), Tensor([1, 4],"float32"), p=1, ) 	 25524 	 167036 	 9.872276544570923 	 3.613771438598633 	 2.7318486274824414 	 17.025442123413086 	 19.20486044883728 	 0.8865173568310858 	 
2025-07-17 17:19:45.080981 test begin: paddle.cdist(Tensor([8550, 4],"float32"), Tensor([1, 4],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([8550, 4],"float32"), Tensor([1, 4],"float32"), p=1, ) 	 34204 	 167036 	 9.929096460342407 	 3.627420663833618 	 2.737233252084113 	 17.11181330680847 	 19.48842453956604 	 0.8780501098006925 	 
2025-07-17 17:20:36.020066 test begin: paddle.cdist(Tensor([900, 4],"float32"), Tensor([1, 4],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([900, 4],"float32"), Tensor([1, 4],"float32"), p=1, ) 	 3604 	 167036 	 9.90515947341919 	 3.610999822616577 	 2.7430517751290786 	 20.295769453048706 	 19.20045018196106 	 1.0570465411335357 	 
2025-07-17 17:21:29.039756 test begin: paddle.ceil(Tensor([3, 6, 3, 1, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 1, 2, 5],"float64"), ) 	 540 	 1095350 	 9.768501281738281 	 12.037637948989868 	 0.8114965180987188 	 55.23085141181946 	 80.60764336585999 	 0.6851813191106336 	 
2025-07-17 17:24:06.693740 test begin: paddle.ceil(Tensor([3, 6, 3, 4, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 4, 1, 5],"float64"), ) 	 1080 	 1095350 	 9.78670597076416 	 11.80908989906311 	 0.8287434556273978 	 55.56949019432068 	 79.25466847419739 	 0.701151001753445 	 
2025-07-17 17:26:43.124403 test begin: paddle.ceil(Tensor([32, 32, 128],"float32"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([32, 32, 128],"float32"), ) 	 131072 	 1095350 	 10.027294635772705 	 11.984398603439331 	 0.8366956880835916 	 56.00717115402222 	 75.7265157699585 	 0.7395978883296365 	 
2025-07-17 17:29:16.881305 test begin: paddle.chunk(Tensor([16, 128, 25500],"float32"), 2, axis=1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcfb7d12a40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 17:39:21.702857 test begin: paddle.chunk(Tensor([4, 216, 64, 64],"float16"), 3, axis=1, )
W0717 17:39:22.149016 32215 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 64, 64],"float16"), 3, axis=1, ) 	 3538944 	 636359 	 9.727683305740356 	 5.414917469024658 	 1.796460123609699 	 38.29503059387207 	 58.84766888618469 	 0.6507484717523341 	 
2025-07-17 17:41:14.941876 test begin: paddle.chunk(Tensor([4, 216, 64, 64],"float32"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 64, 64],"float32"), 3, axis=1, ) 	 3538944 	 636359 	 10.62271237373352 	 4.870377779006958 	 2.1810859148383 	 38.860902309417725 	 57.29753613471985 	 0.6782299018590728 	 
2025-07-17 17:43:06.734864 test begin: paddle.clip(Tensor([1408, 6144],"float32"), min=-2, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([1408, 6144],"float32"), min=-2, max=2, ) 	 8650752 	 14674 	 0.7998039722442627 	 0.8022952079772949 	 0.9968948639998574 	 1.1762216091156006 	 2.04011607170105 	 0.5765464158786154 	 
2025-07-17 17:43:11.980266 test begin: paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, ) 	 26738688 	 14674 	 4.60167932510376 	 4.60228419303894 	 0.9998685722328717 	 6.914196252822876 	 11.084235191345215 	 0.6237864979824331 	 
2025-07-17 17:43:40.990424 test begin: paddle.clip(Tensor([3, 3840, 10240],"float32"), 0, 255, )
[Prof] paddle.clip 	 paddle.clip(Tensor([3, 3840, 10240],"float32"), 0, 255, ) 	 117964800 	 14674 	 10.004115581512451 	 10.037794351577759 	 0.9966448037401748 	 15.238106966018677 	 24.404199361801147 	 0.6244051173369054 	 
2025-07-17 17:44:45.045124 test begin: paddle.clone(Tensor([145, 12, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([145, 12, 112, 112],"float32"), ) 	 21826560 	 47105 	 6.398516416549683 	 6.852386474609375 	 0.9337646731191463 	 6.394933700561523 	 3.0435259342193604 	 2.101159588837797 	 
2025-07-17 17:45:09.455805 test begin: paddle.clone(Tensor([22, 64, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([22, 64, 112, 112],"float32"), ) 	 17661952 	 47105 	 5.215501308441162 	 5.2198591232299805 	 0.9991651470497691 	 5.218907594680786 	 3.0401663780212402 	 1.7166519676063348 	 
2025-07-17 17:45:28.742226 test begin: paddle.clone(Tensor([43, 256, 56, 56],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([43, 256, 56, 56],"float32"), ) 	 34521088 	 47105 	 9.989166498184204 	 9.998111486434937 	 0.9991053322157021 	 9.99014925956726 	 3.0155749320983887 	 3.31285061207735 	 
2025-07-17 17:46:04.788582 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 360 	 623842 	 9.842177152633667 	 8.765878438949585 	 1.1227827560215464 	 42.68716263771057 	 57.53982162475586 	 0.7418716539667704 	 
2025-07-17 17:48:03.633517 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),], ) 	 120 	 623842 	 8.139404058456421 	 11.528661251068115 	 0.7060146777842324 	 39.599940061569214 	 39.18489646911621 	 1.010591927754106 	 
2025-07-17 17:49:42.093885 test begin: paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 72 	 623842 	 9.863518953323364 	 8.630375862121582 	 1.142884054055398 	 42.7041380405426 	 56.236865282058716 	 0.7593619919310569 	 
2025-07-17 17:51:41.086673 test begin: paddle.combinations(Tensor([10],"float64"), 5, False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd4087d3a60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 18:01:45.618856 test begin: paddle.combinations(Tensor([10],"int32"), 1, True, )
W0717 18:01:45.829272 33512 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.combinations 	 paddle.combinations(Tensor([10],"int32"), 1, True, ) 	 10 	 832855 	 176.4207000732422 	 97.26629281044006 	 1.813790728274843 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 18:07:47.984377 test begin: paddle.combinations(Tensor([10],"int64"), 0, True, )
[Prof] paddle.combinations 	 paddle.combinations(Tensor([10],"int64"), 0, True, ) 	 10 	 832855 	 15.948782920837402 	 3.5334632396698 	 4.513640538772892 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 18:08:31.802794 test begin: paddle.complex(Tensor([1, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([1, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), ) 	 1048576 	 987329 	 14.913980484008789 	 12.14509129524231 	 1.2279842218930999 	 59.70066833496094 	 67.87833189964294 	 0.8795246828284974 	 
2025-07-17 18:11:06.494731 test begin: paddle.complex(Tensor([20, 64, 1001],"float32"), Tensor([20, 64, 1001],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 64, 1001],"float32"), Tensor([20, 64, 1001],"float32"), ) 	 2562560 	 987329 	 9.572291851043701 	 12.309959411621094 	 0.7776054762623404 	 56.81171226501465 	 67.07940244674683 	 0.8469322950531123 	 
2025-07-17 18:13:32.919065 test begin: paddle.complex(Tensor([20, 64, 1051],"float32"), Tensor([20, 64, 1051],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 64, 1051],"float32"), Tensor([20, 64, 1051],"float32"), ) 	 2690560 	 987329 	 9.03642988204956 	 19.059300899505615 	 0.4741217912291819 	 63.26319432258606 	 69.44141054153442 	 0.9110297994990606 	 
2025-07-17 18:16:13.837640 test begin: paddle.concat(list[Tensor([469762048],"bfloat16"),], )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([469762048],"bfloat16"),], ) 	 469762048 	 3901 	 5.524983882904053 	 5.782220363616943 	 0.955512508251764 	 11.028831481933594 	 8.110794067382812 	 1.35977210003217 	 
2025-07-17 18:17:03.731515 test begin: paddle.concat(list[Tensor([512, 32, 112, 112],"float16"),Tensor([512, 32, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 32, 112, 112],"float16"),Tensor([512, 32, 112, 112],"float16"),], axis=1, ) 	 411041792 	 3901 	 4.740739107131958 	 7.194653272628784 	 0.6589253057082743 	 7.484720706939697 	 0.23839187622070312 	 31.396710431568337 	 
2025-07-17 18:17:40.888815 test begin: paddle.concat(list[Tensor([822083584],"bfloat16"),], )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([822083584],"bfloat16"),], ) 	 822083584 	 3901 	 9.673828601837158 	 10.620033502578735 	 0.9109037744078847 	 19.318655014038086 	 14.197454452514648 	 1.3607125896161176 	 
2025-07-17 18:19:06.843440 test begin: paddle.conj(Tensor([2, 20, 2, 3],"complex128"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 20, 2, 3],"complex128"), ) 	 240 	 1208025 	 9.680851697921753 	 4.283797264099121 	 2.2598762502262413 	 59.04217267036438 	 65.95988392829895 	 0.8951224464637566 	 
2025-07-17 18:21:25.822049 test begin: paddle.conj(Tensor([2, 20, 2, 3],"complex64"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 20, 2, 3],"complex64"), ) 	 240 	 1208025 	 9.652415752410889 	 4.349619626998901 	 2.2191401961901547 	 58.654375314712524 	 65.86961722373962 	 0.8904617604726766 	 
2025-07-17 18:23:44.354792 test begin: paddle.conj(Tensor([2, 20, 2, 3],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 20, 2, 3],"float32"), ) 	 240 	 1208025 	 9.738910436630249 	 2.221442222595215 	 4.384048496770131 	 58.812864542007446 	 55.214500188827515 	 1.0651706406989816 	 
2025-07-17 18:25:51.394169 test begin: paddle.copysign(Tensor([12, 20, 2],"float32"), Tensor([12, 20, 2],"float32"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 20, 2],"float32"), Tensor([12, 20, 2],"float32"), ) 	 960 	 767933 	 9.683826923370361 	 7.998744487762451 	 1.2106683665400206 	 45.88656163215637 	 82.4118595123291 	 0.5567956105309283 	 
2025-07-17 18:28:17.860456 test begin: paddle.copysign(Tensor([12, 20, 2],"float64"), Tensor([12, 20, 2],"float64"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 20, 2],"float64"), Tensor([12, 20, 2],"float64"), ) 	 960 	 767933 	 9.52544116973877 	 7.93710994720459 	 1.200114554680395 	 45.13783383369446 	 82.0979859828949 	 0.5498043988935236 	 
2025-07-17 18:30:42.566561 test begin: paddle.copysign(Tensor([8, 17, 5, 6, 7],"float16"), Tensor([8, 17, 5, 6, 7],"float16"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([8, 17, 5, 6, 7],"float16"), Tensor([8, 17, 5, 6, 7],"float16"), ) 	 57120 	 767933 	 9.818360090255737 	 8.311751365661621 	 1.1812624870876645 	 46.89141249656677 	 83.72575879096985 	 0.5600595703603727 	 
2025-07-17 18:33:11.598300 test begin: paddle.cos(Tensor([32768, 32],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([32768, 32],"float32"), ) 	 1048576 	 1122531 	 15.4438955783844 	 14.125856161117554 	 1.093306869490491 	 65.78911519050598 	 87.50368428230286 	 0.7518439449733144 	 
2025-07-17 18:36:14.562087 test begin: paddle.cos(Tensor([5000, 256],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([5000, 256],"float32"), ) 	 1280000 	 1122531 	 9.830914974212646 	 17.11968469619751 	 0.5742462637992517 	 69.11134266853333 	 87.60710787773132 	 0.7888782582001044 	 
2025-07-17 18:39:18.280019 test begin: paddle.cos(Tensor([8192, 128],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([8192, 128],"float32"), ) 	 1048576 	 1122531 	 9.686923265457153 	 11.415384769439697 	 0.8485849107241803 	 58.578086614608765 	 88.11302947998047 	 0.6648061808829064 	 
2025-07-17 18:42:06.755136 test begin: paddle.cosh(Tensor([10, 20, 1],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([10, 20, 1],"float32"), ) 	 200 	 1104941 	 9.750326871871948 	 11.085310935974121 	 0.8795717980476421 	 57.65739822387695 	 77.94006752967834 	 0.7397658232964954 	 
2025-07-17 18:44:43.200797 test begin: paddle.cosh(Tensor([8, 32, 241, 241],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([8, 32, 241, 241],"float32"), ) 	 14868736 	 1104941 	 99.48182845115662 	 100.679452419281 	 0.9881045840104804 	 149.0555624961853 	 247.06864380836487 	 0.6032961536462638 	 
2025-07-17 18:54:41.790817 test begin: paddle.cosh(x=Tensor([3, 3, 3],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(x=Tensor([3, 3, 3],"float32"), ) 	 27 	 1104941 	 10.09688925743103 	 11.974953651428223 	 0.8431672932802374 	 58.36743664741516 	 78.48798751831055 	 0.7436480217281474 	 
2025-07-17 18:57:20.724662 test begin: paddle.count_nonzero(Tensor([1, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 980 	 272799 	 9.78887939453125 	 8.779062032699585 	 1.1150256551406488 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 18:58:01.297588 test begin: paddle.count_nonzero(Tensor([1, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 980 	 272799 	 9.791569232940674 	 8.768768548965454 	 1.1166413138017983 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1, 5, 1]) and output[0] has a shape of torch.Size([1, 5]).
2025-07-17 18:58:41.629111 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 5],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([2, 3, 4, 5],"float32"), axis=-1, keepdim=False, ) 	 120 	 272799 	 9.399309158325195 	 8.300486326217651 	 1.1323805363833728 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 18:59:21.126563 test begin: paddle.crop(x=Tensor([2, 3, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([2, 3, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 54 	 501632 	 8.950815916061401 	 9.407588243484497 	 0.9514463945911485 	 25.93584442138672 	 61.009483337402344 	 0.42511168760359824 	 combined
2025-07-17 19:01:06.446988 test begin: paddle.crop(x=Tensor([3, 3],"float32"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([3, 3],"float32"), shape=list[2,2,], ) 	 9 	 501632 	 9.225975751876831 	 6.725694417953491 	 1.3717506592700848 	 25.324286222457886 	 49.72808384895325 	 0.5092552188292482 	 combined
2025-07-17 19:02:37.714999 test begin: paddle.crop(x=Tensor([3, 3],"float64"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([3, 3],"float64"), shape=list[2,2,], ) 	 9 	 501632 	 9.335306406021118 	 6.827483654022217 	 1.367312889943206 	 26.289130449295044 	 49.68148589134216 	 0.5291534658764376 	 combined
2025-07-17 19:04:09.856269 test begin: paddle.cross(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 3, 3],"float64"), axis=0, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 3, 3],"float64"), axis=0, ) 	 54 	 945349 	 16.44808578491211 	 12.35668396949768 	 1.331108396517545 	 58.17404627799988 	 77.56259059906006 	 0.7500271178243092 	 
2025-07-17 19:06:54.409654 test begin: paddle.cross(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 3, 3],"float64"), axis=1, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 3, 3],"float64"), axis=1, ) 	 54 	 945349 	 10.271864652633667 	 12.737036943435669 	 0.8064563758627954 	 58.33710336685181 	 98.92461037635803 	 0.58971274331947 	 
2025-07-17 19:09:54.691213 test begin: paddle.cross(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 3, 3],"float64"), axis=2, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 3, 3],"float64"), axis=2, ) 	 54 	 945349 	 13.242380380630493 	 12.52430009841919 	 1.0573349629574862 	 56.62755084037781 	 89.25925922393799 	 0.634416544935779 	 
2025-07-17 19:12:46.353562 test begin: paddle.cummax(Tensor([100, 100],"float32"), )
[Prof] paddle.cummax 	 paddle.cummax(Tensor([100, 100],"float32"), ) 	 10000 	 426784 	 234.3822991847992 	 17.856138229370117 	 13.126147220303364 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-17 19:17:33.566337 test begin: paddle.cummax(Tensor([100, 100],"float32"), axis=-1, )
[Prof] paddle.cummax 	 paddle.cummax(Tensor([100, 100],"float32"), axis=-1, ) 	 10000 	 426784 	 5.715848684310913 	 6.116703987121582 	 0.9344654729647454 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-17 19:18:10.693899 test begin: paddle.cummax(Tensor([100, 100],"float32"), axis=-2, )
[Prof] paddle.cummax 	 paddle.cummax(Tensor([100, 100],"float32"), axis=-2, ) 	 10000 	 426784 	 10.008235216140747 	 9.41710638999939 	 1.0627718113888056 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-17 19:18:55.504958 test begin: paddle.cummin(Tensor([100, 100],"float32"), )
[Prof] paddle.cummin 	 paddle.cummin(Tensor([100, 100],"float32"), ) 	 10000 	 426816 	 234.36983227729797 	 17.86082124710083 	 13.12200760731206 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-17 19:23:37.548773 test begin: paddle.cummin(Tensor([100, 100],"float32"), axis=-1, )
[Prof] paddle.cummin 	 paddle.cummin(Tensor([100, 100],"float32"), axis=-1, ) 	 10000 	 426816 	 7.4675681591033936 	 6.072139263153076 	 1.2298084473157676 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-17 19:24:16.571870 test begin: paddle.cummin(Tensor([100, 100],"float32"), axis=-2, )
[Prof] paddle.cummin 	 paddle.cummin(Tensor([100, 100],"float32"), axis=-2, ) 	 10000 	 426816 	 10.009300231933594 	 9.417558193206787 	 1.0628339136947038 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-17 19:25:01.758590 test begin: paddle.cumprod(Tensor([2, 3, 10, 10],"float64"), 1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 10, 10],"float64"), 1, ) 	 600 	 1108697 	 9.55167031288147 	 11.84520149230957 	 0.8063746588931254 	 138.06560111045837 	 181.61146712303162 	 0.7602251294899052 	 
2025-07-17 19:30:42.877287 test begin: paddle.cumprod(Tensor([2, 3, 3, 4, 5],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 4, 5],"float32"), dim=0, ) 	 360 	 1108697 	 9.809507608413696 	 12.138642072677612 	 0.8081223212350521 	 130.33770036697388 	 171.07115292549133 	 0.7618917516955148 	 
2025-07-17 19:36:06.244636 test begin: paddle.cumprod(Tensor([2, 3, 3, 4, 5],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 4, 5],"float32"), dim=1, ) 	 360 	 1108697 	 10.213229179382324 	 12.047831296920776 	 0.8477234555892772 	 131.73116493225098 	 170.63459634780884 	 0.7720073639916492 	 
2025-07-17 19:41:30.880149 test begin: paddle.cumsum(Tensor([1034011],"float32"), axis=0, )
[Prof] paddle.cumsum 	 paddle.cumsum(Tensor([1034011],"float32"), axis=0, ) 	 1034011 	 302088 	 10.188829183578491 	 5.115564346313477 	 1.9917312135700247 	 25.19622254371643 	 26.71361780166626 	 0.9431976878154189 	 
2025-07-17 19:42:38.950581 test begin: paddle.cumsum(Tensor([1037889],"float32"), axis=0, )
[Prof] paddle.cumsum 	 paddle.cumsum(Tensor([1037889],"float32"), axis=0, ) 	 1037889 	 302088 	 9.970224142074585 	 5.0897204875946045 	 1.9588942391582098 	 25.86157512664795 	 26.79229235649109 	 0.9652617544829959 	 
2025-07-17 19:43:46.706022 test begin: paddle.cumsum(Tensor([1038765],"float32"), axis=0, )
[Prof] paddle.cumsum 	 paddle.cumsum(Tensor([1038765],"float32"), axis=0, ) 	 1038765 	 302088 	 12.235965967178345 	 7.979234933853149 	 1.533476087446097 	 28.622320652008057 	 26.796767711639404 	 1.0681258635374784 	 
2025-07-17 19:45:02.722952 test begin: paddle.deg2rad(Tensor([1],"int64"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([1],"int64"), ) 	 1 	 658731 	 16.46505832672119 	 11.733872652053833 	 1.4032075185202593 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 19:46:13.757575 test begin: paddle.deg2rad(Tensor([6],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([6],"float32"), ) 	 6 	 658731 	 9.604092359542847 	 9.634057760238647 	 0.9968896386713112 	 36.565006494522095 	 42.170679569244385 	 0.8670717870335061 	 
2025-07-17 19:47:51.750537 test begin: paddle.deg2rad(Tensor([8, 16, 32],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([8, 16, 32],"float32"), ) 	 4096 	 658731 	 9.719619274139404 	 9.610234022140503 	 1.0113821631967437 	 34.04146122932434 	 43.6279022693634 	 0.7802681187637367 	 
2025-07-17 19:49:29.257288 test begin: paddle.diag(Tensor([2000, 2000],"float32"), )
[Prof] paddle.diag 	 paddle.diag(Tensor([2000, 2000],"float32"), ) 	 4000000 	 1091289 	 10.670546770095825 	 19.143568515777588 	 0.5573959087774811 	 61.87861609458923 	 79.77297782897949 	 0.7756839192746081 	 
2025-07-17 19:52:20.836593 test begin: paddle.diag(Tensor([2000, 2000],"float32"), offset=-1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([2000, 2000],"float32"), offset=-1, ) 	 4000000 	 1091289 	 12.03584098815918 	 19.468556880950928 	 0.6182194736753029 	 63.94341063499451 	 78.59772682189941 	 0.8135529260265855 	 
2025-07-17 19:55:15.104417 test begin: paddle.diag(Tensor([2000, 2000],"float32"), offset=1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([2000, 2000],"float32"), offset=1, ) 	 4000000 	 1091289 	 12.282093286514282 	 18.94273042678833 	 0.6483803026170534 	 61.821739196777344 	 78.83026838302612 	 0.7842385985088047 	 
2025-07-17 19:58:07.067664 test begin: paddle.diag_embed(Tensor([2, 3, 12],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 3, 12],"float64"), ) 	 72 	 160783 	 64.74007034301758 	 3.3574023246765137 	 19.282785940542677 	 None 	 None 	 None 	 
2025-07-17 19:59:15.171658 test begin: paddle.diag_embed(Tensor([2, 3, 6],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 3, 6],"float64"), ) 	 36 	 160783 	 65.58472895622253 	 4.273552179336548 	 15.346654540299612 	 None 	 None 	 None 	 
2025-07-17 20:00:25.035720 test begin: paddle.diag_embed(Tensor([2, 3, 8],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 3, 8],"float64"), ) 	 48 	 160783 	 62.504087686538696 	 3.3136165142059326 	 18.8628006344654 	 None 	 None 	 None 	 
2025-07-17 20:01:30.861256 test begin: paddle.diagflat(Tensor([10, 10],"float64"), )
[Prof] paddle.diagflat 	 paddle.diagflat(Tensor([10, 10],"float64"), ) 	 100 	 612607 	 9.454890251159668 	 13.769316911697388 	 0.6866637111916203 	 39.496346950531006 	 37.30140018463135 	 1.058843548902596 	 
2025-07-17 20:03:11.429158 test begin: paddle.diagflat(Tensor([10, 10],"float64"), offset=1, )
[Prof] paddle.diagflat 	 paddle.diagflat(Tensor([10, 10],"float64"), offset=1, ) 	 100 	 612607 	 9.6691153049469 	 13.302319288253784 	 0.7268743965185772 	 37.1521520614624 	 41.148783445358276 	 0.9028736441454454 	 
2025-07-17 20:04:53.138661 test begin: paddle.diagflat(x=Tensor([3, 2, 2, 1, 2, 4, 2, 2],"float64"), offset=2, )
[Prof] paddle.diagflat 	 paddle.diagflat(x=Tensor([3, 2, 2, 1, 2, 4, 2, 2],"float64"), offset=2, ) 	 384 	 612607 	 13.383318185806274 	 19.927468061447144 	 0.6716015373623121 	 40.345781087875366 	 41.48248910903931 	 0.9725978829723542 	 
2025-07-17 20:06:48.292279 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 2, 2],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 6, 6, 2, 2],"float64"), ) 	 864 	 2676121 	 9.817169189453125 	 11.95712161064148 	 0.821031140196495 	 160.86494994163513 	 201.7112534046173 	 0.7975011171982178 	 
2025-07-17 20:13:14.156111 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 2, 2],"float64"), axis1=-1, axis2=2, )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 6, 6, 2, 2],"float64"), axis1=-1, axis2=2, ) 	 864 	 2676121 	 13.696549654006958 	 12.666009664535522 	 1.0813626403868077 	 167.96023988723755 	 201.94435334205627 	 0.8317154558055112 	 
2025-07-17 20:19:50.434143 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 6, 6, 6],"float64"), ) 	 1296 	 2676121 	 10.644430160522461 	 11.681491613388062 	 0.9112218296097535 	 166.7918028831482 	 199.52124118804932 	 0.8359601308110672 	 
2025-07-17 20:26:19.084092 test begin: paddle.diagonal_scatter(Tensor([10, 10],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10, 10],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, ) 	 110 	 322949 	 11.873907089233398 	 7.788813829421997 	 1.5244820776663182 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 20:27:02.984336 test begin: paddle.diagonal_scatter(Tensor([10, 10],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10, 10],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, ) 	 110 	 322949 	 10.018267393112183 	 7.520464897155762 	 1.332134054225968 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 20:27:42.523141 test begin: paddle.diagonal_scatter(Tensor([10, 10],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10, 10],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, ) 	 110 	 322949 	 9.667738437652588 	 7.478925943374634 	 1.2926640149735618 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 20:28:20.956537 test begin: paddle.diff(x=Tensor([4, 4, 4, 4],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 4],"float64"), ) 	 256 	 243702 	 9.675770044326782 	 4.3953094482421875 	 2.2013853992001415 	 None 	 None 	 None 	 
2025-07-17 20:28:35.057524 test begin: paddle.diff(x=Tensor([4, 4, 4, 4],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 4],"float64"), axis=-2, ) 	 256 	 243702 	 9.693009614944458 	 4.506844520568848 	 2.1507308651777097 	 None 	 None 	 None 	 
2025-07-17 20:28:49.262074 test begin: paddle.diff(x=Tensor([4, 4, 4, 4],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 4],"float64"), axis=2, ) 	 256 	 243702 	 9.683825969696045 	 4.4585137367248535 	 2.171985226810048 	 None 	 None 	 None 	 
2025-07-17 20:29:03.411681 test begin: paddle.digamma(Tensor([8, 3, 32, 32],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 32, 32],"float32"), ) 	 24576 	 1079216 	 11.18777084350586 	 16.58683180809021 	 0.6744971537029173 	 55.53461217880249 	 86.62691259384155 	 0.6410780497185875 	 
2025-07-17 20:31:53.367004 test begin: paddle.digamma(Tensor([8, 3, 32, 32],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 32, 32],"float64"), ) 	 24576 	 1079216 	 10.998531103134155 	 17.95624852180481 	 0.612518315826288 	 56.856024503707886 	 83.48960614204407 	 0.6809952415750596 	 
2025-07-17 20:34:42.681881 test begin: paddle.digamma(x=Tensor([3, 6, 6, 6, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 6, 6, 6, 6],"float64"), ) 	 3888 	 1079216 	 9.411655902862549 	 17.866455793380737 	 0.5267780029629285 	 55.24472689628601 	 83.80896925926208 	 0.659174398451162 	 
2025-07-17 20:37:29.022095 test begin: paddle.dist(x=Tensor([10],"float64"), y=Tensor([4, 10],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([10],"float64"), y=Tensor([4, 10],"float64"), ) 	 50 	 412370 	 9.768606424331665 	 8.977490663528442 	 1.0881221479869843 	 35.38544702529907 	 75.67195796966553 	 0.46761637963013947 	 
2025-07-17 20:39:40.301708 test begin: paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 8, 7, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 8, 7, 1, 4],"float64"), ) 	 480 	 412370 	 12.244768142700195 	 9.262024402618408 	 1.3220401513127686 	 39.509647369384766 	 80.40069890022278 	 0.4914092527779667 	 
2025-07-17 20:42:01.856893 test begin: paddle.dist(x=Tensor([2, 1, 4, 4],"float64"), y=Tensor([7, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 4, 4],"float64"), y=Tensor([7, 1, 4],"float64"), ) 	 60 	 412370 	 10.518004179000854 	 9.321245431900024 	 1.1283904340728088 	 40.19283580780029 	 81.09182167053223 	 0.49564598475910027 	 
2025-07-17 20:44:22.989436 test begin: paddle.divide(Tensor([128, 93431],"float32"), Tensor([1, 93431],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc808574f10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 20:54:28.022951 test begin: paddle.divide(Tensor([512, 995],"float32"), Tensor([1, 995],"float32"), )
W0717 20:54:28.397548 110853 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.divide 	 paddle.divide(Tensor([512, 995],"float32"), Tensor([1, 995],"float32"), ) 	 510435 	 861137 	 8.58978009223938 	 10.508266687393188 	 0.8174307283754585 	 68.12880992889404 	 97.40625953674316 	 0.6994294848494289 	 
2025-07-17 20:57:34.062089 test begin: paddle.divide(x=Tensor([187679, 3],"float32"), y=Tensor([3],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0821e1ab60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 21:07:43.191816 test begin: paddle.dot(x=Tensor([6],"float32"), y=Tensor([6],"float32"), )
Warning: The core code of paddle.dot is too complex.
W0717 21:07:43.409716 157371 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.dot 	 paddle.dot(x=Tensor([6],"float32"), y=Tensor([6],"float32"), ) 	 12 	 1035204 	 9.604336500167847 	 17.674110889434814 	 0.5434127102772169 	 62.942214250564575 	 78.10789656639099 	 0.8058367593738013 	 
2025-07-17 21:10:33.052972 test begin: paddle.dot(x=Tensor([6],"float64"), y=Tensor([6],"float64"), )
[Prof] paddle.dot 	 paddle.dot(x=Tensor([6],"float64"), y=Tensor([6],"float64"), ) 	 12 	 1035204 	 10.15283489227295 	 17.024720907211304 	 0.5963583748367016 	 64.61679339408875 	 76.88712358474731 	 0.840411116730959 	 
2025-07-17 21:13:21.744665 test begin: paddle.dot(x=Tensor([6],"int32"), y=Tensor([6],"int32"), )
[Prof] paddle.dot 	 paddle.dot(x=Tensor([6],"int32"), y=Tensor([6],"int32"), ) 	 12 	 1035204 	 9.523349523544312 	 18.081207990646362 	 0.526698743163115 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-17 21:14:54.850992 test begin: paddle.dsplit(Tensor([4, 3, 6],"int64"), list[-1,1,3,], )
W0717 21:14:58.136349 20278 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-17 21:14:58.137268 test begin: paddle.dsplit(Tensor([4, 3, 6],"int64"), list[-1,], )
W0717 21:14:59.829479 20670 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-17 21:14:59.829956 test begin: paddle.dsplit(Tensor([4, 3, 6],"int64"), list[2,4,], )
W0717 21:15:02.234804 20811 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

