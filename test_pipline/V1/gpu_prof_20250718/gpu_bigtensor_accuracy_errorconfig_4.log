2025-07-17 14:35:42.910831 test begin: paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
W0717 14:35:43.450935 110521 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 360 	 525367 	 14.110496044158936 	 7.797411918640137 	 1.8096384019968252 	 39.749754905700684 	 38.216294288635254 	 1.0401258323343363 	 
2025-07-17 14:37:25.343005 test begin: paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),], ) 	 120 	 525367 	 7.682278156280518 	 9.577155828475952 	 0.8021460957582671 	 30.188363313674927 	 29.134838104248047 	 1.0361603248199711 	 
2025-07-17 14:38:41.933292 test begin: paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 72 	 525367 	 9.66611385345459 	 7.284537315368652 	 1.3269358690855182 	 37.234618186950684 	 36.48914408683777 	 1.0204300242926723 	 
2025-07-17 14:40:12.615763 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 457, 457],"float32"), Tensor([30, 8, 457, 64],"float32"), )
Warning: The core code of paddle.einsum is too complex.
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 457, 457],"float32"), Tensor([30, 8, 457, 64],"float32"), ) 	 57143280 	 10834 	 9.218406915664673 	 9.216963529586792 	 1.0001566010403804 	 18.875105619430542 	 14.909770011901855 	 1.2659555180504678 	 
2025-07-17 14:41:06.087072 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), ) 	 61933680 	 10834 	 9.524685621261597 	 10.490467309951782 	 0.9079372100255251 	 19.549486875534058 	 15.196946620941162 	 1.2864088664097273 	 
2025-07-17 14:42:03.868205 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 498, 498],"float32"), Tensor([30, 8, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 498, 498],"float32"), Tensor([30, 8, 498, 64],"float32"), ) 	 67170240 	 10834 	 9.984377384185791 	 9.99064040184021 	 0.9993731114921056 	 20.370506763458252 	 15.65220856666565 	 1.301446161843327 	 
2025-07-17 14:43:02.930995 test begin: paddle.equal(Tensor([4148, 30],"int64"), Tensor([4148, 30],"int64"), )
[Prof] paddle.equal 	 paddle.equal(Tensor([4148, 30],"int64"), Tensor([4148, 30],"int64"), ) 	 248880 	 411320 	 4.515341758728027 	 4.523792028427124 	 0.9981320384213076 	 None 	 None 	 None 	 
2025-07-17 14:43:12.022542 test begin: paddle.equal(Tensor([416, 30],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([416, 30],"int64"), 0, ) 	 12480 	 411320 	 10.326361179351807 	 5.675567388534546 	 1.819441206920127 	 None 	 None 	 None 	 
2025-07-17 14:43:28.030359 test begin: paddle.equal(Tensor([512, 30],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([512, 30],"int64"), 0, ) 	 15360 	 411320 	 10.361701011657715 	 5.678637504577637 	 1.8246808329119422 	 None 	 None 	 None 	 
2025-07-17 14:43:44.743791 test begin: paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), ) 	 640 	 609465 	 10.435971975326538 	 23.02575397491455 	 0.453230412636912 	 None 	 None 	 None 	 
2025-07-17 14:44:18.228087 test begin: paddle.equal_all(Tensor([128],"float32"), Tensor([128],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([128],"float32"), Tensor([128],"float32"), ) 	 256 	 609465 	 10.307896137237549 	 22.898901224136353 	 0.4501480676449495 	 None 	 None 	 None 	 
2025-07-17 14:44:52.498958 test begin: paddle.equal_all(Tensor([16, 16],"float32"), Tensor([16, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([16, 16],"float32"), Tensor([16, 16],"float32"), ) 	 512 	 609465 	 10.440814733505249 	 23.08965039253235 	 0.4521859168938312 	 None 	 None 	 None 	 
2025-07-17 14:45:26.035008 test begin: paddle.erf(Tensor([11, 17],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([11, 17],"float64"), ) 	 187 	 1159878 	 10.295511484146118 	 12.754720687866211 	 0.8071922338480072 	 62.4214551448822 	 120.5806212425232 	 0.5176740217595516 	 
2025-07-17 14:48:52.292307 test begin: paddle.erf(Tensor([4, 2, 3, 5, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 5, 4],"float64"), ) 	 480 	 1159878 	 10.099957704544067 	 12.904158353805542 	 0.7826901551905946 	 62.169296741485596 	 125.72525978088379 	 0.49448533134737876 	 
2025-07-17 14:52:23.660865 test begin: paddle.erf(Tensor([4, 2, 3, 5],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 5],"float64"), ) 	 120 	 1159878 	 10.328147888183594 	 12.92122197151184 	 0.7993166521676243 	 63.17883014678955 	 123.94605040550232 	 0.5097284660551383 	 
2025-07-17 14:55:54.242620 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 5, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 5, 4],"float64"), ) 	 480 	 1114643 	 10.244274616241455 	 12.025610446929932 	 0.8518714839009905 	 59.34917330741882 	 120.63840770721436 	 0.49195918974210445 	 
2025-07-17 14:59:16.516128 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 5],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 5],"float64"), ) 	 120 	 1114643 	 10.300490379333496 	 11.513770818710327 	 0.8946235374595781 	 60.11829233169556 	 118.29361176490784 	 0.5082125013747347 	 
2025-07-17 15:02:37.137547 test begin: paddle.erfinv(x=Tensor([4, 2, 3],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3],"float64"), ) 	 24 	 1114643 	 10.408416748046875 	 11.430511474609375 	 0.910581890510072 	 59.913586378097534 	 116.99839973449707 	 0.512088938943256 	 
2025-07-17 15:05:55.896541 test begin: paddle.exp(Tensor([13, 64, 1007, 16],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([13, 64, 1007, 16],"float32"), ) 	 13405184 	 210648 	 17.17607831954956 	 17.484169483184814 	 0.9823788505406815 	 25.61143469810486 	 25.40282893180847 	 1.0082119108409686 	 
2025-07-17 15:07:23.449105 test begin: paddle.exp(Tensor([16, 1, 640, 640],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([16, 1, 640, 640],"float32"), ) 	 6553600 	 210648 	 8.916610717773438 	 8.947791814804077 	 0.9965152187627957 	 13.035517454147339 	 14.551963329315186 	 0.8957909774200062 	 
2025-07-17 15:08:09.677035 test begin: paddle.exp(Tensor([8, 1, 960, 960],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([8, 1, 960, 960],"float32"), ) 	 7372800 	 210648 	 9.906709432601929 	 9.94269347190857 	 0.9963808560116726 	 14.536983013153076 	 15.22989821434021 	 0.9545029657168229 	 
2025-07-17 15:08:59.537871 test begin: paddle.expand(Tensor([16, 32, 128],"float32"), list[8,16,32,128,], )
[Prof] paddle.expand 	 paddle.expand(Tensor([16, 32, 128],"float32"), list[8,16,32,128,], ) 	 65536 	 903336 	 14.938065528869629 	 8.023384809494019 	 1.8618159147986413 	 63.22563695907593 	 69.24140191078186 	 0.9131189608284174 	 
2025-07-17 15:11:34.991780 test begin: paddle.expand(Tensor([2, 2, 1, 8, 128, 96],"float16"), list[2,2,1,8,128,96,], )
[Prof] paddle.expand 	 paddle.expand(Tensor([2, 2, 1, 8, 128, 96],"float16"), list[2,2,1,8,128,96,], ) 	 393216 	 903336 	 9.467307329177856 	 4.471853494644165 	 2.117087990587489 	 52.06060171127319 	 51.131399154663086 	 1.0181728364952314 	 
2025-07-17 15:13:32.783007 test begin: paddle.expand(Tensor([2, 2, 1, 8, 128, 96],"float16"), list[2,2,2,8,128,96,], )
[Prof] paddle.expand 	 paddle.expand(Tensor([2, 2, 1, 8, 128, 96],"float16"), list[2,2,2,8,128,96,], ) 	 393216 	 903336 	 9.937266826629639 	 4.5455262660980225 	 2.1861642073757066 	 54.62221932411194 	 66.30712032318115 	 0.8237760749959135 	 
2025-07-17 15:15:49.076174 test begin: paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 28, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 28, 28],"float32"), ) 	 32090800 	 112898 	 9.952465534210205 	 0.4150223731994629 	 23.980551837447507 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 15:16:11.582480 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 28],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 28],"float16"), ) 	 32153600 	 112898 	 9.97239637374878 	 0.4119253158569336 	 24.209234028268142 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 15:16:34.193867 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 28],"float32"), ) 	 32153600 	 112898 	 9.974887371063232 	 0.41928720474243164 	 23.79010677702605 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-17 15:16:56.744426 test begin: paddle.expm1(Tensor([8, 16, 32],"complex128"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([8, 16, 32],"complex128"), ) 	 4096 	 1141120 	 9.970824480056763 	 12.679572582244873 	 0.7863691315603845 	 63.64857578277588 	 109.0554563999176 	 0.5836349494459955 	 
2025-07-17 15:20:12.125244 test begin: paddle.expm1(Tensor([8, 16, 32],"complex64"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([8, 16, 32],"complex64"), ) 	 4096 	 1141120 	 10.217332363128662 	 12.637153625488281 	 0.8085153244098415 	 63.876933336257935 	 108.90620517730713 	 0.5865316235402904 	 
2025-07-17 15:23:27.771597 test begin: paddle.expm1(Tensor([8, 16, 32],"float16"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([8, 16, 32],"float16"), ) 	 4096 	 1141120 	 10.436279535293579 	 12.213001489639282 	 0.8545220881326381 	 66.07918429374695 	 93.92820191383362 	 0.7035073912557768 	 
2025-07-17 15:26:30.436509 test begin: paddle.eye(1, dtype="float32", )
[Error] 'm'
2025-07-17 15:26:32.994967 test begin: paddle.eye(Tensor([],"int64"), Tensor([],"int64"), )
[Prof] paddle.eye 	 paddle.eye(Tensor([],"int64"), Tensor([],"int64"), ) 	 2 	 158930 	 10.061557292938232 	 3.4899909496307373 	 2.882975181927852 	 None 	 None 	 None 	 
2025-07-17 15:26:46.552659 test begin: paddle.eye(num_rows=Tensor([1],"float64"), num_columns=Tensor([1],"float64"), dtype="float32", )
[Prof] paddle.eye 	 paddle.eye(num_rows=Tensor([1],"float64"), num_columns=Tensor([1],"float64"), dtype="float32", ) 	 2 	 158930 	 10.7411367893219 	 3.3772568702697754 	 3.1804322862962735 	 None 	 None 	 None 	 
2025-07-17 15:27:00.680607 test begin: paddle.fft.fft(x=Tensor([7],"complex128"), )
[Prof] paddle.fft.fft 	 paddle.fft.fft(x=Tensor([7],"complex128"), ) 	 7 	 147315 	 7.805521488189697 	 2.5654523372650146 	 3.042551746064022 	 11.536918640136719 	 10.870453119277954 	 1.0613098197053843 	 
2025-07-17 15:27:33.812578 test begin: paddle.fft.fft(x=Tensor([7],"complex128"), axis=0, )
[Prof] paddle.fft.fft 	 paddle.fft.fft(x=Tensor([7],"complex128"), axis=0, ) 	 7 	 147315 	 7.7158308029174805 	 2.4609851837158203 	 3.1352609735209436 	 11.098448514938354 	 10.69804334640503 	 1.0374278880322425 	 
2025-07-17 15:28:05.792906 test begin: paddle.fft.fft(x=Tensor([7],"complex128"), n=1, )
[Prof] paddle.fft.fft 	 paddle.fft.fft(x=Tensor([7],"complex128"), n=1, ) 	 7 	 147315 	 10.246210098266602 	 3.1059048175811768 	 3.2989452993753257 	 15.72929859161377 	 15.391494750976562 	 1.0219474356521334 	 
2025-07-17 15:28:50.274518 test begin: paddle.fft.fft2(x=Tensor([2, 2, 4],"complex128"), )
[Prof] paddle.fft.fft2 	 paddle.fft.fft2(x=Tensor([2, 2, 4],"complex128"), ) 	 16 	 81301 	 7.310371398925781 	 1.6457247734069824 	 4.442037646302083 	 7.006332635879517 	 6.912558317184448 	 1.013565790607791 	 
2025-07-17 15:29:13.158461 test begin: paddle.fft.fft2(x=Tensor([3, 2, 4, 2],"complex128"), )
[Prof] paddle.fft.fft2 	 paddle.fft.fft2(x=Tensor([3, 2, 4, 2],"complex128"), ) 	 48 	 81301 	 7.428727388381958 	 1.7063488960266113 	 4.353580563553225 	 7.137296438217163 	 6.437519550323486 	 1.1087028757619093 	 
2025-07-17 15:29:36.966095 test begin: paddle.fft.fft2(x=Tensor([3, 3, 2, 2],"complex128"), s=tuple(1,2,), )
[Prof] paddle.fft.fft2 	 paddle.fft.fft2(x=Tensor([3, 3, 2, 2],"complex128"), s=tuple(1,2,), ) 	 36 	 81301 	 10.06019139289856 	 1.9571504592895508 	 5.140223811178231 	 8.340052843093872 	 8.884135007858276 	 0.9387580035329103 	 
2025-07-17 15:30:06.501934 test begin: paddle.fft.fftfreq(n=10, d=0.1, )
2025-07-17 15:30:06.503143 test begin: paddle.fft.fftfreq(n=14, d=0.03703703731298447, )
2025-07-17 15:30:06.503608 test begin: paddle.fft.fftfreq(n=39, d=0.5, )
2025-07-17 15:30:06.504008 test begin: paddle.fft.fftn(Tensor([39, 7, 32],"float32"), )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([39, 7, 32],"float32"), ) 	 8736 	 19248 	 3.5596747398376465 	 0.7176878452301025 	 4.959920616596699 	 1.8380539417266846 	 1.898876428604126 	 0.9679692232937178 	 
2025-07-17 15:30:14.582225 test begin: paddle.fft.fftn(Tensor([8, 32, 481, 481],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([8, 32, 481, 481],"float32"), axes=list[2,3,], ) 	 59228416 	 19248 	 130.7825710773468 	 94.97420358657837 	 1.3770325639859202 	 121.29734635353088 	 85.274423122406 	 1.422435261501755 	 
2025-07-17 15:37:29.676949 test begin: paddle.fft.fftn(x=Tensor([50, 8, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 8, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], ) 	 3057600 	 19248 	 10.25032925605774 	 3.5018246173858643 	 2.927139527538555 	 5.095830678939819 	 2.9509804248809814 	 1.7268263238802555 	 
2025-07-17 15:37:51.626626 test begin: paddle.fft.fftshift(x=Tensor([16, 128, 10, 6],"complex64"), axes=list[-2,], )
[Prof] paddle.fft.fftshift 	 paddle.fft.fftshift(x=Tensor([16, 128, 10, 6],"complex64"), axes=list[-2,], ) 	 122880 	 220265 	 10.651235103607178 	 2.585048198699951 	 4.1203236013022115 	 12.590323209762573 	 15.145631313323975 	 0.8312841471775801 	 
2025-07-17 15:38:33.452379 test begin: paddle.fft.fftshift(x=Tensor([16, 64, 19, 10],"complex64"), axes=list[-2,], )
[Prof] paddle.fft.fftshift 	 paddle.fft.fftshift(x=Tensor([16, 64, 19, 10],"complex64"), axes=list[-2,], ) 	 194560 	 220265 	 13.95545744895935 	 2.5138111114501953 	 5.551513948440053 	 13.47213625907898 	 15.01809549331665 	 0.897060234107204 	 
2025-07-17 15:39:18.426400 test begin: paddle.fft.fftshift(x=Tensor([16, 96, 20, 11],"complex64"), axes=list[-2,], )
[Prof] paddle.fft.fftshift 	 paddle.fft.fftshift(x=Tensor([16, 96, 20, 11],"complex64"), axes=list[-2,], ) 	 337920 	 220265 	 13.009268522262573 	 2.594748020172119 	 5.0136924360769415 	 12.44894289970398 	 15.0808687210083 	 0.825479163701098 	 
2025-07-17 15:40:01.583609 test begin: paddle.fft.hfft(x=Tensor([3, 2, 2, 3],"complex128"), )
[Prof] paddle.fft.hfft 	 paddle.fft.hfft(x=Tensor([3, 2, 2, 3],"complex128"), ) 	 36 	 178823 	 13.932562351226807 	 7.904677152633667 	 1.762571966216834 	 17.59253239631653 	 19.015044450759888 	 0.9251901799058633 	 
2025-07-17 15:41:00.039528 test begin: paddle.fft.hfft(x=Tensor([3, 2, 2, 3],"complex128"), n=2, )
[Prof] paddle.fft.hfft 	 paddle.fft.hfft(x=Tensor([3, 2, 2, 3],"complex128"), n=2, ) 	 36 	 178823 	 15.526288032531738 	 9.486960411071777 	 1.6365924763859825 	 20.720686674118042 	 23.935304880142212 	 0.8656955396172472 	 
2025-07-17 15:42:10.429857 test begin: paddle.fft.hfft(x=Tensor([3, 2, 2, 3],"complex128"), n=2, axis=1, )
[Prof] paddle.fft.hfft 	 paddle.fft.hfft(x=Tensor([3, 2, 2, 3],"complex128"), n=2, axis=1, ) 	 36 	 178823 	 10.947957038879395 	 11.559757709503174 	 0.9470749572786613 	 17.98412799835205 	 18.57668662071228 	 0.9681020283940437 	 
2025-07-17 15:43:10.112816 test begin: paddle.fft.hfft2(x=Tensor([3, 3, 3, 3],"complex128"), )
[Prof] paddle.fft.hfft2 	 paddle.fft.hfft2(x=Tensor([3, 3, 3, 3],"complex128"), ) 	 81 	 119158 	 9.605794191360474 	 8.36796236038208 	 1.147925119362257 	 11.746299743652344 	 19.56172490119934 	 0.6004736189154859 	 
2025-07-17 15:43:59.892375 test begin: paddle.fft.hfft2(x=Tensor([3, 4, 5],"complex128"), )
[Prof] paddle.fft.hfft2 	 paddle.fft.hfft2(x=Tensor([3, 4, 5],"complex128"), ) 	 60 	 119158 	 9.59228801727295 	 8.35264539718628 	 1.148413174645755 	 10.72347640991211 	 19.161824703216553 	 0.5596271010720623 	 
2025-07-17 15:44:47.733386 test begin: paddle.fft.hfft2(x=Tensor([3, 4],"complex128"), )
[Prof] paddle.fft.hfft2 	 paddle.fft.hfft2(x=Tensor([3, 4],"complex128"), ) 	 12 	 119158 	 10.542290449142456 	 7.117215633392334 	 1.481238027927728 	 14.335388422012329 	 17.63511300086975 	 0.8128889461215995 	 
2025-07-17 15:45:37.885363 test begin: paddle.fft.hfftn(Tensor([3, 3, 3, 3],"complex128"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.hfftn 	 paddle.fft.hfftn(Tensor([3, 3, 3, 3],"complex128"), None, tuple(-2,-1,), "backward", None, ) 	 81 	 149664 	 11.552844047546387 	 11.033209562301636 	 1.0470973094737765 	 13.727186441421509 	 24.268770694732666 	 0.5656317171599005 	 
2025-07-17 15:46:38.764191 test begin: paddle.fft.hfftn(Tensor([3, 4, 5],"complex128"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.hfftn 	 paddle.fft.hfftn(Tensor([3, 4, 5],"complex128"), None, tuple(-2,-1,), "backward", None, ) 	 60 	 149664 	 12.65919280052185 	 10.784031391143799 	 1.173883155692406 	 13.401964902877808 	 24.022976636886597 	 0.557881111298234 	 
2025-07-17 15:47:39.920542 test begin: paddle.fft.hfftn(x=Tensor([3, 4, 5],"complex128"), )
[Prof] paddle.fft.hfftn 	 paddle.fft.hfftn(x=Tensor([3, 4, 5],"complex128"), ) 	 60 	 149664 	 10.554742097854614 	 9.248857736587524 	 1.1411941234755019 	 16.77885413169861 	 22.46227526664734 	 0.7469792766992022 	 
2025-07-17 15:48:39.696798 test begin: paddle.fft.ifft(x=Tensor([7],"complex128"), )
[Prof] paddle.fft.ifft 	 paddle.fft.ifft(x=Tensor([7],"complex128"), ) 	 7 	 144249 	 7.985332012176514 	 5.452521800994873 	 1.4645208774258365 	 13.49435567855835 	 12.278290748596191 	 1.099041874383142 	 
2025-07-17 15:49:18.919483 test begin: paddle.fft.ifft(x=Tensor([7],"complex128"), axis=0, )
[Prof] paddle.fft.ifft 	 paddle.fft.ifft(x=Tensor([7],"complex128"), axis=0, ) 	 7 	 144249 	 8.241267681121826 	 3.8814282417297363 	 2.1232564839197314 	 11.53820276260376 	 12.119735479354858 	 0.9520177055232187 	 
2025-07-17 15:49:55.118037 test begin: paddle.fft.ifft(x=Tensor([7],"complex128"), n=1, )
[Prof] paddle.fft.ifft 	 paddle.fft.ifft(x=Tensor([7],"complex128"), n=1, ) 	 7 	 144249 	 10.034729957580566 	 3.037637948989868 	 3.303464772988335 	 15.570576906204224 	 15.303066968917847 	 1.0174808055032183 	 
2025-07-17 15:50:40.047681 test begin: paddle.fft.ifft2(x=Tensor([2, 2, 4],"complex128"), )
[Prof] paddle.fft.ifft2 	 paddle.fft.ifft2(x=Tensor([2, 2, 4],"complex128"), ) 	 16 	 77185 	 7.418696880340576 	 2.446629047393799 	 3.032211559918791 	 6.6465020179748535 	 6.984932899475098 	 0.9515484420006846 	 
2025-07-17 15:51:03.559739 test begin: paddle.fft.ifft2(x=Tensor([3, 2, 4, 2],"complex128"), )
[Prof] paddle.fft.ifft2 	 paddle.fft.ifft2(x=Tensor([3, 2, 4, 2],"complex128"), ) 	 48 	 77185 	 7.699981689453125 	 2.4111380577087402 	 3.193505102221428 	 7.064908027648926 	 6.950601100921631 	 1.0164456174462577 	 
2025-07-17 15:51:27.694946 test begin: paddle.fft.ifft2(x=Tensor([3, 3, 2, 2],"complex128"), s=tuple(1,2,), )
[Prof] paddle.fft.ifft2 	 paddle.fft.ifft2(x=Tensor([3, 3, 2, 2],"complex128"), s=tuple(1,2,), ) 	 36 	 77185 	 10.043609380722046 	 2.8833861351013184 	 3.4832689449583993 	 9.857961654663086 	 9.455499172210693 	 1.0425638535969854 	 
2025-07-17 15:52:00.305301 test begin: paddle.fft.ifftn(Tensor([8, 32, 481, 481],"complex64"), axes=list[2,3,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb6bdd3b250>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 16:02:05.464814 test begin: paddle.fft.ifftn(x=Tensor([4, 4, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )
W0717 16:02:05.665907 27835 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuFFT, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/native/cuda/SpectralOps.cpp:269.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([4, 4, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 192 	 37347 	 8.22244906425476 	 2.442636251449585 	 3.366219206554042 	 5.166370630264282 	 5.047879934310913 	 1.0234733586169467 	 
2025-07-17 16:02:27.086372 test begin: paddle.fft.ifftn(x=Tensor([50, 8, 39, 14, 14],"complex64"), s=tuple(39,14,14,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([50, 8, 39, 14, 14],"complex64"), s=tuple(39,14,14,), ) 	 3057600 	 37347 	 10.047269821166992 	 6.406649827957153 	 1.5682564352624684 	 10.006979703903198 	 6.4237449169158936 	 1.5578108771958605 	 
2025-07-17 16:03:00.486253 test begin: paddle.fft.ifftshift(x=Tensor([4, 5, 4, 4],"complex128"), )
[Prof] paddle.fft.ifftshift 	 paddle.fft.ifftshift(x=Tensor([4, 5, 4, 4],"complex128"), ) 	 320 	 139474 	 4.560133934020996 	 4.968376636505127 	 0.9178317723570775 	 7.946159839630127 	 13.603297472000122 	 0.5841348287785245 	 
2025-07-17 16:03:31.573148 test begin: paddle.fft.ifftshift(x=Tensor([4, 5, 4, 4],"complex128"), axes=3, )
[Prof] paddle.fft.ifftshift 	 paddle.fft.ifftshift(x=Tensor([4, 5, 4, 4],"complex128"), axes=3, ) 	 320 	 139474 	 5.775867700576782 	 1.518691062927246 	 3.8031880489531 	 7.88010573387146 	 10.439846992492676 	 0.7548104622163588 	 
2025-07-17 16:03:57.363556 test begin: paddle.fft.ifftshift(x=Tensor([4, 5, 4, 4],"complex128"), axes=tuple(0,3,), )
[Prof] paddle.fft.ifftshift 	 paddle.fft.ifftshift(x=Tensor([4, 5, 4, 4],"complex128"), axes=tuple(0,3,), ) 	 320 	 139474 	 9.730245113372803 	 2.5263853073120117 	 3.85144937520455 	 7.858644723892212 	 9.958964109420776 	 0.7891026252879305 	 
2025-07-17 16:04:27.445945 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 3, 3],"float64"), )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 4, 3, 3],"float64"), ) 	 72 	 147903 	 9.488390445709229 	 6.2702929973602295 	 1.5132291983331252 	 17.622854232788086 	 22.678202629089355 	 0.777083374772533 	 
2025-07-17 16:05:23.913827 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 3, 3],"float64"), n=2, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 4, 3, 3],"float64"), n=2, ) 	 72 	 147903 	 10.464405059814453 	 5.470121383666992 	 1.9130114902129385 	 17.955952644348145 	 20.945732831954956 	 0.8572606548745059 	 
2025-07-17 16:06:18.766724 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 3, 3],"float64"), n=2, axis=1, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 4, 3, 3],"float64"), n=2, axis=1, ) 	 72 	 147903 	 10.39511775970459 	 7.244706392288208 	 1.4348570110128835 	 18.02822780609131 	 22.883448839187622 	 0.7878282654325336 	 
2025-07-17 16:07:17.327649 test begin: paddle.fft.ihfft2(x=Tensor([3, 4, 5],"float64"), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([3, 4, 5],"float64"), ) 	 60 	 106488 	 6.950752258300781 	 7.826882839202881 	 0.8880613650540695 	 11.450078964233398 	 18.87406015396118 	 0.6066569074609164 	 
2025-07-17 16:08:03.593656 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 3, 3],"float64"), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([4, 3, 3, 3],"float64"), ) 	 108 	 106488 	 6.923583507537842 	 7.860445976257324 	 0.8808130643541984 	 10.667647361755371 	 19.333208560943604 	 0.5517784245759314 	 
2025-07-17 16:08:48.389355 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 3],"float64"), s=tuple(1,2,), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([4, 3, 3],"float64"), s=tuple(1,2,), ) 	 36 	 106488 	 10.159181118011475 	 6.4676430225372314 	 1.5707702299911517 	 12.86206865310669 	 19.719696760177612 	 0.6522447484628989 	 
2025-07-17 16:09:37.846801 test begin: paddle.fft.ihfftn(Tensor([3, 4, 5],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([3, 4, 5],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 60 	 105773 	 6.687530517578125 	 7.8822245597839355 	 0.8484318693099301 	 10.588587284088135 	 19.054133892059326 	 0.5557107630329423 	 
2025-07-17 16:10:22.067848 test begin: paddle.fft.ihfftn(Tensor([4, 3, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([4, 3, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 108 	 105773 	 6.611245393753052 	 8.089269876480103 	 0.8172857989292124 	 10.516727924346924 	 19.22669816017151 	 0.5469856465595604 	 
2025-07-17 16:11:06.523285 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 5, 2],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([4, 3, 5, 2],"float64"), ) 	 120 	 105773 	 7.4710307121276855 	 7.231216192245483 	 1.0331637878755957 	 14.349663972854614 	 16.387465238571167 	 0.8756487817945035 	 
2025-07-17 16:11:51.972976 test begin: paddle.fft.irfft(Tensor([20, 64, 1001],"complex64"), n=2001, )
[Prof] paddle.fft.irfft 	 paddle.fft.irfft(Tensor([20, 64, 1001],"complex64"), n=2001, ) 	 1281280 	 142255 	 15.78072738647461 	 13.017791509628296 	 1.212243057879885 	 15.452231884002686 	 15.18918228149414 	 1.017318220140727 	 
2025-07-17 16:12:51.695428 test begin: paddle.fft.irfft(Tensor([20, 64, 1051],"complex64"), n=2101, )
[Prof] paddle.fft.irfft 	 paddle.fft.irfft(Tensor([20, 64, 1051],"complex64"), n=2101, ) 	 1345280 	 142255 	 22.441099643707275 	 19.614140510559082 	 1.144128626570526 	 21.978312730789185 	 18.474770069122314 	 1.1896393107225995 	 
2025-07-17 16:14:15.364298 test begin: paddle.fft.irfft(Tensor([4, 32, 32, 16],"complex64"), n=64, axis=-1, norm="forward", )
[Prof] paddle.fft.irfft 	 paddle.fft.irfft(Tensor([4, 32, 32, 16],"complex64"), n=64, axis=-1, norm="forward", ) 	 65536 	 142255 	 10.420283794403076 	 7.284982204437256 	 1.4303787575563491 	 13.75278615951538 	 15.932841539382935 	 0.863172217304812 	 
2025-07-17 16:15:02.783130 test begin: paddle.fft.irfft2(Tensor([32, 15, 8, 256],"complex64"), s=tuple(15,15,), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.irfft2 	 paddle.fft.irfft2(Tensor([32, 15, 8, 256],"complex64"), s=tuple(15,15,), axes=tuple(1,2,), norm="ortho", ) 	 983040 	 14844 	 1.2079098224639893 	 0.8624660968780518 	 1.4005302084758717 	 1.2232110500335693 	 1.7020926475524902 	 0.7186512742373192 	 
2025-07-17 16:15:07.831308 test begin: paddle.fft.irfft2(Tensor([32, 32, 250, 126],"complex64"), s=tuple(250,250,), )
[Prof] paddle.fft.irfft2 	 paddle.fft.irfft2(Tensor([32, 32, 250, 126],"complex64"), s=tuple(250,250,), ) 	 32256000 	 14844 	 38.76002240180969 	 26.65202260017395 	 1.4542994722493112 	 38.28863501548767 	 26.655275583267212 	 1.4364374097682664 	 
2025-07-17 16:17:20.002546 test begin: paddle.fft.irfft2(Tensor([8, 32, 250, 126],"complex64"), s=tuple(250,250,), )
[Prof] paddle.fft.irfft2 	 paddle.fft.irfft2(Tensor([8, 32, 250, 126],"complex64"), s=tuple(250,250,), ) 	 8064000 	 14844 	 9.994296789169312 	 6.9205567836761475 	 1.4441463456731318 	 9.963494777679443 	 6.9723734855651855 	 1.4289961371556956 	 
2025-07-17 16:17:54.787411 test begin: paddle.fft.irfftn(Tensor([32, 15, 8, 256],"complex64"), tuple(15,15,), tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.irfftn 	 paddle.fft.irfftn(Tensor([32, 15, 8, 256],"complex64"), tuple(15,15,), tuple(1,2,), "ortho", None, ) 	 983040 	 14842 	 1.1851134300231934 	 0.8600800037384033 	 1.3779106883917862 	 1.225886344909668 	 1.6866371631622314 	 0.7268227996419135 	 
2025-07-17 16:17:59.793666 test begin: paddle.fft.irfftn(Tensor([32, 32, 250, 126],"complex64"), tuple(250,250,), tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.irfftn 	 paddle.fft.irfftn(Tensor([32, 32, 250, 126],"complex64"), tuple(250,250,), tuple(-2,-1,), "backward", None, ) 	 32256000 	 14842 	 38.74271321296692 	 26.645668983459473 	 1.4539966415186194 	 38.26315259933472 	 26.641972064971924 	 1.4361982103285056 	 
2025-07-17 16:20:12.061164 test begin: paddle.fft.irfftn(Tensor([8, 32, 250, 126],"complex64"), tuple(250,250,), tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.irfftn 	 paddle.fft.irfftn(Tensor([8, 32, 250, 126],"complex64"), tuple(250,250,), tuple(-2,-1,), "backward", None, ) 	 8064000 	 14842 	 9.991419792175293 	 6.906970977783203 	 1.4465704031931585 	 9.961058139801025 	 6.970577955245972 	 1.4290146676151085 	 
2025-07-17 16:20:46.347433 test begin: paddle.fft.rfft(Tensor([20, 64, 2001],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 64, 2001],"float32"), ) 	 2561280 	 248298 	 22.398752212524414 	 15.621615171432495 	 1.4338307509639197 	 49.383050203323364 	 27.78617787361145 	 1.7772523600744123 	 
2025-07-17 16:22:42.943563 test begin: paddle.fft.rfft(Tensor([20, 64, 2101],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 64, 2101],"float32"), ) 	 2689280 	 248298 	 33.35142111778259 	 27.137548208236694 	 1.2289769459593216 	 68.6655330657959 	 47.69274544715881 	 1.4397479621271518 	 
2025-07-17 16:25:40.885000 test begin: paddle.fft.rfft(Tensor([4, 32, 32, 64],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([4, 32, 32, 64],"float32"), axis=-1, norm="forward", ) 	 262144 	 248298 	 10.307964086532593 	 7.100005626678467 	 1.4518247771241384 	 23.972976684570312 	 28.80298614501953 	 0.832308725347757 	 
2025-07-17 16:26:51.105942 test begin: paddle.fft.rfft2(Tensor([32, 32, 250, 250],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f10e246e5c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 16:36:55.797981 test begin: paddle.fft.rfft2(Tensor([8, 32, 250, 250],"float32"), )
W0717 16:36:56.258721 28719 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([8, 32, 250, 250],"float32"), ) 	 16000000 	 183097 	 86.14780616760254 	 48.523964643478394 	 1.7753661886566554 	 209.41297054290771 	 112.22164463996887 	 1.8660657773707512 	 
2025-07-17 16:44:33.258790 test begin: paddle.fft.rfft2(x=Tensor([32, 15, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([32, 15, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 1843200 	 183097 	 10.199872493743896 	 9.65369987487793 	 1.0565765070330482 	 22.161866664886475 	 24.35044550895691 	 0.9101216097559528 	 
2025-07-17 16:45:40.211970 test begin: paddle.fft.rfftfreq(n=5, d=0.3, )
2025-07-17 16:45:40.215360 test begin: paddle.fft.rfftfreq(n=8, d=0.3, )
2025-07-17 16:45:40.216107 test begin: paddle.fft.rfftfreq(n=8, d=1, )
2025-07-17 16:45:40.216640 test begin: paddle.fft.rfftn(Tensor([32, 15, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 15, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, ) 	 1843200 	 21196 	 1.089390516281128 	 1.1126585006713867 	 0.9790879372455981 	 2.5751190185546875 	 2.887751579284668 	 0.8917384158067283 	 
2025-07-17 16:45:47.948875 test begin: paddle.fft.rfftn(Tensor([32, 32, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 32, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 64000000 	 21196 	 38.48930239677429 	 21.401996612548828 	 1.798397742676331 	 93.7949812412262 	 49.87347888946533 	 1.8806584848252548 	 
2025-07-17 16:49:13.392064 test begin: paddle.fft.rfftn(Tensor([8, 32, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([8, 32, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 16000000 	 21196 	 9.978280067443848 	 5.607218265533447 	 1.7795419394993992 	 24.25044274330139 	 12.983423709869385 	 1.8678003033103936 	 
2025-07-17 16:50:08.355386 test begin: paddle.flatten(Tensor([4096, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4096, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 51380224 	 1786109 	 15.328653573989868 	 7.358607530593872 	 2.0830916053424553 	 75.63375926017761 	 94.41772818565369 	 0.8010546399873005 	 
2025-07-17 16:53:22.860510 test begin: paddle.flatten(Tensor([416, 50, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([416, 50, 7, 256],"float32"), start_axis=2, ) 	 37273600 	 1786109 	 14.515263080596924 	 7.150116682052612 	 2.0300735954465527 	 73.0634298324585 	 111.36145830154419 	 0.6560926100178895 	 
2025-07-17 16:56:50.746130 test begin: paddle.flatten(Tensor([512, 50, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([512, 50, 7, 256],"float32"), start_axis=2, ) 	 45875200 	 1786109 	 19.68851637840271 	 7.215104818344116 	 2.7287914554401818 	 85.50131487846375 	 120.47466325759888 	 0.7097037050491262 	 
2025-07-17 17:00:45.424669 test begin: paddle.flip(Tensor([3, 8, 224, 224],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([3, 8, 224, 224],"float32"), axis=list[3,], ) 	 1204224 	 200116 	 5.364289283752441 	 2.7338945865631104 	 1.962141960453606 	 10.137190580368042 	 16.295629739761353 	 0.6220803210589209 	 
2025-07-17 17:01:20.011674 test begin: paddle.flip(Tensor([52, 3, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([52, 3, 112, 112],"float32"), axis=-1, ) 	 1956864 	 200116 	 8.209608793258667 	 3.4602653980255127 	 2.372537319808825 	 9.920377254486084 	 12.821948528289795 	 0.7737027825839567 	 
2025-07-17 17:01:54.510609 test begin: paddle.flip(Tensor([64, 3, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([64, 3, 112, 112],"float32"), axis=-1, ) 	 2408448 	 200116 	 9.968121528625488 	 2.792130470275879 	 3.570077270651532 	 9.944104671478271 	 13.08238935470581 	 0.7601137989293462 	 
2025-07-17 17:02:30.379567 test begin: paddle.floor(Tensor([100000, 2, 3],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([100000, 2, 3],"float32"), ) 	 600000 	 1094992 	 9.494508743286133 	 11.092076063156128 	 0.8559721993634233 	 54.42484474182129 	 68.2337634563446 	 0.7976233756568601 	 
2025-07-17 17:04:53.764509 test begin: paddle.floor(Tensor([4, 157920],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([4, 157920],"float32"), ) 	 631680 	 1094992 	 9.3856201171875 	 11.156943559646606 	 0.841235779943731 	 53.91356325149536 	 69.71802425384521 	 0.7733088226251883 	 
2025-07-17 17:07:17.964497 test begin: paddle.floor(x=Tensor([100, 4, 38, 38],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([100, 4, 38, 38],"float32"), ) 	 577600 	 1094992 	 9.807981967926025 	 11.180131196975708 	 0.8772689510637535 	 54.61948871612549 	 71.10011053085327 	 0.7682053981114958 	 
2025-07-17 17:09:44.693138 test begin: paddle.floor_divide(Tensor([10, 1024],"int64"), Tensor([10, 1024],"int64"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 1024],"int64"), Tensor([10, 1024],"int64"), ) 	 20480 	 1021779 	 9.20499300956726 	 13.788641691207886 	 0.6675779395614205 	 None 	 None 	 None 	 
2025-07-17 17:10:07.724595 test begin: paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), ) 	 400 	 1021779 	 9.405189752578735 	 11.761423110961914 	 0.7996642637414246 	 None 	 None 	 None 	 
2025-07-17 17:10:29.289247 test begin: paddle.floor_divide(Tensor([128],"int64"), Tensor([],"int64"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([128],"int64"), Tensor([],"int64"), ) 	 129 	 1021779 	 10.149895906448364 	 19.56200408935547 	 0.5188576722551327 	 None 	 None 	 None 	 
2025-07-17 17:10:59.427490 test begin: paddle.floor_mod(Tensor([2, 3, 4],"float32"), Tensor([],"float32"), )
[Prof] paddle.floor_mod 	 paddle.floor_mod(Tensor([2, 3, 4],"float32"), Tensor([],"float32"), ) 	 25 	 1003670 	 16.490639209747314 	 11.619459629058838 	 1.4192259998482422 	 76.73566555976868 	 95.38126611709595 	 0.80451506552202 	 
2025-07-17 17:14:19.798043 test begin: paddle.floor_mod(Tensor([],"float32"), Tensor([2, 3, 4],"float32"), )
[Prof] paddle.floor_mod 	 paddle.floor_mod(Tensor([],"float32"), Tensor([2, 3, 4],"float32"), ) 	 25 	 1003670 	 10.999865293502808 	 11.700096607208252 	 0.940151663938053 	 76.36808252334595 	 105.84940481185913 	 0.7214786201120881 	 
2025-07-17 17:17:44.727011 test begin: paddle.floor_mod(Tensor([],"float32"), Tensor([],"float32"), )
[Prof] paddle.floor_mod 	 paddle.floor_mod(Tensor([],"float32"), Tensor([],"float32"), ) 	 2 	 1003670 	 17.40952444076538 	 11.015096426010132 	 1.5805149376319556 	 61.38928747177124 	 81.3671305179596 	 0.7544728083807898 	 
2025-07-17 17:20:36.241480 test begin: paddle.fmax(Tensor([10, 15],"float32"), Tensor([10, 15],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([10, 15],"float32"), Tensor([10, 15],"float32"), ) 	 300 	 1016307 	 17.609506130218506 	 11.136104345321655 	 1.5812985927719299 	 59.96559524536133 	 146.75994157791138 	 0.40859647803502985 	 
2025-07-17 17:24:32.296365 test begin: paddle.fmax(Tensor([10, 15],"float32"), Tensor([15],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([10, 15],"float32"), Tensor([15],"float32"), ) 	 165 	 1016307 	 11.018031358718872 	 11.799179077148438 	 0.9337964350467042 	 59.1010537147522 	 164.74711561203003 	 0.35873801793247645 	 
2025-07-17 17:28:39.370175 test begin: paddle.fmax(Tensor([30, 200, 40],"float32"), Tensor([30, 200, 40],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([30, 200, 40],"float32"), Tensor([30, 200, 40],"float32"), ) 	 480000 	 1016307 	 10.123447895050049 	 11.033290386199951 	 0.9175366133490059 	 58.04471158981323 	 147.49711966514587 	 0.39353115316142284 	 
2025-07-17 17:32:26.088435 test begin: paddle.fmin(Tensor([10, 15],"float32"), Tensor([10, 15],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([10, 15],"float32"), Tensor([10, 15],"float32"), ) 	 300 	 994715 	 15.833679676055908 	 17.377069234848022 	 0.9111824014778628 	 65.8723533153534 	 144.59473276138306 	 0.4555653726616654 	 
2025-07-17 17:36:30.071612 test begin: paddle.fmin(Tensor([10, 15],"float32"), Tensor([15],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([10, 15],"float32"), Tensor([15],"float32"), ) 	 165 	 994715 	 13.90234088897705 	 11.619668006896973 	 1.1964490621182269 	 57.50991487503052 	 162.7031388282776 	 0.35346530674941945 	 
2025-07-17 17:40:36.264631 test begin: paddle.fmin(Tensor([30, 200, 40],"float32"), Tensor([30, 200, 40],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([30, 200, 40],"float32"), Tensor([30, 200, 40],"float32"), ) 	 480000 	 994715 	 9.558152914047241 	 10.960209131240845 	 0.8720776035926906 	 57.45115876197815 	 144.3511312007904 	 0.39799590265811213 	 
2025-07-17 17:44:18.614366 test begin: paddle.frac(Tensor([10, 20, 1],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([10, 20, 1],"float32"), ) 	 200 	 441349 	 12.241860628128052 	 7.136173963546753 	 1.7154655548845 	 39.92192029953003 	 22.987278938293457 	 1.7366962138796658 	 
2025-07-17 17:45:40.916180 test begin: paddle.frac(Tensor([2, 3],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2, 3],"float32"), ) 	 6 	 441349 	 9.738870859146118 	 4.467592716217041 	 2.179892277063376 	 32.303972244262695 	 25.385316371917725 	 1.2725455838714175 	 
2025-07-17 17:46:52.819010 test begin: paddle.frac(Tensor([2, 3],"float64"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2, 3],"float64"), ) 	 6 	 441349 	 10.045185565948486 	 4.5099687576293945 	 2.2273293022164093 	 32.310423374176025 	 24.02860188484192 	 1.344665142359305 	 
2025-07-17 17:48:03.721733 test begin: paddle.full(list[Tensor([],"int32"),Tensor([],"int32"),Tensor([],"int32"),], 0.5, )
[Prof] paddle.full 	 paddle.full(list[Tensor([],"int32"),Tensor([],"int32"),Tensor([],"int32"),], 0.5, ) 	 3 	 63250 	 4.579261779785156 	 0.4583883285522461 	 9.989917924498862 	 None 	 None 	 None 	 
2025-07-17 17:48:08.765740 test begin: paddle.full(shape=Tensor([2],"int32"), fill_value=Tensor([1],"float32"), )
[Prof] paddle.full 	 paddle.full(shape=Tensor([2],"int32"), fill_value=Tensor([1],"float32"), ) 	 3 	 63250 	 3.7132985591888428 	 0.8437924385070801 	 4.400725095094207 	 None 	 None 	 None 	 
2025-07-17 17:48:13.693626 test begin: paddle.full(shape=Tensor([5],"int64"), fill_value=-2, )
[Prof] paddle.full 	 paddle.full(shape=Tensor([5],"int64"), fill_value=-2, ) 	 5 	 63250 	 3.496077299118042 	 1.7698559761047363 	 1.9753456475099935 	 None 	 None 	 None 	 
2025-07-17 17:48:18.964207 test begin: paddle.full_like(Tensor([1, 1, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([1, 1, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), ) 	 4194304 	 946534 	 11.216802835464478 	 11.59896731376648 	 0.967051853155201 	 None 	 None 	 None 	 
2025-07-17 17:48:41.846152 test begin: paddle.full_like(Tensor([1, 300, 4096],"float32"), 1, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([1, 300, 4096],"float32"), 1, ) 	 1228800 	 946534 	 9.982597827911377 	 11.226764678955078 	 0.8891785045271385 	 None 	 None 	 None 	 
2025-07-17 17:49:03.080042 test begin: paddle.full_like(Tensor([6, 256000],"float32"), 0.0, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([6, 256000],"float32"), 0.0, ) 	 1536000 	 946534 	 9.757080793380737 	 11.537574529647827 	 0.8456786795446652 	 None 	 None 	 None 	 
2025-07-17 17:49:24.412327 test begin: paddle.gammainc(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), ) 	 240 	 46705 	 9.930278301239014 	 0.5158712863922119 	 19.24952708782694 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-07-17 17:49:39.443239 test begin: paddle.gammainc(Tensor([2, 3, 4, 5],"float64"), Tensor([2, 3, 4, 5],"float64"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 3, 4, 5],"float64"), Tensor([2, 3, 4, 5],"float64"), ) 	 240 	 46705 	 11.635950803756714 	 0.8227927684783936 	 14.142018804170217 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-07-17 17:49:55.610462 test begin: paddle.gammainc(Tensor([3, 40],"float32"), y=Tensor([3, 40],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([3, 40],"float32"), y=Tensor([3, 40],"float32"), ) 	 240 	 46705 	 9.874944925308228 	 0.8158643245697021 	 12.103660655239958 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-07-17 17:50:10.030768 test begin: paddle.gammaincc(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), ) 	 240 	 47579 	 9.231759786605835 	 0.5770730972290039 	 15.997557035555813 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-07-17 17:50:22.558890 test begin: paddle.gammaincc(Tensor([2, 3, 4, 5],"float64"), Tensor([2, 3, 4, 5],"float64"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 3, 4, 5],"float64"), Tensor([2, 3, 4, 5],"float64"), ) 	 240 	 47579 	 9.245412588119507 	 1.069106101989746 	 8.647797043635412 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-07-17 17:50:36.335938 test begin: paddle.gammaincc(Tensor([3, 40],"float32"), Tensor([3, 40],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([3, 40],"float32"), Tensor([3, 40],"float32"), ) 	 240 	 47579 	 9.189050674438477 	 0.5652782917022705 	 16.255799681899525 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-07-17 17:50:48.556354 test begin: paddle.gather(Tensor([2048, 2048, 7, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 7],"float32"), Tensor([482, 1],"int64"), ) 	 205521378 	 32707 	 9.3954017162323 	 515.4805464744568 	 0.018226491339955665 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 18:00:19.889809 test begin: paddle.gather(Tensor([2048, 2048, 7, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 7],"float32"), Tensor([496, 1],"int64"), ) 	 205521392 	 32707 	 9.659730672836304 	 540.5525422096252 	 0.017870104973237325 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 18:10:17.865107 test begin: paddle.gather(Tensor([2048, 2048, 7, 7],"float32"), Tensor([512, 1],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcc6fd736a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 18:20:22.739094 test begin: paddle.gather_nd(Tensor([1, 8192, 7168],"bfloat16"), Tensor([7780, 2],"int64"), )
W0717 18:20:23.861040 33858 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1bb226ab60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 18:30:27.500420 test begin: paddle.gather_nd(Tensor([1, 8192, 7168],"bfloat16"), Tensor([8162, 2],"int64"), )
W0717 18:30:28.589063 34170 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f04e7012b90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 18:40:31.848952 test begin: paddle.gather_nd(Tensor([20, 41344, 128],"float32"), index=Tensor([20, 500, 2],"int64"), )
W0717 18:40:33.757148 34568 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3c7ec12c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 18:50:38.086263 test begin: paddle.gcd(Tensor([10, 20],"int32"), Tensor([10, 20],"int32"), )
W0717 18:50:41.250138 34894 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0717 18:58:54.444217 34894 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-17 18:58:54.445487 test begin: paddle.gcd(x=Tensor([6, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
W0717 19:07:05.228631 35428 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-17 19:07:05.229956 test begin: paddle.gcd(x=Tensor([6, 2, 4, 5],"int32"), y=Tensor([6, 2, 4, 5],"int32"), )
W0717 19:15:30.414566 35730 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-17 19:15:30.415701 test begin: paddle.geometric.segment_max(Tensor([40, 20],"float16"), Tensor([40],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff35f96a950>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 19:25:35.020540 test begin: paddle.geometric.segment_max(Tensor([40, 20],"float32"), Tensor([40],"int64"), )
W0717 19:25:40.039099 114297 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa5a09baa70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 19:35:43.055616 test begin: paddle.geometric.segment_max(Tensor([40, 20],"float64"), Tensor([40],"int64"), )
W0717 19:35:43.343370 114722 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5643f06aa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 19:45:47.624712 test begin: paddle.geometric.segment_mean(Tensor([40, 20],"float16"), Tensor([40],"int64"), )
W0717 19:45:47.828472 115491 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([40, 20],"float16"), Tensor([40],"int64"), ) 	 840 	 246654 	 12.372079372406006 	 71.53503751754761 	 0.17295132290062928 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:47:35.019085 test begin: paddle.geometric.segment_mean(Tensor([40, 20],"float32"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([40, 20],"float32"), Tensor([40],"int64"), ) 	 840 	 246654 	 9.42315411567688 	 67.24712443351746 	 0.14012724254094902 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:49:11.283576 test begin: paddle.geometric.segment_mean(Tensor([40, 20],"float64"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([40, 20],"float64"), Tensor([40],"int64"), ) 	 840 	 246654 	 9.78713059425354 	 64.70350503921509 	 0.15126121202123158 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:50:45.179122 test begin: paddle.geometric.segment_min(Tensor([40, 20],"float16"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 20],"float16"), Tensor([40],"int64"), ) 	 840 	 297097 	 10.24342656135559 	 38.448734283447266 	 0.2664177833746151 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:51:52.399561 test begin: paddle.geometric.segment_min(Tensor([40, 20],"float32"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 20],"float32"), Tensor([40],"int64"), ) 	 840 	 297097 	 9.20474362373352 	 36.64219284057617 	 0.2512061345177065 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:52:57.027976 test begin: paddle.geometric.segment_min(Tensor([40, 20],"float64"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 20],"float64"), Tensor([40],"int64"), ) 	 840 	 297097 	 9.757652997970581 	 36.38862228393555 	 0.2681512073151031 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:54:03.998891 test begin: paddle.geometric.segment_sum(Tensor([30, 15],"float16"), Tensor([30],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([30, 15],"float16"), Tensor([30],"int64"), ) 	 480 	 297265 	 9.250060796737671 	 11.065587282180786 	 0.8359303994315145 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:54:43.531219 test begin: paddle.geometric.segment_sum(Tensor([30, 15],"float32"), Tensor([30],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([30, 15],"float32"), Tensor([30],"int64"), ) 	 480 	 297265 	 9.2109055519104 	 10.995165348052979 	 0.8377232411099179 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:55:23.014389 test begin: paddle.geometric.segment_sum(Tensor([40, 20],"float32"), Tensor([40],"int32"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([40, 20],"float32"), Tensor([40],"int32"), ) 	 840 	 297265 	 9.096798419952393 	 11.08412766456604 	 0.8207049481244445 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 19:56:02.662501 test begin: paddle.geometric.send_u_recv(Tensor([10, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4127f0a440>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 20:06:07.562033 test begin: paddle.geometric.send_u_recv(Tensor([10, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, )
W0717 20:06:07.776175 130142 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f76f77cab00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 20:16:12.207774 test begin: paddle.geometric.send_u_recv(Tensor([10, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, )
W0717 20:16:12.396045 140398 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd18b126a40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-17 20:26:19.327345 test begin: paddle.geometric.send_ue_recv(Tensor([10, 20],"float64"), Tensor([15, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, )
W0717 20:26:19.853035 152331 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 20],"float64"), Tensor([15, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, ) 	 530 	 102398 	 3.9771695137023926 	 161.24088311195374 	 0.024666011726944836 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 20:29:13.313343 test begin: paddle.geometric.send_ue_recv(Tensor([10, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, ) 	 550 	 102398 	 10.37498688697815 	 117.18169665336609 	 0.08853760598524432 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 20:31:37.080277 test begin: paddle.geometric.send_ue_recv(Tensor([10, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, ) 	 550 	 102398 	 10.026127576828003 	 131.3819079399109 	 0.07631284804764424 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 20:34:21.193774 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 2130 	 108809 	 9.864226341247559 	 1.2630188465118408 	 7.810038914692538 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 20:34:43.544377 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", ) 	 2130 	 108809 	 9.839518547058105 	 1.2433433532714844 	 7.913758111279856 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 20:35:15.161757 test begin: paddle.geometric.send_uv(Tensor([100, 20],"float64"), Tensor([100, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 20],"float64"), Tensor([100, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 2130 	 108809 	 9.846107959747314 	 1.2409722805023193 	 7.934188470158107 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 20:35:37.376905 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 	 4992 	 960158 	 9.231672525405884 	 10.996891260147095 	 0.8394802046339777 	 None 	 None 	 None 	 
2025-07-17 20:35:57.626438 test begin: paddle.greater_equal(Tensor([5, 10, 15, 20],"float32"), Tensor([5, 10, 15, 20],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([5, 10, 15, 20],"float32"), Tensor([5, 10, 15, 20],"float32"), ) 	 30000 	 960158 	 8.78165078163147 	 10.298809051513672 	 0.8526860472610455 	 None 	 None 	 None 	 
2025-07-17 20:36:16.712139 test begin: paddle.greater_equal(Tensor([8, 1024, 1, 1],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([8, 1024, 1, 1],"float32"), Tensor([1],"float32"), ) 	 8193 	 960158 	 9.586532354354858 	 10.874448776245117 	 0.8815649005857041 	 None 	 None 	 None 	 
2025-07-17 20:36:37.178537 test begin: paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), ) 	 400 	 599378 	 5.572182655334473 	 6.232566595077515 	 0.8940430190886988 	 None 	 None 	 None 	 
2025-07-17 20:36:48.988417 test begin: paddle.greater_than(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 2],"float32"), )
W0717 20:36:48.990257 16705 dygraph_functions.cc:90428] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 2],"float32"), ) 	 48 	 599378 	 10.108248233795166 	 6.45014762878418 	 1.56713439994559 	 None 	 None 	 None 	 
2025-07-17 20:37:05.551869 test begin: paddle.greater_than(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 2],"float64"), ) 	 48 	 599378 	 10.021718263626099 	 6.465734004974365 	 1.5499737935269164 	 None 	 None 	 None 	 
2025-07-17 20:37:22.049583 test begin: paddle.heaviside(Tensor([300, 2048],"float32"), Tensor([1],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([300, 2048],"float32"), Tensor([1],"float32"), ) 	 614401 	 1030080 	 10.337614059448242 	 12.609533786773682 	 0.8198252397159609 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-07-17 20:40:15.535462 test begin: paddle.heaviside(Tensor([300, 2048],"float32"), Tensor([2048],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([300, 2048],"float32"), Tensor([2048],"float32"), ) 	 616448 	 1030080 	 10.46615219116211 	 12.830734252929688 	 0.8157095287646796 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-07-17 20:41:41.355664 test begin: paddle.heaviside(Tensor([300, 2048],"float32"), Tensor([300, 2048],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([300, 2048],"float32"), Tensor([300, 2048],"float32"), ) 	 1228800 	 1030080 	 9.482253789901733 	 12.051502227783203 	 0.7868109394728904 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-07-17 20:43:02.259867 test begin: paddle.histogram(input=Tensor([4, 4],"int64"), )
[Prof] paddle.histogram 	 paddle.histogram(input=Tensor([4, 4],"int64"), ) 	 16 	 229242 	 9.960460186004639 	 18.68159055709839 	 0.5331698152553714 	 None 	 None 	 None 	 
2025-07-17 20:43:30.929953 test begin: paddle.histogram_bin_edges(Tensor([5, 20],"float32"), bins=10, min=0, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([5, 20],"float32"), bins=10, min=0, max=1, ) 	 100 	 99952 	 9.965403318405151 	 1.598506212234497 	 6.234197428907614 	 None 	 None 	 None 	 combined
2025-07-17 20:43:42.501616 test begin: paddle.histogram_bin_edges(Tensor([5, 20],"float32"), bins=10, min=0.2, max=0.9, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([5, 20],"float32"), bins=10, min=0.2, max=0.9, ) 	 100 	 99952 	 10.054550409317017 	 1.6557083129882812 	 6.072658046374247 	 None 	 None 	 None 	 combined
2025-07-17 20:43:54.215771 test begin: paddle.histogram_bin_edges(Tensor([5, 20],"float32"), bins=10, min=1, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([5, 20],"float32"), bins=10, min=1, max=1, ) 	 100 	 99952 	 10.275673151016235 	 1.68839430809021 	 6.086062421425322 	 None 	 None 	 None 	 combined
2025-07-17 20:44:06.183701 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=5, weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )
/usr/local/lib/python3.10/dist-packages/paddle/tensor/linalg.py:5741: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  edge = paddle.to_tensor(edge)
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=5, weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 	 24 	 3731 	 6.7507641315460205 	 0.20778512954711914 	 32.48915909555096 	 None 	 None 	 None 	 
2025-07-17 20:44:13.229361 test begin: paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=False, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=False, ) 	 40 	 3731 	 9.497043132781982 	 0.2906758785247803 	 32.672278074743495 	 None 	 None 	 None 	 
2025-07-17 20:44:23.028470 test begin: paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 	 40 	 3731 	 9.79318904876709 	 0.40636324882507324 	 24.099593349256722 	 None 	 None 	 None 	 
2025-07-17 20:44:33.236774 test begin: paddle.hsplit(Tensor([4, 6, 3],"int64"), list[-1,1,3,], )
W0717 20:44:36.381058 77123 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-17 20:44:36.381758 test begin: paddle.hsplit(Tensor([4, 6, 3],"int64"), list[-1,], )
W0717 20:44:38.126232 77272 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-17 20:44:38.126674 test begin: paddle.hsplit(Tensor([4, 6, 3],"int64"), list[2,4,], )
W0717 20:44:40.699833 77277 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-17 20:44:40.700241 test begin: paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 360 	 571062 	 10.126017332077026 	 8.365280866622925 	 1.2104814522701033 	 40.03548765182495 	 39.59756660461426 	 1.0110592918899128 	 
2025-07-17 20:46:18.870517 test begin: paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),], ) 	 120 	 571062 	 8.317835330963135 	 10.69968843460083 	 0.7773904241982207 	 33.50403547286987 	 31.716225624084473 	 1.0563689346259344 	 
2025-07-17 20:47:43.114522 test begin: paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 72 	 571062 	 10.06105351448059 	 7.773065090179443 	 1.294348290893873 	 40.255032539367676 	 40.049947023391724 	 1.0051207437517 	 
2025-07-17 20:49:21.260482 test begin: paddle.hypot(Tensor([10, 10],"float32"), Tensor([10, 1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([10, 10],"float32"), Tensor([10, 1],"float32"), ) 	 110 	 269269 	 10.094558000564575 	 3.152601480484009 	 3.201977180767799 	 29.131162643432617 	 27.78419327735901 	 1.0484797004047346 	 
2025-07-17 20:50:31.475414 test begin: paddle.hypot(Tensor([10, 20],"float32"), Tensor([10, 20],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([10, 20],"float32"), Tensor([10, 20],"float32"), ) 	 400 	 269269 	 9.828834056854248 	 3.0580217838287354 	 3.2141151213606634 	 28.801588535308838 	 24.512863397598267 	 1.1749581461842102 	 
2025-07-17 20:51:37.683908 test begin: paddle.hypot(Tensor([6],"float32"), Tensor([1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([6],"float32"), Tensor([1],"float32"), ) 	 7 	 269269 	 9.902636766433716 	 3.0938072204589844 	 3.200793087865572 	 29.279794931411743 	 27.299261808395386 	 1.072548962565987 	 
2025-07-17 20:52:47.266018 test begin: paddle.i0(Tensor([10, 20, 1],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([10, 20, 1],"float32"), ) 	 200 	 1131975 	 10.407872676849365 	 11.736031532287598 	 0.8868306674377735 	 60.164721965789795 	 87.43755054473877 	 0.6880879163581508 	 
2025-07-17 20:55:37.025905 test begin: paddle.i0(Tensor([513],"float64"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([513],"float64"), ) 	 513 	 1131975 	 9.624834775924683 	 11.235098123550415 	 0.8566756311410949 	 59.55842185020447 	 84.33148503303528 	 0.70624182447248 	 
2025-07-17 20:58:21.784901 test begin: paddle.i0(Tensor([9],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([9],"float32"), ) 	 9 	 1131975 	 9.809255123138428 	 11.184109926223755 	 0.8770707001134118 	 59.1917610168457 	 84.27668523788452 	 0.702350369497417 	 
2025-07-17 21:01:06.252550 test begin: paddle.i0e(Tensor([6],"float64"), )
[Prof] paddle.i0e 	 paddle.i0e(Tensor([6],"float64"), ) 	 6 	 1129536 	 9.73967170715332 	 11.152395486831665 	 0.8733255307035662 	 59.83788847923279 	 110.82244920730591 	 0.5399437470227648 	 
2025-07-17 21:04:17.830925 test begin: paddle.i0e(Tensor([9],"float32"), )
[Prof] paddle.i0e 	 paddle.i0e(Tensor([9],"float32"), ) 	 9 	 1129536 	 10.052526950836182 	 10.974119424819946 	 0.9160212825915292 	 60.633992433547974 	 110.58276152610779 	 0.5483132415646242 	 
2025-07-17 21:07:30.096143 test begin: paddle.i0e(Tensor([9],"float64"), )
[Prof] paddle.i0e 	 paddle.i0e(Tensor([9],"float64"), ) 	 9 	 1129536 	 9.81108283996582 	 11.059213876724243 	 0.8871410707242673 	 60.335179805755615 	 110.02916765213013 	 0.5483562321993767 	 
2025-07-17 21:10:41.337130 test begin: paddle.i1(Tensor([6],"float64"), )
[Prof] paddle.i1 	 paddle.i1(Tensor([6],"float64"), ) 	 6 	 1154758 	 9.896570444107056 	 11.360933780670166 	 0.871105371720886 	 61.04407000541687 	 186.99381113052368 	 0.3264496810688962 	 
2025-07-17 21:15:10.681131 test begin: paddle.i1(Tensor([9],"float32"), )
[Prof] paddle.i1 	 paddle.i1(Tensor([9],"float32"), ) 	 9 	 1154758 	 10.259975671768188 	 11.510464668273926 	 0.8913606850336429 	 62.14101243019104 	 185.78015089035034 	 0.3344868229053566 	 
2025-07-17 21:19:40.380783 test begin: paddle.i1(Tensor([9],"float64"), )
[Prof] paddle.i1 	 paddle.i1(Tensor([9],"float64"), ) 	 9 	 1154758 	 10.865710973739624 	 11.577841758728027 	 0.9384919227755415 	 63.07432246208191 	 186.62495279312134 	 0.33797368207241535 	 
2025-07-17 21:24:12.530329 test begin: paddle.i1e(Tensor([6],"float64"), )
[Prof] paddle.i1e 	 paddle.i1e(Tensor([6],"float64"), ) 	 6 	 1162422 	 9.825814723968506 	 11.429771184921265 	 0.8596685414779974 	 62.98637127876282 	 207.59646463394165 	 0.30340772608930383 	 
2025-07-17 21:29:04.377130 test begin: paddle.i1e(Tensor([9],"float32"), )
[Prof] paddle.i1e 	 paddle.i1e(Tensor([9],"float32"), ) 	 9 	 1162422 	 10.315934419631958 	 11.33328652381897 	 0.9102332671066896 	 62.843838691711426 	 206.27833223342896 	 0.3046555496706072 	 
2025-07-17 21:33:55.155225 test begin: paddle.i1e(Tensor([9],"float64"), )
[Prof] paddle.i1e 	 paddle.i1e(Tensor([9],"float64"), ) 	 9 	 1162422 	 9.967646360397339 	 11.50877046585083 	 0.8660913335593614 	 62.59231877326965 	 207.39365100860596 	 0.3018044114121523 	 
2025-07-17 21:38:46.624024 test begin: paddle.imag(Tensor([10, 10, 10, 20],"complex128"), )
[Prof] paddle.imag 	 paddle.imag(Tensor([10, 10, 10, 20],"complex128"), ) 	 20000 	 3253764 	 10.07566523551941 	 21.172942399978638 	 0.47587458772520796 	 187.9908058643341 	 296.87017583847046 	 0.6332424782428178 	 
2025-07-17 21:47:22.760892 test begin: paddle.imag(Tensor([2, 20, 2, 3],"complex128"), )
[Prof] paddle.imag 	 paddle.imag(Tensor([2, 20, 2, 3],"complex128"), ) 	 240 	 3253764 	 9.99050521850586 	 22.21849775314331 	 0.4496480963521702 	 187.25838160514832 	 297.11223554611206 	 0.6302614271706278 	 
2025-07-17 21:55:59.346925 test begin: paddle.imag(Tensor([2, 20, 2, 3],"complex64"), )
[Prof] paddle.imag 	 paddle.imag(Tensor([2, 20, 2, 3],"complex64"), ) 	 240 	 3253764 	 9.95727801322937 	 21.564491271972656 	 0.46174416487027603 	 188.55275058746338 	 296.8208692073822 	 0.6352408814480148 	 
2025-07-17 22:04:36.249023 test begin: paddle.increment(Tensor([1],"float32"), value=2.0, )
[Prof] paddle.increment 	 paddle.increment(Tensor([1],"float32"), value=2.0, ) 	 1 	 1511227 	 10.529881000518799 	 19.633503913879395 	 0.536322046574426 	 None 	 None 	 None 	 combined
2025-07-17 22:05:06.417136 test begin: paddle.increment(Tensor([1],"int64"), )
[Prof] paddle.increment 	 paddle.increment(Tensor([1],"int64"), ) 	 1 	 1511227 	 10.0352942943573 	 19.340368270874023 	 0.518878138916834 	 None 	 None 	 None 	 combined
2025-07-17 22:05:35.797168 test begin: paddle.increment(Tensor([],"float32"), 1.0, )
[Prof] paddle.increment 	 paddle.increment(Tensor([],"float32"), 1.0, ) 	 1 	 1511227 	 10.110523462295532 	 19.742798566818237 	 0.5121119697431503 	 None 	 None 	 None 	 combined
2025-07-17 22:06:05.654495 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, ) 	 3500 	 755928 	 9.968829870223999 	 17.220445156097412 	 0.5788950157710783 	 56.58874487876892 	 84.4418957233429 	 0.6701501001845187 	 
2025-07-17 22:08:54.021089 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, ) 	 3500 	 755928 	 10.028784990310669 	 12.981805562973022 	 0.772526205361986 	 57.56445670127869 	 76.48563885688782 	 0.7526178451485183 	 
2025-07-17 22:11:31.088966 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([40, 50],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([40, 50],"float32"), None, True, True, ) 	 3500 	 755928 	 11.163557052612305 	 20.23841381072998 	 0.5516023714612269 	 57.08822202682495 	 92.13373231887817 	 0.6196234602679566 	 
2025-07-17 22:14:31.721997 test begin: paddle.incubate.nn.functional.swiglu(Tensor([106496, 7168],"bfloat16"), )
W0717 22:14:52.561043 162298 backward.cc:462] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-17 22:14:57.223402 test begin: paddle.incubate.nn.functional.swiglu(Tensor([108544, 7168],"bfloat16"), )
W0717 22:15:15.681162   825 backward.cc:462] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-17 22:15:15.983541 test begin: paddle.incubate.nn.functional.swiglu(Tensor([111616, 7168],"bfloat16"), )
W0717 22:15:35.703125  2765 backward.cc:462] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-17 22:15:37.400670 test begin: paddle.incubate.segment_max(Tensor([3, 3],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/PaddleAPITest/tester/paddle_torch_gpu_performance.py:121: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_max" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_max" instead.
    Reason: paddle.incubate.segment_max will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/PaddleAPITest/tester/paddle_torch_gpu_performance.py:137: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_max" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_max" instead.
    Reason: paddle.incubate.segment_max will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_max 	 paddle.incubate.segment_max(Tensor([3, 3],"float32"), Tensor([3],"int32"), ) 	 12 	 264018 	 10.23336935043335 	 32.731950998306274 	 0.31264159447638423 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 22:16:37.128268 test begin: paddle.incubate.segment_mean(Tensor([3, 3],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/PaddleAPITest/tester/paddle_torch_gpu_performance.py:121: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_mean" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_mean" instead.
    Reason: paddle.incubate.segment_mean will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/PaddleAPITest/tester/paddle_torch_gpu_performance.py:137: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_mean" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_mean" instead.
    Reason: paddle.incubate.segment_mean will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_mean 	 paddle.incubate.segment_mean(Tensor([3, 3],"float32"), Tensor([3],"int32"), ) 	 12 	 225392 	 10.202579259872437 	 61.35479664802551 	 0.16628820919090717 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 22:18:05.947626 test begin: paddle.incubate.segment_min(Tensor([3, 3],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/PaddleAPITest/tester/paddle_torch_gpu_performance.py:121: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_min" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_min" instead.
    Reason: paddle.incubate.segment_min will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/PaddleAPITest/tester/paddle_torch_gpu_performance.py:137: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_min" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_min" instead.
    Reason: paddle.incubate.segment_min will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_min 	 paddle.incubate.segment_min(Tensor([3, 3],"float32"), Tensor([3],"int32"), ) 	 12 	 262264 	 9.97633671760559 	 32.66468071937561 	 0.30541663037557126 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-17 22:19:04.964322 test begin: paddle.incubate.segment_sum(Tensor([3, 3],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/PaddleAPITest/tester/paddle_torch_gpu_performance.py:121: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_sum" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_sum" instead.
    Reason: paddle.incubate.segment_sum will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/PaddleAPITest/tester/paddle_torch_gpu_performance.py:137: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_sum" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_sum" instead.
    Reason: paddle.incubate.segment_sum will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_sum 	 paddle.incubate.segment_sum(Tensor([3, 3],"float32"), Tensor([3],"int32"), ) 	 12 	 256766 	 9.680657148361206 	 10.303717374801636 	 0.9395305399229834 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
