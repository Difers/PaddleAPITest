2025-07-08 17:37:06.564167 GPU 6 49959 test begin: paddle.cumsum(Tensor([10, 429496730],"float16"), dtype="float16", )
[accuracy error] paddle.cumsum(Tensor([10, 429496730],"float16"), dtype="float16", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1987324484 / 4294967300 (46.3%)
Greatest absolute difference: 272.0 at index (3384322167,) (up to 0.01 allowed)
Greatest relative difference: inf at index (1033129,) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([4294967300]), dtype=torch.float16)
tensor([ 3.2153e-01, -1.2305e-01, -3.9307e-01,  ...,  1.3680e+03,  1.3680e+03,  1.3680e+03], dtype=torch.float16)
DESIRED: (shape=torch.Size([4294967300]), dtype=torch.float16)
tensor([ 3.2153e-01, -1.2305e-01, -3.9307e-01,  ...,  1.4180e+03,  1.4180e+03,  1.4180e+03], dtype=torch.float16)

2025-07-08 17:37:19.524845 GPU 7 50200 test begin: paddle.cumsum(Tensor([357913942, 12],"float16"), dtype="float16", )
[accuracy error] paddle.cumsum(Tensor([357913942, 12],"float16"), dtype="float16", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 3333373146 / 4294967304 (77.6%)
Greatest absolute difference: 1256.0 at index (3353229520,) (up to 0.01 allowed)
Greatest relative difference: inf at index (20846644,) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([4294967304]), dtype=torch.float16)
tensor([4.7144e-01, 2.3926e-02, 3.1525e-02,  ..., 1.2232e+04, 1.2232e+04, 1.2232e+04], dtype=torch.float16)
DESIRED: (shape=torch.Size([4294967304]), dtype=torch.float16)
tensor([4.7144e-01, 2.3926e-02, 3.1525e-02,  ..., 1.1328e+04, 1.1328e+04, 1.1328e+04], dtype=torch.float16)

2025-07-08 17:47:38.035301 GPU 6 53748 test begin: paddle.histogramdd(Tensor([4, 268435457, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=False, )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/paddle/tensor/linalg.py:5743: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  edge = paddle.to_tensor(edge)
[accuracy error] paddle.histogramdd(Tensor([4, 268435457, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 12 / 12 (100.0%)
Greatest absolute difference: 44747593.0 at index (2, 2) (up to 0.01 allowed)
Greatest relative difference: 0.5000694882533225 at index (2, 2) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([3, 4]), dtype=torch.float64)
tensor([[44746705., 44727553., 44739588., 44737558.],
        [44747424., 44728796., 44746994., 44744411.],
        [44739621., 44743217., 44735157., 44733888.]], dtype=torch.float64)
DESIRED: (shape=torch.Size([3, 4]), dtype=torch.float64)
tensor([[89488893., 89462801., 89486715., 89472033.],
        [89479053., 89468449., 89484356., 89480456.],
        [89483167., 89480754., 89482750., 89472401.]], dtype=torch.float64)

2025-07-08 17:49:07.178296 GPU 2 53835 test begin: paddle.histogramdd(Tensor([536870913, 2, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=False, )
[accuracy error] paddle.histogramdd(Tensor([536870913, 2, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 12 / 12 (100.0%)
Greatest absolute difference: 44760106.0 at index (0, 1) (up to 0.01 allowed)
Greatest relative difference: 0.5001325023957642 at index (0, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([3, 4]), dtype=torch.float64)
tensor([[44740770., 44736389., 44725188., 44746164.],
        [44737968., 44737994., 44737944., 44739897.],
        [44745203., 44751121., 44736414., 44735860.]], dtype=torch.float64)
DESIRED: (shape=torch.Size([3, 4]), dtype=torch.float64)
tensor([[89486139., 89496495., 89460124., 89478193.],
        [89470399., 89475926., 89471095., 89470334.],
        [89486644., 89491817., 89480696., 89473964.]], dtype=torch.float64)

2025-07-08 17:52:34.300890 GPU 2 53835 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([2, 1073741824]), dtype=torch.int8)
tensor([[   0,    0,  127,  ...,    0,    0,    0],
        [   0, -127, -127,  ..., -127,  127, -127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([2, 1073741824]), dtype=torch.float16)
tensor([[  -0.,   -0.,  127.,  ...,    0.,   -0.,    0.],
        [  -0., -127., -127.,  ..., -127.,  127., -127.]], dtype=torch.float16)

2025-07-08 17:52:46.924372 GPU 7 54087 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([2, 1073741824]), dtype=torch.int8)
tensor([[   0,  127,  127,  ...,    0,    0,    0],
        [ 127, -127,    0,  ...,  127,  127,  127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([2, 1073741824]), dtype=torch.float16)
tensor([[  -0.,  127.,  127.,  ...,   -0.,   -0.,    0.],
        [ 127., -127.,   -0.,  ...,  127.,  127.,  127.]], dtype=torch.float16)

2025-07-08 17:53:06.128060 GPU 2 53835 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([2, 1073741824]), dtype=torch.int8)
tensor([[   0,    0,  127,  ...,    0,    0,    0],
        [   0, -127, -127,  ..., -127,  127, -127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([2, 1073741824]), dtype=torch.float16)
tensor([[  -0.,   -0.,  127.,  ...,    0.,   -0.,    0.],
        [  -0., -127., -127.,  ..., -127.,  127., -127.]], dtype=torch.float16)

2025-07-08 17:53:11.353984 GPU 3 54519 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([2, 1073741824]), dtype=torch.int8)
tensor([[   0, -127,    0,  ...,    0,    0,    0],
        [   0, -127,  127,  ..., -127,    0,    0]], dtype=torch.int8)
DESIRED: (shape=torch.Size([2, 1073741824]), dtype=torch.float16)
tensor([[   0., -127.,    0.,  ...,   -0.,    0.,    0.],
        [   0., -127.,  127.,  ..., -127.,   -0.,    0.]], dtype=torch.float16)

2025-07-08 17:53:15.933454 GPU 7 54087 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([2, 1073741824]), dtype=torch.int8)
tensor([[   0,  127,  127,  ...,    0,    0,    0],
        [ 127, -127,    0,  ...,  127,  127,  127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([2, 1073741824]), dtype=torch.float16)
tensor([[  -0.,  127.,  127.,  ...,   -0.,   -0.,    0.],
        [ 127., -127.,   -0.,  ...,  127.,  127.,  127.]], dtype=torch.float16)

2025-07-08 17:53:24.428332 GPU 6 53748 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([2, 1073741824]), dtype=torch.int8)
tensor([[ 127,    0, -127,  ...,  127, -127,  127],
        [ 127,    0,  127,  ...,    0, -127, -127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([2, 1073741824]), dtype=torch.float16)
tensor([[ 127.,    0., -127.,  ...,  127., -127.,  127.],
        [ 127.,    0.,  127.,  ...,    0., -127., -127.]], dtype=torch.float16)

2025-07-08 17:53:32.432233 GPU 2 53835 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([2, 1073741824]), dtype=torch.int8)
tensor([[   0,    0,  127,  ...,    0,    0,    0],
        [   0, -127, -127,  ..., -127,  127, -127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([2, 1073741824]), dtype=torch.float16)
tensor([[  -0.,   -0.,  127.,  ...,    0.,   -0.,    0.],
        [  -0., -127., -127.,  ..., -127.,  127., -127.]], dtype=torch.float16)

2025-07-08 17:53:35.081058 GPU 7 54087 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 2147483648],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([2147483648],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([2, 1073741824]), dtype=torch.int8)
tensor([[   0,  127,  127,  ...,    0,    0,    0],
        [ 127, -127,    0,  ...,  127,  127,  127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([2, 1073741824]), dtype=torch.float16)
tensor([[  -0.,  127.,  127.,  ...,   -0.,   -0.,    0.],
        [ 127., -127.,   -0.,  ...,  127.,  127.,  127.]], dtype=torch.float16)

2025-07-08 17:53:58.497538 GPU 7 54087 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([464, 4628198]), dtype=torch.int8)
tensor([[   0,  127, -127,  ...,  127, -127,    0],
        [-127,    0,  127,  ...,    0,    0, -127],
        [   0, -127,    0,  ...,  127, -127,  127],
        ...,
        [-127,    0, -127,  ...,    0,    0,  127],
        [ 127,    0,  127,  ..., -127,  127, -127],
        [-127,  127,  127,  ...,    0,    0,    0]], dtype=torch.int8)
DESIRED: (shape=torch.Size([464, 4628198]), dtype=torch.float16)
tensor([[   0.,  127., -127.,  ...,  127., -127.,    0.],
        [-127.,   -0.,  127.,  ...,   -0.,    0., -127.],
        [   0., -127.,    0.,  ...,  127., -127.,  127.],
        ...,
        [-127.,   -0., -127.,  ...,    0.,   -0.,  127.],
        [ 127.,    0.,  127.,  ..., -127.,  127., -127.],
        [-127.,  127.,  127.,  ...,    0.,   -0.,    0.]], dtype=torch.float16)

2025-07-08 17:53:59.795119 GPU 2 53835 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([464, 4628198]), dtype=torch.int8)
tensor([[   0,    0,  127,  ...,    0,    0, -127],
        [-127, -127,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,  127],
        ...,
        [   0,    0,    0,  ..., -127, -127,    0],
        [ 127,  127,    0,  ..., -127,    0, -127],
        [   0, -127, -127,  ...,    0, -127, -127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([464, 4628198]), dtype=torch.float16)
tensor([[   0.,   -0.,  127.,  ...,    0.,   -0., -127.],
        [-127., -127.,    0.,  ...,   -0.,    0.,   -0.],
        [  -0.,   -0.,    0.,  ...,   -0.,    0.,  127.],
        ...,
        [   0.,    0.,    0.,  ..., -127., -127.,   -0.],
        [ 127.,  127.,    0.,  ..., -127.,   -0., -127.],
        [  -0., -127., -127.,  ...,    0., -127., -127.]], dtype=torch.float16)

2025-07-08 17:53:59.931405 GPU 3 54519 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([464, 4628198]), dtype=torch.int8)
tensor([[   0, -127,    0,  ...,    0,    0,  127],
        [   0,  127,    0,  ..., -127,    0,    0],
        [-127,    0,  127,  ...,  127, -127,    0],
        ...,
        [ 127,  127,    0,  ...,  127,    0,  127],
        [-127,    0,  127,  ...,    0,    0,    0],
        [ 127, -127, -127,  ...,    0,    0,  127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([464, 4628198]), dtype=torch.float16)
tensor([[   0., -127.,   -0.,  ...,    0.,   -0.,  127.],
        [   0.,  127.,   -0.,  ..., -127.,   -0.,   -0.],
        [-127.,    0.,  127.,  ...,  127., -127.,    0.],
        ...,
        [ 127.,  127.,   -0.,  ...,  127.,    0.,  127.],
        [-127.,    0.,  127.,  ...,    0.,   -0.,   -0.],
        [ 127., -127., -127.,  ...,    0.,    0.,  127.]], dtype=torch.float16)

2025-07-08 17:54:00.708826 GPU 5 54716 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([464, 4628198]), dtype=torch.int8)
tensor([[   0,  127, -127,  ...,    0, -127,  127],
        [   0,    0,  127,  ...,    0,  127, -127],
        [   0, -127, -127,  ...,  127, -127,  127],
        ...,
        [-127,    0,    0,  ..., -127,    0,    0],
        [ 127,  127,    0,  ...,  127,    0,  127],
        [   0,    0, -127,  ...,  127, -127,    0]], dtype=torch.int8)
DESIRED: (shape=torch.Size([464, 4628198]), dtype=torch.float16)
tensor([[   0.,  127., -127.,  ...,   -0., -127.,  127.],
        [  -0.,   -0.,  127.,  ...,   -0.,  127., -127.],
        [  -0., -127., -127.,  ...,  127., -127.,  127.],
        ...,
        [-127.,    0.,   -0.,  ..., -127.,   -0.,   -0.],
        [ 127.,  127.,    0.,  ...,  127.,   -0.,  127.],
        [   0.,   -0., -127.,  ...,  127., -127.,    0.]], dtype=torch.float16)

2025-07-08 17:54:18.712123 GPU 7 54087 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([464, 4628198]), dtype=torch.int8)
tensor([[   0,  127, -127,  ...,  127, -127,    0],
        [-127,    0,  127,  ...,    0,    0, -127],
        [   0, -127,    0,  ...,  127, -127,  127],
        ...,
        [-127,    0, -127,  ...,    0,    0,  127],
        [ 127,    0,  127,  ..., -127,  127, -127],
        [-127,  127,  127,  ...,    0,    0,    0]], dtype=torch.int8)
DESIRED: (shape=torch.Size([464, 4628198]), dtype=torch.float16)
tensor([[   0.,  127., -127.,  ...,  127., -127.,    0.],
        [-127.,   -0.,  127.,  ...,   -0.,    0., -127.],
        [   0., -127.,    0.,  ...,  127., -127.,  127.],
        ...,
        [-127.,   -0., -127.,  ...,    0.,   -0.,  127.],
        [ 127.,    0.,  127.,  ..., -127.,  127., -127.],
        [-127.,  127.,  127.,  ...,    0.,   -0.,    0.]], dtype=torch.float16)

2025-07-08 17:54:20.254099 GPU 2 53835 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([464, 4628198]), dtype=torch.int8)
tensor([[   0,    0,  127,  ...,    0,    0, -127],
        [-127, -127,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,  127],
        ...,
        [   0,    0,    0,  ..., -127, -127,    0],
        [ 127,  127,    0,  ..., -127,    0, -127],
        [   0, -127, -127,  ...,    0, -127, -127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([464, 4628198]), dtype=torch.float16)
tensor([[   0.,   -0.,  127.,  ...,    0.,   -0., -127.],
        [-127., -127.,    0.,  ...,   -0.,    0.,   -0.],
        [  -0.,   -0.,    0.,  ...,   -0.,    0.,  127.],
        ...,
        [   0.,    0.,    0.,  ..., -127., -127.,   -0.],
        [ 127.,  127.,    0.,  ..., -127.,   -0., -127.],
        [  -0., -127., -127.,  ...,    0., -127., -127.]], dtype=torch.float16)

2025-07-08 17:54:21.340225 GPU 3 54519 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([464, 4628198]), dtype=torch.int8)
tensor([[   0, -127,    0,  ...,    0,    0,  127],
        [   0,  127,    0,  ..., -127,    0,    0],
        [-127,    0,  127,  ...,  127, -127,    0],
        ...,
        [ 127,  127,    0,  ...,  127,    0,  127],
        [-127,    0,  127,  ...,    0,    0,    0],
        [ 127, -127, -127,  ...,    0,    0,  127]], dtype=torch.int8)
DESIRED: (shape=torch.Size([464, 4628198]), dtype=torch.float16)
tensor([[   0., -127.,   -0.,  ...,    0.,   -0.,  127.],
        [   0.,  127.,   -0.,  ..., -127.,   -0.,   -0.],
        [-127.,    0.,  127.,  ...,  127., -127.,    0.],
        ...,
        [ 127.,  127.,   -0.,  ...,  127.,    0.,  127.],
        [-127.,    0.,  127.,  ...,    0.,   -0.,   -0.],
        [ 127., -127., -127.,  ...,    0.,    0.,  127.]], dtype=torch.float16)

2025-07-08 17:54:22.888084 GPU 5 54716 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 9256396],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([9256396],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'dtype' do not match: torch.int8 != torch.float16.
ACTUAL: (shape=torch.Size([464, 4628198]), dtype=torch.int8)
tensor([[   0,  127, -127,  ...,    0, -127,  127],
        [   0,    0,  127,  ...,    0,  127, -127],
        [   0, -127, -127,  ...,  127, -127,  127],
        ...,
        [-127,    0,    0,  ..., -127,    0,    0],
        [ 127,  127,    0,  ...,  127,    0,  127],
        [   0,    0, -127,  ...,  127, -127,    0]], dtype=torch.int8)
DESIRED: (shape=torch.Size([464, 4628198]), dtype=torch.float16)
tensor([[   0.,  127., -127.,  ...,   -0., -127.,  127.],
        [  -0.,   -0.,  127.,  ...,   -0.,  127., -127.],
        [  -0., -127., -127.,  ...,  127., -127.,  127.],
        ...,
        [-127.,    0.,   -0.,  ..., -127.,   -0.,   -0.],
        [ 127.,  127.,    0.,  ...,  127.,   -0.,  127.],
        [   0.,   -0., -127.,  ...,  127., -127.,    0.]], dtype=torch.float16)

2025-07-08 17:54:41.093914 GPU 2 53835 test begin: paddle.incubate.nn.functional.fused_bias_dropout_residual_layer_norm(Tensor([17409, 128, 1024],"float32"), Tensor([17409, 128, 1024],"float32"), None, Tensor([1024],"float32"), Tensor([1024],"float32"), 0.0, 1e-05, )
[cuda error] paddle.incubate.nn.functional.fused_bias_dropout_residual_layer_norm(Tensor([17409, 128, 1024],"float32"), Tensor([17409, 128, 1024],"float32"), None, Tensor([1024],"float32"), Tensor([1024],"float32"), 0.0, 1e-05, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<FusedBiasDropoutResidualLayerNormGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751968522 (unix time) try "date -d @1751968522" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xd24b) received by PID 53835 (TID 0x7f0967d6e740) from PID 53835 ***]


2025-07-08 17:57:24.175745 GPU 3 54519 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([67108864, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([67108864, 64],"float16"), )
[cuda error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([67108864, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([67108864, 64],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 17:59:00.814064 GPU 3 54519 test begin: paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([1, 2, 4],"float32"), qkv_weight=Tensor([3, 2, 2, 4],"float32"), linear_weight=Tensor([4, 4],"float32"), pre_layer_norm=False, pre_ln_scale=None, pre_ln_bias=None, ln_scale=Tensor([2281701379],"float32"), ln_bias=None, pre_ln_epsilon=1e-05, qkv_bias=None, linear_bias=None, cache_kv=None, attn_mask=Tensor([1, 2, 2, 2],"float32"), dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=2, transpose_qkv_wb=False, name=None, )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([1, 2, 4],"float32"), qkv_weight=Tensor([3, 2, 2, 4],"float32"), linear_weight=Tensor([4, 4],"float32"), pre_layer_norm=False, pre_ln_scale=None, pre_ln_bias=None, ln_scale=Tensor([2281701379],"float32"), ln_bias=None, pre_ln_epsilon=1e-05, qkv_bias=None, linear_bias=None, cache_kv=None, attn_mask=Tensor([1, 2, 2, 2],"float32"), dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=2, transpose_qkv_wb=False, name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 8 / 8 (100.0%)
Greatest absolute difference: 0.8690087795257568 at index (0, 0, 1) (up to 0.01 allowed)
Greatest relative difference: 6.719898223876953 at index (0, 1, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 2, 4]), dtype=torch.float32)
tensor([[[ 0.4696,  0.4959,  0.0389, -0.0045],
         [ 0.3546,  0.4918,  0.0894, -0.0528]]])
DESIRED: (shape=torch.Size([1, 2, 4]), dtype=torch.float32)
tensor([[[ 0.3294, -0.3731, -0.1230, -0.0327],
         [ 0.4057,  0.0637,  0.1497,  0.3583]]])

2025-07-08 17:59:05.831882 GPU 5 56381 test begin: paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([1, 2, 4],"float32"), qkv_weight=Tensor([4, 12],"float32"), linear_weight=Tensor([4, 4],"float32"), pre_layer_norm=False, pre_ln_scale=None, pre_ln_bias=None, ln_scale=Tensor([2281701379],"float32"), ln_bias=None, pre_ln_epsilon=1e-05, qkv_bias=None, linear_bias=None, cache_kv=None, attn_mask=Tensor([1, 2, 2, 2],"float32"), dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=2, transpose_qkv_wb=True, name=None, )
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([1, 2, 4],"float32"), qkv_weight=Tensor([4, 12],"float32"), linear_weight=Tensor([4, 4],"float32"), pre_layer_norm=False, pre_ln_scale=None, pre_ln_bias=None, ln_scale=Tensor([2281701379],"float32"), ln_bias=None, pre_ln_epsilon=1e-05, qkv_bias=None, linear_bias=None, cache_kv=None, attn_mask=Tensor([1, 2, 2, 2],"float32"), dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=2, transpose_qkv_wb=True, name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 7 / 8 (87.5%)
Greatest absolute difference: 0.36327216029167175 at index (0, 0, 1) (up to 0.01 allowed)
Greatest relative difference: 3.0172553062438965 at index (0, 0, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 2, 4]), dtype=torch.float32)
tensor([[[-0.0144,  0.2429,  0.2592,  0.4111],
         [ 0.0368,  0.0648,  0.0796, -0.5175]]])
DESIRED: (shape=torch.Size([1, 2, 4]), dtype=torch.float32)
tensor([[[ 0.0538, -0.1204,  0.3643,  0.4072],
         [-0.1295, -0.2829, -0.2292, -0.3659]]])

2025-07-08 17:59:14.142628 GPU 2 56567 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([16777216, 256],"float16"), Tensor([256],"float16"), None, 1e-05, begin_norm_axis=1, )
[cuda error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([16777216, 256],"float16"), Tensor([256],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<RmsNormGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751968781 (unix time) try "date -d @1751968781" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xdcf7) received by PID 56567 (TID 0x7fa87fee1740) from PID 56567 ***]


2025-07-08 17:59:16.626432 GPU 3 54519 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([5592406, 768],"float16"), Tensor([768],"float16"), None, 1e-06, begin_norm_axis=1, )
[cuda error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([5592406, 768],"float16"), Tensor([768],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<RmsNormGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751968783 (unix time) try "date -d @1751968783" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xd4f7) received by PID 54519 (TID 0x7f90e23e0740) from PID 54519 ***]


2025-07-08 17:59:17.040206 GPU 7 54087 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([67108864, 1, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )
[cuda error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([67108864, 1, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<RmsNormGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751968781 (unix time) try "date -d @1751968781" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xd347) received by PID 54087 (TID 0x7f26cd7ff740) from PID 54087 ***]


2025-07-08 17:59:29.016805 GPU 5 57531 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([67108864, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )
[cuda error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([67108864, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<RmsNormGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751968900 (unix time) try "date -d @1751968900" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe0bb) received by PID 57531 (TID 0x7ff9f16ba740) from PID 57531 ***]


2025-07-08 17:59:50.457016 GPU 7 57705 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([8388608, 1, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, )
[cuda error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([8388608, 1, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<RmsNormGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751968925 (unix time) try "date -d @1751968925" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe169) received by PID 57705 (TID 0x7ff24b391740) from PID 57705 ***]


2025-07-08 17:59:53.164779 GPU 3 57797 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([8388608, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, )
[cuda error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([8388608, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<RmsNormGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751968914 (unix time) try "date -d @1751968914" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe1c5) received by PID 57797 (TID 0x7f24cd556740) from PID 57797 ***]


2025-07-08 18:04:55.413926 GPU 6 60083 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 67108864, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
One of the differentiated Tensors does not require grad
[accuracy error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 67108864, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 478 / 512 (93.4%)
Greatest absolute difference: 1.5737621784210205 at index (0, 7, 1, 12) (up to 0.01 allowed)
Greatest relative difference: 397.08251953125 at index (1, 0, 1, 2) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 8, 2, 16]), dtype=torch.float32)
tensor([[[[ 0.2674,  0.2998, -0.5472,  ...,  0.2203, -0.0646,  0.4424],
          [ 0.0128,  0.4361, -0.6082,  ...,  0.5275, -0.3154,  0.6015]],

         [[-0.3165,  0.1223, -0.1802,  ...,  0.6836, -0.1371,  0.1076],
          [ 0.0151,  0.2611, -0.1999,  ...,  0.7138,  0.0838,  0.1198]],

         [[-0.0015,  0.5556, -0.4731,  ...,  0.8071,  0.3292,  0.0484],
          [ 0.1324,  0.8753, -0.4313,  ...,  0.4965,  0.4301,  0.1022]],

         ...,

         [[ 0.6226,  0.3263, -0.0583,  ...,  0.3986,  0.1096,  0.3217],
          [ 0.3010,  0.7680,  0.0297,  ...,  0.5507, -0.0181,  0.3578]],

         [[-0.3585,  0.4751,  0.1357,  ...,  0.2705, -0.4526,  0.6141],
          [-0.2695,  0.3918, -0.2561,  ...,  0.1157, -0.3935,  0.6122]],

         [[-0.5292,  0.8047, -0.6482,  ...,  0.7135,  0.3043,  0.6102],
          [ 0.0424,  1.3180,  0.1754,  ...,  0.7155,  0.5559,  0.8763]]],


        [[[ 0.3892,  0.5851, -0.5957,  ...,  0.0900,  0.0491,  0.1034],
          [ 0.2425,  0.3540, -0.0459,  ...,  0.2617, -0.0541,  0.2281]],

         [[-0.1317,  0.1751, -0.3966,  ...,  0.5920, -0.1717,  0.4514],
          [-0.0522,  0.2325, -0.0964,  ...,  0.9233,  0.2518,  0.3671]],

         [[ 0.1019,  0.8753, -0.1190,  ...,  0.8434,  0.8770,  0.2697],
          [ 0.2071,  1.0273, -0.5185,  ...,  0.6642,  0.0278,  0.0776]],

         ...,

         [[ 0.3239,  0.0692, -0.1313,  ...,  0.6435, -0.0792,  0.0973],
          [ 0.4111,  0.5037,  0.0227,  ...,  0.7028,  0.0768,  0.7813]],

         [[-0.2420,  0.2325, -0.2916,  ...,  0.1075,  0.1813,  0.3166],
          [-0.5111,  0.5489,  0.1342,  ...,  0.1885, -0.5096,  0.8547]],

         [[-0.1100,  1.0102, -0.6091,  ...,  0.7268,  0.0053,  0.9055],
          [ 0.0196,  1.2966, -0.2817,  ...,  0.5449,  0.2212,  0.8793]]]])
DESIRED: (shape=torch.Size([2, 8, 2, 16]), dtype=torch.float32)
tensor([[[[ 1.4879e-01, -4.2885e-01, -2.8160e-01,  ...,  9.0293e-01,  2.5256e-01,  4.1914e-01],
          [ 7.6177e-03, -1.7909e-01, -5.8401e-02,  ...,  6.5220e-01,  1.0940e-01,  2.4894e-01]],

         [[-2.7926e-01, -2.3734e-01,  3.9845e-02,  ...,  5.2933e-01,  1.2569e-01,  3.1130e-01],
          [ 2.2646e-01, -1.1015e-01, -4.1492e-01,  ...,  4.5256e-01,  1.8191e-01,  1.9425e-01]],

         [[-2.4373e-02, -4.4367e-02, -5.0774e-01,  ...,  5.6344e-01,  2.3675e-01,  6.1219e-02],
          [ 6.8912e-02, -8.3607e-02,  6.3637e-02,  ...,  2.6290e-01,  4.2023e-01,  2.2811e-01]],

         ...,

         [[ 5.6124e-01,  3.1826e-01, -1.7911e-02,  ...,  3.3665e-01,  4.4827e-01,  1.9860e-01],
          [ 3.3847e-01,  7.5655e-01, -8.8383e-02,  ...,  7.8604e-01,  3.4201e-01,  3.8597e-01]],

         [[-6.7064e-01,  3.6773e-01, -3.8756e-01,  ...,  5.4049e-01,  4.8271e-01,  4.6182e-01],
          [-5.3700e-01,  2.6355e-01, -2.3922e-01,  ...,  2.5705e-01,  9.0593e-01,  9.7594e-01]],

         [[ 5.9360e-02, -9.5624e-02, -1.7808e-01,  ...,  4.8629e-01,  4.4292e-01,  6.6373e-01],
          [-2.4669e-01,  9.0348e-02, -5.9683e-01,  ...,  4.0531e-01,  1.1990e+00,  3.7280e-01]]],


        [[[ 3.3590e-01,  1.0979e-01, -4.0568e-01,  ...,  8.5449e-01,  1.1909e-01,  3.2194e-01],
          [ 1.7805e-01,  6.9667e-02,  1.1598e-04,  ...,  6.8848e-01,  1.3495e-01,  3.2997e-01]],

         [[-4.7155e-02, -8.7669e-02, -5.8741e-01,  ...,  4.6077e-01,  4.8949e-01,  7.1873e-01],
          [-3.0743e-02, -8.0611e-02, -4.4616e-01,  ...,  1.1116e+00,  5.1452e-01,  3.8879e-01]],

         [[ 1.1121e-01, -3.8875e-01, -4.7658e-01,  ...,  4.5901e-01,  7.2138e-01,  8.1960e-01],
          [ 1.2178e-01,  1.6293e-02, -8.4365e-02,  ...,  3.4570e-01,  1.3871e-01,  4.5474e-01]],

         ...,

         [[ 2.7819e-01,  5.9165e-02, -1.3147e-01,  ...,  1.1388e-01,  2.8911e-01,  1.5397e-01],
          [ 3.7201e-01,  4.9357e-01,  5.6635e-02,  ...,  8.1037e-01,  4.9716e-01,  6.6754e-01]],

         [[-2.3836e-01,  1.4893e-01, -1.1012e-02,  ...,  1.3466e-01,  5.2820e-01,  7.9607e-01],
          [-2.8989e-01,  3.9659e-01, -1.4577e-01,  ...,  3.3085e-01,  5.6364e-01,  6.9101e-01]],

         [[-4.8068e-01,  4.4323e-01,  3.6324e-02,  ...,  8.7878e-01,  7.9191e-01,  6.1314e-01],
          [-1.9355e-01, -2.6408e-01, -3.0125e-01,  ...,  7.2157e-01,  7.7373e-01,  7.0972e-01]]]])

2025-07-08 18:05:24.158154 GPU 5 60404 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([16777216, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
One of the differentiated Tensors does not require grad
[accuracy error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([16777216, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 485 / 512 (94.7%)
Greatest absolute difference: 1.7100298404693604 at index (1, 6, 0, 5) (up to 0.01 allowed)
Greatest relative difference: 565.9315795898438 at index (1, 5, 0, 9) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 8, 2, 16]), dtype=torch.float32)
tensor([[[[ 4.5544e-02,  9.8322e-01, -7.0975e-02,  ...,  1.0882e+00, -3.2810e-01,  3.6939e-01],
          [-2.8883e-02,  5.3106e-01, -2.3115e-04,  ...,  4.0117e-01, -2.6648e-01,  4.0362e-01]],

         [[ 3.6391e-01,  1.0573e+00, -6.9586e-01,  ...,  2.1949e-01,  4.6233e-02,  1.5483e-01],
          [ 1.1052e-01,  7.1313e-01, -1.8276e-01,  ...,  5.9220e-01, -7.9967e-01,  5.2157e-01]],

         [[-2.1026e-01,  2.6991e-01, -1.2162e-01,  ...,  3.0966e-01,  6.9910e-02,  1.3082e-01],
          [-2.0882e-01,  3.5213e-01, -6.2408e-01,  ...,  1.9129e-01,  1.2182e-01,  3.6784e-01]],

         ...,

         [[ 2.4121e-01,  1.0723e-02,  1.4774e-01,  ...,  8.4760e-02,  3.3669e-01,  6.6963e-02],
          [ 4.3428e-02,  3.5119e-03,  3.1414e-02,  ...,  1.4174e-01, -1.6578e-01,  6.8552e-02]],

         [[ 7.6875e-02,  8.8934e-01,  2.1291e-01,  ...,  3.6913e-01, -6.5117e-02,  4.0764e-01],
          [ 6.2662e-02,  1.1084e+00, -9.9353e-02,  ...,  2.8149e-01, -4.0571e-01,  9.1696e-01]],

         [[-3.1435e-01,  1.0603e+00, -9.2009e-02,  ...,  2.1015e-01, -4.4546e-01,  5.1298e-01],
          [-3.4411e-01,  1.0100e+00,  4.0984e-01,  ...,  9.4795e-02,  4.6654e-01,  1.8506e-01]]],


        [[[ 3.9483e-01,  8.8836e-01,  2.8460e-03,  ...,  8.2456e-01, -3.9245e-01,  4.3040e-01],
          [ 2.2720e-01,  1.1052e+00,  4.7664e-01,  ...,  5.0448e-01, -8.8929e-02,  1.9471e-01]],

         [[-2.9588e-01,  7.7982e-01, -8.5162e-01,  ...,  1.2836e+00, -5.6128e-01,  3.6049e-01],
          [-1.5500e-01,  5.2089e-01, -2.6724e-01,  ...,  8.5566e-01,  6.4003e-01,  5.2565e-01]],

         [[ 3.3747e-01,  6.3431e-01, -4.2347e-02,  ...,  9.8288e-01, -2.3307e-01,  3.2358e-01],
          [ 1.4997e-01,  6.6377e-01, -3.8682e-01,  ...,  7.7921e-01, -1.2106e-01,  4.7197e-02]],

         ...,

         [[ 1.8468e-01,  7.3574e-03, -7.1030e-02,  ...,  1.1492e-01,  1.5474e-01,  9.2669e-02],
          [ 2.3173e-01,  1.0815e-02, -8.5636e-02,  ...,  3.5312e-01,  2.6202e-01,  5.2160e-02]],

         [[ 1.7374e-01,  8.1524e-01,  6.1070e-02,  ...,  3.0131e-01,  1.5326e-02,  3.4914e-01],
          [-1.0400e-01,  4.5356e-01,  3.8721e-01,  ...,  2.0378e-01,  2.1379e-01,  6.8650e-02]],

         [[-2.9481e-01,  6.5330e-01,  1.1291e-01,  ...,  2.5548e-01,  4.9809e-01,  2.3682e-01],
          [ 4.1057e-02,  3.8534e-01,  1.0036e-01,  ...,  3.4585e-01, -2.1056e-01,  2.7035e-01]]]])
DESIRED: (shape=torch.Size([2, 8, 2, 16]), dtype=torch.float32)
tensor([[[[ 0.3572, -0.4596,  0.1371,  ...,  0.1150,  0.1382,  0.1632],
          [ 0.1363, -0.5406, -0.2165,  ...,  0.0468,  0.2043,  0.6120]],

         [[ 0.3596,  0.2565,  0.1723,  ...,  0.2965,  0.2295,  0.3067],
          [ 0.1477, -0.2881, -0.2929,  ...,  0.8816,  0.0191,  0.6733]],

         [[-0.1549,  0.0385, -0.0572,  ...,  0.9577,  0.3649,  0.2485],
          [-0.0152, -0.5026, -0.0782,  ...,  0.9250,  0.7312,  0.5732]],

         ...,

         [[ 0.2388, -0.0115,  0.0078,  ...,  0.6545,  0.5082,  0.8244],
          [ 0.0595, -0.0040, -0.3532,  ...,  0.8741,  0.5329,  0.5983]],

         [[-0.0100, -0.1502,  0.0126,  ...,  1.5311,  0.5570,  0.3292],
          [-0.0722,  0.1510, -0.1404,  ...,  1.4637,  0.4255,  0.5352]],

         [[-0.2520,  0.6939, -0.1016,  ...,  0.1934,  0.0226,  0.5626],
          [-0.1226,  0.6282,  0.1164,  ...,  0.5567,  0.1188,  0.1867]]],


        [[[ 0.4301, -0.6828, -0.0514,  ...,  0.0838,  0.1159,  0.1741],
          [ 0.4909,  0.2523,  0.5646,  ...,  0.1276,  0.1970,  0.5122]],

         [[-0.0906,  0.0310, -0.1732,  ...,  0.5003,  0.0585,  0.9472],
          [ 0.1168, -0.3126, -0.2310,  ...,  0.2263,  0.6649,  0.2271]],

         [[-0.0476,  0.0063, -0.6005,  ...,  0.9240,  0.4710,  0.7989],
          [ 0.0019, -0.5616, -0.7001,  ...,  1.0395,  0.0801,  0.2165]],

         ...,

         [[ 0.1717, -0.0054, -0.2846,  ...,  0.5924,  0.7012,  0.2458],
          [ 0.2445, -0.0103, -0.1355,  ...,  1.0236,  0.3912,  0.4211]],

         [[ 0.0262, -0.7768,  0.0580,  ...,  1.1152,  0.5601,  0.3660],
          [-0.2697,  0.1168,  0.3811,  ...,  1.3664,  0.2009,  0.4178]],

         [[-0.3946,  0.5968,  0.4403,  ...,  0.5527,  0.1637,  0.2132],
          [-0.0752,  0.0499,  0.1522,  ...,  0.7151,  0.0111,  0.3105]]]])

2025-07-08 18:05:39.121195 GPU 4 60725 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 134217728],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
One of the differentiated Tensors does not require grad
[accuracy error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 134217728],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 495 / 512 (96.7%)
Greatest absolute difference: 1.6685465574264526 at index (1, 2, 0, 10) (up to 0.01 allowed)
Greatest relative difference: 337.0708312988281 at index (1, 3, 1, 2) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 8, 2, 16]), dtype=torch.float32)
tensor([[[[ 0.0806,  0.2652,  0.6856,  ...,  0.9122,  0.0783,  0.8002],
          [ 0.4825,  0.9996, -0.1303,  ...,  1.1344, -0.6738,  0.3027]],

         [[ 0.1006,  0.9154, -0.2937,  ...,  0.6000, -0.2492,  0.4266],
          [-0.1737,  0.4395, -0.2102,  ...,  0.8044, -0.2128,  0.7896]],

         [[ 0.4620,  0.8528,  0.4379,  ...,  0.2084, -0.1690,  0.3303],
          [ 0.0740,  0.5043,  0.4012,  ...,  0.0963,  0.0439,  0.1989]],

         ...,

         [[-0.4767,  0.7411, -0.3013,  ...,  1.1144, -0.0024,  0.3338],
          [-0.3693,  0.6942, -0.2708,  ...,  0.7023,  0.1037,  0.1617]],

         [[-0.0675,  0.4210, -0.1694,  ...,  0.3437,  0.0069,  0.3407],
          [-0.0442,  0.1829,  0.3903,  ...,  0.7012,  0.0091,  0.1231]],

         [[ 0.2516,  0.7386, -0.6795,  ...,  0.2957, -0.1386,  0.2085],
          [-0.3134,  0.7251,  0.0051,  ...,  0.1649,  0.0016,  0.3211]]],


        [[[ 0.3131,  0.7403,  0.1062,  ...,  1.0210,  0.4687,  0.5827],
          [ 0.4541,  1.1832, -0.6553,  ...,  0.4008, -0.1408,  0.8438]],

         [[ 0.1022,  0.5298,  0.0796,  ...,  1.0172, -0.3912,  0.8812],
          [-0.0931,  0.6127,  0.0827,  ...,  0.6458, -0.0105,  0.2724]],

         [[ 0.3093,  0.8208,  0.3642,  ...,  0.3984,  0.2089,  0.6949],
          [ 0.3227,  0.6967,  0.8093,  ...,  0.3150,  0.2996,  0.7114]],

         ...,

         [[ 0.0275,  0.1695, -0.6237,  ...,  0.9805,  0.6416,  0.3269],
          [-0.0512,  0.5038, -0.1157,  ...,  0.3941,  0.0517,  0.0334]],

         [[-0.2017,  0.4344,  0.4158,  ...,  0.6476, -0.0347,  0.3262],
          [ 0.4439,  0.3665,  0.1154,  ...,  0.4800,  0.0219,  0.1589]],

         [[ 0.0019,  0.4565,  0.0636,  ...,  0.4074, -0.2133,  0.4479],
          [-0.1217,  1.3086,  0.2364,  ...,  0.3758, -0.1324,  0.1326]]]])
DESIRED: (shape=torch.Size([2, 8, 2, 16]), dtype=torch.float32)
tensor([[[[ 0.0826,  0.0813,  0.6080,  ...,  0.0771,  0.4764,  0.8103],
          [ 0.5090,  0.2313, -0.3093,  ...,  0.3100,  0.1088,  1.0517]],

         [[ 0.3680,  0.5126, -0.0358,  ...,  0.5145,  0.3370,  0.6765],
          [-0.2884,  0.1266, -0.2114,  ...,  0.3101,  0.7364,  0.8622]],

         [[ 0.4443, -0.4649,  0.4928,  ...,  0.3433,  0.1193,  0.4732],
          [ 0.2118, -0.3151,  0.3511,  ...,  0.3417,  0.9190,  0.1629]],

         ...,

         [[ 0.0207,  0.4044, -0.0761,  ...,  1.1418,  0.9311,  0.4232],
          [ 0.0432,  0.4308, -0.1900,  ...,  0.5175,  0.5837,  0.1403]],

         [[ 0.4383,  0.1228, -0.2619,  ...,  0.9151,  1.0166,  0.5904],
          [-0.4354, -0.1158,  0.2350,  ...,  0.3465,  0.2234,  0.1872]],

         [[ 0.3402, -0.2563, -0.5006,  ...,  1.2406,  0.2873,  0.7462],
          [-0.2145,  0.2542,  0.2681,  ...,  0.8784,  0.7562,  0.1576]]],


        [[[ 0.3558,  0.4184, -0.1156,  ...,  0.1489,  0.3712,  0.3100],
          [ 0.5056,  0.5965, -0.6720,  ...,  0.1289,  0.5270,  1.1148]],

         [[ 0.0370, -0.2404, -0.1858,  ...,  0.5632,  0.5562,  1.2633],
          [ 0.0158,  0.3629, -0.2668,  ...,  0.2793,  0.6743,  0.4730]],

         [[ 0.3462,  0.0384,  0.0183,  ...,  0.8404,  0.3371,  0.6697],
          [ 0.2810, -0.7699,  0.7505,  ...,  0.5844,  0.9034,  0.6869]],

         ...,

         [[ 0.0355, -0.1246, -0.5516,  ...,  0.9525,  0.8048,  0.0408],
          [-0.0418, -0.0431, -0.2044,  ...,  1.1066,  0.1428,  0.0118]],

         [[ 0.4029, -0.0305,  0.3589,  ...,  0.8146,  1.0918,  0.6944],
          [-0.0829,  0.0265,  0.4843,  ...,  0.2786,  1.1015,  0.2147]],

         [[ 0.1901, -0.3787,  0.6287,  ...,  1.2841,  0.4932,  0.5428],
          [ 0.2494,  0.1569,  0.2711,  ...,  1.5640,  0.6399,  0.8506]]]])

2025-07-08 18:08:25.799183 GPU 4 61697 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 16777216, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
One of the differentiated Tensors does not require grad
[accuracy error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 16777216, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 482 / 512 (94.1%)
Greatest absolute difference: 1.78311026096344 at index (6, 1, 1, 3) (up to 0.01 allowed)
Greatest relative difference: 414.00579833984375 at index (7, 0, 0, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([8, 2, 2, 16]), dtype=torch.float32)
tensor([[[[-0.0816,  0.7420,  0.4347,  ...,  0.6159,  0.2759,  0.6131],
          [-0.0551,  0.2005, -0.0094,  ...,  0.7796,  0.1673,  1.0532]],

         [[-0.2809,  0.5899, -0.0050,  ...,  0.1863,  0.1170,  1.1643],
          [-0.0292,  0.7635,  0.2140,  ...,  0.4463,  0.0800,  0.3676]]],


        [[[-0.3784,  0.5286, -0.6235,  ...,  0.2724,  0.4954,  0.3069],
          [-0.4128,  0.5245,  0.0259,  ...,  0.1209,  0.0472,  0.1406]],

         [[-0.2349,  0.3411, -0.4133,  ...,  0.4385,  0.1499,  0.4058],
          [ 0.4823,  0.8538, -0.3187,  ...,  0.8007,  0.1666,  0.4036]]],


        [[[ 0.2228,  0.3243, -0.2013,  ...,  0.3835,  0.1299,  0.3581],
          [-0.0440,  0.3217, -0.0178,  ...,  0.7106,  0.1621,  0.2045]],

         [[ 0.1372,  0.1408,  0.0218,  ...,  0.6927, -0.0352,  0.1214],
          [-0.0447,  0.0697, -0.2438,  ...,  0.6388,  0.1529,  0.4763]]],


        ...,


        [[[ 0.3677,  0.1327, -0.0383,  ...,  0.7337,  0.0417,  0.4404],
          [ 0.0447,  0.2015, -0.2583,  ...,  0.3176,  0.0285,  0.9335]],

         [[-0.3507,  0.2307,  0.0529,  ...,  0.9261,  0.0600,  0.7411],
          [-0.1118,  0.0579,  0.2395,  ...,  0.7291,  0.0191,  0.7534]]],


        [[[-0.4172,  0.3292, -0.3037,  ...,  0.7457,  0.0445,  1.0242],
          [-0.1927,  0.1098,  0.4564,  ...,  0.3742,  0.5406,  0.2612]],

         [[-0.5972,  0.1375, -0.2736,  ...,  0.4636,  0.6452,  0.3923],
          [-0.3034,  0.1415,  0.6741,  ...,  0.4954,  0.4725,  0.5732]]],


        [[[ 0.0830,  0.6008, -0.3922,  ...,  0.3055,  0.2977,  0.7503],
          [-0.0858,  0.6198, -0.2181,  ...,  0.7199,  0.1361,  0.5685]],

         [[-0.0669,  0.3784,  0.0716,  ...,  0.8077,  0.1787,  0.3371],
          [-0.0827,  0.2496,  0.0726,  ...,  0.9864, -0.0251,  0.5916]]]])
DESIRED: (shape=torch.Size([8, 2, 2, 16]), dtype=torch.float32)
tensor([[[[-0.2400,  0.2496,  0.1442,  ...,  1.0089,  0.4770,  0.1857],
          [-0.3100, -0.2775,  0.1927,  ...,  0.7928,  0.4986,  0.8844]],

         [[-0.0648, -0.0040, -0.2809,  ...,  0.2773,  0.4197,  1.0504],
          [ 0.1643,  0.1958, -0.0959,  ...,  0.6628,  0.4531,  0.3943]]],


        [[[-0.2822,  0.1739, -0.0075,  ...,  0.5195,  0.2913,  0.1425],
          [-0.3740, -0.1101, -0.1199,  ...,  0.1110,  0.0349,  0.3490]],

         [[-0.1898, -0.5540, -0.9604,  ...,  0.5700,  0.1002,  0.9502],
          [ 0.2101, -0.3070, -0.5585,  ...,  0.8666,  0.1156,  0.9074]]],


        [[[ 0.1643,  0.0100, -0.2661,  ...,  0.6736,  0.1946,  0.1316],
          [ 0.1096,  0.2266, -0.1779,  ...,  0.8550,  0.1014,  0.5275]],

         [[ 0.1012, -0.0460, -0.1213,  ...,  0.8423,  0.1615,  0.5254],
          [-0.0377, -0.0404, -0.0921,  ...,  0.7324,  0.2037,  0.2120]]],


        ...,


        [[[ 0.6691,  0.1210, -0.2277,  ...,  0.4488,  0.5765,  0.7798],
          [ 0.4523,  0.1898, -0.0380,  ...,  0.2076,  0.2436,  1.1410]],

         [[ 0.1586,  0.2271,  0.3391,  ...,  0.9715,  0.8140,  0.7258],
          [-0.5571,  0.0556,  0.0947,  ...,  0.3518,  0.3497,  0.4252]]],


        [[[-0.4640, -0.0362, -0.0214,  ...,  0.9715,  0.5179,  0.2861],
          [-0.2297, -0.1930,  0.3754,  ...,  0.5973,  0.4012,  0.0485]],

         [[-0.6619,  0.1238, -0.2160,  ...,  0.4885,  0.2536,  0.0311],
          [-0.6808, -0.0881,  0.8074,  ...,  0.6924,  0.2148,  0.1012]]],


        [[[ 0.2243,  0.0014, -0.2128,  ...,  0.2703,  0.4621,  0.1609],
          [ 0.0806,  0.1350, -0.4138,  ...,  0.7274,  0.6815,  0.0995]],

         [[-0.1280, -0.1048, -0.1874,  ...,  0.2530,  0.2968,  0.6358],
          [-0.1265,  0.1018,  0.0206,  ...,  0.0692,  0.7466,  0.3449]]]])

2025-07-08 18:09:42.709200 GPU 4 61697 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([67108864, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
One of the differentiated Tensors does not require grad
[accuracy error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([67108864, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 499 / 512 (97.5%)
Greatest absolute difference: 1.7209835052490234 at index (1, 0, 1, 7) (up to 0.01 allowed)
Greatest relative difference: 72.78863525390625 at index (3, 0, 1, 4) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([8, 2, 2, 16]), dtype=torch.float32)
tensor([[[[ 0.5150,  0.5935,  0.1500,  ...,  0.6069, -0.3453,  0.5161],
          [ 0.4762,  0.5066,  0.2977,  ...,  1.1349, -0.3642,  1.1929]],

         [[ 0.3471,  0.6227,  0.0561,  ...,  0.9922,  0.4708,  1.0675],
          [ 0.3971,  0.4071,  0.0283,  ...,  0.1294,  0.2272,  1.8241]]],


        [[[-0.0169,  0.5358,  0.0626,  ...,  0.1684,  0.0231,  0.1178],
          [-0.0204,  0.3639,  0.4513,  ...,  0.1436, -0.0097,  0.3737]],

         [[-0.0064,  0.5535, -0.0232,  ...,  0.1463, -0.0653,  0.0497],
          [-0.0082,  0.4784,  0.5177,  ...,  0.0484, -0.0102,  0.0092]]],


        [[[-0.6235,  0.2674,  0.2828,  ...,  0.3247, -0.3069,  0.5750],
          [-0.2789,  0.1404, -0.0445,  ...,  0.4765, -0.4320,  0.2458]],

         [[-0.0646,  0.3021,  0.0823,  ...,  0.6186,  0.0566,  0.7476],
          [-0.2710,  0.2119, -0.3905,  ...,  0.6235,  0.3209,  0.4434]]],


        ...,


        [[[-0.2263,  0.2188,  0.4682,  ...,  0.7456, -0.0205,  0.2233],
          [-0.1374,  0.1489,  0.4941,  ...,  0.2955, -0.0167,  0.3593]],

         [[ 0.4501,  0.4291, -0.4135,  ...,  0.1202,  0.0430,  0.4016],
          [ 0.3140,  0.2506, -0.2718,  ...,  0.6421, -0.0241,  0.0461]]],


        [[[ 0.0944,  0.1908,  0.0048,  ...,  0.7675,  0.2212,  0.0681],
          [ 0.4770,  0.3238, -0.3557,  ...,  0.7439,  0.3084,  0.0526]],

         [[ 0.1216,  0.0925, -0.3530,  ...,  0.9306,  0.0621,  0.0369],
          [ 0.3638,  0.3192, -0.2578,  ...,  0.1765,  0.4011,  0.1134]]],


        [[[ 0.1041,  1.0428,  0.0763,  ...,  0.0432, -0.0697,  0.6175],
          [ 0.0869,  0.5114,  0.0098,  ...,  0.3660, -0.1099,  0.6670]],

         [[-0.5023,  0.6145,  0.5010,  ...,  0.0147,  0.0908,  1.0075],
          [-0.1469,  0.3945, -0.0209,  ...,  0.3883, -0.0209,  1.1917]]]])
DESIRED: (shape=torch.Size([8, 2, 2, 16]), dtype=torch.float32)
tensor([[[[ 0.0805, -0.4033,  0.2454,  ...,  0.5555,  0.2174,  0.8380],
          [ 0.3606, -0.4141,  0.3010,  ...,  0.5496,  0.6070,  0.9036]],

         [[ 0.0236, -0.4791,  0.0212,  ...,  0.6485,  0.8070,  0.6257],
          [ 0.1903, -0.2455, -0.0142,  ...,  0.8698,  0.7841,  0.7268]]],


        [[[ 0.0036,  0.1637,  0.0912,  ...,  0.4605,  0.1517,  1.0609],
          [-0.0352,  0.1541,  0.5918,  ...,  1.0393,  0.7183,  1.6231]],

         [[-0.0263,  0.1233, -0.0113,  ...,  0.3874,  0.1813,  1.0736],
          [-0.0201,  0.1842,  0.5159,  ...,  0.1559,  0.1385,  0.1402]]],


        [[[-0.3174,  0.0132,  0.1445,  ...,  0.1652,  0.3064,  1.1831],
          [-0.1681, -0.4393, -0.5705,  ...,  0.2480,  0.4850,  1.2144]],

         [[-0.1884, -0.1962,  0.1185,  ...,  0.9046,  0.8535,  1.1614],
          [-0.5783, -0.1299, -0.4390,  ...,  0.9311,  0.8048,  0.5416]]],


        ...,


        [[[ 0.0045, -0.4820,  0.0695,  ...,  0.1505,  0.8843,  0.3375],
          [-0.1812, -0.4162,  0.5803,  ...,  0.1826,  0.6090,  0.4838]],

         [[ 0.1333, -0.0286, -0.1750,  ...,  0.2619,  0.9127,  0.4164],
          [-0.2370, -0.2738,  0.0087,  ...,  0.1199,  0.8054,  0.1222]]],


        [[[ 0.1076, -0.1360, -0.2638,  ...,  0.3996,  1.0187,  0.4833],
          [ 0.4754, -0.2612, -0.4908,  ...,  0.2700,  0.6525,  0.2871]],

         [[ 0.0616, -0.2178, -0.2667,  ...,  0.0238,  0.8296,  0.3352],
          [ 0.4013, -0.2380, -0.4826,  ...,  0.1235,  0.9623,  0.8008]]],


        [[[ 0.0798,  0.3897,  0.0305,  ...,  0.5819,  0.2960,  0.7541],
          [ 0.1072, -0.1781,  0.0595,  ...,  0.7391,  0.2883,  0.9820]],

         [[-0.5109,  0.1935,  0.5429,  ...,  0.2594,  0.8742,  0.4677],
          [-0.2457,  0.1928,  0.0624,  ...,  0.7121,  0.7013,  0.6089]]]])

2025-07-08 18:10:02.157137 GPU 3 62331 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 134217728],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
One of the differentiated Tensors does not require grad
[accuracy error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 134217728],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 489 / 512 (95.5%)
Greatest absolute difference: 1.6650176048278809 at index (4, 0, 0, 12) (up to 0.01 allowed)
Greatest relative difference: 335.5147705078125 at index (7, 1, 0, 7) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([8, 2, 2, 16]), dtype=torch.float32)
tensor([[[[-0.0625,  0.7289,  0.4094,  ...,  0.4536,  0.2480,  0.8859],
          [-0.0834,  0.8701,  0.1011,  ...,  1.0282,  0.5842,  0.6176]],

         [[-0.0309,  0.7884,  0.0743,  ...,  0.8768,  0.3067,  0.7719],
          [-0.0988,  0.9719,  0.3390,  ...,  0.7765,  0.2678,  1.0086]]],


        [[[-0.4769,  0.2561, -0.3153,  ...,  0.2975,  0.1920,  0.5671],
          [ 0.0082,  0.8444, -0.3622,  ...,  0.4282,  0.0285,  0.2185]],

         [[-0.2683,  0.3501,  0.2031,  ...,  0.3878, -0.0579,  0.5012],
          [ 0.3930,  0.8174,  0.4380,  ...,  0.3457,  0.1855,  0.9326]]],


        [[[ 0.5729,  0.7002,  0.0574,  ...,  0.3865,  0.3272,  0.7807],
          [-0.0354,  0.8063, -0.0546,  ...,  0.5021,  0.1080,  0.7975]],

         [[ 0.6696,  0.6236,  0.1035,  ...,  0.2923,  0.1274,  0.4788],
          [ 0.1507,  0.2960,  0.0840,  ...,  0.0437,  0.0965,  0.5724]]],


        ...,


        [[[-0.1148,  0.5581,  0.3847,  ...,  0.2432,  0.1834,  0.3398],
          [-0.2484,  0.6187,  0.0243,  ...,  0.7562,  0.3249,  0.1525]],

         [[-0.1376,  0.3413, -0.0030,  ...,  0.8710,  0.2508,  0.1331],
          [-0.1828,  0.6851,  0.1245,  ...,  0.9683,  0.2117,  0.1197]]],


        [[[ 0.2211,  0.5719,  0.2214,  ...,  0.4671, -0.3086,  1.1907],
          [ 0.0876,  0.0223, -0.2912,  ...,  0.1326,  0.1334,  1.2002]],

         [[ 0.4388,  0.3253,  0.0963,  ...,  0.1869, -0.0573,  1.1216],
          [ 0.5952,  0.2068, -0.2038,  ...,  0.1288, -0.1759,  1.2241]]],


        [[[-0.2817,  0.4215,  0.0051,  ...,  1.0381,  0.3993,  0.5049],
          [ 0.1246,  0.3912, -0.0266,  ...,  0.8541, -0.4170,  0.8063]],

         [[ 0.2529,  0.2440, -0.1355,  ...,  0.6965, -0.1207,  0.6642],
          [ 0.0561,  0.3685, -0.0810,  ...,  1.0172, -0.0731,  0.8162]]]])
DESIRED: (shape=torch.Size([8, 2, 2, 16]), dtype=torch.float32)
tensor([[[[-2.8106e-01, -1.3234e-01,  1.6087e-02,  ...,  5.8647e-01,  5.5865e-01,  4.0738e-01],
          [ 1.3326e-01,  2.1143e-01,  6.0996e-01,  ...,  2.9745e-01,  1.8229e-01,  1.7137e-01]],

         [[-1.6092e-01,  5.5184e-02, -2.1768e-01,  ...,  4.4762e-01,  4.8414e-01,  2.7730e-01],
          [ 7.3974e-02, -1.2708e-01, -1.4666e-01,  ...,  7.0865e-01,  5.4666e-01,  4.0859e-01]]],


        [[[-9.3572e-02, -6.0381e-02, -6.7556e-02,  ...,  6.6847e-01,  3.2077e-01,  7.0314e-01],
          [ 1.0854e-01, -3.0382e-01, -4.7876e-01,  ...,  7.2226e-01,  3.1076e-01,  2.5260e-01]],

         [[ 7.8939e-02, -6.5999e-01,  2.0841e-01,  ...,  7.0138e-01,  5.0132e-01,  7.6733e-01],
          [ 4.9654e-01, -4.7668e-01,  6.1437e-02,  ...,  1.4951e+00,  4.1625e-01,  9.9888e-01]]],


        [[[ 6.0090e-01,  2.0596e-01,  1.8846e-01,  ...,  7.8323e-01,  6.1775e-01,  3.5071e-01],
          [-3.3543e-02,  6.2949e-01, -7.1869e-02,  ...,  8.0671e-01,  4.0354e-01,  9.5826e-01]],

         [[ 6.4090e-01,  2.2876e-01,  5.8225e-02,  ...,  9.7228e-01,  3.0512e-01,  9.1349e-01],
          [ 1.2338e-01,  1.2780e-01,  2.1003e-02,  ...,  7.7601e-01,  3.8645e-01,  4.4191e-01]]],


        ...,


        [[[-4.7790e-02,  5.1026e-01,  1.0916e-01,  ...,  9.4134e-02,  6.8695e-01,  9.2819e-01],
          [-3.1045e-01,  5.9875e-01, -2.8983e-01,  ...,  1.7619e-01,  7.0259e-01,  7.9056e-01]],

         [[-2.2616e-01,  3.1364e-01,  2.8300e-01,  ...,  3.7430e-01,  6.0169e-01,  4.2683e-01],
          [-9.3705e-02,  6.2227e-01,  3.6630e-02,  ...,  6.7691e-01,  4.6368e-01,  4.0643e-01]]],


        [[[ 3.8985e-01,  4.9508e-01,  2.0650e-01,  ...,  5.8765e-01,  2.8560e-01,  8.3125e-01],
          [-8.2615e-02,  1.0546e-02,  1.8332e-02,  ...,  2.2211e-01,  6.1742e-01,  2.8843e-01]],

         [[ 3.6002e-01,  2.4339e-01,  2.6568e-01,  ...,  3.4002e-01,  4.8916e-01,  6.6932e-01],
          [ 6.1554e-01,  1.1023e-01,  1.1654e-01,  ...,  2.7094e-01,  4.8937e-01,  6.8014e-01]]],


        [[[-7.9345e-02,  2.7554e-01,  8.7290e-03,  ...,  3.3774e-01,  5.7511e-01,  2.7416e-01],
          [-1.4312e-03, -3.9110e-01,  6.1208e-02,  ...,  7.1419e-01,  1.0200e-01,  8.6426e-01]],

         [[-1.1317e-01, -2.7965e-01, -2.2835e-01,  ...,  4.6916e-01,  2.6530e-01,  5.7671e-01],
          [-2.3107e-02, -1.6459e-01, -7.3266e-03,  ...,  1.1370e-01,  3.8387e-01,  6.3484e-01]]]])

2025-07-08 18:10:13.490150 GPU 6 62504 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), None, Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), None, Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969479 (unix time) try "date -d @1751969479" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf428) received by PID 62504 (TID 0x7fa9f3d1d740) from PID 62504 ***]


2025-07-08 18:10:36.974332 GPU 5 61522 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), None, Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), None, Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969482 (unix time) try "date -d @1751969482" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf052) received by PID 61522 (TID 0x7f591a7da740) from PID 61522 ***]


2025-07-08 18:10:44.434002 GPU 7 62693 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969506 (unix time) try "date -d @1751969506" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf4e5) received by PID 62693 (TID 0x7f6f9b5ec740) from PID 62693 ***]


2025-07-08 18:11:25.041585 GPU 2 62112 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969529 (unix time) try "date -d @1751969529" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf2a0) received by PID 62112 (TID 0x7f68050b4740) from PID 62112 ***]


2025-07-08 18:11:27.381378 GPU 3 62331 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969542 (unix time) try "date -d @1751969542" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf37b) received by PID 62331 (TID 0x7f148eb50740) from PID 62331 ***]


2025-07-08 18:11:27.410012 GPU 5 63116 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969623 (unix time) try "date -d @1751969623" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf68c) received by PID 63116 (TID 0x7fe0c2316740) from PID 63116 ***]


2025-07-08 18:11:56.426370 GPU 7 63367 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), None, Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), None, Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969581 (unix time) try "date -d @1751969581" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf787) received by PID 63367 (TID 0x7f47c2776740) from PID 63367 ***]


2025-07-08 18:11:56.437316 GPU 4 63303 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), None, Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), None, Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969570 (unix time) try "date -d @1751969570" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf747) received by PID 63303 (TID 0x7fc860a0b740) from PID 63303 ***]


2025-07-08 18:11:59.162897 GPU 6 63477 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), None, Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), None, Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969582 (unix time) try "date -d @1751969582" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf7f5) received by PID 63477 (TID 0x7fdc29173740) from PID 63477 ***]


2025-07-08 18:12:31.565435 GPU 3 63813 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), None, Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), None, Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969605 (unix time) try "date -d @1751969605" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf945) received by PID 63813 (TID 0x7f65d3376740) from PID 63813 ***]


2025-07-08 18:12:52.493052 GPU 2 64214 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969629 (unix time) try "date -d @1751969629" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfad6) received by PID 64214 (TID 0x7f2674ab2740) from PID 64214 ***]


2025-07-08 18:13:08.807416 GPU 6 64386 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969654 (unix time) try "date -d @1751969654" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfb82) received by PID 64386 (TID 0x7fb8e9a78740) from PID 64386 ***]


2025-07-08 18:13:08.809795 GPU 7 64450 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969728 (unix time) try "date -d @1751969728" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfbc2) received by PID 64450 (TID 0x7fe902c56740) from PID 64450 ***]


2025-07-08 18:13:35.158586 GPU 4 64796 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969769 (unix time) try "date -d @1751969769" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfd1c) received by PID 64796 (TID 0x7ff83c984740) from PID 64796 ***]


2025-07-08 18:13:35.162708 GPU 3 64732 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969672 (unix time) try "date -d @1751969672" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfcdc) received by PID 64732 (TID 0x7f3b659a2740) from PID 64732 ***]


2025-07-08 18:13:50.567680 GPU 5 65062 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969688 (unix time) try "date -d @1751969688" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfe26) received by PID 65062 (TID 0x7fd09eea3740) from PID 65062 ***]


2025-07-08 18:14:24.425298 GPU 6 65259 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969792 (unix time) try "date -d @1751969792" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfeeb) received by PID 65259 (TID 0x7fb9b41e2740) from PID 65259 ***]


2025-07-08 18:14:32.901134 GPU 2 65425 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 8912897, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969818 (unix time) try "date -d @1751969818" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xff91) received by PID 65425 (TID 0x7fa16a3b1740) from PID 65425 ***]


2025-07-08 18:14:39.116229 GPU 3 65589 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969752 (unix time) try "date -d @1751969752" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10035) received by PID 65589 (TID 0x7f4a8b57d740) from PID 65589 ***]


2025-07-08 18:15:32.432113 GPU 5 66022 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969792 (unix time) try "date -d @1751969792" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x101e6) received by PID 66022 (TID 0x7f908e0b7740) from PID 66022 ***]


2025-07-08 18:15:59.625959 GPU 3 66209 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969892 (unix time) try "date -d @1751969892" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x102a1) received by PID 66209 (TID 0x7fc11c712740) from PID 66209 ***]


2025-07-08 18:16:13.439499 GPU 7 66382 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969908 (unix time) try "date -d @1751969908" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1034e) received by PID 66382 (TID 0x7f469b6d6740) from PID 66382 ***]


2025-07-08 18:16:16.652300 GPU 4 66476 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969837 (unix time) try "date -d @1751969837" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x103ac) received by PID 66476 (TID 0x7f6a62de5740) from PID 66476 ***]


2025-07-08 18:16:39.890385 GPU 6 66727 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969855 (unix time) try "date -d @1751969855" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x104a7) received by PID 66727 (TID 0x7ff2a8a5d740) from PID 66727 ***]


2025-07-08 18:17:07.803637 GPU 2 67145 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969962 (unix time) try "date -d @1751969962" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10649) received by PID 67145 (TID 0x7ff31cf7c740) from PID 67145 ***]


2025-07-08 18:17:19.062418 GPU 5 67316 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969977 (unix time) try "date -d @1751969977" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x106f4) received by PID 67316 (TID 0x7fd9a4bef740) from PID 67316 ***]


2025-07-08 18:17:24.314019 GPU 4 67481 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969907 (unix time) try "date -d @1751969907" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10799) received by PID 67481 (TID 0x7fbef91a0740) from PID 67481 ***]


2025-07-08 18:18:15.890137 GPU 6 67696 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
outputs format not support
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751969957 (unix time) try "date -d @1751969957" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10870) received by PID 67696 (TID 0x7f66ccf33740) from PID 67696 ***]


2025-07-08 18:18:35.374601 GPU 4 67874 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751970062 (unix time) try "date -d @1751970062" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10922) received by PID 67874 (TID 0x7f7d8e1d8740) from PID 67874 ***]


2025-07-08 18:18:35.412257 GPU 7 67939 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
[cuda error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8912897, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751970050 (unix time) try "date -d @1751970050" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10963) received by PID 67939 (TID 0x7f80a8819740) from PID 67939 ***]


2025-07-08 18:19:29.312626 GPU 2 68599 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 138547333],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(716), misaligned address. 
  [Hint: 'cudaErrorMisalignedAddress'. The device encountered a load or store instruction on a memory address which is not aligned. This leaves the process in aninconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminatedand relaunched.] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_variable_length_memory_efficient_attention(_object*, _object*, _object*)
1   variable_length_memory_efficient_attention_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
2   paddle::experimental::variable_length_memory_efficient_attention(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
3   void phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)
4   void phi::dispatch_cutlass_forward<phi::dtype::float16, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::GPUContext const&, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
5   void phi::dispatch_cutlass_forward_f16_sm80<phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
6   phi::fmha_cutlassF_variable_f16_aligned_32x128_urf_sm_mua_sm80(cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, false, 32, 128, false, (cutlass::gemm::kernel::GroupScheduleMode)0, true>, phi::Params&, phi::GPUContext const&)
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751970086 (unix time) try "date -d @1751970086" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10bf7) received by PID 68599 (TID 0x7fa8510c0740) from PID 68599 ***]


2025-07-08 18:21:37.338412 GPU 7 69572 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([2, 8, 112, 96],"float16"), Tensor([2, 8, 240, 96],"float16"), Tensor([2, 8, 240, 1118482],"float16"), Tensor([2, 1],"int32"), Tensor([2, 1],"int32"), mask=Tensor([2, 1, 4096, 4096],"float16"), scale=0.10206207261596575, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(716), misaligned address. 
  [Hint: 'cudaErrorMisalignedAddress'. The device encountered a load or store instruction on a memory address which is not aligned. This leaves the process in aninconsistent state and any further CUDA work will return the same error. To continue using CUDA, the process must be terminatedand relaunched.] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_variable_length_memory_efficient_attention(_object*, _object*, _object*)
1   variable_length_memory_efficient_attention_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
2   paddle::experimental::variable_length_memory_efficient_attention(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
3   void phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)
4   void phi::dispatch_cutlass_forward<phi::dtype::float16, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::GPUContext const&, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
5   void phi::dispatch_cutlass_forward_f16_sm80<phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
6   phi::fmha_cutlassF_variable_f16_aligned_32x128_urf_sm_ma_sm80(cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, true, 32, 128, false, (cutlass::gemm::kernel::GroupScheduleMode)0, true>, phi::Params&, phi::GPUContext const&)
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751970199 (unix time) try "date -d @1751970199" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10fc4) received by PID 69572 (TID 0x7f2199469740) from PID 69572 ***]


2025-07-08 18:34:33.841140 GPU 3 70905 test begin: paddle.kthvalue(Tensor([4294967295],"float32"), k=2, )
element 1 of tensors does not require grad and does not have a grad_fn
[accuracy error] paddle.kthvalue(Tensor([4294967295],"float32"), k=2, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Scalars are not close!

Expected -1.8997858841629522e-10 but got -0.5.
Absolute difference: 0.4999999998100214 (up to 0.01 allowed)
Relative difference: 2631875539.123305 (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([]), dtype=torch.float32)
-0.5
DESIRED: (shape=torch.Size([]), dtype=torch.float32)
-1.8997858841629522e-10

2025-07-08 18:37:25.062926 GPU 4 69321 test begin: paddle.linalg.triangular_solve(Tensor([1073741824, 2, 2],"float32"), Tensor([1, 2, 1],"float32"), upper=False, )
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::ThrowExceptionToPython(std::__exception_ptr::exception_ptr)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1751971085 (unix time) try "date -d @1751971085" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10ec9) received by PID 69321 (TID 0x7f0a7d9a7740) from PID 69321 ***]


2025-07-08 18:39:09.728661 GPU 2 72062 test begin: paddle.logcumsumexp(Tensor([10, 429496730],"float16"), dtype="float16", axis=1, )
"logcumsumexp_backward" not implemented for 'Half'
[accuracy error] paddle.logcumsumexp(Tensor([10, 429496730],"float16"), dtype="float16", axis=1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4294342548 / 4294967300 (100.0%)
Greatest absolute difference: 8.1171875 at index (0, 428687808) (up to 0.01 allowed)
Greatest relative difference: 0.68310546875 at index (0, 428687808) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([10, 429496730]), dtype=torch.float16)
tensor([[ 0.3359,  0.7832,  1.1025,  ..., 20.0000, 20.0000, 20.0000],
        [ 0.1409,  0.7109,  1.0156,  ..., 20.0000, 20.0000, 20.0000],
        [-0.0525,  0.7661,  1.0703,  ..., 20.0000, 20.0000, 20.0000],
        ...,
        [ 0.0587,  0.9004,  1.2217,  ..., 20.0000, 20.0000, 20.0000],
        [ 0.4883,  0.9458,  1.3008,  ..., 20.0000, 20.0000, 20.0000],
        [-0.1281,  0.7700,  1.0820,  ..., 20.0000, 20.0000, 20.0000]], dtype=torch.float16)
DESIRED: (shape=torch.Size([10, 429496730]), dtype=torch.float16)
tensor([[ 0.3359,  0.7832,  1.1025,  ..., 11.8828, 11.8828, 11.8828],
        [ 0.1409,  0.7109,  1.0156,  ..., 11.8828, 11.8828, 11.8828],
        [-0.0525,  0.7661,  1.0703,  ..., 11.8828, 11.8828, 11.8828],
        ...,
        [ 0.0587,  0.9004,  1.2227,  ..., 11.8828, 11.8828, 11.8828],
        [ 0.4883,  0.9458,  1.3008,  ..., 11.8828, 11.8828, 11.8828],
        [-0.1281,  0.7700,  1.0830,  ..., 11.8828, 11.8828, 11.8828]], dtype=torch.float16)

2025-07-08 18:39:20.817162 GPU 6 72441 test begin: paddle.logcumsumexp(Tensor([10, 429496730],"float16"), dtype="float16", axis=None, )
"logcumsumexp_backward" not implemented for 'Half'
[accuracy error] paddle.logcumsumexp(Tensor([10, 429496730],"float16"), dtype="float16", axis=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4272987060 / 4294967300 (99.5%)
Greatest absolute difference: 2.453125 at index (512396288,) (up to 0.01 allowed)
Greatest relative difference: 0.138427734375 at index (512396288,) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([4294967300]), dtype=torch.float16)
tensor([ 0.1598,  0.6943,  1.0869,  ..., 20.1875, 20.1875, 20.1875], dtype=torch.float16)
DESIRED: (shape=torch.Size([4294967300]), dtype=torch.float16)
tensor([ 0.1598,  0.6943,  1.0869,  ..., 17.9062, 17.9062, 17.9062], dtype=torch.float16)

2025-07-08 18:41:44.039714 GPU 2 72062 test begin: paddle.logcumsumexp(Tensor([357913942, 12],"float16"), dtype="float16", axis=None, )
"logcumsumexp_backward" not implemented for 'Half'
[accuracy error] paddle.logcumsumexp(Tensor([357913942, 12],"float16"), dtype="float16", axis=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4260451176 / 4294967304 (99.2%)
Greatest absolute difference: 2.453125 at index (513117184,) (up to 0.01 allowed)
Greatest relative difference: 0.138427734375 at index (513117184,) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([4294967304]), dtype=torch.float16)
tensor([ 0.3359,  0.7832,  1.1025,  ..., 20.1875, 20.1875, 20.1875], dtype=torch.float16)
DESIRED: (shape=torch.Size([4294967304]), dtype=torch.float16)
tensor([ 0.3359,  0.7832,  1.1025,  ..., 17.9688, 17.9688, 17.9688], dtype=torch.float16)

2025-07-08 19:02:56.175014 GPU 6 79269 test begin: paddle.matmul(x=Tensor([4, 1073741825],"float16"), y=Tensor([1073741825],"float16"), transpose_x=False, transpose_y=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 19:58:48.718310 GPU 4 101035 test begin: paddle.nanmedian(Tensor([2, 107374183, 4, 5],"float32"), axis=list[0,-1,], keepdim=False, mode="min", )
[cuda error] paddle.nanmedian(Tensor([2, 107374183, 4, 5],"float32"), axis=list[0,-1,], keepdim=False, mode="min", ) 
 (External) CUDA error(1), invalid argument. 
  [Hint: 'cudaErrorInvalidValue'. This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 19:59:52.726628 GPU 2 99739 test begin: paddle.nanmedian(Tensor([2, 107374183, 4, 5],"float32"), axis=list[0,3,], keepdim=False, mode="min", )
[cuda error] paddle.nanmedian(Tensor([2, 107374183, 4, 5],"float32"), axis=list[0,3,], keepdim=False, mode="min", ) 
 (External) CUDA error(1), invalid argument. 
  [Hint: 'cudaErrorInvalidValue'. This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 20:00:28.657340 GPU 4 101035 test begin: paddle.nanmedian(Tensor([2, 3, 143165577, 5],"float32"), axis=list[0,-1,], keepdim=False, mode="min", )
[cuda error] paddle.nanmedian(Tensor([2, 3, 143165577, 5],"float32"), axis=list[0,-1,], keepdim=False, mode="min", ) 
 (External) CUDA error(1), invalid argument. 
  [Hint: 'cudaErrorInvalidValue'. This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 20:00:57.426721 GPU 4 101035 test begin: paddle.nanmedian(Tensor([2, 3, 143165577, 5],"float32"), axis=list[0,1,3,], keepdim=False, mode="min", )
[cuda error] paddle.nanmedian(Tensor([2, 3, 143165577, 5],"float32"), axis=list[0,1,3,], keepdim=False, mode="min", ) 
 (External) CUDA error(1), invalid argument. 
  [Hint: 'cudaErrorInvalidValue'. This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 20:01:25.858656 GPU 4 101035 test begin: paddle.nanmedian(Tensor([2, 3, 143165577, 5],"float32"), axis=list[0,3,], keepdim=False, mode="min", )
[cuda error] paddle.nanmedian(Tensor([2, 3, 143165577, 5],"float32"), axis=list[0,3,], keepdim=False, mode="min", ) 
 (External) CUDA error(1), invalid argument. 
  [Hint: 'cudaErrorInvalidValue'. This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 20:02:37.683364 GPU 5 102531 test begin: paddle.nanmedian(Tensor([2, 3, 4, 178956971],"float32"), axis=tuple(1,2,), keepdim=False, mode="min", )
[cuda error] paddle.nanmedian(Tensor([2, 3, 4, 178956971],"float32"), axis=tuple(1,2,), keepdim=False, mode="min", ) 
 (External) CUDA error(1), invalid argument. 
  [Hint: 'cudaErrorInvalidValue'. This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 20:02:43.585531 GPU 3 102047 test begin: paddle.nanmedian(Tensor([42949673, 100],"float32"), axis=1, mode="min", )
element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nanmedian(Tensor([42949673, 100],"float32"), axis=1, mode="min", ) 
 (External) CUDA error(1), invalid argument. 
  [Hint: 'cudaErrorInvalidValue'. This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 20:03:51.626770 GPU 3 102047 test begin: paddle.nanmedian(Tensor([71582789, 3, 4, 5],"float32"), axis=tuple(1,2,), keepdim=False, mode="min", )
[cuda error] paddle.nanmedian(Tensor([71582789, 3, 4, 5],"float32"), axis=tuple(1,2,), keepdim=False, mode="min", ) 
 (External) CUDA error(1), invalid argument. 
  [Hint: 'cudaErrorInvalidValue'. This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 20:04:16.551110 GPU 3 102047 test begin: paddle.nanmedian(Tensor([71582789, 3, 4, 5],"float32"), axis=tuple(1,2,3,), keepdim=False, mode="min", )
[cuda error] paddle.nanmedian(Tensor([71582789, 3, 4, 5],"float32"), axis=tuple(1,2,3,), keepdim=False, mode="min", ) 
 (External) CUDA error(1), invalid argument. 
  [Hint: 'cudaErrorInvalidValue'. This indicates that one or more of the parameters passed to the API call is not within an acceptable range of values. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 20:09:01.375093 GPU 4 102699 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 1048576, 32],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:09:13.319243 GPU 4 104608 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 1048576, 32],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:09:16.378304 GPU 7 103290 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 1048576, 32],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:09:40.116153 GPU 2 103737 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 32, 1048576],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:09:53.609644 GPU 2 105187 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 32, 1048576],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:10:02.638827 GPU 5 102531 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 128, 32, 1048576],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:11:58.131035 GPU 7 104768 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 16, 1677722],"float32"), output_size=2, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:12:11.737406 GPU 7 106203 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 16, 1677722],"float32"), output_size=4, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:13:34.837026 GPU 4 105514 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 838861, 32],"float32"), output_size=2, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:13:48.364574 GPU 4 106800 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 160, 838861, 32],"float32"), output_size=4, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:15:37.284097 GPU 3 102047 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 262144, 128, 128],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:15:50.763869 GPU 3 107385 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 262144, 128, 128],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:16:04.546035 GPU 2 105683 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 262144, 128, 128],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:16:51.505669 GPU 5 105348 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 4194304, 32, 32],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:16:56.742591 GPU 6 103118 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 4194304, 32, 32],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:17:04.769797 GPU 5 107829 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 4194304, 32, 32],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:17:14.837084 GPU 3 108146 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 128, 65536],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:17:41.614754 GPU 7 106395 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 128, 65536],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:17:53.432132 GPU 7 108555 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 128, 65536],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:17:59.014155 GPU 4 106979 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 2, 4194304],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:18:12.272563 GPU 4 108723 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 4194304, 2],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:18:25.459434 GPU 5 108885 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 65536, 128],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:18:34.741095 GPU 3 109046 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 65536, 128],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:18:57.437882 GPU 2 107548 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 512, 65536, 128],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:19:26.915829 GPU 7 109381 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 8388608, 16, 32],"float32"), output_size=2, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:19:33.721598 GPU 4 109785 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([1, 8388608, 16, 32],"float32"), output_size=4, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:20:42.890526 GPU 7 110279 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 4, 8192],"float16"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:20:52.907655 GPU 4 110439 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 4, 8192],"float32"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:21:47.557292 GPU 6 107987 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 410, 80],"float16"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:21:47.856303 GPU 2 109218 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 410, 80],"float32"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:21:59.979904 GPU 2 110878 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 104858, 4, 80],"float16"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:22:15.114076 GPU 7 111041 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 104858, 4, 80],"float32"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:22:20.140329 GPU 3 110110 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 128, 28, 9363],"float16"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:22:25.259525 GPU 4 111201 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 128, 28, 9363],"float32"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:22:34.414077 GPU 5 109946 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 128, 9363, 28],"float16"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:23:26.313478 GPU 6 111370 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 128, 9363, 28],"float32"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:23:42.503382 GPU 7 111771 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 171197, 14, 14],"float16"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:23:47.238638 GPU 2 111931 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 171197, 14, 14],"float32"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:23:47.576580 GPU 4 112012 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 320, 14, 7490],"float16"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:23:55.705764 GPU 3 112246 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 320, 14, 7490],"float32"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:24:02.732748 GPU 5 112405 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 320, 7490, 14],"float16"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:24:45.436651 GPU 6 112572 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 320, 7490, 14],"float32"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:25:13.261128 GPU 7 112739 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 42800, 28, 28],"float16"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:25:15.336937 GPU 2 112827 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 42800, 28, 28],"float32"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:25:23.730522 GPU 3 113058 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 512, 7, 9363],"float16"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:25:26.657765 GPU 4 113147 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 512, 9363, 7],"float16"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:26:33.766870 GPU 2 113942 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([13, 105352, 56, 56],"float32"), output_size=list[7,7,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:26:55.518620 GPU 7 114106 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([13, 256, 23046, 56],"float32"), output_size=list[7,7,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:27:00.978157 GPU 3 114265 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([13, 256, 56, 23046],"float32"), output_size=list[7,7,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:27:11.925826 GPU 4 114428 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([13108, 1024, 4, 80],"float16"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:27:12.050946 GPU 5 114493 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([13108, 1024, 4, 80],"float32"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:27:55.951241 GPU 2 115077 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([15521779, 3, 7, 7],"float32"), output_size=5, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:28:17.445501 GPU 7 115242 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([15521779, 3, 7, 7],"float32"), output_size=list[2,5,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:29:21.749949 GPU 2 115889 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([192, 480, 4, 11651],"float16"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:29:59.900793 GPU 7 116292 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([192, 480, 583, 80],"float16"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:30:02.422556 GPU 3 116380 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([192, 69906, 4, 80],"float16"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:31:47.408110 GPU 5 115491 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 102261127, 7],"float32"), list[2,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:31:48.588084 GPU 3 117201 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 102261127, 7],"float32"), output_size=5, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:32:14.327007 GPU 4 115723 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 102261127, 7],"float32"), output_size=list[2,5,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:32:59.971099 GPU 7 117376 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 102261127, 7],"float32"), output_size=list[3,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:33:40.379841 GPU 2 116638 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 178956971, 4],"float32"), output_size=list[3,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:33:58.708228 GPU 4 118102 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 4, 178956971],"float32"), output_size=list[3,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:34:31.633744 GPU 3 117544 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 7, 102261127],"float32"), list[2,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:34:42.685930 GPU 7 118272 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 7, 102261127],"float32"), output_size=5, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:34:47.951119 GPU 5 118430 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 7, 102261127],"float32"), output_size=list[2,5,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:34:54.780997 GPU 3 118589 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 7, 102261127],"float32"), output_size=list[3,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:35:36.691893 GPU 4 119150 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 3, 7, 102261127],"float32"), output_size=list[None,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:38:50.688282 GPU 2 118754 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 43826197, 7, 7],"float32"), list[2,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:39:32.520334 GPU 4 119808 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 43826197, 7, 7],"float32"), output_size=list[2,5,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:40:17.099327 GPU 2 120651 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 43826197, 7, 7],"float32"), output_size=list[3,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:40:36.935588 GPU 5 119321 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 43826197, 7, 7],"float32"), output_size=list[None,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:40:48.679796 GPU 4 120822 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 512, 599187, 7],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:41:25.069236 GPU 7 119402 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 512, 7, 599187],"float32"), output_size=tuple(7,7,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:41:36.272944 GPU 7 121312 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 67108865, 4, 4],"float64"), output_size=list[1,4,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:41:54.952505 GPU 5 121543 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 67108865, 4, 4],"float64"), output_size=list[2,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:41:58.277363 GPU 3 121695 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2, 67108865, 4, 4],"float64"), output_size=list[3,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:46:51.276612 GPU 2 121225 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([27963, 480, 4, 80],"float16"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:46:54.630098 GPU 3 122453 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([27963, 480, 4, 80],"float32"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:47:00.258698 GPU 4 121861 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([29217465, 3, 7, 7],"float32"), list[2,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:47:07.110808 GPU 3 123229 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([29217465, 3, 7, 7],"float32"), output_size=list[2,5,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:47:14.185187 GPU 4 123387 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([29217465, 3, 7, 7],"float32"), output_size=list[3,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:48:24.479479 GPU 3 123954 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([29217465, 3, 7, 7],"float32"), output_size=list[None,3,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:48:31.269996 GPU 4 124113 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([32768, 128, 32, 32],"float16"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:48:42.428370 GPU 7 122291 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([32768, 128, 32, 32],"float16"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:48:49.537617 GPU 7 124274 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([32768, 128, 32, 32],"float16"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:49:30.748862 GPU 2 123794 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([32768, 128, 32, 32],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:49:39.614102 GPU 2 124679 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([32768, 128, 32, 32],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:49:48.384084 GPU 3 124839 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([32768, 128, 32, 32],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:50:17.647202 GPU 7 125162 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 1024, 13108, 80],"float32"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:50:27.181811 GPU 5 125322 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 1024, 4, 262144],"float32"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:51:08.375663 GPU 2 125490 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 1048576, 32, 32],"float16"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:51:13.891074 GPU 3 125649 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 1048576, 32, 32],"float16"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:51:23.353399 GPU 4 125809 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 1048576, 32, 32],"float16"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:51:33.649897 GPU 7 126205 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 1048576, 32, 32],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:51:55.845367 GPU 5 126368 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 1048576, 32, 32],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:52:43.782434 GPU 3 126539 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 1048576, 32, 32],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:52:52.328732 GPU 4 126701 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 262144, 32],"float16"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:52:52.775081 GPU 2 126782 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 262144, 32],"float16"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:52:54.743722 GPU 7 126876 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 262144, 32],"float16"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:53:23.582079 GPU 5 127237 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 262144, 32],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:54:04.985207 GPU 3 127657 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 262144, 32],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:54:18.236241 GPU 2 127819 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 262144, 32],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:54:27.509106 GPU 7 127980 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 32, 262144],"float16"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:54:36.664496 GPU 4 128141 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 32, 262144],"float16"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:54:43.803095 GPU 5 128301 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 32, 262144],"float16"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:55:33.113773 GPU 2 128707 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 32, 262144],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:55:33.845971 GPU 3 128793 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 32, 262144],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:56:00.344906 GPU 7 129028 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 128, 32, 262144],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:56:12.673311 GPU 4 129190 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 160, 16, 419431],"float16"), output_size=2, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:56:24.613664 GPU 5 129351 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 160, 16, 419431],"float16"), output_size=4, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:56:47.443699 GPU 2 129517 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 160, 16, 419431],"float32"), output_size=2, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:56:48.134217 GPU 3 129602 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 160, 16, 419431],"float32"), output_size=4, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:57:18.991486 GPU 7 129841 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 160, 209716, 32],"float16"), output_size=4, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:57:43.259345 GPU 4 130327 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 160, 209716, 32],"float32"), output_size=2, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:58:14.348503 GPU 3 130653 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 2097152, 16, 32],"float16"), output_size=2, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:58:15.079978 GPU 2 130738 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 2097152, 16, 32],"float16"), output_size=4, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:58:48.119345 GPU 7 130976 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 2097152, 16, 32],"float32"), output_size=2, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:58:59.000590 GPU 4 131137 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 2097152, 16, 32],"float32"), output_size=4, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:59:03.135137 GPU 5 131296 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 3355444, 4, 80],"float32"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:59:53.629030 GPU 3 131632 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 480, 27963, 80],"float32"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 20:59:58.329771 GPU 2 131792 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 480, 4, 559241],"float32"), list[1,40,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:00:04.886941 GPU 7 131951 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 512, 128, 16384],"float16"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:00:30.679900 GPU 4 132184 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 512, 128, 16384],"float16"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:01:11.035918 GPU 3 132351 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 512, 128, 16384],"float16"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:01:23.701379 GPU 2 132514 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 512, 16384, 128],"float16"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:01:35.361511 GPU 5 132909 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 512, 16384, 128],"float16"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:01:53.066168 GPU 7 133072 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 512, 16384, 128],"float16"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:02:08.114988 GPU 4 133234 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 65536, 128, 128],"float16"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:02:40.656575 GPU 3 133399 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 65536, 128, 128],"float16"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:03:01.325493 GPU 2 133563 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([4, 65536, 128, 128],"float16"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:03:04.122687 GPU 5 133653 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([42800, 128, 28, 28],"float16"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:03:21.821372 GPU 7 133884 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([42800, 128, 28, 28],"float32"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:06:46.158006 GPU 5 134611 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 256, 1311, 25],"float16"), output_size=list[1,25,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:06:53.129435 GPU 5 135535 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 256, 1311, 25],"float32"), output_size=list[1,25,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:07:19.614133 GPU 4 135705 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 256, 2, 16384],"float16"), output_size=list[1,25,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:08:12.072852 GPU 7 134772 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 256, 2, 16384],"float32"), output_size=list[1,25,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:08:20.019228 GPU 3 134447 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 512, 128, 128],"float16"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:08:22.292067 GPU 5 136121 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 512, 128, 128],"float16"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:08:28.170081 GPU 3 136279 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 512, 128, 128],"float16"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:08:46.040309 GPU 4 136441 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 512, 128, 128],"float32"), output_size=tuple(2,2,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:08:58.220555 GPU 2 135373 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 512, 128, 128],"float32"), output_size=tuple(3,3,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:09:10.680406 GPU 2 136603 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([512, 512, 128, 128],"float32"), output_size=tuple(6,6,), data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:09:27.871190 GPU 7 136764 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([52429, 160, 16, 32],"float16"), output_size=2, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:09:52.206297 GPU 5 137164 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([52429, 160, 16, 32],"float16"), output_size=4, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:10:04.373228 GPU 3 137325 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([52429, 160, 16, 32],"float32"), output_size=2, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:10:22.140231 GPU 4 137488 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([52429, 160, 16, 32],"float32"), output_size=4, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:10:55.956929 GPU 7 137812 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([5350, 256, 56, 56],"float32"), output_size=list[7,7,], data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:11:36.291401 GPU 5 138373 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([68479, 320, 14, 14],"float16"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:11:52.462038 GPU 4 138538 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([68479, 320, 14, 14],"float32"), output_size=7, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:13:26.973753 GPU 4 139042 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([15521779, 3, 7, 7],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:13:38.717915 GPU 2 139454 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([15521779, 3, 7, 7],"float32"), output_size=list[2,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:14:21.418858 GPU 3 139627 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 102261127, 7],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:14:43.811495 GPU 7 138710 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 102261127, 7],"float32"), output_size=list[2,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:14:55.214913 GPU 2 139803 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 102261127, 7],"float32"), output_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:15:31.069647 GPU 5 138883 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 178956971, 4],"float32"), output_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:15:51.252395 GPU 3 140523 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 4, 178956971],"float32"), output_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:16:02.120156 GPU 5 140683 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 7, 102261127],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:16:27.347414 GPU 4 139963 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 7, 102261127],"float32"), output_size=list[2,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:16:28.935359 GPU 2 140848 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 7, 102261127],"float32"), output_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:16:52.275644 GPU 4 141169 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 3, 7, 102261127],"float32"), output_size=list[None,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:20:28.090749 GPU 7 142483 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 43826197, 7, 7],"float32"), output_size=list[2,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:20:33.840418 GPU 4 142059 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 43826197, 7, 7],"float32"), output_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:22:00.764588 GPU 2 141895 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 43826197, 7, 7],"float32"), output_size=list[None,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:22:25.988398 GPU 5 141732 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 67108865, 4, 4],"float64"), output_size=list[1,4,], )
[accuracy error] backward  paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 67108865, 4, 4],"float64"), output_size=list[1,4,], ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 32 / 2147483680 (0.0%)
Greatest absolute difference: 0.12156348827847643 at index (1, 67108863, 0, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 67108863, 0, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 67108865, 4, 4]), dtype=torch.float64)
tensor([[[[ 0.0303,  0.1244,  0.0773,  0.0864],
          [ 0.0303,  0.1244,  0.0773,  0.0864],
          [ 0.0303,  0.1244,  0.0773,  0.0864],
          [ 0.0303,  0.1244,  0.0773,  0.0864]],

         [[ 0.0153, -0.0517, -0.0949, -0.0996],
          [ 0.0153, -0.0517, -0.0949, -0.0996],
          [ 0.0153, -0.0517, -0.0949, -0.0996],
          [ 0.0153, -0.0517, -0.0949, -0.0996]],

         [[-0.1011, -0.0927, -0.1132, -0.0438],
          [-0.1011, -0.0927, -0.1132, -0.0438],
          [-0.1011, -0.0927, -0.1132, -0.0438],
          [-0.1011, -0.0927, -0.1132, -0.0438]],

         ...,

         [[ 0.0787, -0.0793,  0.0431,  0.0984],
          [ 0.0787, -0.0793,  0.0431,  0.0984],
          [ 0.0787, -0.0793,  0.0431,  0.0984],
          [ 0.0787, -0.0793,  0.0431,  0.0984]],

         [[ 0.1200,  0.1131,  0.1219, -0.1141],
          [ 0.1200,  0.1131,  0.1219, -0.1141],
          [ 0.1200,  0.1131,  0.1219, -0.1141],
          [ 0.1200,  0.1131,  0.1219, -0.1141]],

         [[-0.0353,  0.1149, -0.0846, -0.0951],
          [-0.0353,  0.1149, -0.0846, -0.0951],
          [-0.0353,  0.1149, -0.0846, -0.0951],
          [-0.0353,  0.1149, -0.0846, -0.0951]]],


        [[[-0.0060, -0.0303,  0.0361,  0.0592],
          [-0.0060, -0.0303,  0.0361,  0.0592],
          [-0.0060, -0.0303,  0.0361,  0.0592],
          [-0.0060, -0.0303,  0.0361,  0.0592]],

         [[ 0.1138, -0.0731,  0.0444,  0.0007],
          [ 0.1138, -0.0731,  0.0444,  0.0007],
          [ 0.1138, -0.0731,  0.0444,  0.0007],
          [ 0.1138, -0.0731,  0.0444,  0.0007]],

         [[ 0.0757,  0.0555, -0.1114,  0.0732],
          [ 0.0757,  0.0555, -0.1114,  0.0732],
          [ 0.0757,  0.0555, -0.1114,  0.0732],
          [ 0.0757,  0.0555, -0.1114,  0.0732]],

         ...,

         [[ 0.0133,  0.0598, -0.0140,  0.0665],
          [ 0.0133,  0.0598, -0.0140,  0.0665],
          [ 0.0133,  0.0598, -0.0140,  0.0665],
          [ 0.0133,  0.0598, -0.0140,  0.0665]],

         [[-0.1216, -0.1009,  0.0199,  0.0319],
          [-0.1216, -0.1009,  0.0199,  0.0319],
          [-0.1216, -0.1009,  0.0199,  0.0319],
          [-0.1216, -0.1009,  0.0199,  0.0319]],

         [[-0.1128, -0.0535, -0.0602,  0.0707],
          [-0.1128, -0.0535, -0.0602,  0.0707],
          [-0.1128, -0.0535, -0.0602,  0.0707],
          [-0.1128, -0.0535, -0.0602,  0.0707]]]], dtype=torch.float64)
DESIRED: (shape=torch.Size([2, 67108865, 4, 4]), dtype=torch.float64)
tensor([[[[ 0.0303,  0.1244,  0.0773,  0.0864],
          [ 0.0303,  0.1244,  0.0773,  0.0864],
          [ 0.0303,  0.1244,  0.0773,  0.0864],
          [ 0.0303,  0.1244,  0.0773,  0.0864]],

         [[ 0.0153, -0.0517, -0.0949, -0.0996],
          [ 0.0153, -0.0517, -0.0949, -0.0996],
          [ 0.0153, -0.0517, -0.0949, -0.0996],
          [ 0.0153, -0.0517, -0.0949, -0.0996]],

         [[-0.1011, -0.0927, -0.1132, -0.0438],
          [-0.1011, -0.0927, -0.1132, -0.0438],
          [-0.1011, -0.0927, -0.1132, -0.0438],
          [-0.1011, -0.0927, -0.1132, -0.0438]],

         ...,

         [[ 0.0787, -0.0793,  0.0431,  0.0984],
          [ 0.0787, -0.0793,  0.0431,  0.0984],
          [ 0.0787, -0.0793,  0.0431,  0.0984],
          [ 0.0787, -0.0793,  0.0431,  0.0984]],

         [[ 0.1200,  0.1131,  0.1219, -0.1141],
          [ 0.1200,  0.1131,  0.1219, -0.1141],
          [ 0.1200,  0.1131,  0.1219, -0.1141],
          [ 0.1200,  0.1131,  0.1219, -0.1141]],

         [[-0.0353,  0.1149, -0.0846, -0.0951],
          [-0.0353,  0.1149, -0.0846, -0.0951],
          [-0.0353,  0.1149, -0.0846, -0.0951],
          [-0.0353,  0.1149, -0.0846, -0.0951]]],


        [[[-0.0060, -0.0303,  0.0361,  0.0592],
          [-0.0060, -0.0303,  0.0361,  0.0592],
          [-0.0060, -0.0303,  0.0361,  0.0592],
          [-0.0060, -0.0303,  0.0361,  0.0592]],

         [[ 0.1138, -0.0731,  0.0444,  0.0007],
          [ 0.1138, -0.0731,  0.0444,  0.0007],
          [ 0.1138, -0.0731,  0.0444,  0.0007],
          [ 0.1138, -0.0731,  0.0444,  0.0007]],

         [[ 0.0757,  0.0555, -0.1114,  0.0732],
          [ 0.0757,  0.0555, -0.1114,  0.0732],
          [ 0.0757,  0.0555, -0.1114,  0.0732],
          [ 0.0757,  0.0555, -0.1114,  0.0732]],

         ...,

         [[ 0.0133,  0.0598, -0.0140,  0.0665],
          [ 0.0133,  0.0598, -0.0140,  0.0665],
          [ 0.0133,  0.0598, -0.0140,  0.0665],
          [ 0.0133,  0.0598, -0.0140,  0.0665]],

         [[ 0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000]]]], dtype=torch.float64)

2025-07-08 21:22:31.657878 GPU 3 141572 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 67108865, 4, 4],"float64"), output_size=list[2,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:23:29.609747 GPU 4 142907 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([2, 67108865, 4, 4],"float64"), output_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:24:57.216137 GPU 5 141732 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([29217465, 3, 7, 7],"float32"), output_size=list[2,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:25:18.868297 GPU 2 143487 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([29217465, 3, 7, 7],"float32"), output_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:25:54.663407 GPU 4 144075 test begin: paddle.nn.functional.adaptive_avg_pool2d(x=Tensor([29217465, 3, 7, 7],"float32"), output_size=list[None,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:26:33.691252 GPU 2 144555 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 3834793, 16, 7, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 3834793, 16, 7, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:26:53.389649 GPU 7 143078 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 3834793, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 3834793, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:27:23.526378 GPU 7 143078 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 4793491, 16, 7, 8],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 4793491, 16, 7, 8],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:27:26.128666 GPU 4 144727 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 5478275, 16, 7, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 5478275, 16, 7, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:27:52.030569 GPU 5 145212 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 114131, 7, 7],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 114131, 7, 7],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:28:26.175396 GPU 2 144555 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 114131, 7, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 114131, 7, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:29:36.378519 GPU 7 143078 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 34953, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 34953, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:29:37.059070 GPU 4 144727 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 34953, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 34953, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:29:53.083605 GPU 2 144555 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 43691, 8],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 43691, 8],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:29:55.014415 GPU 7 143078 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 49933, 7],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 49933, 7],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:30:09.875629 GPU 2 144555 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 49933, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 49933, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:30:11.874305 GPU 7 143078 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 49933],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 49933],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:30:12.529919 GPU 4 144727 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 49933],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 49933],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:30:27.659520 GPU 3 144235 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 79892, 7, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 79892, 7, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:30:30.134728 GPU 5 145212 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 79892, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 79892, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:30:30.276107 GPU 7 143078 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 99865, 7, 8],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 99865, 7, 8],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:30:37.730737 GPU 2 144555 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([174763, 3, 8, 32, 32],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:30:45.042071 GPU 4 144727 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=3, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:30:46.259010 GPU 7 143078 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:30:48.332064 GPU 2 145677 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:32:26.573805 GPU 7 143078 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:32:48.786139 GPU 5 145212 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:32:54.064673 GPU 3 144235 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[3,3,3,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:34:03.906954 GPU 2 146358 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:34:06.166687 GPU 4 146445 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 131073, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:34:16.389602 GPU 7 143078 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 262144, 8, 32, 32],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:34:29.207312 GPU 3 144235 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 14608733, 7, 7],"float32"), list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:34:35.985359 GPU 7 146678 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 14608733, 7, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:35:56.126674 GPU 7 147090 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 14608733, 7, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:36:13.854612 GPU 5 147251 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 14608733, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:36:33.947352 GPU 3 147412 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 14608733, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:36:48.742318 GPU 2 147573 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 14608733, 7, 7],"float32"), output_size=list[None,3,None,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:37:34.687013 GPU 7 147884 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=3, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=3, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:37:36.788107 GPU 4 146445 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:37:53.368099 GPU 5 148149 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:38:11.005982 GPU 3 148312 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:38:18.783635 GPU 2 148472 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:39:21.999122 GPU 4 146445 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[3,3,3,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:40:28.505966 GPU 7 147884 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:40:46.848200 GPU 4 146445 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 349526, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:41:00.088578 GPU 2 148472 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 10865245, 7],"float32"), list[2,3,5,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 10865245, 7],"float32"), list[2,3,5,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:41:17.465724 GPU 5 148149 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:41:38.417829 GPU 3 148312 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:41:57.600853 GPU 7 147884 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:42:03.844529 GPU 4 146445 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:42:33.945494 GPU 2 148472 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 20452226, 7],"float32"), list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:42:54.474651 GPU 5 149162 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 20452226, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:42:58.339191 GPU 2 149250 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 20452226, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:43:39.044923 GPU 3 148312 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 20452226, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:43:41.772930 GPU 4 149729 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 20452226, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:43:58.965517 GPU 7 147884 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 20452226, 7],"float32"), output_size=list[None,3,None,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:44:07.215585 GPU 3 149891 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 10865245],"float32"), list[2,3,5,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 10865245],"float32"), list[2,3,5,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:44:17.478241 GPU 7 150050 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=5, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:44:32.387711 GPU 5 150212 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:44:58.272733 GPU 2 150377 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:45:17.724350 GPU 4 150539 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:45:58.270161 GPU 7 150944 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 20452226],"float32"), list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:46:07.798199 GPU 3 149891 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 20452226],"float32"), output_size=5, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:46:10.993711 GPU 5 150212 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 20452226],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:46:20.966410 GPU 3 151108 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 20452226],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:46:26.349861 GPU 5 151267 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 20452226],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:46:32.556362 GPU 2 150377 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 5, 7, 20452226],"float32"), output_size=list[None,3,None,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:46:45.236952 GPU 2 151429 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 699051, 32, 32],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:46:50.285979 GPU 4 151588 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 7760890, 7, 7],"float32"), list[2,3,5,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 7760890, 7, 7],"float32"), list[2,3,5,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:47:24.429721 GPU 7 151754 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 7760890, 7, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:47:43.806275 GPU 5 152153 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 7760890, 7, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 7760890, 7, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:47:58.700942 GPU 3 152314 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 7760890, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:48:22.686037 GPU 2 152479 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 7760890, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:48:37.247051 GPU 4 151588 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=3, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=3, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:48:44.966669 GPU 7 152643 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:49:15.531036 GPU 3 152810 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:49:17.541686 GPU 5 152153 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:49:50.195538 GPU 2 153212 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:51:19.230399 GPU 4 151588 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[3,3,3,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:51:37.717824 GPU 7 152643 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:52:03.891489 GPU 5 152153 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:52:16.292425 GPU 3 152810 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 2796203, 32],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:52:30.565459 GPU 2 153212 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=3, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=3, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:52:31.504457 GPU 4 151588 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:52:40.425934 GPU 7 152643 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:53:06.827808 GPU 5 152153 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:53:47.472799 GPU 3 153892 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:53:48.423097 GPU 2 153212 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[3,3,3,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:54:00.277482 GPU 7 152643 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:54:03.737656 GPU 4 151588 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:54:36.479373 GPU 5 152153 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 3, 8, 32, 2796203],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:54:56.730027 GPU 5 154064 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 4656534, 5, 7, 7],"float32"), list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:54:58.333022 GPU 4 154151 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 4656534, 5, 7, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:55:02.927889 GPU 7 152643 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 4656534, 5, 7, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:55:04.213680 GPU 2 153212 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 4656534, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:56:13.799299 GPU 7 154630 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 4656534, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:56:19.760169 GPU 2 154790 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 8765240, 5, 7, 7],"float32"), list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:56:25.538595 GPU 5 154949 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 8765240, 5, 7, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:56:29.645104 GPU 4 155108 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 8765240, 5, 7, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:56:33.237555 GPU 3 153892 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 8765240, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:57:29.382482 GPU 7 155279 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([2, 8765240, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:57:41.952691 GPU 2 155764 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([3104356, 3, 5, 7, 7],"float32"), list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:57:47.528608 GPU 3 155925 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([3104356, 3, 5, 7, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:57:56.447428 GPU 4 156084 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([3104356, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:58:02.424140 GPU 5 156244 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([3104356, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:58:53.271737 GPU 7 156413 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([3104356, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:59:03.486906 GPU 2 156575 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([4994, 768, 16, 7, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([4994, 768, 16, 7, 10],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:59:09.995229 GPU 4 156735 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([4994, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([4994, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 21:59:16.594125 GPU 3 156894 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([5843493, 3, 5, 7, 7],"float32"), list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 21:59:32.146998 GPU 5 157292 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([5843493, 3, 5, 7, 7],"float32"), output_size=5, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:00:06.472635 GPU 7 157457 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([5843493, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:00:32.333996 GPU 3 157623 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([5843493, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:00:52.164999 GPU 5 157788 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([5843493, 3, 5, 7, 7],"float32"), output_size=list[None,3,None,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:00:54.424617 GPU 4 156735 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([6242, 768, 16, 7, 8],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([6242, 768, 16, 7, 8],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:01:27.295120 GPU 7 157969 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([7134, 768, 16, 7, 7],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([7134, 768, 16, 7, 7],"float16"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:01:51.933376 GPU 3 158367 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([7134, 768, 16, 7, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([7134, 768, 16, 7, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:02:03.787141 GPU 2 158528 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([87382, 3, 8, 32, 32],"float64"), output_size=3, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:02:10.684532 GPU 5 158687 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[1,1,1,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:02:26.881649 GPU 4 156735 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[1,3,2,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:03:20.686698 GPU 7 157969 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[2,2,2,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:03:39.305360 GPU 3 158367 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[2,3,3,], data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:04:27.680390 GPU 5 158687 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[3,3,3,], data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[3,3,3,], data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:04:31.019941 GPU 4 159136 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([87382, 3, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:05:26.019089 GPU 5 158687 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([174763, 3, 8, 32, 32],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:05:48.043748 GPU 7 157969 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[1,1,1,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[1,1,1,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:05:49.443261 GPU 3 159703 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[1,3,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:06:37.146202 GPU 5 159870 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[2,2,2,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[2,2,2,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:06:51.044312 GPU 7 157969 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[2,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:06:57.304050 GPU 2 160033 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:07:05.732509 GPU 4 160192 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:08:01.319040 GPU 7 160597 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:08:06.762832 GPU 3 160756 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 131073, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:09:14.641526 GPU 5 159870 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 262144, 8, 32, 32],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:09:20.217576 GPU 2 160940 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 14608733, 7, 7],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:09:36.718082 GPU 4 161336 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 14608733, 7, 7],"float32"), output_size=list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:10:30.186520 GPU 5 161507 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 14608733, 7, 7],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:10:31.288187 GPU 3 160756 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 14608733, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:10:32.446096 GPU 7 161594 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 14608733, 7, 7],"float32"), output_size=list[None,3,None,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:10:40.505073 GPU 2 161824 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=3, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=3, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:11:11.493005 GPU 4 161989 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[1,1,1,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[1,1,1,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:12:03.501798 GPU 3 162396 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[1,3,2,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[1,3,2,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:12:08.010486 GPU 7 162555 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[2,2,2,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[2,2,2,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:12:18.375007 GPU 5 162715 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[2,3,3,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[2,3,3,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:13:18.652519 GPU 2 161824 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[3,3,3,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=list[3,3,3,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:14:02.644135 GPU 4 161989 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=tuple(3,3,3,), )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=tuple(3,3,3,), ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:14:21.078967 GPU 2 161824 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:14:34.856026 GPU 7 162555 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 349526, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:14:51.600267 GPU 3 162396 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:15:07.722113 GPU 4 161989 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=list[2,3,5,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=list[2,3,5,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:15:21.567946 GPU 2 161824 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=list[3,3,3,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=list[3,3,3,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:15:22.410374 GPU 5 162715 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 10865245, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:15:38.646198 GPU 7 162555 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 20452226, 7],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:16:07.399486 GPU 3 163396 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 20452226, 7],"float32"), output_size=list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:16:38.642824 GPU 4 161989 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 20452226, 7],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:16:41.024427 GPU 5 163559 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 20452226, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:16:53.322452 GPU 2 161824 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 20452226, 7],"float32"), output_size=list[None,3,None,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:16:53.917205 GPU 7 163719 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:16:59.025132 GPU 4 357 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=list[2,3,5,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=list[2,3,5,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:17:07.379946 GPU 2 528 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=list[3,3,3,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=list[3,3,3,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:17:33.682106 GPU 3 938 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 10865245],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:18:09.507493 GPU 5 1105 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 20452226],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:18:20.279089 GPU 7 1268 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 20452226],"float32"), output_size=list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:18:31.493530 GPU 4 357 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 20452226],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:18:44.681383 GPU 4 1446 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 5, 7, 20452226],"float32"), output_size=list[None,3,None,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:18:45.685631 GPU 2 1532 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 699051, 32, 32],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:18:51.614092 GPU 3 1763 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 7760890, 7, 7],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:19:25.393573 GPU 5 1929 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 7760890, 7, 7],"float32"), output_size=list[2,3,5,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 7760890, 7, 7],"float32"), output_size=list[2,3,5,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:19:49.370387 GPU 7 2338 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 7760890, 7, 7],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:20:03.475933 GPU 4 2501 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 7760890, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:20:12.253903 GPU 3 2661 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=3, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=3, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:21:09.371337 GPU 7 2995 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[1,3,2,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[1,3,2,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:21:13.881274 GPU 5 1929 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[2,2,2,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[2,2,2,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:21:32.353348 GPU 4 3396 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[2,3,3,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[2,3,3,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:23:01.856770 GPU 3 2661 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[3,3,3,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=list[3,3,3,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:23:19.935551 GPU 2 3579 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=tuple(3,3,3,), )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=tuple(3,3,3,), ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:23:39.742060 GPU 7 2995 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:23:48.418185 GPU 5 1929 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 1398102, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:23:55.949392 GPU 4 3396 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 2796203, 32],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:24:08.535089 GPU 3 2661 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=3, )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=3, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:24:49.264591 GPU 7 2995 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[1,1,1,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[1,1,1,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:24:53.098468 GPU 5 1929 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[1,3,2,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[1,3,2,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:25:09.007551 GPU 4 3995 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[2,2,2,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[2,2,2,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:25:16.340905 GPU 3 2661 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[2,3,3,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=list[2,3,3,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:26:11.353271 GPU 7 2995 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=tuple(3,3,3,), )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=tuple(3,3,3,), ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:26:15.609315 GPU 5 1929 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:26:41.833180 GPU 3 2661 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 1398102],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:27:05.638250 GPU 2 4412 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 3, 8, 32, 2796203],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:27:17.713859 GPU 7 2995 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 4656534, 5, 7, 7],"float32"), output_size=5, )
[accuracy error] paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 4656534, 5, 7, 7],"float32"), output_size=5, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 121505530 / 1164133500 (10.4%)
Greatest absolute difference: 0.4947296977043152 at index (0, 683502, 2, 1, 3) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 980107, 3, 0, 4) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 4656534, 5, 5, 5]), dtype=torch.float32)
tensor([[[[[-2.4754e-01, -1.7566e-01,  4.6519e-02,  1.2580e-02,  8.1822e-02],
           [-3.4602e-01, -2.3454e-01, -7.1511e-02, -5.7672e-02,  3.7645e-02],
           [ 6.3560e-02,  9.9701e-02,  1.1868e-01, -3.3195e-02,  5.3566e-02],
           [ 1.0611e-02,  3.5513e-02,  9.0208e-02, -1.1322e-01, -6.0836e-03],
           [-1.5166e-01, -1.1733e-01,  3.5179e-02,  4.5582e-03, -5.0136e-02]],

          [[-2.0960e-01, -4.5137e-02,  2.4983e-01,  2.7442e-01,  1.1999e-01],
           [-4.1285e-02,  1.2926e-01,  1.6961e-01,  1.2842e-01,  2.8929e-02],
           [-1.2275e-01,  1.3262e-01,  1.3973e-01,  1.1505e-01,  9.6643e-02],
           [ 9.3592e-02,  2.2300e-01,  2.1936e-01, -7.6244e-03, -1.7907e-01],
           [ 3.2811e-01,  1.0505e-01, -1.0153e-02, -1.2139e-01, -9.0433e-02]],

          [[ 9.1510e-02,  6.1883e-02,  7.7000e-02, -9.6091e-02,  9.2363e-02],
           [-6.5151e-02,  3.5165e-02,  1.1023e-01,  7.9093e-02,  2.4825e-01],
           [ 6.4633e-02,  1.5951e-01,  4.0216e-02, -1.2902e-02,  1.4156e-01],
           [ 3.0358e-02,  5.0625e-02, -3.0713e-03, -1.1605e-01, -8.0821e-02],
           [ 8.9311e-02,  1.3191e-01, -2.2650e-02, -1.0282e-01,  7.1118e-02]],

          [[ 6.0034e-02, -1.0745e-01, -3.1576e-02, -2.0240e-01, -3.3309e-01],
           [-1.6399e-01, -1.3018e-01,  1.1229e-01,  9.0084e-02, -2.0122e-01],
           [-2.6764e-01,  5.9525e-03,  1.6083e-01,  7.8409e-02, -1.1271e-01],
           [-3.1441e-01, -1.2508e-01,  2.1194e-01,  1.3370e-01,  8.8221e-02],
           [-2.6036e-01, -6.9433e-02,  9.4897e-02, -6.3106e-03,  1.2915e-02]],

          [[ 2.3452e-01, -7.7806e-02, -2.7210e-01, -2.7994e-01, -1.4421e-01],
           [ 1.6823e-01,  1.6437e-01, -6.7291e-02, -3.2078e-01, -5.0070e-02],
           [ 1.1170e-01,  1.7896e-02, -7.8833e-02, -1.7938e-01, -1.7644e-01],
           [ 7.8387e-03,  1.0744e-01,  1.5471e-02, -1.0094e-01, -3.2161e-02],
           [-1.8837e-01, -1.0063e-01,  2.3288e-01,  1.6828e-01, -5.0075e-03]]],


         [[[ 3.1129e-02,  2.1662e-01,  6.3783e-02, -1.8702e-01, -4.6790e-02],
           [-9.2469e-03,  2.4741e-01,  7.9033e-02,  9.0584e-02, -1.0863e-02],
           [ 6.1783e-02,  8.1618e-02, -4.2475e-02,  1.2256e-01,  6.0270e-02],
           [-3.1781e-02,  3.3202e-02,  7.5815e-02, -1.5382e-01, -7.3878e-02],
           [-1.4287e-03,  1.0521e-01,  2.1996e-01, -1.4069e-01, -8.8796e-02]],

          [[-8.7271e-02, -9.9203e-02, -3.0476e-02, -1.4728e-02,  6.8144e-03],
           [ 8.9082e-02,  2.9677e-01, -5.7230e-02, -5.1549e-02,  2.1995e-01],
           [ 9.7541e-02,  1.2236e-01, -8.2362e-03, -3.6898e-02, -1.2448e-01],
           [-1.8839e-01, -5.5363e-02, -9.9658e-02, -5.2465e-02, -1.0887e-01],
           [-7.1538e-02, -1.1200e-01, -4.7094e-02, -8.7519e-02, -2.0478e-01]],

          [[ 7.5525e-02,  2.1332e-02, -1.1093e-01,  2.1780e-01,  1.7665e-01],
           [ 2.0276e-02,  8.1922e-02,  3.0965e-02,  3.2977e-01,  2.5512e-01],
           [ 1.0785e-01, -7.8272e-02, -1.0161e-01,  9.7080e-02,  1.0245e-01],
           [ 1.4420e-01,  7.7238e-02, -2.5647e-02, -1.9817e-03, -8.1989e-02],
           [ 1.8355e-01,  2.3926e-01,  1.2023e-01, -8.1921e-02, -9.3139e-02]],

          [[ 6.7390e-03, -1.3981e-01, -1.5200e-01,  7.4757e-03,  2.2691e-01],
           [-1.2792e-01, -1.0078e-01,  6.7479e-02, -3.7067e-02,  4.2799e-02],
           [ 1.0500e-02, -7.5304e-03,  1.2718e-01, -3.9375e-02, -2.6503e-01],
           [ 6.3481e-02,  1.5665e-01,  1.1692e-01, -3.2910e-02, -3.7631e-01],
           [ 6.9712e-02, -6.2756e-03,  1.1888e-01,  1.6393e-01, -1.3914e-02]],

          [[-1.2300e-02,  8.7132e-02, -7.2857e-02, -1.4511e-01, -9.2310e-02],
           [ 3.0199e-01,  3.1244e-01, -1.0331e-01, -2.4511e-01, -5.0446e-02],
           [ 7.7405e-02,  6.5146e-02, -2.1144e-02, -8.7259e-02, -1.0893e-01],
           [-4.0410e-02, -2.5565e-01, -6.7481e-02, -1.0902e-02, -2.1084e-01],
           [-2.3432e-01, -3.2645e-01, -6.9122e-02,  6.1833e-02, -7.9549e-02]]],


         [[[-9.5413e-02,  2.8862e-01,  1.7075e-01,  1.9523e-02, -2.1943e-01],
           [-5.0268e-02,  2.0639e-02,  6.2907e-02, -8.7810e-02, -1.0560e-01],
           [ 2.0509e-01, -3.9978e-02, -5.4234e-02, -1.2724e-01, -2.2644e-02],
           [-7.2181e-03, -2.1400e-01, -3.8350e-02,  8.8922e-02,  2.5122e-01],
           [-7.6365e-02, -2.5885e-01, -7.5640e-02,  1.2346e-01,  1.7524e-01]],

          [[-2.2136e-01,  1.2141e-02, -2.0800e-02,  1.5029e-01,  7.0275e-02],
           [-6.3907e-02,  1.4406e-02,  7.4754e-02,  2.3490e-01,  3.2993e-02],
           [-4.9101e-02, -2.2282e-01, -1.7476e-02,  1.2947e-02, -7.9863e-02],
           [ 4.7638e-02,  3.3926e-02,  2.7644e-02,  1.2219e-01, -6.6442e-02],
           [ 6.0129e-03,  4.9139e-04,  1.6937e-02,  2.0493e-01,  1.8648e-01]],

          [[ 1.7662e-01,  9.7327e-02,  1.2405e-01,  8.4724e-03,  1.7184e-02],
           [ 1.4101e-01,  1.0104e-01,  8.6869e-02,  7.7830e-02,  2.1667e-01],
           [ 1.0373e-01,  3.7344e-02, -3.6442e-02, -3.6413e-02,  1.8845e-01],
           [ 6.1342e-02, -1.6533e-01,  5.8115e-02, -1.8089e-02, -4.0023e-03],
           [ 1.1542e-01,  2.6284e-01,  2.1514e-01,  1.1906e-01,  3.8085e-02]],

          [[ 9.4938e-02, -7.4811e-02, -1.7831e-01, -2.8634e-01, -6.8554e-02],
           [ 1.7537e-01,  6.8882e-02, -1.3707e-01, -2.8818e-01, -1.1975e-01],
           [-5.5504e-02,  3.0843e-02, -2.4364e-02, -1.5223e-01, -9.4794e-02],
           [-9.4114e-02, -1.0492e-01, -2.0520e-01, -8.9957e-02,  4.2126e-04],
           [ 4.6780e-02,  9.1626e-02, -1.5783e-01,  5.6628e-02, -4.6900e-02]],

          [[-6.8037e-02, -1.0241e-01, -2.2016e-03, -1.2051e-01, -2.5656e-01],
           [ 1.1485e-01, -7.8107e-02,  2.6106e-02,  1.3226e-01, -7.5378e-02],
           [-1.5598e-01, -1.6103e-01, -3.2854e-03,  8.5915e-02,  5.0178e-02],
           [-9.2333e-02, -8.1332e-06, -6.4187e-03, -2.1768e-02,  7.2079e-02],
           [-8.9128e-02, -1.0906e-01, -2.4136e-02,  1.6446e-02,  6.0312e-02]]],


         ...,


         [[[-2.1839e-01, -2.0352e-01, -1.5219e-01, -3.0351e-02,  2.5606e-02],
           [-1.2025e-02, -7.9242e-02,  8.0475e-02,  2.2749e-02, -8.3821e-02],
           [ 1.3204e-01,  1.4043e-01,  2.4031e-01,  4.3581e-02, -7.1294e-02],
           [ 2.0583e-01,  1.2373e-02,  1.3116e-01,  6.7905e-02,  1.0802e-01],
           [ 3.3834e-01,  1.4438e-01, -4.6122e-02,  5.8686e-02,  8.0239e-02]],

          [[ 2.5209e-01, -7.0877e-02, -1.0708e-01,  3.0074e-02,  3.0325e-02],
           [ 5.2055e-02, -1.1646e-01, -6.9905e-02, -6.6992e-02, -1.1665e-01],
           [ 1.2255e-02,  9.4167e-02,  3.7935e-02, -4.6106e-02, -5.1754e-02],
           [-1.0337e-01, -1.2945e-01, -9.3917e-02, -1.3973e-02,  2.4698e-01],
           [-5.4941e-02, -2.2897e-01, -2.3873e-01, -1.5673e-01,  9.7679e-02]],

          [[-6.3695e-02,  2.0061e-01,  1.4863e-01, -1.9276e-01, -4.6766e-02],
           [-1.3249e-01,  1.1557e-01,  1.1513e-02, -1.4148e-01, -6.5750e-02],
           [-7.0721e-02, -5.8031e-02,  3.7133e-02,  2.0844e-01,  1.7670e-01],
           [ 8.1747e-02, -2.3260e-03, -3.9468e-02, -9.4769e-02,  6.4387e-02],
           [ 4.3390e-02, -9.7429e-03, -3.2869e-02, -1.7672e-01, -1.4304e-01]],

          [[-2.2551e-01, -2.0813e-01,  9.7632e-02,  3.8782e-02, -1.5129e-01],
           [-3.5756e-01, -1.2134e-01,  2.2455e-01,  2.9848e-02, -4.1815e-02],
           [-8.7242e-02,  5.9001e-02, -3.0805e-02, -1.5966e-01, -1.7188e-01],
           [-1.7400e-01,  2.5460e-02, -1.3547e-01, -2.2748e-01, -3.5421e-01],
           [-3.0634e-02, -3.5065e-02,  3.0685e-02, -2.7435e-02, -2.6322e-01]],

          [[-2.9253e-03,  1.1419e-01, -2.1354e-02, -1.0351e-01,  1.7339e-01],
           [ 5.4164e-02,  2.1300e-01,  3.0884e-02, -9.2353e-02,  5.8271e-02],
           [ 1.1828e-01,  1.9007e-01,  5.0983e-02, -3.7902e-02, -4.0447e-02],
           [-1.0411e-01,  1.3247e-01,  6.1293e-02, -7.4833e-02,  1.0857e-01],
           [-1.8026e-01,  4.5653e-02,  5.3323e-02,  2.2507e-02,  3.5289e-01]]],


         [[[ 5.6470e-02,  2.4565e-01,  1.4215e-01, -2.9696e-02,  1.4750e-01],
           [-4.5081e-02,  1.7838e-01,  1.6967e-01, -1.2194e-01,  3.6710e-02],
           [-1.2337e-01, -9.5513e-02, -5.7521e-02, -2.2184e-01, -5.5876e-02],
           [-2.1541e-02, -1.2687e-01, -1.1839e-01, -7.2142e-02, -1.6206e-02],
           [ 1.1011e-01,  9.9139e-02, -1.1201e-01, -3.7293e-03,  1.2387e-01]],

          [[-1.5800e-01,  1.2633e-02,  1.7303e-01,  1.8624e-01,  6.4500e-02],
           [-9.0169e-02, -5.1099e-02,  2.1415e-01,  1.1445e-01,  1.5510e-01],
           [ 1.5045e-02,  9.0271e-02,  1.5953e-01,  2.9668e-02, -1.2879e-01],
           [-7.4506e-02, -1.4549e-02,  8.8397e-02, -1.8182e-01, -3.6516e-01],
           [-1.8590e-01, -1.3566e-01, -5.5301e-02, -4.0263e-01, -3.5360e-01]],

          [[-1.3279e-01, -7.4134e-02,  1.5088e-02, -1.1418e-01, -1.6689e-04],
           [ 8.9338e-02, -1.3636e-01, -2.4542e-02,  3.9530e-02,  1.3942e-01],
           [ 1.2747e-01,  6.8863e-02,  4.1529e-03, -4.1435e-02,  2.5195e-02],
           [ 1.0701e-01,  4.3009e-02, -1.0672e-01, -2.5304e-01, -2.7326e-01],
           [-7.4054e-02, -7.9777e-02, -8.4971e-03, -1.4689e-01, -1.0226e-01]],

          [[-1.1994e-01, -1.4387e-02, -1.0589e-01, -1.7318e-01,  7.2091e-02],
           [ 5.9157e-02,  1.7422e-01, -8.6786e-02, -1.5075e-01,  7.0953e-02],
           [ 6.3158e-02,  8.7681e-02,  7.3040e-04,  9.5737e-03, -9.8679e-02],
           [-1.6717e-01,  2.2341e-02,  4.5142e-03, -1.1875e-01, -1.8926e-01],
           [-2.6400e-01, -4.3387e-02, -2.9892e-02, -3.5387e-02,  7.1090e-02]],

          [[ 6.8541e-02,  1.4673e-03, -7.5193e-02, -2.1410e-01, -1.9204e-01],
           [ 2.3103e-01, -5.7551e-02, -8.6097e-02, -4.9569e-02, -5.8217e-02],
           [ 2.3904e-01,  2.3762e-01,  5.2644e-02, -7.9401e-02, -1.0261e-01],
           [ 2.8217e-01,  1.7829e-01, -6.0230e-02, -9.7476e-02, -3.5172e-02],
           [ 3.2894e-01,  1.7406e-01, -1.1044e-01,  9.1031e-02,  2.5069e-01]]],


         [[[ 5.8382e-02,  9.2564e-02,  2.3178e-02, -9.5792e-02, -1.5941e-01],
           [-1.2089e-01, -7.7372e-02,  1.0922e-01, -3.3806e-02, -9.5909e-02],
           [-1.3035e-01, -2.1905e-01, -1.1214e-01, -4.5339e-02,  2.0398e-01],
           [-1.4917e-01, -1.2109e-01, -2.3920e-02, -4.9141e-02,  8.8827e-02],
           [-1.8042e-01, -1.0000e-01,  2.1993e-01,  5.6054e-03, -2.0091e-01]],

          [[-1.3989e-01, -2.9135e-01, -1.1608e-01,  1.8714e-01, -1.7109e-02],
           [-4.5054e-02, -2.2046e-01,  2.4333e-02,  2.6713e-01,  1.3647e-01],
           [ 4.0234e-02, -1.2642e-01, -2.6267e-02,  8.3683e-02, -1.5345e-01],
           [ 1.8191e-01,  3.9864e-02,  5.2119e-02,  1.4822e-01, -2.8342e-01],
           [ 1.5623e-01,  2.4821e-01,  5.9593e-02, -5.5342e-02, -3.5828e-01]],

          [[-3.1575e-02,  8.6273e-02, -2.1839e-02, -3.1774e-02,  1.6411e-01],
           [-1.6058e-01,  6.8948e-03,  2.0701e-02,  1.1199e-01,  1.0817e-01],
           [ 9.8833e-02,  8.5513e-02,  5.9436e-02,  1.3552e-01, -3.2647e-02],
           [ 2.6905e-01,  3.2356e-01,  1.8071e-01,  1.2670e-01,  7.8495e-02],
           [-6.3894e-02,  4.9920e-02,  6.2031e-02,  2.4700e-01,  4.9384e-02]],

          [[ 2.4988e-01,  1.4659e-01, -7.5618e-02, -2.4282e-01, -1.3520e-01],
           [ 1.0123e-01,  6.4468e-03, -1.1852e-02, -1.6555e-01, -1.7387e-01],
           [ 3.1209e-02,  2.9417e-02,  1.7171e-02, -6.5201e-02, -1.1780e-01],
           [ 1.6786e-01,  2.4282e-01,  6.1920e-02, -5.2885e-02, -1.6534e-01],
           [-4.7915e-02, -3.3843e-02, -9.4171e-02, -1.0187e-01, -1.8676e-01]],

          [[ 5.7031e-02, -5.6153e-02, -1.4205e-01, -1.0630e-01,  1.4958e-01],
           [-1.1515e-01, -2.5926e-01, -2.4161e-01, -1.1882e-01, -2.1935e-03],
           [-1.4113e-01, -1.2279e-01, -4.3878e-02,  6.2548e-02,  6.3222e-02],
           [ 2.0829e-02,  4.2590e-02,  1.0259e-01,  1.5163e-01, -1.3747e-01],
           [ 3.6319e-02, -1.0194e-01, -1.7106e-02, -1.3708e-01, -3.1096e-01]]]],



        [[[[ 2.3460e-01,  1.1313e-01, -1.7657e-02, -1.5669e-01, -1.4719e-01],
           [ 3.2099e-01,  1.6893e-01,  2.2988e-02, -1.5693e-01, -2.0504e-01],
           [ 8.2922e-02,  9.2410e-02,  1.2636e-01,  2.1014e-02, -1.6902e-01],
           [ 1.7143e-01,  2.2987e-01,  4.1381e-02, -2.1796e-01, -5.6744e-02],
           [ 1.0137e-01,  3.3573e-01, -1.5216e-01, -4.3050e-01, -1.0611e-01]],

          [[-1.6639e-01, -1.2663e-01,  9.1996e-02,  2.3114e-01, -7.0565e-02],
           [-2.0457e-01, -2.4152e-01, -1.0408e-01, -9.1833e-02, -8.3914e-02],
           [-1.0278e-01,  1.9987e-02,  5.4393e-02, -3.7822e-02,  4.3637e-02],
           [-4.6868e-02,  3.7993e-03,  2.1472e-01,  1.0496e-01,  1.4932e-01],
           [-9.8083e-02, -7.4983e-02,  9.6208e-03, -2.0191e-02, -2.7804e-03]],

          [[ 1.9409e-02, -4.1642e-04, -1.0141e-02,  1.5490e-02,  2.1655e-01],
           [ 1.3940e-01, -2.3405e-01, -1.0833e-01,  2.0145e-01,  3.8305e-01],
           [ 1.2942e-02, -1.2558e-01, -6.3514e-02,  2.7769e-02, -1.4696e-02],
           [ 3.2118e-02,  1.8587e-01,  3.2439e-02, -1.2127e-01, -9.3524e-02],
           [ 2.5296e-01,  8.7610e-02,  1.6039e-02,  4.4221e-02, -2.1316e-02]],

          [[ 3.4956e-02,  1.0685e-01, -3.1463e-02,  1.2823e-01, -1.0281e-01],
           [ 3.7548e-02, -1.6845e-01, -1.8609e-01,  6.2533e-02,  8.6082e-02],
           [-2.0648e-01, -1.1369e-01, -2.0879e-02,  2.7427e-02,  2.2745e-01],
           [-1.7353e-01, -4.4902e-02,  2.1823e-03,  7.5949e-02,  1.9443e-01],
           [ 2.7501e-02,  5.5503e-02, -1.5037e-01, -1.1259e-01, -3.8437e-02]],

          [[ 3.4217e-02,  7.5341e-02, -4.1842e-02, -3.4280e-02,  5.5343e-03],
           [-7.5145e-02,  1.5396e-01, -7.7044e-03, -1.4748e-01,  1.3788e-01],
           [-5.2580e-02,  1.3417e-01,  9.1498e-02,  4.9499e-03,  1.7670e-01],
           [-1.7408e-03,  2.2080e-01, -7.5412e-02, -1.8303e-01,  4.8599e-02],
           [-1.3946e-01, -3.2543e-03, -1.1811e-01, -2.5458e-01, -1.5899e-01]]],


         [[[ 1.7876e-01,  1.2952e-01,  8.5433e-02,  6.7565e-02,  1.3002e-01],
           [-1.2873e-01, -9.4756e-02,  1.0468e-01,  8.8164e-02,  4.1358e-02],
           [-2.6944e-02, -1.5306e-01,  2.5137e-02,  9.3890e-02,  2.1213e-05],
           [ 1.7208e-01, -1.6035e-01, -8.4465e-02,  1.0338e-01, -2.1469e-01],
           [ 6.9539e-02, -1.1727e-01, -1.1820e-01, -1.6000e-01, -1.5867e-01]],

          [[-1.0290e-01,  1.1322e-03,  1.5491e-01,  3.1457e-01,  2.3732e-01],
           [-1.6854e-01,  2.4953e-02,  2.2100e-01,  3.1638e-01,  1.9514e-01],
           [ 2.8077e-03, -3.5917e-02,  1.8383e-02,  1.5807e-01,  1.7516e-01],
           [ 1.3611e-01, -2.1428e-01, -2.1947e-01,  3.2111e-02, -9.8724e-02],
           [ 9.4765e-02, -1.2994e-01,  4.4923e-02,  1.7370e-01, -1.6837e-02]],

          [[-2.1819e-02,  6.7075e-03,  9.2127e-02, -1.2199e-01, -8.2447e-02],
           [-1.3376e-01, -1.0031e-01,  1.1039e-01, -9.0568e-02, -2.6138e-01],
           [-2.9389e-01, -7.5599e-02,  1.3363e-01, -3.7297e-02, -6.1135e-02],
           [ 8.6681e-02,  2.9143e-02,  1.3048e-01,  1.8482e-01,  1.0813e-01],
           [ 2.5140e-01,  1.0453e-01,  2.0538e-01,  1.8585e-01,  2.2779e-02]],

          [[ 9.6545e-02, -6.4148e-02,  7.4449e-02,  8.1512e-02, -2.2085e-01],
           [ 1.7519e-01,  2.5260e-01,  1.9965e-01,  2.2706e-01, -9.4862e-02],
           [-4.6058e-02,  4.2110e-02,  1.9325e-02,  1.4486e-01, -4.6321e-02],
           [ 1.1186e-01, -4.3316e-03, -1.7601e-01,  2.5979e-01,  1.6015e-01],
           [ 1.1830e-01, -1.1618e-01, -2.0441e-01,  1.5297e-03,  1.6980e-01]],

          [[ 2.6061e-02, -7.6520e-02, -1.5767e-01,  2.1374e-02,  9.3966e-03],
           [ 4.8727e-02, -3.6156e-02, -5.5114e-03,  1.0755e-01,  8.2969e-02],
           [-5.4237e-02, -1.2404e-01,  5.5708e-02, -1.1389e-02,  1.3761e-01],
           [ 2.3445e-01,  1.3971e-01, -6.6114e-02, -8.1390e-02,  2.0923e-02],
           [ 3.5463e-01,  2.0237e-01, -1.3753e-01, -3.1330e-01, -1.1377e-01]]],


         [[[ 1.6053e-02, -1.5014e-01, -2.3054e-01, -1.5807e-01, -1.9809e-01],
           [ 1.3138e-02, -2.3705e-02, -1.4611e-02, -2.0728e-01, -4.4230e-01],
           [ 4.3882e-02,  1.5924e-01,  1.2073e-01, -1.7352e-02, -2.7302e-01],
           [ 8.4622e-02, -5.4906e-02, -9.9474e-02, -5.7309e-02, -5.7340e-02],
           [ 2.0831e-01, -2.6124e-02, -2.5327e-01, -8.0944e-02, -7.6111e-03]],

          [[-5.8115e-02, -1.9936e-01, -2.3839e-01, -3.1739e-01, -9.8906e-02],
           [ 1.7586e-01, -1.3735e-01, -1.2268e-01, -2.1490e-01, -7.7297e-02],
           [ 9.7420e-02,  5.1519e-02, -1.7276e-02,  7.7271e-02,  1.1081e-01],
           [-4.6786e-02,  1.0418e-01, -9.0308e-03, -6.8263e-03,  5.8657e-02],
           [ 8.0125e-02, -1.2066e-02, -9.3309e-02, -1.2197e-01,  6.0807e-02]],

          [[-1.9457e-01, -1.6079e-01,  1.3489e-01, -6.4541e-03,  6.3035e-02],
           [-8.7274e-02,  3.3860e-02, -7.8685e-02, -1.9299e-01, -6.6165e-02],
           [ 9.6051e-02,  5.5405e-02, -9.5750e-02,  7.4729e-02,  2.8794e-02],
           [-8.9374e-02,  1.0518e-01, -3.2077e-02,  8.6472e-03, -1.5723e-01],
           [-1.3352e-01, -1.0071e-01, -6.5512e-02, -4.1128e-02, -2.2810e-01]],

          [[ 8.0539e-02,  2.0371e-02,  2.3385e-02,  2.6167e-01, -4.9690e-03],
           [-2.0403e-01, -1.1684e-01, -2.3563e-02,  2.9943e-01,  6.5721e-02],
           [-4.5533e-02,  7.4159e-02,  1.1471e-01,  2.1836e-01, -3.8446e-03],
           [-1.2756e-01,  1.0413e-01,  2.3488e-01,  2.0579e-01, -1.9992e-01],
           [-2.9913e-02, -6.5656e-02,  2.1716e-01,  9.5433e-02, -1.3029e-01]],

          [[-3.2719e-01, -2.9223e-01, -1.1740e-01,  5.8288e-02,  1.4016e-01],
           [-1.7338e-01, -1.3911e-01, -1.8751e-02, -8.0098e-03,  6.7860e-02],
           [ 2.0250e-01,  2.1914e-02, -6.4629e-02, -3.8304e-02, -1.1254e-01],
           [ 3.9100e-01,  2.5064e-01,  1.1626e-01, -1.8743e-02, -2.5547e-01],
           [ 2.6109e-01,  2.9738e-01,  8.4271e-02,  4.1345e-02, -2.5369e-02]]],


         ...,


         [[[-1.0479e-01,  1.0051e-01, -5.9363e-02, -3.4990e-02,  1.5675e-01],
           [ 8.4578e-02,  1.2322e-02, -7.1868e-02, -3.4663e-02, -1.0388e-01],
           [ 1.7101e-01,  2.7926e-02, -8.6573e-02, -6.1130e-02, -1.7971e-01],
           [ 8.8679e-02, -1.5895e-01, -2.9883e-01, -3.9026e-02, -7.9610e-02],
           [-5.3678e-02,  7.6659e-02, -6.0150e-02, -3.7884e-02,  1.4952e-01]],

          [[ 2.3783e-02,  3.8414e-02, -6.7909e-03, -1.3166e-01, -1.2630e-01],
           [ 1.7933e-01,  9.6162e-02, -5.1626e-02, -1.0012e-01, -5.6817e-02],
           [ 8.0887e-02,  7.0375e-02,  4.6448e-03,  2.7087e-02, -3.2805e-02],
           [ 3.6046e-02, -2.2969e-02, -1.4198e-02, -9.8678e-02,  4.6774e-02],
           [ 1.5982e-01,  1.2297e-01,  2.4707e-01,  1.6977e-01,  2.6697e-02]],

          [[ 2.6956e-01, -3.9469e-02, -5.0035e-02,  1.5085e-01,  2.6902e-01],
           [ 4.8042e-02, -6.5235e-02,  1.3477e-02,  1.0718e-01,  7.4791e-02],
           [ 1.1920e-01,  1.3180e-01,  1.2789e-01,  1.9311e-02, -8.9509e-02],
           [ 2.1460e-03,  1.0745e-02,  6.9946e-02, -7.9205e-02, -1.2691e-01],
           [-1.4829e-01, -8.8365e-02,  9.2068e-02,  2.3136e-01,  5.5471e-02]],

          [[-2.0094e-01, -1.4818e-01, -1.9373e-02,  2.2044e-01,  3.0658e-01],
           [-1.2566e-01, -1.6422e-01, -1.4622e-02, -3.0199e-02,  2.2404e-01],
           [-1.1106e-01, -3.9787e-02,  8.3726e-02,  1.4284e-03,  8.5765e-02],
           [ 5.1737e-02,  4.5422e-02, -5.7327e-04,  7.6312e-02,  1.0244e-01],
           [-6.8874e-02,  1.3433e-01, -8.4078e-02, -1.1942e-02, -1.7127e-01]],

          [[ 5.7438e-02,  5.9603e-02, -1.4619e-01, -1.0007e-01, -2.1053e-01],
           [ 4.2728e-02,  1.1936e-02, -1.3317e-01, -3.0908e-01, -1.7722e-01],
           [-1.5700e-01, -9.9405e-02,  1.3382e-02, -1.2423e-01, -3.2186e-02],
           [-1.1901e-01,  7.1846e-02,  2.1726e-01,  7.1097e-02,  1.1333e-01],
           [ 2.3581e-01,  2.2743e-01,  1.3978e-01,  3.0935e-02,  2.8744e-01]]],


         [[[ 2.6436e-01,  1.6235e-01, -2.1291e-02, -1.0607e-01, -1.7187e-02],
           [ 7.0326e-02,  3.7983e-03,  4.6443e-03,  4.5394e-02,  1.0442e-01],
           [ 6.4809e-02,  1.4686e-01,  1.9403e-01,  2.9725e-01,  1.5064e-01],
           [ 2.3816e-01,  2.0990e-01,  2.1127e-02,  3.6023e-02,  2.4076e-01],
           [ 1.2839e-01,  1.9181e-01, -1.2803e-01, -2.3893e-01, -4.7531e-02]],

          [[-1.8029e-01, -6.6741e-02, -3.2001e-02, -1.2360e-01,  2.2573e-01],
           [-1.0727e-01, -6.3408e-02, -1.1581e-03,  1.1547e-01,  1.9329e-01],
           [ 7.9185e-02,  3.1672e-02,  7.5760e-02,  2.5611e-01,  1.6262e-01],
           [ 1.1321e-01,  1.2269e-01,  1.0954e-01,  1.9822e-01, -4.3747e-02],
           [-1.0454e-02, -1.2350e-02,  1.3518e-01,  1.3583e-01, -8.8714e-02]],

          [[-1.5332e-01, -8.7367e-02, -2.0825e-01, -2.0929e-01, -1.0884e-03],
           [-5.1320e-04, -1.0337e-02, -2.1317e-01, -2.7475e-01,  1.0815e-01],
           [ 3.7816e-02,  1.0557e-01, -1.6685e-01, -2.0474e-01,  7.7536e-02],
           [ 1.0184e-01,  1.7537e-01, -7.1631e-02, -4.8032e-02,  5.5396e-02],
           [-1.1160e-01, -2.2539e-04,  1.1052e-01,  7.9020e-03, -2.9301e-01]],

          [[ 1.1717e-01,  2.5460e-01, -7.0550e-02, -1.2273e-01, -1.0429e-02],
           [ 1.2043e-01,  1.7939e-01, -7.2791e-02,  1.3983e-02,  1.8927e-01],
           [ 6.8165e-02, -1.2582e-02, -1.0906e-01,  1.0885e-02,  6.1820e-02],
           [-3.2805e-02, -1.6118e-02,  7.7707e-02, -6.5398e-02, -6.9683e-02],
           [ 1.4291e-02,  1.6825e-01,  9.7252e-02, -1.1792e-01, -6.6803e-03]],

          [[-1.4461e-01,  2.3065e-02, -5.4612e-02, -1.8642e-01, -1.8271e-01],
           [ 1.6075e-02, -4.6612e-02, -6.2039e-02, -3.3393e-02,  5.5170e-02],
           [-1.2731e-01, -1.2816e-01, -1.0495e-01, -1.4475e-01,  7.2920e-02],
           [-4.5781e-02,  6.5532e-03, -1.0611e-01, -1.6441e-02,  1.7273e-01],
           [ 8.0112e-02,  1.1778e-01,  9.8364e-02,  2.4068e-01,  2.5714e-01]]],


         [[[ 1.3585e-01,  2.1218e-01,  1.9059e-01, -6.7154e-02,  1.2160e-01],
           [ 2.3178e-01,  8.9135e-02, -3.1822e-02, -9.2346e-02, -4.7104e-02],
           [ 1.4718e-01,  1.0080e-01, -1.9004e-01,  9.0704e-03,  6.7536e-02],
           [ 1.3220e-01, -3.9739e-02, -3.0004e-02,  1.5870e-01,  8.8572e-02],
           [ 6.4795e-02, -3.8852e-02,  2.3828e-01,  4.0043e-01,  2.3304e-01]],

          [[-2.7895e-01, -3.2158e-02,  8.7026e-02,  2.0703e-01,  2.0064e-01],
           [-2.7009e-01,  1.0184e-01,  1.0463e-02,  1.3679e-01,  1.5250e-01],
           [-1.8537e-01, -1.3589e-01, -1.5547e-01, -1.6027e-01, -9.2191e-02],
           [-5.0652e-02, -7.5620e-02, -1.0351e-01, -2.1753e-01, -2.1726e-02],
           [ 6.8334e-02,  3.0196e-02, -8.7949e-02, -2.8791e-01, -8.9017e-02]],

          [[ 1.2057e-01, -1.5497e-01, -2.6320e-02, -1.7202e-02, -1.0887e-01],
           [ 5.7035e-03, -1.5839e-01, -5.4324e-02,  4.5631e-02, -8.5717e-02],
           [-8.0339e-02, -1.4786e-01,  7.3986e-02, -1.9479e-02, -1.6699e-01],
           [-1.1801e-01, -1.6715e-01,  1.2061e-01, -7.1590e-02, -3.5157e-01],
           [ 5.0610e-02,  3.7073e-02,  2.9230e-02, -1.3933e-01, -2.0951e-01]],

          [[-3.2288e-02,  5.7667e-02,  2.6267e-02, -1.5993e-02,  2.8085e-02],
           [ 2.4960e-01,  2.3777e-01,  6.0405e-02, -1.9032e-02, -9.3823e-02],
           [-1.0050e-01,  1.3708e-01,  5.7569e-02, -8.0784e-02, -1.1917e-01],
           [-8.5680e-02,  1.9178e-01,  2.0999e-01,  1.4298e-01,  8.7479e-03],
           [-1.5245e-02,  6.8275e-02,  2.1059e-01, -6.0931e-02, -9.5732e-02]],

          [[-1.5025e-01, -1.8897e-01, -9.1721e-02, -2.1037e-01, -2.0626e-01],
           [-9.5751e-02, -3.2014e-02, -5.3245e-02, -2.3315e-01, -2.7067e-01],
           [ 8.5627e-02,  6.2798e-02, -1.1401e-01, -8.4107e-02, -2.3793e-02],
           [ 1.3163e-02, -5.4790e-04, -2.0570e-01, -1.8009e-01, -6.7027e-03],
           [-1.6170e-01, -1.3329e-01,  3.4279e-02, -4.8787e-02, -1.7672e-01]]]]])
DESIRED: (shape=torch.Size([2, 4656534, 5, 5, 5]), dtype=torch.float32)
tensor([[[[[-2.4754e-01, -1.7566e-01,  4.6519e-02,  1.2580e-02,  8.1822e-02],
           [-3.4602e-01, -2.3454e-01, -7.1511e-02, -5.7672e-02,  3.7645e-02],
           [ 6.3560e-02,  9.9701e-02,  1.1868e-01, -3.3195e-02,  5.3566e-02],
           [ 1.0611e-02,  3.5513e-02,  9.0208e-02, -1.1322e-01, -6.0836e-03],
           [-1.5166e-01, -1.1733e-01,  3.5179e-02,  4.5582e-03, -5.0136e-02]],

          [[-2.0960e-01, -4.5137e-02,  2.4983e-01,  2.7442e-01,  1.1999e-01],
           [-4.1285e-02,  1.2926e-01,  1.6961e-01,  1.2842e-01,  2.8929e-02],
           [-1.2275e-01,  1.3262e-01,  1.3973e-01,  1.1505e-01,  9.6643e-02],
           [ 9.3592e-02,  2.2300e-01,  2.1936e-01, -7.6244e-03, -1.7907e-01],
           [ 3.2811e-01,  1.0505e-01, -1.0153e-02, -1.2139e-01, -9.0433e-02]],

          [[ 9.1510e-02,  6.1883e-02,  7.7000e-02, -9.6091e-02,  9.2363e-02],
           [-6.5151e-02,  3.5165e-02,  1.1023e-01,  7.9093e-02,  2.4825e-01],
           [ 6.4633e-02,  1.5951e-01,  4.0216e-02, -1.2902e-02,  1.4156e-01],
           [ 3.0358e-02,  5.0625e-02, -3.0713e-03, -1.1605e-01, -8.0821e-02],
           [ 8.9311e-02,  1.3191e-01, -2.2650e-02, -1.0282e-01,  7.1118e-02]],

          [[ 6.0034e-02, -1.0745e-01, -3.1576e-02, -2.0240e-01, -3.3309e-01],
           [-1.6399e-01, -1.3018e-01,  1.1229e-01,  9.0084e-02, -2.0122e-01],
           [-2.6764e-01,  5.9525e-03,  1.6083e-01,  7.8409e-02, -1.1271e-01],
           [-3.1441e-01, -1.2508e-01,  2.1194e-01,  1.3370e-01,  8.8221e-02],
           [-2.6036e-01, -6.9433e-02,  9.4897e-02, -6.3106e-03,  1.2915e-02]],

          [[ 2.3452e-01, -7.7806e-02, -2.7210e-01, -2.7994e-01, -1.4421e-01],
           [ 1.6823e-01,  1.6437e-01, -6.7291e-02, -3.2078e-01, -5.0070e-02],
           [ 1.1170e-01,  1.7896e-02, -7.8833e-02, -1.7938e-01, -1.7644e-01],
           [ 7.8387e-03,  1.0744e-01,  1.5471e-02, -1.0094e-01, -3.2161e-02],
           [-1.8837e-01, -1.0063e-01,  2.3288e-01,  1.6828e-01, -5.0075e-03]]],


         [[[ 3.1129e-02,  2.1662e-01,  6.3783e-02, -1.8702e-01, -4.6790e-02],
           [-9.2469e-03,  2.4741e-01,  7.9033e-02,  9.0584e-02, -1.0863e-02],
           [ 6.1783e-02,  8.1618e-02, -4.2475e-02,  1.2256e-01,  6.0270e-02],
           [-3.1781e-02,  3.3202e-02,  7.5815e-02, -1.5382e-01, -7.3878e-02],
           [-1.4287e-03,  1.0521e-01,  2.1996e-01, -1.4069e-01, -8.8796e-02]],

          [[-8.7271e-02, -9.9203e-02, -3.0476e-02, -1.4728e-02,  6.8144e-03],
           [ 8.9082e-02,  2.9677e-01, -5.7230e-02, -5.1549e-02,  2.1995e-01],
           [ 9.7541e-02,  1.2236e-01, -8.2362e-03, -3.6898e-02, -1.2448e-01],
           [-1.8839e-01, -5.5363e-02, -9.9658e-02, -5.2465e-02, -1.0887e-01],
           [-7.1538e-02, -1.1200e-01, -4.7094e-02, -8.7519e-02, -2.0478e-01]],

          [[ 7.5525e-02,  2.1332e-02, -1.1093e-01,  2.1780e-01,  1.7665e-01],
           [ 2.0276e-02,  8.1922e-02,  3.0965e-02,  3.2977e-01,  2.5512e-01],
           [ 1.0785e-01, -7.8272e-02, -1.0161e-01,  9.7080e-02,  1.0245e-01],
           [ 1.4420e-01,  7.7238e-02, -2.5647e-02, -1.9817e-03, -8.1989e-02],
           [ 1.8355e-01,  2.3926e-01,  1.2023e-01, -8.1921e-02, -9.3139e-02]],

          [[ 6.7390e-03, -1.3981e-01, -1.5200e-01,  7.4757e-03,  2.2691e-01],
           [-1.2792e-01, -1.0078e-01,  6.7479e-02, -3.7067e-02,  4.2799e-02],
           [ 1.0500e-02, -7.5304e-03,  1.2718e-01, -3.9375e-02, -2.6503e-01],
           [ 6.3481e-02,  1.5665e-01,  1.1692e-01, -3.2910e-02, -3.7631e-01],
           [ 6.9712e-02, -6.2756e-03,  1.1888e-01,  1.6393e-01, -1.3914e-02]],

          [[-1.2300e-02,  8.7132e-02, -7.2857e-02, -1.4511e-01, -9.2310e-02],
           [ 3.0199e-01,  3.1244e-01, -1.0331e-01, -2.4511e-01, -5.0446e-02],
           [ 7.7405e-02,  6.5146e-02, -2.1144e-02, -8.7259e-02, -1.0893e-01],
           [-4.0410e-02, -2.5565e-01, -6.7481e-02, -1.0902e-02, -2.1084e-01],
           [-2.3432e-01, -3.2645e-01, -6.9122e-02,  6.1833e-02, -7.9549e-02]]],


         [[[-9.5413e-02,  2.8862e-01,  1.7075e-01,  1.9523e-02, -2.1943e-01],
           [-5.0268e-02,  2.0639e-02,  6.2907e-02, -8.7810e-02, -1.0560e-01],
           [ 2.0509e-01, -3.9978e-02, -5.4234e-02, -1.2724e-01, -2.2644e-02],
           [-7.2181e-03, -2.1400e-01, -3.8350e-02,  8.8922e-02,  2.5122e-01],
           [-7.6365e-02, -2.5885e-01, -7.5640e-02,  1.2346e-01,  1.7524e-01]],

          [[-2.2136e-01,  1.2141e-02, -2.0800e-02,  1.5029e-01,  7.0275e-02],
           [-6.3907e-02,  1.4406e-02,  7.4754e-02,  2.3490e-01,  3.2993e-02],
           [-4.9101e-02, -2.2282e-01, -1.7476e-02,  1.2947e-02, -7.9863e-02],
           [ 4.7638e-02,  3.3926e-02,  2.7644e-02,  1.2219e-01, -6.6442e-02],
           [ 6.0129e-03,  4.9139e-04,  1.6937e-02,  2.0493e-01,  1.8648e-01]],

          [[ 1.7662e-01,  9.7327e-02,  1.2405e-01,  8.4724e-03,  1.7184e-02],
           [ 1.4101e-01,  1.0104e-01,  8.6869e-02,  7.7830e-02,  2.1667e-01],
           [ 1.0373e-01,  3.7344e-02, -3.6442e-02, -3.6413e-02,  1.8845e-01],
           [ 6.1342e-02, -1.6533e-01,  5.8115e-02, -1.8089e-02, -4.0023e-03],
           [ 1.1542e-01,  2.6284e-01,  2.1514e-01,  1.1906e-01,  3.8085e-02]],

          [[ 9.4938e-02, -7.4811e-02, -1.7831e-01, -2.8634e-01, -6.8554e-02],
           [ 1.7537e-01,  6.8882e-02, -1.3707e-01, -2.8818e-01, -1.1975e-01],
           [-5.5504e-02,  3.0843e-02, -2.4364e-02, -1.5223e-01, -9.4794e-02],
           [-9.4114e-02, -1.0492e-01, -2.0520e-01, -8.9957e-02,  4.2126e-04],
           [ 4.6780e-02,  9.1626e-02, -1.5783e-01,  5.6628e-02, -4.6900e-02]],

          [[-6.8037e-02, -1.0241e-01, -2.2016e-03, -1.2051e-01, -2.5656e-01],
           [ 1.1485e-01, -7.8107e-02,  2.6106e-02,  1.3226e-01, -7.5378e-02],
           [-1.5598e-01, -1.6103e-01, -3.2854e-03,  8.5915e-02,  5.0178e-02],
           [-9.2333e-02, -8.1332e-06, -6.4187e-03, -2.1768e-02,  7.2079e-02],
           [-8.9128e-02, -1.0906e-01, -2.4136e-02,  1.6446e-02,  6.0312e-02]]],


         ...,


         [[[-2.1839e-01, -2.0352e-01, -1.5219e-01, -3.0351e-02,  2.5606e-02],
           [-1.2025e-02, -7.9242e-02,  8.0475e-02,  2.2749e-02, -8.3821e-02],
           [ 1.3204e-01,  1.4043e-01,  2.4031e-01,  4.3581e-02, -7.1294e-02],
           [ 2.0583e-01,  1.2373e-02,  1.3116e-01,  6.7905e-02,  1.0802e-01],
           [ 3.3834e-01,  1.4438e-01, -4.6122e-02,  5.8686e-02,  8.0239e-02]],

          [[ 2.5209e-01, -7.0877e-02, -1.0708e-01,  3.0074e-02,  3.0325e-02],
           [ 5.2055e-02, -1.1646e-01, -6.9905e-02, -6.6992e-02, -1.1665e-01],
           [ 1.2255e-02,  9.4167e-02,  3.7935e-02, -4.6106e-02, -5.1754e-02],
           [-1.0337e-01, -1.2945e-01, -9.3917e-02, -1.3973e-02,  2.4698e-01],
           [-5.4941e-02, -2.2897e-01, -2.3873e-01, -1.5673e-01,  9.7679e-02]],

          [[-6.3695e-02,  2.0061e-01,  1.4863e-01, -1.9276e-01, -4.6766e-02],
           [-1.3249e-01,  1.1557e-01,  1.1513e-02, -1.4148e-01, -6.5750e-02],
           [-7.0721e-02, -5.8031e-02,  3.7133e-02,  2.0844e-01,  1.7670e-01],
           [ 8.1747e-02, -2.3260e-03, -3.9468e-02, -9.4769e-02,  6.4387e-02],
           [ 4.3390e-02, -9.7429e-03, -3.2869e-02, -1.7672e-01, -1.4304e-01]],

          [[-2.2551e-01, -2.0813e-01,  9.7632e-02,  3.8782e-02, -1.5129e-01],
           [-3.5756e-01, -1.2134e-01,  2.2455e-01,  2.9848e-02, -4.1815e-02],
           [-8.7242e-02,  5.9001e-02, -3.0805e-02, -1.5966e-01, -1.7188e-01],
           [-1.7400e-01,  2.5460e-02, -1.3547e-01, -2.2748e-01, -3.5421e-01],
           [-3.0634e-02, -3.5065e-02,  3.0685e-02, -2.7435e-02, -2.6322e-01]],

          [[-2.9253e-03,  1.1419e-01, -2.1354e-02, -1.0351e-01,  1.7339e-01],
           [ 5.4164e-02,  2.1300e-01,  3.0884e-02, -9.2353e-02,  5.8271e-02],
           [ 1.1828e-01,  1.9007e-01,  5.0983e-02, -3.7902e-02, -4.0447e-02],
           [-1.0411e-01,  1.3247e-01,  6.1293e-02, -7.4833e-02,  1.0857e-01],
           [-1.8026e-01,  4.5653e-02,  5.3323e-02,  2.2507e-02,  3.5289e-01]]],


         [[[ 5.6470e-02,  2.4565e-01,  1.4215e-01, -2.9696e-02,  1.4750e-01],
           [-4.5081e-02,  1.7838e-01,  1.6967e-01, -1.2194e-01,  3.6710e-02],
           [-1.2337e-01, -9.5513e-02, -5.7521e-02, -2.2184e-01, -5.5876e-02],
           [-2.1541e-02, -1.2687e-01, -1.1839e-01, -7.2142e-02, -1.6206e-02],
           [ 1.1011e-01,  9.9139e-02, -1.1201e-01, -3.7293e-03,  1.2387e-01]],

          [[-1.5800e-01,  1.2633e-02,  1.7303e-01,  1.8624e-01,  6.4500e-02],
           [-9.0169e-02, -5.1099e-02,  2.1415e-01,  1.1445e-01,  1.5510e-01],
           [ 1.5045e-02,  9.0271e-02,  1.5953e-01,  2.9668e-02, -1.2879e-01],
           [-7.4506e-02, -1.4549e-02,  8.8397e-02, -1.8182e-01, -3.6516e-01],
           [-1.8590e-01, -1.3566e-01, -5.5301e-02, -4.0263e-01, -3.5360e-01]],

          [[-1.3279e-01, -7.4134e-02,  1.5088e-02, -1.1418e-01, -1.6689e-04],
           [ 8.9338e-02, -1.3636e-01, -2.4542e-02,  3.9530e-02,  1.3942e-01],
           [ 1.2747e-01,  6.8863e-02,  4.1529e-03, -4.1435e-02,  2.5195e-02],
           [ 1.0701e-01,  4.3009e-02, -1.0672e-01, -2.5304e-01, -2.7326e-01],
           [-7.4054e-02, -7.9777e-02, -8.4971e-03, -1.4689e-01, -1.0226e-01]],

          [[-1.1994e-01, -1.4387e-02, -1.0589e-01, -1.7318e-01,  7.2091e-02],
           [ 5.9157e-02,  1.7422e-01, -8.6786e-02, -1.5075e-01,  7.0953e-02],
           [ 6.3158e-02,  8.7681e-02,  7.3040e-04,  9.5737e-03, -9.8679e-02],
           [-1.6717e-01,  2.2341e-02,  4.5142e-03, -1.1875e-01, -1.8926e-01],
           [-2.6400e-01, -4.3387e-02, -2.9892e-02, -3.5387e-02,  7.1090e-02]],

          [[ 6.8541e-02,  1.4673e-03, -7.5193e-02, -2.1410e-01, -1.9204e-01],
           [ 2.3103e-01, -5.7551e-02, -8.6097e-02, -4.9569e-02, -5.8217e-02],
           [ 2.3904e-01,  2.3762e-01,  5.2644e-02, -7.9401e-02, -1.0261e-01],
           [ 2.8217e-01,  1.7829e-01, -6.0230e-02, -9.7476e-02, -3.5172e-02],
           [ 3.2894e-01,  1.7406e-01, -1.1044e-01,  9.1031e-02,  2.5069e-01]]],


         [[[ 5.8382e-02,  9.2564e-02,  2.3178e-02, -9.5792e-02, -1.5941e-01],
           [-1.2089e-01, -7.7372e-02,  1.0922e-01, -3.3806e-02, -9.5909e-02],
           [-1.3035e-01, -2.1905e-01, -1.1214e-01, -4.5339e-02,  2.0398e-01],
           [-1.4917e-01, -1.2109e-01, -2.3920e-02, -4.9141e-02,  8.8827e-02],
           [-1.8042e-01, -1.0000e-01,  2.1993e-01,  5.6054e-03, -2.0091e-01]],

          [[-1.3989e-01, -2.9135e-01, -1.1608e-01,  1.8714e-01, -1.7109e-02],
           [-4.5054e-02, -2.2046e-01,  2.4333e-02,  2.6713e-01,  1.3647e-01],
           [ 4.0234e-02, -1.2642e-01, -2.6267e-02,  8.3683e-02, -1.5345e-01],
           [ 1.8191e-01,  3.9864e-02,  5.2119e-02,  1.4822e-01, -2.8342e-01],
           [ 1.5623e-01,  2.4821e-01,  5.9593e-02, -5.5342e-02, -3.5828e-01]],

          [[-3.1575e-02,  8.6273e-02, -2.1839e-02, -3.1774e-02,  1.6411e-01],
           [-1.6058e-01,  6.8948e-03,  2.0701e-02,  1.1199e-01,  1.0817e-01],
           [ 9.8833e-02,  8.5513e-02,  5.9436e-02,  1.3552e-01, -3.2647e-02],
           [ 2.6905e-01,  3.2356e-01,  1.8071e-01,  1.2670e-01,  7.8495e-02],
           [-6.3894e-02,  4.9920e-02,  6.2031e-02,  2.4700e-01,  4.9384e-02]],

          [[ 2.4988e-01,  1.4659e-01, -7.5618e-02, -2.4282e-01, -1.3520e-01],
           [ 1.0123e-01,  6.4468e-03, -1.1852e-02, -1.6555e-01, -1.7387e-01],
           [ 3.1209e-02,  2.9417e-02,  1.7171e-02, -6.5201e-02, -1.1780e-01],
           [ 1.6786e-01,  2.4282e-01,  6.1920e-02, -5.2885e-02, -1.6534e-01],
           [-4.7915e-02, -3.3843e-02, -9.4171e-02, -1.0187e-01, -1.8676e-01]],

          [[ 5.7031e-02, -5.6153e-02, -1.4205e-01, -1.0630e-01,  1.4958e-01],
           [-1.1515e-01, -2.5926e-01, -2.4161e-01, -1.1882e-01, -2.1935e-03],
           [-1.4113e-01, -1.2279e-01, -4.3878e-02,  6.2548e-02,  6.3222e-02],
           [ 2.0829e-02,  4.2590e-02,  1.0259e-01,  1.5163e-01, -1.3747e-01],
           [ 3.6319e-02, -1.0194e-01, -1.7106e-02, -1.3708e-01, -3.1096e-01]]]],



        [[[[ 2.3460e-01,  1.1313e-01, -1.7657e-02, -1.5669e-01, -1.4719e-01],
           [ 3.2099e-01,  1.6893e-01,  2.2988e-02, -1.5693e-01, -2.0504e-01],
           [ 8.2922e-02,  9.2410e-02,  1.2636e-01,  2.1014e-02, -1.6902e-01],
           [ 1.7143e-01,  2.2987e-01,  4.1381e-02, -2.1796e-01, -5.6744e-02],
           [ 1.0137e-01,  3.3573e-01, -1.5216e-01, -4.3050e-01, -1.0611e-01]],

          [[-1.6639e-01, -1.2663e-01,  9.1996e-02,  2.3114e-01, -7.0565e-02],
           [-2.0457e-01, -2.4152e-01, -1.0408e-01, -9.1833e-02, -8.3914e-02],
           [-1.0278e-01,  1.9987e-02,  5.4393e-02, -3.7822e-02,  4.3637e-02],
           [-4.6868e-02,  3.7993e-03,  2.1472e-01,  1.0496e-01,  1.4932e-01],
           [-9.8083e-02, -7.4983e-02,  9.6208e-03, -2.0191e-02, -2.7804e-03]],

          [[ 1.9409e-02, -4.1642e-04, -1.0141e-02,  1.5490e-02,  2.1655e-01],
           [ 1.3940e-01, -2.3405e-01, -1.0833e-01,  2.0145e-01,  3.8305e-01],
           [ 1.2942e-02, -1.2558e-01, -6.3514e-02,  2.7769e-02, -1.4696e-02],
           [ 3.2118e-02,  1.8587e-01,  3.2439e-02, -1.2127e-01, -9.3524e-02],
           [ 2.5296e-01,  8.7610e-02,  1.6039e-02,  4.4221e-02, -2.1316e-02]],

          [[ 3.4956e-02,  1.0685e-01, -3.1463e-02,  1.2823e-01, -1.0281e-01],
           [ 3.7548e-02, -1.6845e-01, -1.8609e-01,  6.2533e-02,  8.6082e-02],
           [-2.0648e-01, -1.1369e-01, -2.0879e-02,  2.7427e-02,  2.2745e-01],
           [-1.7353e-01, -4.4902e-02,  2.1823e-03,  7.5949e-02,  1.9443e-01],
           [ 2.7501e-02,  5.5503e-02, -1.5037e-01, -1.1259e-01, -3.8437e-02]],

          [[ 3.4217e-02,  7.5341e-02, -4.1842e-02, -3.4280e-02,  5.5343e-03],
           [-7.5145e-02,  1.5396e-01, -7.7044e-03, -1.4748e-01,  1.3788e-01],
           [-5.2580e-02,  1.3417e-01,  9.1498e-02,  4.9499e-03,  1.7670e-01],
           [-1.7408e-03,  2.2080e-01, -7.5412e-02, -1.8303e-01,  4.8599e-02],
           [-1.3946e-01, -3.2543e-03, -1.1811e-01, -2.5458e-01, -1.5899e-01]]],


         [[[ 1.7876e-01,  1.2952e-01,  8.5433e-02,  6.7565e-02,  1.3002e-01],
           [-1.2873e-01, -9.4756e-02,  1.0468e-01,  8.8164e-02,  4.1358e-02],
           [-2.6944e-02, -1.5306e-01,  2.5137e-02,  9.3890e-02,  2.1213e-05],
           [ 1.7208e-01, -1.6035e-01, -8.4465e-02,  1.0338e-01, -2.1469e-01],
           [ 6.9539e-02, -1.1727e-01, -1.1820e-01, -1.6000e-01, -1.5867e-01]],

          [[-1.0290e-01,  1.1322e-03,  1.5491e-01,  3.1457e-01,  2.3732e-01],
           [-1.6854e-01,  2.4953e-02,  2.2100e-01,  3.1638e-01,  1.9514e-01],
           [ 2.8077e-03, -3.5917e-02,  1.8383e-02,  1.5807e-01,  1.7516e-01],
           [ 1.3611e-01, -2.1428e-01, -2.1947e-01,  3.2111e-02, -9.8724e-02],
           [ 9.4765e-02, -1.2994e-01,  4.4923e-02,  1.7370e-01, -1.6837e-02]],

          [[-2.1819e-02,  6.7075e-03,  9.2127e-02, -1.2199e-01, -8.2447e-02],
           [-1.3376e-01, -1.0031e-01,  1.1039e-01, -9.0568e-02, -2.6138e-01],
           [-2.9389e-01, -7.5599e-02,  1.3363e-01, -3.7297e-02, -6.1135e-02],
           [ 8.6681e-02,  2.9143e-02,  1.3048e-01,  1.8482e-01,  1.0813e-01],
           [ 2.5140e-01,  1.0453e-01,  2.0538e-01,  1.8585e-01,  2.2779e-02]],

          [[ 9.6545e-02, -6.4148e-02,  7.4449e-02,  8.1512e-02, -2.2085e-01],
           [ 1.7519e-01,  2.5260e-01,  1.9965e-01,  2.2706e-01, -9.4862e-02],
           [-4.6058e-02,  4.2110e-02,  1.9325e-02,  1.4486e-01, -4.6321e-02],
           [ 1.1186e-01, -4.3316e-03, -1.7601e-01,  2.5979e-01,  1.6015e-01],
           [ 1.1830e-01, -1.1618e-01, -2.0441e-01,  1.5297e-03,  1.6980e-01]],

          [[ 2.6061e-02, -7.6520e-02, -1.5767e-01,  2.1374e-02,  9.3966e-03],
           [ 4.8727e-02, -3.6156e-02, -5.5114e-03,  1.0755e-01,  8.2969e-02],
           [-5.4237e-02, -1.2404e-01,  5.5708e-02, -1.1389e-02,  1.3761e-01],
           [ 2.3445e-01,  1.3971e-01, -6.6114e-02, -8.1390e-02,  2.0923e-02],
           [ 3.5463e-01,  2.0237e-01, -1.3753e-01, -3.1330e-01, -1.1377e-01]]],


         [[[ 1.6053e-02, -1.5014e-01, -2.3054e-01, -1.5807e-01, -1.9809e-01],
           [ 1.3138e-02, -2.3705e-02, -1.4611e-02, -2.0728e-01, -4.4230e-01],
           [ 4.3882e-02,  1.5924e-01,  1.2073e-01, -1.7352e-02, -2.7302e-01],
           [ 8.4622e-02, -5.4906e-02, -9.9474e-02, -5.7309e-02, -5.7340e-02],
           [ 2.0831e-01, -2.6124e-02, -2.5327e-01, -8.0944e-02, -7.6111e-03]],

          [[-5.8115e-02, -1.9936e-01, -2.3839e-01, -3.1739e-01, -9.8906e-02],
           [ 1.7586e-01, -1.3735e-01, -1.2268e-01, -2.1490e-01, -7.7297e-02],
           [ 9.7420e-02,  5.1519e-02, -1.7276e-02,  7.7271e-02,  1.1081e-01],
           [-4.6786e-02,  1.0418e-01, -9.0308e-03, -6.8263e-03,  5.8657e-02],
           [ 8.0125e-02, -1.2066e-02, -9.3309e-02, -1.2197e-01,  6.0807e-02]],

          [[-1.9457e-01, -1.6079e-01,  1.3489e-01, -6.4541e-03,  6.3035e-02],
           [-8.7274e-02,  3.3860e-02, -7.8685e-02, -1.9299e-01, -6.6165e-02],
           [ 9.6051e-02,  5.5405e-02, -9.5750e-02,  7.4729e-02,  2.8794e-02],
           [-8.9374e-02,  1.0518e-01, -3.2077e-02,  8.6472e-03, -1.5723e-01],
           [-1.3352e-01, -1.0071e-01, -6.5512e-02, -4.1128e-02, -2.2810e-01]],

          [[ 8.0539e-02,  2.0371e-02,  2.3385e-02,  2.6167e-01, -4.9690e-03],
           [-2.0403e-01, -1.1684e-01, -2.3563e-02,  2.9943e-01,  6.5721e-02],
           [-4.5533e-02,  7.4159e-02,  1.1471e-01,  2.1836e-01, -3.8446e-03],
           [-1.2756e-01,  1.0413e-01,  2.3488e-01,  2.0579e-01, -1.9992e-01],
           [-2.9913e-02, -6.5656e-02,  2.1716e-01,  9.5433e-02, -1.3029e-01]],

          [[-3.2719e-01, -2.9223e-01, -1.1740e-01,  5.8288e-02,  1.4016e-01],
           [-1.7338e-01, -1.3911e-01, -1.8751e-02, -8.0098e-03,  6.7860e-02],
           [ 2.0250e-01,  2.1914e-02, -6.4629e-02, -3.8304e-02, -1.1254e-01],
           [ 3.9100e-01,  2.5064e-01,  1.1626e-01, -1.8743e-02, -2.5547e-01],
           [ 2.6109e-01,  2.9738e-01,  8.4271e-02,  4.1345e-02, -2.5369e-02]]],


         ...,


         [[[-1.0479e-01,  1.0051e-01, -5.9363e-02, -3.4990e-02,  1.5675e-01],
           [ 8.4578e-02,  1.2322e-02, -7.1868e-02, -3.4663e-02, -1.0388e-01],
           [ 1.7101e-01,  2.7926e-02, -8.6573e-02, -6.1130e-02, -1.7971e-01],
           [ 8.8679e-02, -1.5895e-01, -2.9883e-01, -3.9026e-02, -7.9610e-02],
           [-5.3678e-02,  7.6659e-02, -6.0150e-02, -3.7884e-02,  1.4952e-01]],

          [[ 2.3783e-02,  3.8414e-02, -6.7909e-03, -1.3166e-01, -1.2630e-01],
           [ 1.7933e-01,  9.6162e-02, -5.1626e-02, -1.0012e-01, -5.6817e-02],
           [ 8.0887e-02,  7.0375e-02,  4.6448e-03,  2.7087e-02, -3.2805e-02],
           [ 3.6046e-02, -2.2969e-02, -1.4198e-02, -9.8678e-02,  4.6774e-02],
           [ 1.5982e-01,  1.2297e-01,  2.4707e-01,  1.6977e-01,  2.6697e-02]],

          [[ 2.6956e-01, -3.9469e-02, -5.0035e-02,  1.5085e-01,  2.6902e-01],
           [ 4.8042e-02, -6.5235e-02,  1.3477e-02,  1.0718e-01,  7.4791e-02],
           [ 1.1920e-01,  1.3180e-01,  1.2789e-01,  1.9311e-02, -8.9509e-02],
           [ 2.1460e-03,  1.0745e-02,  6.9946e-02, -7.9205e-02, -1.2691e-01],
           [-1.4829e-01, -8.8365e-02,  9.2068e-02,  2.3136e-01,  5.5471e-02]],

          [[-2.0094e-01, -1.4818e-01, -1.9373e-02,  2.2044e-01,  3.0658e-01],
           [-1.2566e-01, -1.6422e-01, -1.4622e-02, -3.0199e-02,  2.2404e-01],
           [-1.1106e-01, -3.9787e-02,  8.3726e-02,  1.4284e-03,  8.5765e-02],
           [ 5.1737e-02,  4.5422e-02, -5.7327e-04,  7.6312e-02,  1.0244e-01],
           [-6.8874e-02,  1.3433e-01, -8.4078e-02, -1.1942e-02, -1.7127e-01]],

          [[ 5.7438e-02,  5.9603e-02, -1.4619e-01, -1.0007e-01, -2.1053e-01],
           [ 4.2728e-02,  1.1936e-02, -1.3317e-01, -3.0908e-01, -1.7722e-01],
           [-1.5700e-01, -9.9405e-02,  1.3382e-02, -1.2423e-01, -3.2186e-02],
           [-1.1901e-01,  7.1846e-02,  2.1726e-01,  7.1097e-02,  1.1333e-01],
           [ 2.3581e-01,  2.2743e-01,  1.3978e-01,  3.0935e-02,  2.8744e-01]]],


         [[[ 2.6436e-01,  1.6235e-01, -2.1291e-02, -1.0607e-01, -1.7187e-02],
           [ 7.0326e-02,  3.7983e-03,  4.6443e-03,  4.5394e-02,  1.0442e-01],
           [ 6.4809e-02,  1.4686e-01,  1.9403e-01,  2.9725e-01,  1.5064e-01],
           [ 2.3816e-01,  2.0990e-01,  2.1127e-02,  3.6023e-02,  2.4076e-01],
           [ 1.2839e-01,  1.9181e-01, -1.2803e-01, -2.3893e-01, -4.7531e-02]],

          [[-1.8029e-01, -6.6741e-02, -3.2001e-02, -1.2360e-01,  2.2573e-01],
           [-1.0727e-01, -6.3408e-02, -1.1581e-03,  1.1547e-01,  1.9329e-01],
           [ 7.9185e-02,  3.1672e-02,  7.5760e-02,  2.5611e-01,  1.6262e-01],
           [ 1.1321e-01,  1.2269e-01,  1.0954e-01,  1.9822e-01, -4.3747e-02],
           [-1.0454e-02, -1.2350e-02,  1.3518e-01,  1.3583e-01, -8.8714e-02]],

          [[-1.5332e-01, -8.7367e-02, -2.0825e-01, -2.0929e-01, -1.0884e-03],
           [-5.1320e-04, -1.0337e-02, -2.1317e-01, -2.7475e-01,  1.0815e-01],
           [ 3.7816e-02,  1.0557e-01, -1.6685e-01, -2.0474e-01,  7.7536e-02],
           [ 1.0184e-01,  1.7537e-01, -7.1631e-02, -4.8032e-02,  5.5396e-02],
           [-1.1160e-01, -2.2539e-04,  1.1052e-01,  7.9020e-03, -2.9301e-01]],

          [[ 1.1717e-01,  2.5460e-01, -7.0550e-02, -1.2273e-01, -1.0429e-02],
           [ 1.2043e-01,  1.7939e-01, -7.2791e-02,  1.3983e-02,  1.8927e-01],
           [ 6.8165e-02, -1.2582e-02, -1.0906e-01,  1.0885e-02,  6.1820e-02],
           [-3.2805e-02, -1.6118e-02,  7.7707e-02, -6.5398e-02, -6.9683e-02],
           [ 1.4291e-02,  1.6825e-01,  9.7252e-02, -1.1792e-01, -6.6803e-03]],

          [[-1.4461e-01,  2.3065e-02, -5.4612e-02, -1.8642e-01, -1.8271e-01],
           [ 1.6075e-02, -4.6612e-02, -6.2039e-02, -3.3393e-02,  5.5170e-02],
           [-1.2731e-01, -1.2816e-01, -1.0495e-01, -1.4475e-01,  7.2920e-02],
           [-4.5781e-02,  6.5532e-03, -1.0611e-01, -1.6441e-02,  1.7273e-01],
           [ 8.0112e-02,  1.1778e-01,  9.8364e-02,  2.4068e-01,  2.5714e-01]]],


         [[[ 1.3585e-01,  2.1218e-01,  1.9059e-01, -6.7154e-02,  1.2160e-01],
           [ 2.3178e-01,  8.9135e-02, -3.1822e-02, -9.2346e-02, -4.7104e-02],
           [ 1.4718e-01,  1.0080e-01, -1.9004e-01,  9.0704e-03,  6.7536e-02],
           [ 1.3220e-01, -3.9739e-02, -3.0004e-02,  1.5870e-01,  8.8572e-02],
           [ 6.4795e-02, -3.8852e-02,  2.3828e-01,  4.0043e-01,  2.3304e-01]],

          [[-2.7895e-01, -3.2158e-02,  8.7026e-02,  2.0703e-01,  2.0064e-01],
           [-2.7009e-01,  1.0184e-01,  1.0463e-02,  1.3679e-01,  1.5250e-01],
           [-1.8537e-01, -1.3589e-01, -1.5547e-01, -1.6027e-01, -9.2191e-02],
           [-5.0652e-02, -7.5620e-02, -1.0351e-01, -2.1753e-01, -2.1726e-02],
           [ 6.8334e-02,  3.0196e-02, -8.7949e-02, -2.8791e-01, -8.9017e-02]],

          [[ 1.2057e-01, -1.5497e-01, -2.6320e-02, -1.7202e-02, -1.0887e-01],
           [ 5.7035e-03, -1.5839e-01, -5.4324e-02,  4.5631e-02, -8.5717e-02],
           [-8.0339e-02, -1.4786e-01,  7.3986e-02, -1.9479e-02, -1.6699e-01],
           [-1.1801e-01, -1.6715e-01,  1.2061e-01, -7.1590e-02, -3.5157e-01],
           [ 5.0610e-02,  3.7073e-02,  2.9230e-02, -1.3933e-01, -2.0951e-01]],

          [[-3.2288e-02,  5.7667e-02,  2.6267e-02, -1.5993e-02,  2.8085e-02],
           [ 2.4960e-01,  2.3777e-01,  6.0405e-02, -1.9032e-02, -9.3823e-02],
           [-1.0050e-01,  1.3708e-01,  5.7569e-02, -8.0784e-02, -1.1917e-01],
           [-8.5680e-02,  1.9178e-01,  2.0999e-01,  1.4298e-01,  8.7479e-03],
           [-1.5245e-02,  6.8275e-02,  2.1059e-01, -6.0931e-02, -9.5732e-02]],

          [[-1.5025e-01, -1.8897e-01, -9.1721e-02, -2.1037e-01, -2.0626e-01],
           [-9.5751e-02, -3.2014e-02, -5.3245e-02, -2.3315e-01, -2.7067e-01],
           [ 8.5627e-02,  6.2798e-02, -1.1401e-01, -8.4107e-02, -2.3793e-02],
           [ 1.3163e-02, -5.4790e-04, -2.0570e-01, -1.8009e-01, -6.7027e-03],
           [-1.6170e-01, -1.3329e-01,  3.4279e-02, -4.8787e-02, -1.7672e-01]]]]])

2025-07-08 22:27:26.058673 GPU 5 1929 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 4656534, 5, 7, 7],"float32"), output_size=list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:27:35.641718 GPU 3 4810 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 4656534, 5, 7, 7],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:27:38.673388 GPU 5 4901 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 8765240, 5, 7, 7],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:28:25.778051 GPU 2 5226 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 8765240, 5, 7, 7],"float32"), output_size=list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:28:37.738124 GPU 4 3995 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 8765240, 5, 7, 7],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:28:49.381770 GPU 3 5398 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 8765240, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:28:57.976066 GPU 7 2995 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([2, 8765240, 5, 7, 7],"float32"), output_size=list[None,3,None,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:28:59.433919 GPU 5 5559 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([3104356, 3, 5, 7, 7],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:29:12.705830 GPU 7 5719 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([3104356, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:29:40.806782 GPU 2 6118 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([3104356, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:29:53.375297 GPU 4 6278 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([3104356, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:30:23.497774 GPU 5 6445 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([5843493, 3, 5, 7, 7],"float32"), output_size=5, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:30:24.499645 GPU 3 6531 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([5843493, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:30:39.576640 GPU 7 6765 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([5843493, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:31:13.698650 GPU 2 6933 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([5843493, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], data_format="NDHWC", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:31:14.007545 GPU 4 7014 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([5843493, 3, 5, 7, 7],"float32"), output_size=list[None,3,None,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:31:46.258507 GPU 5 7501 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([87382, 3, 8, 32, 32],"float64"), output_size=3, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:31:55.573133 GPU 7 7662 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[1,1,1,], )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[1,1,1,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:31:58.339741 GPU 3 7751 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[1,3,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:32:32.996858 GPU 4 8055 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[2,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:34:03.874310 GPU 5 8564 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([87382, 3, 8, 32, 32],"float64"), output_size=list[3,3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:34:22.361015 GPU 7 7662 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([87382, 3, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), )
[cuda error] backward paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([87382, 3, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-08 22:34:32.809232 GPU 3 8735 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([87382, 3, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NCDHW", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:34:38.669270 GPU 4 8896 test begin: paddle.nn.functional.adaptive_avg_pool3d(x=Tensor([87382, 3, 8, 32, 32],"float64"), output_size=tuple(3,3,3,), data_format="NDHWC", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:34:58.037026 GPU 2 9065 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([285212673, 8],"float32"), Tensor([4],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([285212673, 8],"float32"), Tensor([4],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, ) 
 Input and label should have the same size in the batch dimension.

2025-07-08 22:36:44.807542 GPU 4 9641 test begin: paddle.nn.functional.avg_pool1d(Tensor([2, 134217729, 8],"float64"), 2, 1, 0, True, True, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:36:52.750141 GPU 7 7662 test begin: paddle.nn.functional.avg_pool1d(Tensor([2, 134217729, 8],"float64"), 2, 2, 0, True, False, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:37:30.589263 GPU 5 9477 test begin: paddle.nn.functional.avg_pool1d(Tensor([2, 134217729, 8],"float64"), 3, 4, 0, True, False, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:38:50.637457 GPU 7 10289 test begin: paddle.nn.functional.avg_pool1d(Tensor([2, 3, 357913942],"float64"), 2, 1, 0, True, True, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:39:12.567420 GPU 2 10206 test begin: paddle.nn.functional.avg_pool1d(Tensor([2, 3, 357913942],"float64"), 2, 2, 0, True, False, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:39:34.984738 GPU 5 10780 test begin: paddle.nn.functional.avg_pool1d(Tensor([2, 3, 357913942],"float64"), 3, 4, 0, True, False, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:40:00.687486 GPU 4 10943 test begin: paddle.nn.functional.avg_pool1d(Tensor([2, 3, 715827883],"float32"), 2, 2, 0, True, False, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:40:00.877412 GPU 3 9727 test begin: paddle.nn.functional.avg_pool1d(Tensor([2, 3, 715827883],"float32"), 2, None, 0, True, False, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:41:01.421164 GPU 7 11273 test begin: paddle.nn.functional.avg_pool1d(Tensor([2, 3, 715827883],"float32"), kernel_size=2, stride=2, padding=list[0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:42:45.212890 GPU 7 12167 test begin: paddle.nn.functional.avg_pool1d(Tensor([89478486, 3, 8],"float64"), 2, 2, 0, True, False, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:42:52.211636 GPU 4 11596 test begin: paddle.nn.functional.avg_pool1d(Tensor([89478486, 3, 8],"float64"), 3, 4, 0, True, False, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:44:06.125795 GPU 2 11436 test begin: paddle.nn.functional.avg_pool1d(x=Tensor([2, 134217729, 8],"float64"), kernel_size=2, stride=1, padding=0, ceil_mode=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:44:19.882970 GPU 3 12589 test begin: paddle.nn.functional.avg_pool1d(x=Tensor([2, 134217729, 8],"float64"), kernel_size=2, stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:44:54.532960 GPU 5 11997 test begin: paddle.nn.functional.avg_pool1d(x=Tensor([2, 134217729, 8],"float64"), kernel_size=3, stride=4, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:46:20.114590 GPU 2 13324 test begin: paddle.nn.functional.avg_pool1d(x=Tensor([2, 3, 357913942],"float64"), kernel_size=2, stride=1, padding=0, ceil_mode=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:46:55.638855 GPU 7 13159 test begin: paddle.nn.functional.avg_pool1d(x=Tensor([2, 3, 357913942],"float64"), kernel_size=2, stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:46:57.582823 GPU 3 13490 test begin: paddle.nn.functional.avg_pool1d(x=Tensor([2, 3, 357913942],"float64"), kernel_size=3, stride=4, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:47:05.712422 GPU 4 12756 test begin: paddle.nn.functional.avg_pool1d(x=Tensor([2, 3, 715827883],"float32"), kernel_size=2, stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:47:24.932856 GPU 4 13813 test begin: paddle.nn.functional.avg_pool1d(x=Tensor([89478486, 3, 8],"float64"), kernel_size=2, stride=1, padding=0, ceil_mode=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:48:38.940424 GPU 2 14224 test begin: paddle.nn.functional.avg_pool1d(x=Tensor([89478486, 3, 8],"float64"), kernel_size=2, stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:48:46.828337 GPU 5 13653 test begin: paddle.nn.functional.avg_pool1d(x=Tensor([89478486, 3, 8],"float64"), kernel_size=3, stride=4, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:49:13.628875 GPU 3 14389 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1024, 40, 104858],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:49:24.578733 GPU 7 14550 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1024, 42, 99865],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:49:47.237633 GPU 4 14947 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1024, 44, 95326],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:50:38.908139 GPU 3 15118 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1024, 65536, 64],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 22:50:49.847408 GPU 7 15279 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1024, 67651, 62],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:50:51.669990 GPU 5 15367 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1024, 69906, 60],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:50:57.064114 GPU 2 15596 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1024, 77673, 54],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:51:08.720641 GPU 4 15757 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 10737419, 20, 20],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:51:57.291956 GPU 3 16161 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 128, 1677722, 20],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:52:08.950640 GPU 7 16324 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 128, 20, 1677722],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:52:10.423452 GPU 5 16411 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1597831, 42, 64],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:52:30.663562 GPU 2 16645 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1626882, 44, 60],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:52:47.577127 GPU 4 16808 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 171798692, 5, 5],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:53:21.175539 GPU 3 16974 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1731842, 40, 62],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:53:44.239421 GPU 5 17376 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1789570, 40, 60],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:53:46.932987 GPU 7 17463 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 1988411, 40, 54],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:54:46.485242 GPU 3 18021 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 21913099, 14, 14],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:55:16.364049 GPU 5 18189 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 256, 10, 1677722],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:55:17.478965 GPU 7 18274 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 256, 1677722, 10],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:55:35.231022 GPU 4 17858 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 42949673, 10, 10],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:55:36.474985 GPU 2 17693 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 477218589, 3, 3],"float32"), kernel_size=2, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:55:50.657962 GPU 4 18748 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 512, 1677722, 5],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:55:55.120379 GPU 2 18906 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 512, 5, 1677722],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:56:19.362703 GPU 3 19070 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 8, 14, 38347923],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:56:55.354697 GPU 7 19238 test begin: paddle.nn.functional.avg_pool2d(Tensor([1, 8, 38347923, 14],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:57:00.345657 GPU 5 19396 test begin: paddle.nn.functional.avg_pool2d(Tensor([105352, 208, 14, 14],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:57:15.240335 GPU 4 19559 test begin: paddle.nn.functional.avg_pool2d(Tensor([10700, 512, 28, 28],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:57:25.340133 GPU 2 19719 test begin: paddle.nn.functional.avg_pool2d(Tensor([119304648, 4, 3, 3],"float32"), kernel_size=2, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:59:27.380486 GPU 3 20208 test begin: paddle.nn.functional.avg_pool2d(Tensor([1398102, 3, 32, 32],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 22:59:42.461775 GPU 3 21261 test begin: paddle.nn.functional.avg_pool2d(Tensor([1398102, 3, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:00:27.799804 GPU 4 20384 test begin: paddle.nn.functional.avg_pool2d(Tensor([1398102, 3, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[list[0,0,],list[0,0,],list[0,0,],list[0,0,],], divisor_override=4, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:00:32.002294 GPU 2 20789 test begin: paddle.nn.functional.avg_pool2d(Tensor([1398102, 3, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:00:32.372426 GPU 5 20448 test begin: paddle.nn.functional.avg_pool2d(Tensor([1398102, 3, 32, 32],"float32"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:00:54.780241 GPU 7 20702 test begin: paddle.nn.functional.avg_pool2d(Tensor([1561, 1024, 42, 64],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:01:04.369125 GPU 2 21759 test begin: paddle.nn.functional.avg_pool2d(Tensor([1589, 1024, 44, 60],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:01:05.797935 GPU 7 21846 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 1024, 32768, 8],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:01:07.656562 GPU 3 21933 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 1024, 4, 65536],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:02:01.587097 GPU 5 21595 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 1024, 65536, 4],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:02:11.888878 GPU 5 22482 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 1024, 8, 32768],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:02:22.495526 GPU 2 22646 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 256, 8192],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:02:23.556908 GPU 7 22733 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 32, 65536],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:02:25.769072 GPU 4 21435 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 32768, 64],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:02:32.562827 GPU 3 22963 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 64, 32768],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:03:37.894238 GPU 5 23382 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 65536, 32],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:03:40.931365 GPU 4 23470 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 8192, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:03:43.755191 GPU 7 23629 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 16777216, 4, 4],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:03:48.855841 GPU 2 23859 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 262144, 32, 32],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:03:55.996221 GPU 3 24018 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 4096, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:04:55.314223 GPU 5 24189 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 4194304, 8, 8],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:05:08.025485 GPU 2 24355 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 65536, 64, 64],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:05:08.087797 GPU 4 24419 test begin: paddle.nn.functional.avg_pool2d(Tensor([167773, 256, 10, 10],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:05:15.288342 GPU 3 24672 test begin: paddle.nn.functional.avg_pool2d(Tensor([1692, 1024, 40, 62],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:05:15.561291 GPU 7 24737 test begin: paddle.nn.functional.avg_pool2d(Tensor([1748, 1024, 40, 60],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:06:30.589275 GPU 4 25400 test begin: paddle.nn.functional.avg_pool2d(Tensor([1942, 1024, 40, 54],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:06:36.765186 GPU 2 25561 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 1048577, 32, 32],"float64"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:06:45.249395 GPU 3 25722 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 1048577, 32, 32],"float64"), kernel_size=list[3,3,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:06:45.634892 GPU 7 25786 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 1056, 14, 145258],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:07:39.259204 GPU 5 25237 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 1056, 145258, 14],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:07:49.549636 GPU 5 26299 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 10956550, 14, 14],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:07:54.007060 GPU 4 26458 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 12707004, 13, 13],"float32"), kernel_size=5, stride=3, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:09:42.672037 GPU 7 26621 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 2097152, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:09:54.480966 GPU 7 27669 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 2097152, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[list[0,0,],list[0,0,],list[0,0,],list[0,0,],], divisor_override=4, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:10:19.057951 GPU 2 26790 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 2097152, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:10:22.585579 GPU 5 27109 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 2097152, 32, 32],"float32"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:10:33.997701 GPU 4 27267 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 11184811, 32],"float64"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:10:35.079772 GPU 5 27927 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 11184811, 32],"float64"), kernel_size=list[3,3,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:10:40.086367 GPU 3 26952 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 22369622, 32],"float32"), kernel_size=2, stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:11:21.328507 GPU 7 28324 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 22369622, 32],"float32"), kernel_size=2, stride=None, padding="SAME", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:11:44.757521 GPU 2 27838 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 22369622, 32],"float32"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:12:02.651710 GPU 2 28728 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 32, 11184811],"float64"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:12:23.432850 GPU 3 28891 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 32, 11184811],"float64"), kernel_size=list[3,3,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:12:41.370189 GPU 7 29054 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 32, 22369622],"float32"), kernel_size=2, stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:12:44.094639 GPU 4 29142 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 32, 22369622],"float32"), kernel_size=2, stride=2, padding=list[list[0,0,],list[0,0,],list[0,0,],list[0,0,],], divisor_override=4, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:13:35.172731 GPU 5 29617 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 32, 22369622],"float32"), kernel_size=2, stride=None, padding="SAME", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:14:08.908705 GPU 7 29784 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 32, 22369622],"float32"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:14:21.141617 GPU 4 29945 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 4, 89478486],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:14:43.128106 GPU 3 30265 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 4, 89478486],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:14:56.318320 GPU 5 30426 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 4, 89478486],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=True, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:15:11.244689 GPU 2 30103 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 4, 89478486],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:15:32.036859 GPU 7 30970 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 4, 89478486],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:16:44.127644 GPU 4 31145 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 89478486, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:17:06.671629 GPU 5 31466 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 89478486, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:17:14.800657 GPU 2 31626 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 89478486, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=True, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:17:45.190750 GPU 7 32029 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 89478486, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:17:56.488587 GPU 3 31306 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 3, 89478486, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:19:37.075745 GPU 5 32623 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 512, 13, 322639],"float32"), kernel_size=5, stride=3, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:19:38.034503 GPU 2 32710 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 512, 322639, 13],"float32"), kernel_size=5, stride=3, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:20:06.897886 GPU 3 32944 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 528, 13, 312862],"float32"), kernel_size=5, stride=3, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:20:12.655408 GPU 4 32205 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 528, 312862, 13],"float32"), kernel_size=5, stride=3, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:20:13.901510 GPU 7 33105 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 67108865, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:20:54.681182 GPU 5 33434 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 67108865, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:20:56.919538 GPU 2 33521 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 67108865, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=True, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:21:26.403278 GPU 4 33267 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 67108865, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:21:35.163192 GPU 3 33996 test begin: paddle.nn.functional.avg_pool2d(Tensor([2, 67108865, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:23:03.748473 GPU 2 34333 test begin: paddle.nn.functional.avg_pool2d(Tensor([20752, 1056, 14, 14],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:23:16.296804 GPU 5 34493 test begin: paddle.nn.functional.avg_pool2d(Tensor([21400, 1024, 14, 14],"float16"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:23:32.302269 GPU 4 34890 test begin: paddle.nn.functional.avg_pool2d(Tensor([21400, 1024, 14, 14],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:23:46.855835 GPU 3 35052 test begin: paddle.nn.functional.avg_pool2d(Tensor([22369622, 3, 8, 8],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:26:03.274611 GPU 7 34169 test begin: paddle.nn.functional.avg_pool2d(Tensor([262144, 1024, 4, 4],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:26:26.748468 GPU 4 35476 test begin: paddle.nn.functional.avg_pool2d(Tensor([2739138, 8, 14, 14],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:27:14.200142 GPU 2 35219 test begin: paddle.nn.functional.avg_pool2d(Tensor([32, 1024, 14, 9363],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:27:26.519295 GPU 2 36446 test begin: paddle.nn.functional.avg_pool2d(Tensor([32, 1024, 9363, 14],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:27:34.885085 GPU 3 35949 test begin: paddle.nn.functional.avg_pool2d(Tensor([32, 171197, 28, 28],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:27:37.419756 GPU 7 36114 test begin: paddle.nn.functional.avg_pool2d(Tensor([32, 256, 56, 9363],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:27:48.758952 GPU 7 36855 test begin: paddle.nn.functional.avg_pool2d(Tensor([32, 256, 9363, 56],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:27:48.758974 GPU 5 35388 test begin: paddle.nn.functional.avg_pool2d(Tensor([32, 42800, 56, 56],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:27:53.431133 GPU 4 36279 test begin: paddle.nn.functional.avg_pool2d(Tensor([32, 512, 9363, 28],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:28:01.361864 GPU 5 37106 test begin: paddle.nn.functional.avg_pool2d(Tensor([32, 684785, 14, 14],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:28:04.084335 GPU 4 37196 test begin: paddle.nn.functional.avg_pool2d(Tensor([32768, 128, 32, 32],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:28:57.499250 GPU 3 37525 test begin: paddle.nn.functional.avg_pool2d(Tensor([335545, 512, 5, 5],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:29:09.208630 GPU 7 37757 test begin: paddle.nn.functional.avg_pool2d(Tensor([349526, 3, 64, 64],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:29:23.757345 GPU 5 37922 test begin: paddle.nn.functional.avg_pool2d(Tensor([34953, 384, 4, 80],"float32"), list[4,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:29:32.387932 GPU 4 38316 test begin: paddle.nn.functional.avg_pool2d(Tensor([34953, 512, 3, 80],"float16"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:30:30.694880 GPU 3 38492 test begin: paddle.nn.functional.avg_pool2d(Tensor([34953, 512, 3, 80],"float32"), kernel_size=list[2,2,], stride=list[2,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:30:36.220178 GPU 2 37438 test begin: paddle.nn.functional.avg_pool2d(Tensor([34953, 512, 3, 80],"float32"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:30:43.933715 GPU 5 38812 test begin: paddle.nn.functional.avg_pool2d(Tensor([4, 3355444, 4, 80],"float32"), list[4,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:30:49.691774 GPU 2 38971 test begin: paddle.nn.functional.avg_pool2d(Tensor([4, 384, 34953, 80],"float32"), list[4,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:31:21.610103 GPU 4 39136 test begin: paddle.nn.functional.avg_pool2d(Tensor([4, 384, 4, 699051],"float32"), list[4,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:32:14.208969 GPU 5 39703 test begin: paddle.nn.functional.avg_pool2d(Tensor([44739243, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:33:05.045773 GPU 2 39862 test begin: paddle.nn.functional.avg_pool2d(Tensor([44739243, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:33:08.981731 GPU 4 40034 test begin: paddle.nn.functional.avg_pool2d(Tensor([44739243, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=True, exclusive=False, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:33:12.229862 GPU 3 39539 test begin: paddle.nn.functional.avg_pool2d(Tensor([44739243, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:33:12.316816 GPU 7 38653 test begin: paddle.nn.functional.avg_pool2d(Tensor([44739243, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:34:23.542892 GPU 5 40443 test begin: paddle.nn.functional.avg_pool2d(Tensor([48133, 528, 13, 13],"float32"), kernel_size=5, stride=3, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:35:11.549279 GPU 2 40611 test begin: paddle.nn.functional.avg_pool2d(Tensor([49637, 512, 13, 13],"float32"), kernel_size=5, stride=3, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:35:19.993727 GPU 7 40838 test begin: paddle.nn.functional.avg_pool2d(Tensor([512, 128, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:35:22.634844 GPU 3 40926 test begin: paddle.nn.functional.avg_pool2d(Tensor([52676, 104, 28, 28],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:35:43.029158 GPU 5 41394 test begin: paddle.nn.functional.avg_pool2d(Tensor([52676, 104, 28, 28],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:36:08.514405 GPU 4 41557 test begin: paddle.nn.functional.avg_pool2d(Tensor([5350, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:36:38.696212 GPU 7 41725 test begin: paddle.nn.functional.avg_pool2d(Tensor([5592406, 3, 16, 16],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:36:38.729548 GPU 2 41724 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 1198373, 8, 8],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:37:03.860236 GPU 3 42046 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 18725, 64, 64],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:37:14.240422 GPU 5 42206 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 299594, 16, 16],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:37:39.747421 GPU 4 42605 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 3, 1597831, 16],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:38:02.084971 GPU 7 42769 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 3, 16, 1597831],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:38:09.097023 GPU 2 42929 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 3, 3195661, 8],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:38:36.374644 GPU 3 43094 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 3, 32, 798916],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:38:44.774389 GPU 5 43254 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 3, 399458, 64],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:39:00.954860 GPU 4 43416 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 3, 4, 6391321],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:39:34.417656 GPU 7 43818 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 3, 6391321, 4],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:39:39.489159 GPU 2 43977 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 3, 64, 399458],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:40:21.465914 GPU 3 44146 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 3, 798916, 32],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:40:21.639223 GPU 5 44211 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 3, 8, 3195661],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:40:38.493698 GPU 4 44466 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 4793491, 4, 4],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:41:09.570711 GPU 7 44632 test begin: paddle.nn.functional.avg_pool2d(Tensor([56, 74899, 32, 32],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:41:14.045223 GPU 2 44791 test begin: paddle.nn.functional.avg_pool2d(Tensor([57066, 1536, 7, 7],"float16"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:42:01.570365 GPU 4 45213 test begin: paddle.nn.functional.avg_pool2d(Tensor([57066, 1536, 7, 7],"float32"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:42:03.952564 GPU 5 45302 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 1024, 14, 4682],"float16"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:42:05.832183 GPU 3 45459 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 1024, 14, 4682],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:43:51.971663 GPU 2 45855 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 1024, 4682, 14],"float16"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:44:00.442339 GPU 5 46583 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 1024, 4682, 14],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:44:07.038320 GPU 7 45696 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 104, 23046, 28],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:44:14.375718 GPU 7 46745 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 104, 23046, 28],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:44:57.378327 GPU 3 46418 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 104, 28, 23046],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:45:06.128835 GPU 4 46024 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 104, 28, 23046],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:46:24.201720 GPU 4 47642 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 128, 64, 8192],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:46:43.491095 GPU 2 46917 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 128, 64, 8192],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:47:10.567626 GPU 7 47476 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 128, 8192, 64],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:47:54.899305 GPU 4 48374 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 128, 8192, 64],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:47:58.692401 GPU 2 48533 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 1369569, 7, 7],"float16"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:48:07.327694 GPU 3 47730 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 1369569, 7, 7],"float32"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:48:23.458122 GPU 3 48696 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 1536, 6242, 7],"float16"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:48:26.196210 GPU 5 47079 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 1536, 7, 6242],"float16"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:48:36.745208 GPU 5 48857 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 1536, 7, 6242],"float32"), kernel_size=7, stride=1, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:49:52.135262 GPU 7 47971 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 16384, 64, 64],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:49:54.697621 GPU 5 49674 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 16384, 64, 64],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:49:59.870150 GPU 7 49903 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 208, 14, 23046],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:50:54.099081 GPU 4 49028 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 208, 23046, 14],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:51:19.093508 GPU 3 49587 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 279621, 3, 80],"float16"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:51:23.697089 GPU 5 50082 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 279621, 3, 80],"float32"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:51:37.769856 GPU 7 50478 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 342393, 14, 14],"float16"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:52:12.676246 GPU 2 49116 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 342393, 14, 14],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:52:22.028833 GPU 2 50645 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 342393, 14, 14],"float32"), kernel_size=2, stride=2, padding="SAME", ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:52:55.861565 GPU 5 51127 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 512, 1639, 80],"float16"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:53:32.036056 GPU 7 51327 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 512, 1639, 80],"float32"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:54:06.970399 GPU 2 51694 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 512, 3, 43691],"float16"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:54:12.315307 GPU 4 50804 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 512, 3, 43691],"float32"), kernel_size=list[3,2,], stride=list[3,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:54:27.332607 GPU 3 50967 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 85599, 28, 28],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:54:28.841315 GPU 5 51860 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 85599, 28, 28],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:54:35.066720 GPU 3 52019 test begin: paddle.nn.functional.avg_pool2d(Tensor([65536, 1024, 8, 8],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:54:49.467702 GPU 7 52181 test begin: paddle.nn.functional.avg_pool2d(Tensor([699051, 3, 32, 32],"float64"), kernel_size=list[2,2,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:55:28.623305 GPU 4 52347 test begin: paddle.nn.functional.avg_pool2d(Tensor([699051, 3, 32, 32],"float64"), kernel_size=list[3,3,], stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:55:46.978092 GPU 5 52746 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 2236963, 3, 80],"float32"), kernel_size=list[2,2,], stride=list[2,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:55:50.759379 GPU 2 52905 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 512, 13108, 80],"float32"), kernel_size=list[2,2,], stride=list[2,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:56:23.435957 GPU 3 53072 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 512, 3, 349526],"float32"), kernel_size=list[2,2,], stride=list[2,2,], padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:57:13.778682 GPU 7 53400 test begin: paddle.nn.functional.avg_pool2d(Tensor([8192, 128, 64, 64],"float16"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:57:42.743468 GPU 3 53959 test begin: paddle.nn.functional.avg_pool2d(Tensor([8192, 128, 64, 64],"float32"), kernel_size=3, stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:57:43.605592 GPU 4 54045 test begin: paddle.nn.functional.avg_pool2d(Tensor([8192, 128, 64, 64],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-08 23:59:03.913619 GPU 4 54627 test begin: paddle.nn.functional.avg_pool2d(Tensor([89478486, 3, 4, 4],"float32"), kernel_size=2, stride=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:59:45.703567 GPU 5 53240 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([1398102, 3, 32, 32],"float32"), kernel_size=list[2,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:59:55.579087 GPU 2 53559 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 1048577, 32, 32],"float64"), kernel_size=list[2,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-08 23:59:58.512720 GPU 5 55107 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 1048577, 32, 32],"float64"), kernel_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:00:15.528268 GPU 7 54379 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 2097152, 32, 32],"float32"), kernel_size=list[2,2,], )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 00:00:26.194822 GPU 4 55273 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 11184811, 32],"float64"), kernel_size=list[2,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:00:28.082159 GPU 7 55362 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 11184811, 32],"float64"), kernel_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:00:42.799032 GPU 3 54544 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 22369622, 32],"float32"), kernel_size=list[2,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:01:59.956187 GPU 2 55848 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 32, 11184811],"float64"), kernel_size=list[2,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:02:04.528554 GPU 3 56006 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 32, 11184811],"float64"), kernel_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:02:11.822775 GPU 5 56165 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 32, 22369622],"float32"), kernel_size=list[2,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:02:50.348224 GPU 7 56334 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 4, 89478486],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:03:32.948971 GPU 5 56896 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 4, 89478486],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:03:53.720451 GPU 4 56422 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 4, 89478486],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,], ceil_mode=True, exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:04:25.678170 GPU 2 57068 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 4, 89478486],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:04:29.411294 GPU 3 57226 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 4, 89478486],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:05:20.426303 GPU 7 57395 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 89478486, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:06:00.608190 GPU 4 57955 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 89478486, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:06:27.002466 GPU 5 57793 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 89478486, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,], ceil_mode=True, exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:06:42.774432 GPU 2 58124 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 89478486, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:07:17.420629 GPU 3 58290 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 3, 89478486, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:07:33.648540 GPU 7 58687 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 67108865, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:08:46.856091 GPU 5 59019 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 67108865, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:08:56.523995 GPU 2 59179 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 67108865, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,], ceil_mode=True, exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:09:32.271004 GPU 4 58858 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 67108865, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:09:53.745773 GPU 3 59587 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([2, 67108865, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:09:57.910568 GPU 7 59747 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([44739243, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[1,1,], padding=list[0,0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:11:26.474656 GPU 2 60082 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([44739243, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,0,0,], ceil_mode=False, exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:11:42.516834 GPU 4 60479 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([44739243, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=list[3,3,], padding=list[0,0,], ceil_mode=True, exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:12:01.530271 GPU 5 59919 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([44739243, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=list[0,0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:12:06.617822 GPU 3 60643 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([44739243, 3, 4, 4],"float64"), kernel_size=list[3,3,], stride=tuple(1,1,), padding=tuple(0,0,), )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:12:24.190467 GPU 7 60806 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([699051, 3, 32, 32],"float64"), kernel_size=list[2,2,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:13:43.593568 GPU 2 61218 test begin: paddle.nn.functional.avg_pool2d(x=Tensor([699051, 3, 32, 32],"float64"), kernel_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:14:05.327432 GPU 5 61380 test begin: paddle.nn.functional.avg_pool3d(Tensor([127827, 1, 3, 1600, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([127827, 1, 3, 1600, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:14:12.385508 GPU 4 61539 test begin: paddle.nn.functional.avg_pool3d(Tensor([127827, 1, 7, 3, 1600],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([127827, 1, 7, 3, 1600],"float32"), kernel_size=tuple(5,1,1,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:14:24.469592 GPU 3 61700 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:14:48.855696 GPU 7 61864 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:15:58.089184 GPU 2 62277 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:16:20.356485 GPU 4 61539 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:16:23.868718 GPU 5 61380 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:16:36.442216 GPU 3 62456 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:17:43.902910 GPU 7 61864 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:18:22.062194 GPU 2 62880 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:18:29.645411 GPU 4 63039 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:18:53.936127 GPU 7 61864 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:19:11.782753 GPU 3 63363 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:20:00.591760 GPU 7 61864 test begin: paddle.nn.functional.avg_pool3d(Tensor([1398102, 8, 8, 8, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:20:29.233031 GPU 2 63775 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:20:49.779343 GPU 7 63937 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:20:54.564165 GPU 4 64095 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:21:06.222199 GPU 5 64255 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:21:33.611183 GPU 3 64655 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:22:54.211216 GPU 2 64833 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:23:18.413784 GPU 4 64996 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:23:30.835140 GPU 5 65156 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:23:45.648712 GPU 3 65553 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:25:12.563412 GPU 2 65739 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:25:34.764708 GPU 7 66136 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:25:42.147614 GPU 4 66295 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 11184811, 8, 8],"float16"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:26:00.080787 GPU 5 66545 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 32, 699051],"float32"), kernel_size=2, stride=2, padding="SAME", )
[accuracy error] paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 32, 699051],"float32"), kernel_size=2, stride=2, padding="SAME", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1372 / 536871936 (0.0%)
Greatest absolute difference: 0.19765149056911469 at index (1, 0, 10, 15, 349525) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 0, 2, 349525) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 3, 16, 16, 349526]), dtype=torch.float32)
tensor([[[[[-5.1719e-02, -3.6504e-02, -1.2205e-02,  ...,  2.2619e-01,  1.6827e-01,  7.9465e-04],
           [-9.1081e-02,  9.3507e-02, -6.2163e-02,  ...,  1.3192e-02,  7.8766e-02, -1.0304e-02],
           [ 5.0813e-02,  1.1063e-02,  6.4948e-02,  ..., -2.0067e-01, -9.0400e-02,  1.0417e-01],
           ...,
           [-8.9781e-02, -1.7561e-01,  8.6241e-02,  ..., -1.7908e-03,  2.0824e-01, -1.7526e-01],
           [ 5.0412e-04,  6.5610e-02, -5.0508e-03,  ...,  5.1210e-02,  1.6117e-01,  1.4290e-01],
           [-9.3719e-02, -8.3497e-03, -1.3110e-02,  ..., -2.2157e-01,  1.1538e-01,  2.3439e-01]],

          [[ 1.2656e-02,  1.7471e-01,  6.7729e-02,  ..., -2.1263e-01, -1.2861e-01,  3.8095e-02],
           [-4.3924e-03,  9.9526e-02, -5.1582e-02,  ...,  3.1134e-02, -2.7181e-02,  6.2557e-02],
           [ 8.5023e-03, -3.8177e-02,  1.4214e-01,  ...,  1.8581e-01, -6.1638e-02, -2.0515e-01],
           ...,
           [-1.3064e-01,  1.6561e-01,  2.9345e-02,  ...,  6.8511e-02, -1.8116e-01,  3.8603e-02],
           [-7.7158e-03,  1.2191e-02,  2.9022e-01,  ...,  5.5245e-02,  1.1469e-01,  1.7381e-01],
           [ 1.0480e-01,  5.2589e-02,  7.0193e-02,  ..., -6.6058e-02, -9.5547e-02, -5.8273e-02]],

          [[-3.9132e-03,  1.3088e-01, -1.2037e-01,  ..., -6.8984e-02,  4.7928e-02, -5.3300e-02],
           [ 1.0116e-02, -4.1906e-02,  1.1713e-01,  ..., -4.7970e-02,  2.4903e-02, -1.6717e-01],
           [-1.4064e-01, -5.9689e-02,  5.6907e-02,  ..., -3.0039e-02, -8.1352e-02,  1.1146e-01],
           ...,
           [ 1.5879e-04, -5.8420e-02, -1.7319e-01,  ...,  3.6327e-02,  2.0435e-01,  4.8136e-02],
           [ 1.7517e-01, -4.9741e-02,  2.5948e-01,  ...,  8.2332e-02, -1.8834e-02, -2.8266e-01],
           [ 1.8687e-01, -8.6144e-02, -8.8821e-02,  ...,  8.2420e-02,  1.2570e-01,  1.7823e-02]],

          ...,

          [[-2.8950e-01, -4.3954e-02, -1.3017e-01,  ...,  3.1760e-02,  1.4518e-01, -7.1694e-02],
           [-6.5251e-02, -2.5360e-02, -8.2801e-02,  ...,  6.3474e-02, -2.3402e-02, -1.3112e-01],
           [ 6.6223e-02, -1.4179e-01,  1.2596e-01,  ..., -5.3316e-02,  8.0961e-02, -5.5966e-03],
           ...,
           [ 1.5036e-02,  2.7336e-02,  1.5879e-04,  ..., -1.7470e-01,  2.2207e-01, -1.9177e-01],
           [-4.7589e-02,  1.2213e-01, -9.8320e-02,  ..., -6.4969e-02, -1.3301e-01, -2.1499e-01],
           [-2.1553e-03,  1.0302e-01,  1.0817e-01,  ...,  4.6941e-02, -6.2216e-02,  1.2015e-01]],

          [[ 8.1714e-03, -4.2011e-03,  4.8066e-02,  ..., -5.8555e-02,  4.1905e-02, -1.1641e-01],
           [ 1.5402e-01,  1.4513e-01, -4.1828e-02,  ..., -4.3817e-02,  1.3004e-01, -1.1415e-01],
           [-2.0078e-02,  9.7600e-02,  8.0821e-02,  ...,  1.4108e-01,  2.0334e-01, -1.4196e-01],
           ...,
           [-9.1110e-03,  4.3867e-02, -6.0003e-02,  ...,  2.1404e-01,  7.0192e-02,  1.4949e-01],
           [-4.3810e-02, -7.5636e-02, -1.4436e-01,  ...,  1.1732e-01, -1.2891e-02, -1.9387e-04],
           [ 2.4612e-02, -1.5513e-01,  1.4056e-01,  ...,  1.2676e-02, -1.2976e-01, -3.0051e-02]],

          [[-1.2349e-01, -2.4868e-01, -2.8894e-02,  ..., -1.2279e-02, -2.8688e-01,  3.6763e-02],
           [ 5.2041e-02, -4.6162e-02, -1.6425e-01,  ..., -2.9597e-02,  8.1415e-03, -4.7537e-02],
           [ 1.8858e-02, -2.5486e-02,  3.6216e-02,  ...,  6.6440e-02,  6.5881e-02, -6.5031e-02],
           ...,
           [ 1.0802e-01,  3.5857e-02, -4.4891e-02,  ..., -5.4661e-02, -2.1709e-03,  1.5700e-01],
           [-6.5687e-02,  5.6275e-02, -7.8978e-02,  ...,  1.2540e-02,  2.3280e-01,  1.3638e-01],
           [-3.7495e-02, -1.6189e-01,  2.4314e-02,  ...,  3.3982e-02,  5.5847e-03, -2.4113e-01]]],


         [[[ 8.3383e-02,  4.9746e-02,  1.1295e-01,  ..., -1.2702e-01, -6.7514e-02,  1.7648e-01],
           [-3.7848e-03,  1.6733e-01, -2.5424e-02,  ..., -6.7140e-02, -9.8193e-02, -4.4049e-02],
           [ 1.0158e-01, -1.3708e-01,  2.0853e-01,  ..., -1.2847e-01,  7.3449e-02, -1.0065e-01],
           ...,
           [-1.3532e-01, -3.7907e-02,  1.0290e-01,  ...,  1.1696e-02, -2.3871e-02, -1.9201e-03],
           [-1.2599e-01,  7.4409e-04,  8.6696e-02,  ..., -7.5205e-03, -2.0164e-02,  9.8728e-02],
           [ 3.9148e-02,  2.2430e-01,  1.4881e-01,  ..., -1.5298e-01,  2.6255e-02, -1.1549e-01]],

          [[-1.0835e-02, -4.3536e-02,  4.0059e-02,  ..., -4.3951e-02, -2.2709e-02, -9.3310e-02],
           [-8.3527e-02, -3.3470e-02,  1.6280e-01,  ..., -9.5824e-02, -7.6204e-02, -2.7983e-02],
           [ 6.5379e-02, -5.9333e-02, -4.7918e-02,  ..., -2.5278e-02, -9.0872e-03, -2.1135e-01],
           ...,
           [-1.0885e-02,  5.1764e-02, -7.3426e-02,  ...,  7.7690e-02,  2.1427e-01,  8.1901e-02],
           [ 5.7892e-02, -8.0033e-02, -6.2248e-02,  ..., -1.4045e-01, -2.9327e-02, -2.5503e-01],
           [ 1.3187e-01,  1.5285e-01, -2.5293e-02,  ...,  9.6747e-02, -1.0868e-01, -1.8188e-03]],

          [[-9.3825e-02, -1.6490e-01, -2.4656e-02,  ...,  1.8937e-02,  3.4579e-02,  3.5023e-02],
           [ 1.4638e-02,  3.2183e-02,  1.5543e-01,  ...,  2.2385e-01,  1.1252e-01,  1.3033e-01],
           [-1.3077e-02,  7.4263e-03,  5.2413e-02,  ..., -5.7532e-02,  7.1045e-02,  1.9331e-01],
           ...,
           [-1.5013e-01,  1.3091e-02, -5.1531e-02,  ..., -6.9028e-02,  8.5918e-02,  8.4155e-02],
           [-1.5379e-01, -8.2807e-02, -1.3177e-02,  ...,  8.9027e-02, -6.9768e-02,  4.8020e-02],
           [-2.4427e-01,  4.8972e-02,  1.1560e-01,  ..., -2.4544e-02, -1.6632e-01,  5.9131e-02]],

          ...,

          [[-7.9054e-02, -1.7772e-01, -2.1592e-01,  ..., -8.3309e-02, -1.8830e-01, -2.2471e-01],
           [ 6.0261e-02,  1.8029e-01, -1.3193e-01,  ...,  1.3377e-02, -5.3249e-02,  1.5116e-01],
           [ 2.4298e-01,  6.8292e-03,  9.1563e-02,  ..., -5.8034e-02,  1.1443e-01, -1.4155e-02],
           ...,
           [ 2.0064e-01, -4.9655e-02, -1.3976e-01,  ...,  1.1479e-01,  1.3087e-01, -2.7952e-01],
           [ 2.0647e-01,  3.8088e-02, -1.0793e-01,  ..., -6.6002e-02, -2.9351e-02,  5.3537e-02],
           [-5.8129e-02, -2.2639e-01, -1.4555e-02,  ..., -2.7416e-03, -6.8416e-02, -1.2223e-01]],

          [[-6.5021e-02, -8.6732e-02, -2.1240e-01,  ..., -8.8598e-02,  8.7518e-02, -2.2041e-01],
           [-3.8789e-02,  1.3117e-01, -5.5422e-02,  ..., -6.2349e-02,  1.7731e-02,  7.3610e-02],
           [ 2.0050e-02, -1.4758e-01,  1.3422e-01,  ...,  1.4205e-02,  7.5924e-02, -2.9304e-01],
           ...,
           [ 1.3738e-01,  3.4833e-02,  1.5551e-01,  ..., -2.0225e-02,  4.6200e-03, -2.4621e-01],
           [ 8.4077e-02, -2.8140e-01, -6.8259e-02,  ..., -1.5302e-01,  1.1189e-01,  4.0439e-02],
           [ 1.4018e-01, -4.9179e-02,  1.5870e-02,  ..., -1.9802e-02,  2.5565e-02,  2.4938e-01]],

          [[-2.2840e-02,  1.9660e-02, -1.2123e-01,  ..., -1.4350e-01,  2.9715e-02, -5.3998e-02],
           [ 7.0689e-02, -9.8229e-02, -5.2063e-02,  ..., -3.0718e-02, -1.8737e-01, -1.2701e-02],
           [-6.0398e-02,  2.1706e-02,  6.4876e-02,  ..., -1.8122e-01, -8.0396e-02,  1.2267e-01],
           ...,
           [ 1.4349e-01, -1.6232e-02,  2.9104e-02,  ..., -9.2567e-02,  1.0523e-01,  8.5975e-02],
           [ 1.8256e-01,  8.2825e-03,  1.0063e-01,  ...,  2.0783e-02,  8.6217e-02,  1.1184e-02],
           [ 1.1896e-01,  2.3185e-02, -1.5677e-01,  ...,  2.6884e-02,  1.7688e-01, -2.7036e-02]]],


         [[[ 5.1738e-02, -8.2156e-02, -5.7586e-02,  ...,  5.0848e-02,  9.0494e-02, -1.7797e-01],
           [ 7.8948e-02, -7.7281e-03,  1.9601e-01,  ...,  2.8718e-02,  4.5148e-03, -2.6059e-01],
           [ 6.3696e-03,  3.7026e-02, -1.8971e-01,  ..., -1.5124e-01,  3.2368e-03,  1.5508e-01],
           ...,
           [-1.4182e-01, -6.2829e-02, -4.5519e-02,  ..., -5.1071e-02,  1.5891e-01, -1.4292e-01],
           [ 7.7593e-02, -9.6499e-03, -1.4091e-01,  ..., -4.0807e-03,  9.7922e-02, -4.5531e-02],
           [ 1.4460e-01, -9.8265e-02,  1.1449e-01,  ..., -1.3788e-01,  2.1757e-01,  1.2829e-01]],

          [[ 1.8890e-01,  1.4227e-01, -4.9771e-02,  ...,  9.5853e-02,  8.2355e-03,  1.0048e-01],
           [ 3.9803e-02, -3.2070e-02, -9.2818e-02,  ...,  1.4189e-01,  1.4247e-01,  1.7101e-02],
           [-1.6882e-01, -3.5612e-02,  1.5701e-01,  ...,  1.1117e-01, -8.2270e-02,  6.0318e-02],
           ...,
           [ 1.6800e-01,  8.4385e-02,  8.9271e-02,  ...,  1.3067e-01,  9.0303e-03,  1.1782e-01],
           [ 3.8606e-02,  1.1027e-01, -2.7406e-02,  ..., -1.0154e-01, -7.7272e-02,  8.3562e-02],
           [-7.1902e-03,  2.2358e-02, -1.0842e-01,  ...,  4.7598e-02, -1.0040e-01, -1.8742e-01]],

          [[-7.2652e-02, -5.7439e-02, -1.2187e-01,  ...,  4.6438e-02,  2.7288e-02,  1.4920e-01],
           [-4.5663e-02,  2.9398e-02, -4.3027e-02,  ...,  2.8081e-02,  6.9825e-02, -1.2443e-01],
           [-3.0488e-02, -1.1072e-01, -6.4673e-02,  ..., -1.4835e-01,  9.3133e-02, -8.7698e-03],
           ...,
           [ 1.0924e-01, -2.3088e-02,  1.4907e-02,  ..., -5.1914e-02, -1.7360e-02, -8.3336e-02],
           [-7.8886e-03,  1.7813e-02,  1.3433e-01,  ...,  4.3630e-02,  2.3882e-03,  6.5298e-02],
           [-2.1786e-02, -7.9214e-02,  4.0749e-02,  ...,  4.6335e-02, -1.3265e-01,  8.1368e-02]],

          ...,

          [[-1.1723e-01,  3.4729e-02,  3.8122e-02,  ..., -1.2132e-01, -9.8686e-02,  5.9137e-04],
           [-1.6197e-01, -4.0773e-02, -1.8454e-01,  ..., -1.0427e-01,  1.4327e-01, -2.1088e-02],
           [-4.0216e-02, -1.9838e-01, -2.2298e-02,  ..., -1.5327e-02,  8.9590e-02,  2.7675e-02],
           ...,
           [-8.9180e-02, -1.0186e-01,  1.6532e-01,  ..., -1.5825e-02,  1.6452e-02, -4.6581e-02],
           [-9.8063e-02,  2.4210e-01, -9.0786e-02,  ...,  5.0904e-03,  2.9339e-02,  5.4376e-02],
           [ 5.7119e-03,  1.6200e-01, -8.3041e-02,  ...,  9.9793e-02, -2.0328e-01,  1.1468e-01]],

          [[ 8.6222e-02, -2.3523e-02, -6.7607e-02,  ..., -6.7261e-02,  1.2968e-01, -4.5482e-02],
           [-1.5546e-01,  1.2857e-01, -2.6060e-02,  ..., -1.7158e-01,  7.0821e-02, -2.1647e-01],
           [-1.9054e-01, -6.7703e-02,  5.8477e-02,  ..., -7.5101e-02,  8.2438e-02,  8.1514e-03],
           ...,
           [ 9.8554e-02,  4.0017e-02,  6.6562e-02,  ..., -1.6033e-01,  1.1978e-01,  7.2377e-02],
           [ 3.9758e-03, -2.1713e-01,  3.2263e-02,  ...,  6.8994e-02,  7.8278e-02, -2.9141e-02],
           [-1.0840e-01,  6.4828e-02,  4.8345e-02,  ...,  2.2109e-01,  7.7200e-02,  3.2395e-02]],

          [[-1.4579e-01, -9.6438e-02,  3.9577e-02,  ..., -4.1615e-02,  2.1097e-01, -2.2321e-01],
           [-1.2009e-01,  1.4932e-01,  9.3814e-02,  ..., -1.5744e-01,  1.8450e-02,  9.2123e-02],
           [-9.4646e-02,  4.4187e-02, -2.7231e-02,  ..., -1.5582e-01,  5.6208e-02, -4.6155e-02],
           ...,
           [-2.0764e-01, -1.1780e-01,  1.1761e-01,  ...,  4.4373e-02,  1.2062e-01, -2.4492e-01],
           [ 3.4541e-02,  9.2316e-02,  8.6572e-02,  ...,  4.9650e-02, -1.4964e-01,  1.4859e-01],
           [ 5.8262e-03,  4.1237e-02, -6.9146e-02,  ..., -2.3451e-02,  1.1813e-01, -1.1932e-01]]]],



        [[[[ 1.2428e-01,  3.3049e-02, -1.4598e-01,  ..., -2.2230e-01, -2.7260e-02,  7.8397e-02],
           [ 5.1835e-02,  1.9952e-01,  1.4810e-01,  ..., -2.4916e-02, -1.0166e-01, -1.0638e-02],
           [-2.0487e-03, -1.1173e-02, -1.0114e-01,  ...,  1.3827e-01,  1.5915e-02,  3.8727e-01],
           ...,
           [ 3.5150e-02, -9.9423e-03, -5.9804e-03,  ...,  1.6197e-01, -1.0956e-01,  1.4232e-01],
           [-9.3933e-02, -7.3832e-02,  2.1533e-01,  ..., -5.8523e-02,  1.2239e-01,  1.2299e-01],
           [-1.1439e-01, -1.7288e-02,  3.8300e-02,  ..., -5.5578e-02, -2.1039e-01,  2.8791e-01]],

          [[ 2.5182e-02,  1.3916e-01, -1.0860e-01,  ...,  1.3204e-01, -1.3758e-02,  5.0162e-02],
           [ 8.5065e-03,  1.6124e-01, -1.8269e-01,  ...,  5.6612e-02,  3.8359e-03,  1.2108e-02],
           [ 1.7990e-02, -4.9614e-02,  5.7091e-02,  ...,  2.4540e-01, -5.7622e-02,  1.7941e-01],
           ...,
           [-1.1682e-01,  2.9700e-02, -1.3789e-01,  ..., -2.5069e-02,  2.2194e-01, -3.7228e-02],
           [-5.3824e-02, -2.8239e-02, -5.3066e-02,  ...,  4.4792e-02,  4.4650e-02, -1.1588e-01],
           [ 1.2627e-02,  3.4649e-02,  6.7262e-02,  ..., -7.3688e-02, -1.8457e-01,  6.5294e-02]],

          [[-2.8479e-02,  8.6385e-02, -3.9592e-02,  ...,  7.8804e-02,  3.8565e-03,  2.1225e-01],
           [ 1.1705e-01,  5.2806e-02,  3.5834e-02,  ...,  2.6951e-02, -1.6272e-01,  1.4029e-01],
           [-1.0407e-01,  1.1199e-01,  6.7765e-02,  ...,  2.9788e-02, -8.6957e-02, -5.1139e-02],
           ...,
           [ 1.6296e-02,  1.3300e-01,  1.1993e-01,  ..., -2.0099e-02, -2.0825e-01,  1.5828e-01],
           [ 7.8322e-02,  6.9653e-02, -1.2177e-01,  ...,  9.8302e-02, -3.0598e-02, -1.0726e-01],
           [ 5.4188e-02,  1.7700e-01,  1.8592e-02,  ..., -8.1934e-02, -1.9203e-01,  1.2907e-01]],

          ...,

          [[ 8.5796e-02, -4.0935e-02,  2.5473e-03,  ..., -4.9643e-02, -5.6363e-02, -3.7115e-02],
           [-3.5314e-02, -3.5665e-02, -3.5016e-02,  ..., -1.4788e-01,  7.7781e-02,  8.9592e-02],
           [-1.1980e-01,  1.4775e-01,  9.7909e-02,  ..., -3.5528e-02,  2.0915e-01,  4.0632e-02],
           ...,
           [-2.6921e-02,  6.4062e-02, -8.0792e-02,  ...,  1.6203e-01,  3.8516e-02,  1.4530e-01],
           [-7.1749e-02,  1.2707e-02, -6.6984e-02,  ..., -6.8698e-02, -6.1401e-02, -2.3295e-01],
           [ 6.0136e-02,  9.3326e-02, -1.4997e-01,  ...,  9.9114e-02, -1.3905e-01,  2.3683e-01]],

          [[-1.6720e-02, -1.1724e-01, -1.4473e-01,  ..., -6.7528e-02,  1.0761e-01,  4.5798e-02],
           [ 8.9777e-02,  2.0750e-01,  9.2466e-03,  ...,  2.4011e-02, -1.2005e-01,  4.3951e-02],
           [-7.6846e-02,  1.3688e-01, -6.0720e-02,  ...,  7.2474e-02,  1.0860e-01, -3.0218e-01],
           ...,
           [-3.3997e-02, -4.3192e-02,  6.9491e-02,  ...,  4.8826e-02, -1.2958e-01, -1.5695e-02],
           [ 2.6201e-01,  5.1996e-03,  2.4976e-02,  ..., -1.5732e-01,  1.5408e-02,  3.0327e-01],
           [-2.8583e-02, -1.6454e-01,  2.1023e-01,  ...,  7.2236e-02, -3.7975e-02, -1.6000e-01]],

          [[-1.5753e-01,  4.9768e-02, -6.6787e-02,  ..., -1.7602e-02, -2.4135e-02, -2.4171e-03],
           [ 4.3996e-02, -2.8216e-03, -5.8728e-03,  ...,  1.1755e-01,  1.6632e-01,  1.5805e-01],
           [ 4.1429e-02, -2.6683e-02, -2.2168e-02,  ..., -8.4676e-02,  7.9474e-02, -3.3234e-01],
           ...,
           [-1.1876e-01, -7.5294e-02, -1.2862e-01,  ...,  1.3111e-03,  2.4694e-02, -1.9855e-02],
           [-7.7103e-02, -9.5762e-02,  1.0150e-02,  ...,  1.7200e-01, -3.9386e-02, -5.0944e-02],
           [ 1.8612e-01, -2.7014e-01,  1.2454e-01,  ..., -1.5889e-02,  2.4181e-03, -1.0794e-01]]],


         [[[-7.0753e-02, -5.6858e-03, -1.5505e-01,  ..., -1.0831e-01,  4.3245e-02, -4.7828e-03],
           [-1.4532e-01,  1.8874e-01,  1.7022e-01,  ...,  6.6515e-02, -5.1983e-02,  1.4893e-02],
           [-1.2205e-02, -4.1273e-02, -3.2776e-02,  ...,  1.0380e-01, -1.5386e-01,  3.2966e-02],
           ...,
           [ 5.9033e-02, -7.7072e-02,  3.2538e-02,  ...,  1.3125e-01, -4.4611e-02, -7.4951e-03],
           [-1.4254e-01,  9.9437e-02,  1.3261e-02,  ...,  1.8991e-01,  1.0106e-01,  1.3676e-01],
           [ 3.8742e-02,  4.8316e-02,  1.5468e-01,  ...,  3.5050e-02,  1.2245e-01, -1.0649e-01]],

          [[ 1.3174e-01,  6.8838e-02,  5.8592e-02,  ...,  1.2019e-01, -8.0251e-02,  1.2621e-02],
           [ 1.2926e-01, -4.2112e-03,  1.5386e-01,  ..., -1.1881e-01, -1.6002e-01, -8.8875e-02],
           [-2.1710e-01,  5.1243e-02,  1.7793e-02,  ..., -2.2988e-01,  2.2042e-02, -5.0317e-02],
           ...,
           [ 1.5861e-02,  7.8712e-02,  1.3116e-01,  ...,  1.6383e-02, -2.0928e-01,  1.6742e-01],
           [-3.1829e-02, -1.7138e-01,  1.4885e-01,  ..., -3.9689e-03,  1.5748e-02,  3.1428e-01],
           [ 5.6229e-02, -1.5602e-01, -1.4553e-02,  ...,  1.9813e-01,  3.5488e-02, -4.5263e-02]],

          [[-1.6881e-01, -4.7530e-02, -2.3560e-01,  ...,  2.2706e-01,  1.9609e-01,  1.0046e-02],
           [-6.1174e-02, -1.1685e-01, -1.3385e-01,  ..., -9.3520e-03, -8.5750e-02,  1.3070e-01],
           [-9.3084e-02,  1.6471e-01,  1.1930e-01,  ...,  6.9835e-03, -1.1076e-01, -1.9448e-01],
           ...,
           [ 4.2203e-02,  3.1718e-02, -1.0242e-01,  ...,  1.5695e-01,  1.4183e-01,  1.5517e-01],
           [-4.3046e-02, -5.5197e-02,  1.6704e-01,  ..., -1.2704e-01,  7.9601e-02,  2.3842e-02],
           [-9.7188e-02,  7.9388e-02,  6.2029e-02,  ...,  1.2480e-01, -1.2471e-01, -6.2743e-02]],

          ...,

          [[ 9.4021e-02,  7.8354e-02,  6.5698e-02,  ..., -1.0745e-01, -3.4012e-02,  1.1481e-01],
           [-1.4672e-01, -4.4642e-02,  7.3621e-02,  ..., -2.6723e-02,  8.1749e-02, -1.1453e-01],
           [ 2.7905e-02, -9.0574e-02,  4.3206e-02,  ...,  5.8372e-02, -6.3585e-02,  2.1225e-01],
           ...,
           [-5.0864e-02,  3.0016e-01, -1.3941e-01,  ...,  3.6436e-02,  1.8815e-02, -9.7777e-02],
           [-1.7614e-01, -5.7894e-02,  1.0916e-01,  ..., -1.9700e-02, -1.0411e-02, -1.9828e-01],
           [-1.2500e-01,  3.1388e-02,  1.0968e-01,  ...,  1.4440e-01,  1.9715e-03, -2.0979e-02]],

          [[ 1.2781e-02,  1.3040e-01,  1.3949e-01,  ..., -1.6671e-01, -6.4652e-02,  1.0282e-01],
           [ 5.5998e-02,  4.4611e-02,  1.0386e-01,  ..., -1.6865e-01,  2.8941e-02, -2.1682e-02],
           [ 8.3916e-02,  5.5815e-03,  9.0352e-02,  ..., -5.7889e-02,  5.3321e-02,  1.7868e-01],
           ...,
           [ 1.9076e-02, -1.1242e-01, -1.0514e-01,  ...,  9.1568e-02,  8.1445e-02, -1.0390e-01],
           [-4.0659e-02, -1.7865e-03,  4.7363e-03,  ...,  9.8566e-02, -7.7189e-02, -8.1157e-02],
           [ 1.5700e-02, -2.0995e-01,  7.7326e-03,  ...,  7.9025e-02,  4.6939e-02,  1.2665e-02]],

          [[-7.5244e-02, -1.0601e-01,  1.5411e-01,  ...,  4.6457e-02, -4.6399e-03,  2.5762e-01],
           [-2.5882e-02,  6.5585e-02, -6.6159e-02,  ...,  4.6849e-02,  2.0188e-01, -8.7930e-02],
           [-5.9892e-02,  2.0089e-02, -6.7414e-02,  ..., -3.3750e-02,  7.7698e-03, -5.6440e-02],
           ...,
           [-1.2892e-02,  5.8407e-02,  1.1737e-01,  ...,  7.7694e-03, -5.8593e-02, -5.0004e-02],
           [-6.5807e-02,  8.2080e-02,  1.7689e-01,  ...,  1.4801e-01,  2.0409e-02,  1.0757e-01],
           [-5.5275e-03,  8.9268e-02, -3.7061e-02,  ..., -1.1030e-01,  8.6875e-02,  1.2886e-01]]],


         [[[ 1.1878e-01, -3.3466e-02, -3.7534e-02,  ..., -1.8839e-01,  4.4484e-02, -2.0334e-01],
           [-3.7850e-04,  3.6875e-02, -4.7738e-02,  ...,  1.0372e-02,  3.7806e-02,  6.7029e-02],
           [-6.8944e-03,  4.6740e-02,  9.1538e-02,  ...,  2.4567e-03, -8.7989e-02,  2.7539e-01],
           ...,
           [-2.9824e-02,  6.5578e-02,  5.4447e-02,  ...,  6.5801e-02, -1.7350e-02,  3.6908e-02],
           [-3.3510e-02,  2.4157e-02,  1.0367e-01,  ..., -9.8515e-02,  1.4797e-02, -2.3162e-01],
           [ 9.1276e-02,  1.1989e-01,  6.5676e-02,  ...,  6.0878e-02, -4.6121e-02, -1.5299e-01]],

          [[ 1.0156e-01, -8.1350e-02, -1.1278e-01,  ...,  6.2826e-02, -4.1725e-03,  9.4395e-02],
           [-3.2365e-02, -5.1138e-02,  4.3056e-02,  ..., -4.8650e-02, -6.5195e-02, -5.1724e-02],
           [ 4.4916e-03,  9.4116e-02, -1.2453e-01,  ..., -6.9053e-02,  1.3909e-02,  2.2541e-02],
           ...,
           [ 7.0467e-03, -1.9157e-01, -4.6999e-02,  ...,  1.0125e-02,  1.6563e-02, -1.2178e-01],
           [ 5.3260e-03, -1.0730e-01,  9.7915e-02,  ...,  1.3055e-01,  1.2415e-01, -1.7917e-02],
           [-2.7611e-02, -6.3880e-02,  3.4756e-02,  ..., -4.0236e-02, -6.2716e-02, -4.9178e-03]],

          [[-5.8793e-02, -1.8193e-03,  2.6872e-02,  ..., -1.2947e-01, -9.1769e-02, -3.9621e-02],
           [-7.0935e-02,  2.5420e-02,  1.6784e-01,  ..., -2.0042e-01,  1.8795e-02, -2.3920e-01],
           [ 1.5561e-02,  4.5933e-03,  1.2832e-01,  ..., -1.1066e-01,  1.8132e-01, -7.5835e-02],
           ...,
           [ 1.0541e-01, -1.4916e-02, -2.2559e-01,  ...,  6.4903e-02,  4.2904e-02,  4.1102e-02],
           [ 1.1839e-02, -1.4796e-01, -1.4244e-02,  ..., -7.0299e-02,  1.2350e-01,  3.9947e-02],
           [ 9.9712e-02, -2.0634e-01,  6.4662e-02,  ..., -1.5305e-02,  9.4067e-02, -1.0225e-02]],

          ...,

          [[ 1.5359e-02, -1.0706e-02,  4.9041e-02,  ...,  1.0676e-01,  2.7318e-02, -2.2994e-01],
           [-1.3571e-01, -1.1272e-01, -4.7973e-02,  ..., -1.2026e-02,  9.6498e-02, -4.8285e-02],
           [-3.7644e-02, -1.3874e-01,  4.8209e-02,  ...,  1.6603e-01, -1.3509e-01,  1.0682e-01],
           ...,
           [-1.1660e-01,  9.8233e-02,  8.4664e-02,  ...,  1.3770e-01, -6.1837e-02, -1.7978e-01],
           [-1.0587e-01, -8.2539e-02, -1.2050e-01,  ...,  1.9447e-02, -1.2695e-02,  7.2291e-02],
           [-6.9729e-02,  8.1041e-02, -1.1917e-01,  ..., -9.8614e-02, -2.1439e-02, -1.5958e-01]],

          [[-1.6277e-01, -1.8310e-02,  1.0467e-01,  ...,  1.4662e-01,  8.4161e-02,  1.6656e-01],
           [-1.9334e-01,  1.4996e-01,  2.5534e-02,  ..., -2.2852e-01,  2.7952e-02, -1.3315e-01],
           [-1.5791e-02,  5.6656e-02, -8.7165e-02,  ..., -3.6142e-02, -4.8529e-02,  6.6179e-02],
           ...,
           [ 9.0500e-02,  1.5181e-03, -1.5838e-01,  ..., -1.4371e-01,  9.7451e-02, -2.7198e-01],
           [ 8.2085e-02, -9.2813e-02,  2.4225e-02,  ..., -3.5731e-02,  2.8473e-01, -6.3544e-02],
           [ 1.4864e-01, -1.4223e-01, -2.9897e-02,  ...,  1.0993e-02,  1.6023e-02,  2.6045e-01]],

          [[ 1.1568e-01, -4.7023e-02,  1.5965e-01,  ...,  2.7939e-01, -9.0137e-03, -2.7626e-02],
           [-6.3169e-02,  7.7513e-02,  7.1934e-02,  ..., -2.5251e-02,  1.6755e-01, -6.8636e-02],
           [-1.1848e-01,  1.3252e-01, -3.4675e-02,  ..., -7.1558e-02,  4.3700e-02, -1.7055e-01],
           ...,
           [-5.8607e-02,  1.2974e-01, -2.0226e-02,  ...,  6.1341e-02, -5.1336e-02,  1.3220e-01],
           [-1.2591e-01,  1.0047e-01, -7.7686e-02,  ..., -9.0199e-02,  2.1356e-02,  2.7975e-01],
           [ 2.7335e-02,  2.8877e-02, -6.1852e-03,  ..., -8.7883e-02,  2.6576e-02, -6.8112e-02]]]]])
DESIRED: (shape=torch.Size([2, 3, 16, 16, 349526]), dtype=torch.float32)
tensor([[[[[-5.1719e-02, -3.6504e-02, -1.2205e-02,  ...,  2.2619e-01,  1.6827e-01,  3.9732e-04],
           [-9.1081e-02,  9.3507e-02, -6.2163e-02,  ...,  1.3192e-02,  7.8766e-02, -5.1519e-03],
           [ 5.0813e-02,  1.1063e-02,  6.4948e-02,  ..., -2.0067e-01, -9.0400e-02,  5.2084e-02],
           ...,
           [-8.9781e-02, -1.7561e-01,  8.6241e-02,  ..., -1.7908e-03,  2.0824e-01, -8.7629e-02],
           [ 5.0412e-04,  6.5610e-02, -5.0508e-03,  ...,  5.1210e-02,  1.6117e-01,  7.1450e-02],
           [-9.3719e-02, -8.3497e-03, -1.3110e-02,  ..., -2.2157e-01,  1.1538e-01,  1.1720e-01]],

          [[ 1.2656e-02,  1.7471e-01,  6.7729e-02,  ..., -2.1263e-01, -1.2861e-01,  1.9047e-02],
           [-4.3924e-03,  9.9526e-02, -5.1582e-02,  ...,  3.1134e-02, -2.7181e-02,  3.1279e-02],
           [ 8.5023e-03, -3.8177e-02,  1.4214e-01,  ...,  1.8581e-01, -6.1638e-02, -1.0257e-01],
           ...,
           [-1.3064e-01,  1.6561e-01,  2.9345e-02,  ...,  6.8511e-02, -1.8116e-01,  1.9301e-02],
           [-7.7158e-03,  1.2191e-02,  2.9022e-01,  ...,  5.5245e-02,  1.1469e-01,  8.6907e-02],
           [ 1.0480e-01,  5.2589e-02,  7.0193e-02,  ..., -6.6058e-02, -9.5547e-02, -2.9137e-02]],

          [[-3.9132e-03,  1.3088e-01, -1.2037e-01,  ..., -6.8984e-02,  4.7928e-02, -2.6650e-02],
           [ 1.0116e-02, -4.1906e-02,  1.1713e-01,  ..., -4.7970e-02,  2.4903e-02, -8.3586e-02],
           [-1.4064e-01, -5.9689e-02,  5.6907e-02,  ..., -3.0039e-02, -8.1352e-02,  5.5731e-02],
           ...,
           [ 1.5879e-04, -5.8420e-02, -1.7319e-01,  ...,  3.6327e-02,  2.0435e-01,  2.4068e-02],
           [ 1.7517e-01, -4.9741e-02,  2.5948e-01,  ...,  8.2332e-02, -1.8834e-02, -1.4133e-01],
           [ 1.8687e-01, -8.6144e-02, -8.8821e-02,  ...,  8.2420e-02,  1.2570e-01,  8.9115e-03]],

          ...,

          [[-2.8950e-01, -4.3954e-02, -1.3017e-01,  ...,  3.1760e-02,  1.4518e-01, -3.5847e-02],
           [-6.5251e-02, -2.5360e-02, -8.2801e-02,  ...,  6.3474e-02, -2.3402e-02, -6.5559e-02],
           [ 6.6223e-02, -1.4179e-01,  1.2596e-01,  ..., -5.3316e-02,  8.0961e-02, -2.7983e-03],
           ...,
           [ 1.5036e-02,  2.7336e-02,  1.5879e-04,  ..., -1.7470e-01,  2.2207e-01, -9.5885e-02],
           [-4.7589e-02,  1.2213e-01, -9.8320e-02,  ..., -6.4969e-02, -1.3301e-01, -1.0750e-01],
           [-2.1553e-03,  1.0302e-01,  1.0817e-01,  ...,  4.6941e-02, -6.2216e-02,  6.0075e-02]],

          [[ 8.1714e-03, -4.2011e-03,  4.8066e-02,  ..., -5.8555e-02,  4.1905e-02, -5.8207e-02],
           [ 1.5402e-01,  1.4513e-01, -4.1828e-02,  ..., -4.3817e-02,  1.3004e-01, -5.7076e-02],
           [-2.0078e-02,  9.7600e-02,  8.0821e-02,  ...,  1.4108e-01,  2.0334e-01, -7.0981e-02],
           ...,
           [-9.1110e-03,  4.3867e-02, -6.0003e-02,  ...,  2.1404e-01,  7.0192e-02,  7.4746e-02],
           [-4.3810e-02, -7.5636e-02, -1.4436e-01,  ...,  1.1732e-01, -1.2891e-02, -9.6936e-05],
           [ 2.4612e-02, -1.5513e-01,  1.4056e-01,  ...,  1.2676e-02, -1.2976e-01, -1.5026e-02]],

          [[-1.2349e-01, -2.4868e-01, -2.8894e-02,  ..., -1.2279e-02, -2.8688e-01,  1.8381e-02],
           [ 5.2041e-02, -4.6162e-02, -1.6425e-01,  ..., -2.9597e-02,  8.1415e-03, -2.3769e-02],
           [ 1.8858e-02, -2.5486e-02,  3.6216e-02,  ...,  6.6440e-02,  6.5881e-02, -3.2515e-02],
           ...,
           [ 1.0802e-01,  3.5857e-02, -4.4891e-02,  ..., -5.4661e-02, -2.1709e-03,  7.8499e-02],
           [-6.5687e-02,  5.6275e-02, -7.8978e-02,  ...,  1.2540e-02,  2.3280e-01,  6.8189e-02],
           [-3.7495e-02, -1.6189e-01,  2.4314e-02,  ...,  3.3982e-02,  5.5847e-03, -1.2057e-01]]],


         [[[ 8.3383e-02,  4.9746e-02,  1.1295e-01,  ..., -1.2702e-01, -6.7514e-02,  8.8242e-02],
           [-3.7848e-03,  1.6733e-01, -2.5424e-02,  ..., -6.7140e-02, -9.8193e-02, -2.2024e-02],
           [ 1.0158e-01, -1.3708e-01,  2.0853e-01,  ..., -1.2847e-01,  7.3449e-02, -5.0325e-02],
           ...,
           [-1.3532e-01, -3.7907e-02,  1.0290e-01,  ...,  1.1696e-02, -2.3871e-02, -9.6007e-04],
           [-1.2599e-01,  7.4409e-04,  8.6696e-02,  ..., -7.5205e-03, -2.0164e-02,  4.9364e-02],
           [ 3.9148e-02,  2.2430e-01,  1.4881e-01,  ..., -1.5298e-01,  2.6255e-02, -5.7747e-02]],

          [[-1.0835e-02, -4.3536e-02,  4.0059e-02,  ..., -4.3951e-02, -2.2709e-02, -4.6655e-02],
           [-8.3527e-02, -3.3470e-02,  1.6280e-01,  ..., -9.5824e-02, -7.6204e-02, -1.3992e-02],
           [ 6.5379e-02, -5.9333e-02, -4.7918e-02,  ..., -2.5278e-02, -9.0872e-03, -1.0567e-01],
           ...,
           [-1.0885e-02,  5.1764e-02, -7.3426e-02,  ...,  7.7690e-02,  2.1427e-01,  4.0950e-02],
           [ 5.7892e-02, -8.0033e-02, -6.2248e-02,  ..., -1.4045e-01, -2.9327e-02, -1.2752e-01],
           [ 1.3187e-01,  1.5285e-01, -2.5293e-02,  ...,  9.6747e-02, -1.0868e-01, -9.0942e-04]],

          [[-9.3825e-02, -1.6490e-01, -2.4656e-02,  ...,  1.8937e-02,  3.4579e-02,  1.7512e-02],
           [ 1.4638e-02,  3.2183e-02,  1.5543e-01,  ...,  2.2385e-01,  1.1252e-01,  6.5164e-02],
           [-1.3077e-02,  7.4263e-03,  5.2413e-02,  ..., -5.7532e-02,  7.1045e-02,  9.6653e-02],
           ...,
           [-1.5013e-01,  1.3091e-02, -5.1531e-02,  ..., -6.9028e-02,  8.5918e-02,  4.2078e-02],
           [-1.5379e-01, -8.2807e-02, -1.3177e-02,  ...,  8.9027e-02, -6.9768e-02,  2.4010e-02],
           [-2.4427e-01,  4.8972e-02,  1.1560e-01,  ..., -2.4544e-02, -1.6632e-01,  2.9566e-02]],

          ...,

          [[-7.9054e-02, -1.7772e-01, -2.1592e-01,  ..., -8.3309e-02, -1.8830e-01, -1.1236e-01],
           [ 6.0261e-02,  1.8029e-01, -1.3193e-01,  ...,  1.3377e-02, -5.3249e-02,  7.5579e-02],
           [ 2.4298e-01,  6.8292e-03,  9.1563e-02,  ..., -5.8034e-02,  1.1443e-01, -7.0776e-03],
           ...,
           [ 2.0064e-01, -4.9655e-02, -1.3976e-01,  ...,  1.1479e-01,  1.3087e-01, -1.3976e-01],
           [ 2.0647e-01,  3.8088e-02, -1.0793e-01,  ..., -6.6002e-02, -2.9351e-02,  2.6768e-02],
           [-5.8129e-02, -2.2639e-01, -1.4555e-02,  ..., -2.7416e-03, -6.8416e-02, -6.1113e-02]],

          [[-6.5021e-02, -8.6732e-02, -2.1240e-01,  ..., -8.8598e-02,  8.7518e-02, -1.1021e-01],
           [-3.8789e-02,  1.3117e-01, -5.5422e-02,  ..., -6.2349e-02,  1.7731e-02,  3.6805e-02],
           [ 2.0050e-02, -1.4758e-01,  1.3422e-01,  ...,  1.4205e-02,  7.5924e-02, -1.4652e-01],
           ...,
           [ 1.3738e-01,  3.4833e-02,  1.5551e-01,  ..., -2.0225e-02,  4.6200e-03, -1.2310e-01],
           [ 8.4077e-02, -2.8140e-01, -6.8259e-02,  ..., -1.5302e-01,  1.1189e-01,  2.0219e-02],
           [ 1.4018e-01, -4.9179e-02,  1.5870e-02,  ..., -1.9802e-02,  2.5565e-02,  1.2469e-01]],

          [[-2.2840e-02,  1.9660e-02, -1.2123e-01,  ..., -1.4350e-01,  2.9715e-02, -2.6999e-02],
           [ 7.0689e-02, -9.8229e-02, -5.2063e-02,  ..., -3.0718e-02, -1.8737e-01, -6.3505e-03],
           [-6.0398e-02,  2.1706e-02,  6.4876e-02,  ..., -1.8122e-01, -8.0396e-02,  6.1333e-02],
           ...,
           [ 1.4349e-01, -1.6232e-02,  2.9104e-02,  ..., -9.2567e-02,  1.0523e-01,  4.2988e-02],
           [ 1.8256e-01,  8.2825e-03,  1.0063e-01,  ...,  2.0783e-02,  8.6217e-02,  5.5922e-03],
           [ 1.1896e-01,  2.3185e-02, -1.5677e-01,  ...,  2.6884e-02,  1.7688e-01, -1.3518e-02]]],


         [[[ 5.1738e-02, -8.2156e-02, -5.7586e-02,  ...,  5.0848e-02,  9.0494e-02, -8.8983e-02],
           [ 7.8948e-02, -7.7281e-03,  1.9601e-01,  ...,  2.8718e-02,  4.5148e-03, -1.3029e-01],
           [ 6.3696e-03,  3.7026e-02, -1.8971e-01,  ..., -1.5124e-01,  3.2368e-03,  7.7541e-02],
           ...,
           [-1.4182e-01, -6.2829e-02, -4.5519e-02,  ..., -5.1071e-02,  1.5891e-01, -7.1460e-02],
           [ 7.7593e-02, -9.6499e-03, -1.4091e-01,  ..., -4.0807e-03,  9.7922e-02, -2.2766e-02],
           [ 1.4460e-01, -9.8265e-02,  1.1449e-01,  ..., -1.3788e-01,  2.1757e-01,  6.4145e-02]],

          [[ 1.8890e-01,  1.4227e-01, -4.9771e-02,  ...,  9.5853e-02,  8.2355e-03,  5.0242e-02],
           [ 3.9803e-02, -3.2070e-02, -9.2818e-02,  ...,  1.4189e-01,  1.4247e-01,  8.5506e-03],
           [-1.6882e-01, -3.5612e-02,  1.5701e-01,  ...,  1.1117e-01, -8.2270e-02,  3.0159e-02],
           ...,
           [ 1.6800e-01,  8.4385e-02,  8.9271e-02,  ...,  1.3067e-01,  9.0303e-03,  5.8911e-02],
           [ 3.8606e-02,  1.1027e-01, -2.7406e-02,  ..., -1.0154e-01, -7.7272e-02,  4.1781e-02],
           [-7.1902e-03,  2.2358e-02, -1.0842e-01,  ...,  4.7598e-02, -1.0040e-01, -9.3710e-02]],

          [[-7.2652e-02, -5.7439e-02, -1.2187e-01,  ...,  4.6438e-02,  2.7288e-02,  7.4599e-02],
           [-4.5663e-02,  2.9398e-02, -4.3027e-02,  ...,  2.8081e-02,  6.9825e-02, -6.2214e-02],
           [-3.0488e-02, -1.1072e-01, -6.4673e-02,  ..., -1.4835e-01,  9.3133e-02, -4.3849e-03],
           ...,
           [ 1.0924e-01, -2.3088e-02,  1.4907e-02,  ..., -5.1914e-02, -1.7360e-02, -4.1668e-02],
           [-7.8886e-03,  1.7813e-02,  1.3433e-01,  ...,  4.3630e-02,  2.3882e-03,  3.2649e-02],
           [-2.1786e-02, -7.9214e-02,  4.0749e-02,  ...,  4.6335e-02, -1.3265e-01,  4.0684e-02]],

          ...,

          [[-1.1723e-01,  3.4729e-02,  3.8122e-02,  ..., -1.2132e-01, -9.8686e-02,  2.9568e-04],
           [-1.6197e-01, -4.0773e-02, -1.8454e-01,  ..., -1.0427e-01,  1.4327e-01, -1.0544e-02],
           [-4.0216e-02, -1.9838e-01, -2.2298e-02,  ..., -1.5327e-02,  8.9590e-02,  1.3838e-02],
           ...,
           [-8.9180e-02, -1.0186e-01,  1.6532e-01,  ..., -1.5825e-02,  1.6452e-02, -2.3290e-02],
           [-9.8063e-02,  2.4210e-01, -9.0786e-02,  ...,  5.0904e-03,  2.9339e-02,  2.7188e-02],
           [ 5.7119e-03,  1.6200e-01, -8.3041e-02,  ...,  9.9793e-02, -2.0328e-01,  5.7341e-02]],

          [[ 8.6222e-02, -2.3523e-02, -6.7607e-02,  ..., -6.7261e-02,  1.2968e-01, -2.2741e-02],
           [-1.5546e-01,  1.2857e-01, -2.6060e-02,  ..., -1.7158e-01,  7.0821e-02, -1.0823e-01],
           [-1.9054e-01, -6.7703e-02,  5.8477e-02,  ..., -7.5101e-02,  8.2438e-02,  4.0757e-03],
           ...,
           [ 9.8554e-02,  4.0017e-02,  6.6562e-02,  ..., -1.6033e-01,  1.1978e-01,  3.6189e-02],
           [ 3.9758e-03, -2.1713e-01,  3.2263e-02,  ...,  6.8994e-02,  7.8278e-02, -1.4571e-02],
           [-1.0840e-01,  6.4828e-02,  4.8345e-02,  ...,  2.2109e-01,  7.7200e-02,  1.6197e-02]],

          [[-1.4579e-01, -9.6438e-02,  3.9577e-02,  ..., -4.1615e-02,  2.1097e-01, -1.1161e-01],
           [-1.2009e-01,  1.4932e-01,  9.3814e-02,  ..., -1.5744e-01,  1.8450e-02,  4.6062e-02],
           [-9.4646e-02,  4.4187e-02, -2.7231e-02,  ..., -1.5582e-01,  5.6208e-02, -2.3077e-02],
           ...,
           [-2.0764e-01, -1.1780e-01,  1.1761e-01,  ...,  4.4373e-02,  1.2062e-01, -1.2246e-01],
           [ 3.4541e-02,  9.2316e-02,  8.6572e-02,  ...,  4.9650e-02, -1.4964e-01,  7.4296e-02],
           [ 5.8262e-03,  4.1237e-02, -6.9146e-02,  ..., -2.3451e-02,  1.1813e-01, -5.9662e-02]]]],



        [[[[ 1.2428e-01,  3.3049e-02, -1.4598e-01,  ..., -2.2230e-01, -2.7260e-02,  3.9198e-02],
           [ 5.1835e-02,  1.9952e-01,  1.4810e-01,  ..., -2.4916e-02, -1.0166e-01, -5.3190e-03],
           [-2.0487e-03, -1.1173e-02, -1.0114e-01,  ...,  1.3827e-01,  1.5915e-02,  1.9364e-01],
           ...,
           [ 3.5150e-02, -9.9423e-03, -5.9804e-03,  ...,  1.6197e-01, -1.0956e-01,  7.1162e-02],
           [-9.3933e-02, -7.3832e-02,  2.1533e-01,  ..., -5.8523e-02,  1.2239e-01,  6.1494e-02],
           [-1.1439e-01, -1.7288e-02,  3.8300e-02,  ..., -5.5578e-02, -2.1039e-01,  1.4396e-01]],

          [[ 2.5182e-02,  1.3916e-01, -1.0860e-01,  ...,  1.3204e-01, -1.3758e-02,  2.5081e-02],
           [ 8.5065e-03,  1.6124e-01, -1.8269e-01,  ...,  5.6612e-02,  3.8359e-03,  6.0542e-03],
           [ 1.7990e-02, -4.9614e-02,  5.7091e-02,  ...,  2.4540e-01, -5.7622e-02,  8.9704e-02],
           ...,
           [-1.1682e-01,  2.9700e-02, -1.3789e-01,  ..., -2.5069e-02,  2.2194e-01, -1.8614e-02],
           [-5.3824e-02, -2.8239e-02, -5.3066e-02,  ...,  4.4792e-02,  4.4650e-02, -5.7939e-02],
           [ 1.2627e-02,  3.4649e-02,  6.7262e-02,  ..., -7.3688e-02, -1.8457e-01,  3.2647e-02]],

          [[-2.8479e-02,  8.6385e-02, -3.9592e-02,  ...,  7.8804e-02,  3.8565e-03,  1.0612e-01],
           [ 1.1705e-01,  5.2806e-02,  3.5834e-02,  ...,  2.6951e-02, -1.6272e-01,  7.0147e-02],
           [-1.0407e-01,  1.1199e-01,  6.7765e-02,  ...,  2.9788e-02, -8.6957e-02, -2.5570e-02],
           ...,
           [ 1.6296e-02,  1.3300e-01,  1.1993e-01,  ..., -2.0099e-02, -2.0825e-01,  7.9142e-02],
           [ 7.8322e-02,  6.9653e-02, -1.2177e-01,  ...,  9.8302e-02, -3.0598e-02, -5.3630e-02],
           [ 5.4188e-02,  1.7700e-01,  1.8592e-02,  ..., -8.1934e-02, -1.9203e-01,  6.4534e-02]],

          ...,

          [[ 8.5796e-02, -4.0935e-02,  2.5473e-03,  ..., -4.9643e-02, -5.6363e-02, -1.8557e-02],
           [-3.5314e-02, -3.5665e-02, -3.5016e-02,  ..., -1.4788e-01,  7.7781e-02,  4.4796e-02],
           [-1.1980e-01,  1.4775e-01,  9.7909e-02,  ..., -3.5528e-02,  2.0915e-01,  2.0316e-02],
           ...,
           [-2.6921e-02,  6.4062e-02, -8.0792e-02,  ...,  1.6203e-01,  3.8516e-02,  7.2652e-02],
           [-7.1749e-02,  1.2707e-02, -6.6984e-02,  ..., -6.8698e-02, -6.1401e-02, -1.1647e-01],
           [ 6.0136e-02,  9.3326e-02, -1.4997e-01,  ...,  9.9114e-02, -1.3905e-01,  1.1841e-01]],

          [[-1.6720e-02, -1.1724e-01, -1.4473e-01,  ..., -6.7528e-02,  1.0761e-01,  2.2899e-02],
           [ 8.9777e-02,  2.0750e-01,  9.2466e-03,  ...,  2.4011e-02, -1.2005e-01,  2.1975e-02],
           [-7.6846e-02,  1.3688e-01, -6.0720e-02,  ...,  7.2474e-02,  1.0860e-01, -1.5109e-01],
           ...,
           [-3.3997e-02, -4.3192e-02,  6.9491e-02,  ...,  4.8826e-02, -1.2958e-01, -7.8475e-03],
           [ 2.6201e-01,  5.1996e-03,  2.4976e-02,  ..., -1.5732e-01,  1.5408e-02,  1.5163e-01],
           [-2.8583e-02, -1.6454e-01,  2.1023e-01,  ...,  7.2236e-02, -3.7975e-02, -8.0000e-02]],

          [[-1.5753e-01,  4.9768e-02, -6.6787e-02,  ..., -1.7602e-02, -2.4135e-02, -1.2085e-03],
           [ 4.3996e-02, -2.8216e-03, -5.8728e-03,  ...,  1.1755e-01,  1.6632e-01,  7.9027e-02],
           [ 4.1429e-02, -2.6683e-02, -2.2168e-02,  ..., -8.4676e-02,  7.9474e-02, -1.6617e-01],
           ...,
           [-1.1876e-01, -7.5294e-02, -1.2862e-01,  ...,  1.3111e-03,  2.4694e-02, -9.9277e-03],
           [-7.7103e-02, -9.5762e-02,  1.0150e-02,  ...,  1.7200e-01, -3.9386e-02, -2.5472e-02],
           [ 1.8612e-01, -2.7014e-01,  1.2454e-01,  ..., -1.5889e-02,  2.4181e-03, -5.3971e-02]]],


         [[[-7.0753e-02, -5.6858e-03, -1.5505e-01,  ..., -1.0831e-01,  4.3245e-02, -2.3914e-03],
           [-1.4532e-01,  1.8874e-01,  1.7022e-01,  ...,  6.6515e-02, -5.1983e-02,  7.4466e-03],
           [-1.2205e-02, -4.1273e-02, -3.2776e-02,  ...,  1.0380e-01, -1.5386e-01,  1.6483e-02],
           ...,
           [ 5.9033e-02, -7.7072e-02,  3.2538e-02,  ...,  1.3125e-01, -4.4611e-02, -3.7476e-03],
           [-1.4254e-01,  9.9437e-02,  1.3261e-02,  ...,  1.8991e-01,  1.0106e-01,  6.8381e-02],
           [ 3.8742e-02,  4.8316e-02,  1.5468e-01,  ...,  3.5050e-02,  1.2245e-01, -5.3243e-02]],

          [[ 1.3174e-01,  6.8838e-02,  5.8592e-02,  ...,  1.2019e-01, -8.0251e-02,  6.3106e-03],
           [ 1.2926e-01, -4.2112e-03,  1.5386e-01,  ..., -1.1881e-01, -1.6002e-01, -4.4437e-02],
           [-2.1710e-01,  5.1243e-02,  1.7793e-02,  ..., -2.2988e-01,  2.2042e-02, -2.5158e-02],
           ...,
           [ 1.5861e-02,  7.8712e-02,  1.3116e-01,  ...,  1.6383e-02, -2.0928e-01,  8.3712e-02],
           [-3.1829e-02, -1.7138e-01,  1.4885e-01,  ..., -3.9689e-03,  1.5748e-02,  1.5714e-01],
           [ 5.6229e-02, -1.5602e-01, -1.4553e-02,  ...,  1.9813e-01,  3.5488e-02, -2.2631e-02]],

          [[-1.6881e-01, -4.7530e-02, -2.3560e-01,  ...,  2.2706e-01,  1.9609e-01,  5.0230e-03],
           [-6.1174e-02, -1.1685e-01, -1.3385e-01,  ..., -9.3520e-03, -8.5750e-02,  6.5352e-02],
           [-9.3084e-02,  1.6471e-01,  1.1930e-01,  ...,  6.9835e-03, -1.1076e-01, -9.7240e-02],
           ...,
           [ 4.2203e-02,  3.1718e-02, -1.0242e-01,  ...,  1.5695e-01,  1.4183e-01,  7.7585e-02],
           [-4.3046e-02, -5.5197e-02,  1.6704e-01,  ..., -1.2704e-01,  7.9601e-02,  1.1921e-02],
           [-9.7188e-02,  7.9388e-02,  6.2029e-02,  ...,  1.2480e-01, -1.2471e-01, -3.1372e-02]],

          ...,

          [[ 9.4021e-02,  7.8354e-02,  6.5698e-02,  ..., -1.0745e-01, -3.4012e-02,  5.7407e-02],
           [-1.4672e-01, -4.4642e-02,  7.3621e-02,  ..., -2.6723e-02,  8.1749e-02, -5.7264e-02],
           [ 2.7905e-02, -9.0574e-02,  4.3206e-02,  ...,  5.8372e-02, -6.3585e-02,  1.0612e-01],
           ...,
           [-5.0864e-02,  3.0016e-01, -1.3941e-01,  ...,  3.6436e-02,  1.8815e-02, -4.8889e-02],
           [-1.7614e-01, -5.7894e-02,  1.0916e-01,  ..., -1.9700e-02, -1.0411e-02, -9.9141e-02],
           [-1.2500e-01,  3.1388e-02,  1.0968e-01,  ...,  1.4440e-01,  1.9715e-03, -1.0489e-02]],

          [[ 1.2781e-02,  1.3040e-01,  1.3949e-01,  ..., -1.6671e-01, -6.4652e-02,  5.1411e-02],
           [ 5.5998e-02,  4.4611e-02,  1.0386e-01,  ..., -1.6865e-01,  2.8941e-02, -1.0841e-02],
           [ 8.3916e-02,  5.5815e-03,  9.0352e-02,  ..., -5.7889e-02,  5.3321e-02,  8.9338e-02],
           ...,
           [ 1.9076e-02, -1.1242e-01, -1.0514e-01,  ...,  9.1568e-02,  8.1445e-02, -5.1950e-02],
           [-4.0659e-02, -1.7865e-03,  4.7363e-03,  ...,  9.8566e-02, -7.7189e-02, -4.0579e-02],
           [ 1.5700e-02, -2.0995e-01,  7.7326e-03,  ...,  7.9025e-02,  4.6939e-02,  6.3324e-03]],

          [[-7.5244e-02, -1.0601e-01,  1.5411e-01,  ...,  4.6457e-02, -4.6399e-03,  1.2881e-01],
           [-2.5882e-02,  6.5585e-02, -6.6159e-02,  ...,  4.6849e-02,  2.0188e-01, -4.3965e-02],
           [-5.9892e-02,  2.0089e-02, -6.7414e-02,  ..., -3.3750e-02,  7.7698e-03, -2.8220e-02],
           ...,
           [-1.2892e-02,  5.8407e-02,  1.1737e-01,  ...,  7.7694e-03, -5.8593e-02, -2.5002e-02],
           [-6.5807e-02,  8.2080e-02,  1.7689e-01,  ...,  1.4801e-01,  2.0409e-02,  5.3787e-02],
           [-5.5275e-03,  8.9268e-02, -3.7061e-02,  ..., -1.1030e-01,  8.6875e-02,  6.4432e-02]]],


         [[[ 1.1878e-01, -3.3466e-02, -3.7534e-02,  ..., -1.8839e-01,  4.4484e-02, -1.0167e-01],
           [-3.7850e-04,  3.6875e-02, -4.7738e-02,  ...,  1.0372e-02,  3.7806e-02,  3.3514e-02],
           [-6.8944e-03,  4.6740e-02,  9.1538e-02,  ...,  2.4567e-03, -8.7989e-02,  1.3770e-01],
           ...,
           [-2.9824e-02,  6.5578e-02,  5.4447e-02,  ...,  6.5801e-02, -1.7350e-02,  1.8454e-02],
           [-3.3510e-02,  2.4157e-02,  1.0367e-01,  ..., -9.8515e-02,  1.4797e-02, -1.1581e-01],
           [ 9.1276e-02,  1.1989e-01,  6.5676e-02,  ...,  6.0878e-02, -4.6121e-02, -7.6496e-02]],

          [[ 1.0156e-01, -8.1350e-02, -1.1278e-01,  ...,  6.2826e-02, -4.1725e-03,  4.7197e-02],
           [-3.2365e-02, -5.1138e-02,  4.3056e-02,  ..., -4.8650e-02, -6.5195e-02, -2.5862e-02],
           [ 4.4916e-03,  9.4116e-02, -1.2453e-01,  ..., -6.9053e-02,  1.3909e-02,  1.1271e-02],
           ...,
           [ 7.0467e-03, -1.9157e-01, -4.6999e-02,  ...,  1.0125e-02,  1.6563e-02, -6.0891e-02],
           [ 5.3260e-03, -1.0730e-01,  9.7915e-02,  ...,  1.3055e-01,  1.2415e-01, -8.9586e-03],
           [-2.7611e-02, -6.3880e-02,  3.4756e-02,  ..., -4.0236e-02, -6.2716e-02, -2.4589e-03]],

          [[-5.8793e-02, -1.8193e-03,  2.6872e-02,  ..., -1.2947e-01, -9.1769e-02, -1.9811e-02],
           [-7.0935e-02,  2.5420e-02,  1.6784e-01,  ..., -2.0042e-01,  1.8795e-02, -1.1960e-01],
           [ 1.5561e-02,  4.5933e-03,  1.2832e-01,  ..., -1.1066e-01,  1.8132e-01, -3.7917e-02],
           ...,
           [ 1.0541e-01, -1.4916e-02, -2.2559e-01,  ...,  6.4903e-02,  4.2904e-02,  2.0551e-02],
           [ 1.1839e-02, -1.4796e-01, -1.4244e-02,  ..., -7.0299e-02,  1.2350e-01,  1.9973e-02],
           [ 9.9712e-02, -2.0634e-01,  6.4662e-02,  ..., -1.5305e-02,  9.4067e-02, -5.1123e-03]],

          ...,

          [[ 1.5359e-02, -1.0706e-02,  4.9041e-02,  ...,  1.0676e-01,  2.7318e-02, -1.1497e-01],
           [-1.3571e-01, -1.1272e-01, -4.7973e-02,  ..., -1.2026e-02,  9.6498e-02, -2.4142e-02],
           [-3.7644e-02, -1.3874e-01,  4.8209e-02,  ...,  1.6603e-01, -1.3509e-01,  5.3409e-02],
           ...,
           [-1.1660e-01,  9.8233e-02,  8.4664e-02,  ...,  1.3770e-01, -6.1837e-02, -8.9890e-02],
           [-1.0587e-01, -8.2539e-02, -1.2050e-01,  ...,  1.9447e-02, -1.2695e-02,  3.6146e-02],
           [-6.9729e-02,  8.1041e-02, -1.1917e-01,  ..., -9.8614e-02, -2.1439e-02, -7.9788e-02]],

          [[-1.6277e-01, -1.8310e-02,  1.0467e-01,  ...,  1.4662e-01,  8.4161e-02,  8.3279e-02],
           [-1.9334e-01,  1.4996e-01,  2.5534e-02,  ..., -2.2852e-01,  2.7952e-02, -6.6576e-02],
           [-1.5791e-02,  5.6656e-02, -8.7165e-02,  ..., -3.6142e-02, -4.8529e-02,  3.3090e-02],
           ...,
           [ 9.0500e-02,  1.5181e-03, -1.5838e-01,  ..., -1.4371e-01,  9.7451e-02, -1.3599e-01],
           [ 8.2085e-02, -9.2813e-02,  2.4225e-02,  ..., -3.5731e-02,  2.8473e-01, -3.1772e-02],
           [ 1.4864e-01, -1.4223e-01, -2.9897e-02,  ...,  1.0993e-02,  1.6023e-02,  1.3022e-01]],

          [[ 1.1568e-01, -4.7023e-02,  1.5965e-01,  ...,  2.7939e-01, -9.0137e-03, -1.3813e-02],
           [-6.3169e-02,  7.7513e-02,  7.1934e-02,  ..., -2.5251e-02,  1.6755e-01, -3.4318e-02],
           [-1.1848e-01,  1.3252e-01, -3.4675e-02,  ..., -7.1558e-02,  4.3700e-02, -8.5276e-02],
           ...,
           [-5.8607e-02,  1.2974e-01, -2.0226e-02,  ...,  6.1341e-02, -5.1336e-02,  6.6101e-02],
           [-1.2591e-01,  1.0047e-01, -7.7686e-02,  ..., -9.0199e-02,  2.1356e-02,  1.3988e-01],
           [ 2.7335e-02,  2.8877e-02, -6.1852e-03,  ..., -8.7883e-02,  2.6576e-02, -3.4056e-02]]]]])

2025-07-09 00:27:11.760639 GPU 4 66790 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 32, 699051],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 32, 699051],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:27:30.414368 GPU 2 66953 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 32, 699051],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 32, 699051],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:28:23.632174 GPU 5 66545 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 699051, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
[accuracy error] paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 699051, 32],"float32"), kernel_size=2, stride=2, padding="SAME", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1388 / 536871936 (0.0%)
Greatest absolute difference: 0.2070731818675995 at index (1, 2, 14, 349525, 10) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 0, 349525, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 3, 16, 349526, 16]), dtype=torch.float32)
tensor([[[[[ 1.7678e-02, -1.1869e-01,  1.7209e-03,  ...,  2.0948e-02, -1.2922e-01, -1.9499e-02],
           [ 1.8190e-01,  1.5827e-01, -1.5013e-01,  ..., -1.1092e-01,  1.4093e-02, -8.5713e-02],
           [ 1.4221e-01, -4.0686e-02,  4.5202e-02,  ..., -1.4705e-02, -1.4522e-01,  2.2603e-02],
           ...,
           [ 1.1512e-01,  3.1642e-02, -4.1150e-02,  ...,  3.2517e-02, -8.2103e-02,  3.2869e-02],
           [ 6.6597e-03, -2.8933e-03, -7.2864e-02,  ..., -2.8429e-02,  1.0583e-01,  3.0074e-02],
           [ 4.6593e-02,  5.6080e-02,  2.0530e-01,  ..., -8.8963e-02, -8.6283e-02, -3.0811e-03]],

          [[-2.1772e-02,  4.5365e-02, -4.0567e-02,  ...,  4.1850e-02, -2.6265e-02, -1.0521e-01],
           [-1.0434e-02,  7.9878e-02, -7.1205e-03,  ...,  9.8341e-02,  5.5836e-02,  9.7038e-02],
           [ 1.1001e-01, -7.6013e-02, -7.1061e-02,  ..., -5.5704e-02, -4.0311e-02, -1.7760e-01],
           ...,
           [-5.0246e-02,  7.4168e-03,  2.3886e-02,  ...,  1.3964e-01, -4.8986e-02, -9.2333e-02],
           [ 1.4277e-01, -1.6291e-01,  7.2377e-02,  ..., -3.7339e-02,  8.6228e-02, -6.8943e-02],
           [-1.0970e-01, -7.1222e-02, -1.4909e-01,  ...,  1.2340e-01,  1.1476e-01, -1.2698e-01]],

          [[-1.3195e-01,  6.7783e-02, -1.2484e-01,  ..., -5.3518e-02,  4.9771e-02,  1.9543e-01],
           [ 1.4253e-02, -1.3843e-02,  1.7344e-01,  ...,  2.0529e-02,  5.8550e-02, -1.6507e-01],
           [ 6.7188e-02,  9.7987e-02,  1.9611e-01,  ...,  9.1967e-02,  2.0288e-01,  5.9768e-02],
           ...,
           [-8.6912e-02,  5.9631e-02,  4.3593e-02,  ...,  2.0400e-01, -9.3281e-06, -2.7839e-02],
           [ 1.6573e-01, -6.0532e-02,  1.2071e-02,  ...,  1.6880e-01,  1.8976e-02, -1.8158e-02],
           [ 1.8896e-02,  1.0078e-01, -2.3211e-01,  ...,  1.5544e-01,  8.4602e-02,  1.4910e-01]],

          ...,

          [[-2.7638e-01, -1.0444e-01, -3.3593e-02,  ...,  9.3273e-02,  5.3615e-02,  6.1194e-02],
           [-4.0035e-02,  3.1824e-03, -4.1431e-02,  ..., -1.5374e-01, -3.9844e-03,  1.2538e-01],
           [ 5.8797e-02, -1.9520e-02, -2.0996e-02,  ...,  1.7589e-02, -8.2646e-03, -8.3580e-02],
           ...,
           [ 1.1046e-01, -1.2869e-01,  5.9641e-02,  ...,  6.0303e-02, -3.3408e-02,  1.1570e-01],
           [ 1.4504e-01, -6.1918e-02,  8.2354e-02,  ...,  4.4248e-02, -3.5073e-02,  9.0225e-03],
           [ 1.0521e-01,  1.6851e-02,  3.9045e-02,  ..., -4.4138e-02,  6.6125e-02, -1.1616e-01]],

          [[-7.6248e-02,  5.0611e-04,  2.3682e-02,  ..., -6.5641e-02,  4.4012e-02, -1.8814e-02],
           [ 1.4566e-02, -8.1274e-02,  2.3463e-02,  ...,  6.7338e-02,  8.5098e-02,  6.0308e-03],
           [-1.5350e-02,  1.1780e-01,  1.3809e-01,  ..., -4.7296e-02, -2.0346e-02, -9.9974e-02],
           ...,
           [ 1.1688e-01, -5.8684e-02, -1.2735e-01,  ..., -5.0013e-02, -3.3897e-02,  8.7869e-02],
           [ 2.1602e-02,  7.2426e-03,  1.5684e-02,  ...,  8.2968e-02, -6.5913e-02, -2.8172e-02],
           [-5.3919e-02, -1.5254e-01, -2.9891e-02,  ...,  1.2791e-02,  8.2589e-02, -8.0033e-02]],

          [[-5.8085e-03, -1.6631e-01,  1.3059e-01,  ..., -6.6204e-02,  1.1470e-01,  1.2214e-01],
           [-2.3676e-02, -3.2635e-02, -5.0514e-02,  ...,  5.5042e-02, -5.0817e-02, -9.0572e-02],
           [ 4.0355e-02, -4.0910e-03, -2.0745e-01,  ..., -1.6287e-02,  1.4495e-03, -5.1589e-02],
           ...,
           [-1.4239e-01,  8.7322e-02, -1.0951e-02,  ...,  6.9901e-02, -6.7363e-02, -2.1616e-02],
           [ 1.0679e-01,  5.0165e-03, -1.1394e-01,  ...,  1.9622e-02,  1.2350e-01, -8.8123e-02],
           [-1.1171e-01, -1.9981e-01,  1.4010e-01,  ..., -4.7713e-02, -8.5166e-02, -1.4303e-02]]],


         [[[-3.0793e-02, -8.1815e-02,  3.9053e-02,  ..., -4.5166e-02, -1.0564e-01, -5.9908e-02],
           [-2.7364e-01,  1.5409e-02, -3.5829e-02,  ..., -3.4169e-02, -2.5662e-02,  1.3411e-01],
           [ 1.5539e-01, -1.8562e-02,  1.2002e-01,  ..., -1.1170e-02,  8.1806e-02,  1.0001e-02],
           ...,
           [ 9.9996e-02, -1.6132e-01, -3.3999e-03,  ...,  8.3698e-02,  6.8587e-02,  1.0060e-01],
           [-1.2652e-01,  9.8358e-02, -4.3663e-02,  ..., -5.9073e-02, -9.3428e-02, -2.3360e-02],
           [-3.4400e-02,  1.3626e-01, -1.2271e-01,  ..., -4.0671e-01, -5.4014e-02,  2.6849e-02]],

          [[ 3.1517e-02, -1.6612e-03, -6.7514e-03,  ...,  5.2971e-02,  2.5678e-02,  8.4137e-02],
           [-6.0259e-02,  6.4350e-02,  1.1563e-01,  ...,  2.2695e-02, -4.2524e-02, -2.2274e-01],
           [-1.3682e-01, -1.5328e-01,  1.0404e-02,  ...,  1.3784e-01, -9.4115e-02, -1.5754e-01],
           ...,
           [-4.7877e-02, -8.8103e-04, -1.2770e-01,  ...,  5.5901e-02,  1.4171e-02,  6.2368e-02],
           [-6.6318e-02,  9.2185e-02, -6.0715e-02,  ..., -5.6738e-02, -6.4205e-02,  5.6472e-02],
           [ 3.4586e-01, -5.7278e-02,  1.3854e-01,  ...,  1.7907e-01, -1.6093e-01, -1.2585e-01]],

          [[-1.3528e-01, -1.0640e-01,  8.3570e-02,  ...,  4.0866e-02,  1.4548e-01, -1.0115e-01],
           [-2.7363e-02,  9.4625e-02,  7.6939e-02,  ..., -1.5638e-01,  5.8590e-02,  2.0230e-02],
           [ 9.1995e-02, -8.6190e-02, -2.0597e-02,  ...,  8.9695e-02,  1.4686e-01,  2.9740e-01],
           ...,
           [-2.7519e-02,  1.9322e-01,  1.2674e-01,  ...,  1.5293e-01, -2.3302e-01,  8.2237e-03],
           [-1.0276e-01,  1.3271e-01,  1.0624e-01,  ...,  1.6120e-01,  1.7135e-01, -7.7796e-02],
           [-1.9331e-01, -3.9915e-02,  1.6595e-01,  ..., -3.6142e-01, -1.9179e-01,  6.7075e-02]],

          ...,

          [[ 3.7141e-02, -1.6830e-01, -2.6803e-02,  ...,  3.6332e-02,  2.9816e-02, -8.8611e-02],
           [-1.1456e-02, -8.0778e-02, -8.7805e-02,  ..., -4.4957e-02, -1.3485e-02,  7.7581e-02],
           [ 7.4461e-02, -7.9231e-05,  2.2535e-01,  ..., -1.9999e-02,  9.1316e-02, -8.3312e-02],
           ...,
           [-3.4507e-02,  4.6922e-02, -7.4580e-02,  ..., -9.9098e-02, -2.9617e-02, -4.1278e-02],
           [ 4.8504e-02, -1.3509e-02,  6.7890e-02,  ...,  8.5848e-02,  2.1691e-02,  1.5425e-01],
           [ 1.6100e-01,  3.7087e-01,  1.8504e-01,  ...,  7.1564e-02, -4.7979e-02, -3.6518e-01]],

          [[-4.6121e-02,  4.5275e-02, -1.1703e-01,  ..., -9.3450e-02,  7.3174e-02, -2.2679e-03],
           [-4.2467e-02,  1.0286e-01, -6.9883e-02,  ...,  7.1122e-03, -1.1983e-01,  6.6457e-02],
           [-1.6718e-01,  5.5855e-02,  1.9853e-01,  ...,  9.1035e-02,  4.4439e-02, -1.2860e-02],
           ...,
           [ 3.3807e-02,  9.1547e-02, -1.7815e-01,  ...,  5.8873e-03,  1.2014e-01,  1.0265e-01],
           [ 9.7783e-02, -3.2251e-02,  1.3523e-01,  ...,  1.4967e-01,  3.1653e-02,  1.8893e-02],
           [ 1.2284e-01,  2.7478e-01,  1.6925e-01,  ...,  1.4014e-02,  3.8529e-02,  8.1932e-02]],

          [[-2.2553e-03, -6.8780e-02, -1.2393e-01,  ..., -6.5132e-03, -1.0480e-02,  1.3866e-01],
           [-1.4786e-01, -9.7939e-02, -2.0115e-02,  ...,  1.4280e-01, -6.0578e-02, -3.6881e-02],
           [-5.4864e-02,  7.8702e-03, -4.0934e-02,  ..., -1.1767e-01,  2.1145e-03, -1.7000e-01],
           ...,
           [-7.7721e-02, -5.0249e-02, -6.0123e-02,  ..., -8.4866e-02, -7.6625e-02, -3.9783e-02],
           [-1.9231e-02,  3.1416e-01, -9.0982e-02,  ...,  2.1785e-02, -1.8539e-02,  1.2924e-01],
           [-2.6320e-03, -2.1724e-01,  1.1475e-02,  ...,  2.5385e-02,  8.2390e-02,  2.5705e-01]]],


         [[[ 3.4323e-02,  7.3804e-02,  9.3262e-02,  ...,  6.1913e-02, -8.7134e-02, -6.3999e-02],
           [ 4.4304e-02,  2.9380e-02,  7.6865e-03,  ...,  1.3464e-01, -2.0552e-02, -1.3955e-01],
           [-1.2601e-01, -1.3200e-01,  1.5847e-01,  ..., -8.7092e-02,  1.3441e-01,  2.7138e-02],
           ...,
           [ 7.1918e-02,  1.5671e-01,  2.7945e-01,  ..., -9.8597e-02,  1.8564e-02,  2.7031e-02],
           [-2.1564e-01, -1.2122e-01,  6.0555e-02,  ..., -1.2828e-01,  8.3378e-03, -2.7371e-02],
           [ 2.7472e-01, -7.1386e-02,  1.2780e-01,  ...,  1.2222e-01,  2.3115e-02,  2.1558e-01]],

          [[ 8.3758e-02,  3.4569e-02, -9.3018e-04,  ..., -1.7446e-01, -1.4403e-02, -8.7807e-02],
           [ 4.3753e-03, -7.1454e-02, -1.3344e-02,  ..., -1.1835e-01,  9.1923e-02, -1.3934e-02],
           [-1.0413e-01, -2.8933e-02,  1.9115e-02,  ..., -3.5910e-02, -6.3158e-02, -1.2236e-01],
           ...,
           [-1.8997e-01, -4.0607e-02,  3.1502e-02,  ..., -2.6670e-02, -6.4269e-02,  1.2755e-01],
           [ 1.7951e-01, -7.7850e-02,  7.1563e-02,  ...,  1.0359e-01, -1.3505e-01, -4.8131e-03],
           [ 1.9696e-02,  1.6824e-01,  7.6495e-02,  ...,  6.1638e-02,  1.6424e-01, -1.7025e-01]],

          [[ 5.2005e-02, -8.7354e-03, -2.0907e-01,  ..., -8.8093e-02, -7.3874e-02, -8.3908e-02],
           [ 7.4007e-02, -7.8830e-03,  1.0050e-01,  ...,  7.8965e-02,  1.7388e-01, -1.8402e-01],
           [-6.8659e-02,  1.1121e-01,  9.9473e-02,  ..., -1.7195e-01, -6.6126e-02, -5.3557e-02],
           ...,
           [ 6.7717e-02, -4.2867e-02, -6.9874e-02,  ..., -1.2003e-01, -2.2582e-04,  1.9686e-01],
           [ 2.0602e-01,  4.7224e-02, -7.7956e-02,  ..., -9.6627e-02, -8.7196e-02, -7.6171e-02],
           [ 7.3078e-02,  1.6399e-02,  1.5834e-01,  ...,  2.1742e-01, -1.4777e-01,  1.7248e-01]],

          ...,

          [[-6.8370e-02,  9.7779e-02,  3.2084e-02,  ...,  1.3662e-01, -1.2817e-01, -6.6525e-02],
           [-1.0421e-01,  4.9575e-02,  8.2023e-02,  ..., -1.2099e-01,  4.9644e-02, -1.6958e-02],
           [-5.7894e-02, -1.1338e-01,  5.7951e-02,  ...,  6.9661e-02,  1.0345e-01, -2.4140e-01],
           ...,
           [-1.6245e-01, -2.1996e-02, -1.1410e-01,  ...,  1.7381e-01, -7.4888e-02,  8.0187e-02],
           [ 4.6791e-02, -1.0249e-01,  1.7948e-01,  ...,  3.4041e-02, -7.1259e-02,  1.3903e-01],
           [-1.7463e-01,  9.7772e-03, -1.2103e-01,  ...,  7.9517e-02,  1.0918e-01, -4.8893e-02]],

          [[ 8.6413e-02,  1.0992e-01,  4.2992e-02,  ..., -1.3379e-01, -1.2311e-02,  2.7374e-01],
           [-6.1332e-02,  4.0560e-02, -7.4819e-02,  ...,  3.8633e-02,  1.0480e-02, -3.8337e-02],
           [-2.7061e-02, -1.5954e-02, -1.5008e-01,  ...,  5.9089e-02, -2.4241e-01, -2.7010e-03],
           ...,
           [ 7.1174e-02, -8.2015e-02, -1.9426e-01,  ..., -8.8654e-02,  8.8175e-02, -6.0884e-02],
           [-2.3830e-02,  2.1831e-01,  2.7088e-02,  ...,  1.2368e-01, -5.9341e-02, -2.9717e-02],
           [ 1.9701e-01, -8.9410e-02,  2.1935e-01,  ..., -4.3288e-02,  7.5494e-02, -2.1325e-01]],

          [[-1.0907e-01,  1.6808e-02,  4.6153e-02,  ...,  1.6254e-02, -2.1828e-02,  4.0576e-02],
           [ 1.6915e-01, -7.0257e-03,  8.3887e-03,  ..., -6.8072e-02, -3.8760e-02, -7.9600e-02],
           [-1.1647e-01, -5.5208e-02, -1.8755e-01,  ..., -1.4966e-03, -2.5315e-01, -9.0283e-02],
           ...,
           [ 9.2383e-02, -1.7990e-01, -1.0841e-02,  ..., -2.1225e-03,  5.5542e-02,  1.7308e-03],
           [-6.1809e-02, -1.4336e-01, -1.6371e-01,  ...,  1.2761e-01,  7.1046e-02,  1.1383e-01],
           [ 2.3332e-01,  2.5573e-01,  7.8653e-02,  ..., -2.3020e-01, -1.5522e-02,  1.7396e-01]]]],



        [[[[-1.3327e-02, -7.5995e-03, -1.1064e-01,  ...,  1.4703e-01, -1.0544e-01, -8.8224e-02],
           [-9.7992e-02, -9.8790e-02,  7.6756e-02,  ...,  7.7151e-02,  2.5536e-02,  7.9893e-02],
           [ 7.8480e-03,  1.9318e-01,  1.8886e-01,  ..., -6.6406e-02, -8.8224e-02,  7.0729e-02],
           ...,
           [ 1.0387e-01,  6.4673e-02, -1.1585e-01,  ...,  2.1255e-01,  1.3834e-01,  1.0606e-01],
           [ 1.2981e-01,  1.6105e-01, -3.6469e-02,  ...,  2.3592e-02,  7.2730e-02, -1.4212e-01],
           [ 2.4307e-02, -2.0563e-01, -5.4778e-02,  ...,  5.3453e-03, -2.6747e-01,  1.4365e-01]],

          [[-5.3494e-03,  5.7382e-02,  2.8021e-02,  ...,  8.6948e-02, -1.7400e-01,  5.0493e-02],
           [-7.1652e-02,  5.9668e-02,  3.0906e-02,  ..., -2.0324e-02, -1.5187e-02, -3.3846e-02],
           [ 1.1716e-01, -1.3411e-01,  1.2435e-01,  ...,  6.3969e-02,  2.3362e-02,  5.0942e-02],
           ...,
           [-6.2221e-02, -3.4528e-02, -1.6936e-02,  ...,  1.2658e-01, -8.3257e-02,  6.9013e-02],
           [-1.2824e-01, -9.1228e-02,  1.0784e-01,  ..., -4.3215e-04,  6.8006e-02,  1.0720e-01],
           [-7.4594e-02, -9.7687e-02,  8.5086e-03,  ..., -3.7773e-02, -1.8176e-01,  9.1329e-02]],

          [[-7.1148e-02,  1.2656e-01, -1.2811e-01,  ..., -1.6136e-03,  1.4728e-02,  3.9614e-02],
           [ 2.8608e-01, -1.7838e-01,  1.4163e-02,  ..., -8.6596e-02, -1.2421e-01, -2.4138e-01],
           [ 8.2303e-02, -1.8556e-01, -1.3375e-01,  ...,  3.7466e-02,  6.7215e-02,  6.2322e-02],
           ...,
           [ 2.0063e-02,  3.2728e-02,  1.0073e-01,  ..., -1.4855e-02, -1.7615e-01, -1.7525e-03],
           [-3.6733e-02,  2.0089e-01,  7.0277e-02,  ...,  3.5889e-02,  2.0179e-01, -2.9671e-02],
           [ 1.0277e-01,  2.4010e-01, -2.9245e-02,  ..., -2.3410e-01, -2.7102e-01,  2.0295e-02]],

          ...,

          [[ 1.3289e-01, -1.0309e-01, -3.1759e-02,  ...,  1.7048e-01, -1.8964e-02, -1.2934e-01],
           [ 8.3651e-04, -1.2118e-02,  4.8383e-02,  ...,  8.3546e-02,  3.1596e-03, -9.3150e-02],
           [ 1.3853e-02, -1.7523e-02,  1.0431e-01,  ...,  1.8272e-02, -1.0487e-01,  2.8880e-02],
           ...,
           [-6.2014e-02,  4.3244e-02,  1.7157e-01,  ...,  1.2507e-02,  1.1046e-01,  5.2350e-02],
           [-2.4355e-02, -3.0559e-02,  2.9041e-02,  ..., -6.5836e-02,  6.2233e-02,  1.8482e-01],
           [-1.5814e-01, -6.8522e-02,  7.0216e-02,  ..., -1.2344e-01, -2.0592e-01,  1.1899e-01]],

          [[-8.6372e-02, -3.5795e-02, -6.5172e-02,  ..., -2.6198e-01,  1.8766e-02,  7.5859e-02],
           [ 9.9940e-02,  4.0150e-02, -9.8056e-02,  ..., -1.7158e-01, -3.4062e-03, -2.6638e-02],
           [-5.5468e-02, -7.0994e-04, -7.5006e-02,  ..., -1.0616e-01,  1.7372e-02, -4.8893e-02],
           ...,
           [-1.4181e-01,  1.1548e-01,  5.5874e-02,  ...,  2.9942e-02, -1.8112e-02, -2.9444e-02],
           [ 3.9031e-02,  5.0681e-02,  5.8297e-02,  ...,  1.5017e-02,  7.6318e-02, -1.7010e-02],
           [ 2.1333e-02,  1.2379e-01, -1.6834e-01,  ..., -4.3487e-02, -8.1982e-02,  4.2028e-02]],

          [[ 1.6978e-02, -4.5087e-02,  8.1483e-02,  ..., -3.4949e-02, -7.6166e-02,  1.2526e-01],
           [ 1.1711e-01, -2.5587e-02, -3.6007e-02,  ..., -1.7148e-01,  1.1170e-02,  4.9115e-02],
           [-2.6843e-02,  4.2615e-02,  4.1452e-02,  ...,  1.1094e-01,  7.9744e-03, -9.2707e-02],
           ...,
           [ 2.5185e-02,  2.3065e-02, -1.1378e-01,  ..., -1.3932e-01, -6.4133e-02,  1.0574e-02],
           [-3.0315e-02,  3.8837e-02,  1.6509e-01,  ...,  1.2289e-01,  5.9516e-02,  4.5134e-02],
           [-5.7822e-02,  1.1070e-02,  2.0424e-01,  ..., -4.9920e-02, -5.5227e-02, -1.1868e-01]]],


         [[[-8.5187e-02,  6.7951e-02,  5.3837e-02,  ..., -7.2646e-02,  2.5646e-02,  6.2759e-02],
           [ 6.9788e-02, -8.1055e-02,  4.5650e-02,  ...,  1.2715e-01,  1.4196e-02, -1.5498e-01],
           [ 1.0499e-01, -1.5657e-02,  1.3404e-02,  ..., -7.7004e-02, -1.4072e-01, -1.9966e-03],
           ...,
           [-9.7883e-02,  5.0957e-02,  1.0122e-01,  ...,  1.4629e-01, -1.1889e-01, -3.2397e-02],
           [ 4.3204e-02,  1.5724e-01, -5.0146e-05,  ..., -4.0239e-02,  2.0330e-01, -1.8656e-02],
           [ 8.0338e-02, -7.9326e-02,  6.1825e-02,  ..., -2.4754e-01,  1.5843e-01, -1.1025e-02]],

          [[ 8.9421e-02,  8.8845e-02, -6.1184e-02,  ..., -2.7354e-02,  8.8321e-02,  1.2828e-01],
           [ 1.3511e-02, -5.0528e-02, -1.4633e-01,  ...,  3.6064e-02,  4.0775e-02,  1.3117e-01],
           [ 1.8151e-02, -1.1443e-01, -1.5861e-01,  ..., -2.4579e-02,  2.5034e-02, -2.1739e-02],
           ...,
           [ 1.2346e-02,  1.2971e-01,  3.8458e-02,  ...,  2.3587e-01,  1.1556e-01,  9.2041e-03],
           [ 1.0450e-01, -1.1313e-02, -8.5782e-02,  ...,  1.4086e-02, -3.6369e-02, -7.4709e-03],
           [ 1.7713e-01, -5.5425e-02, -7.5177e-02,  ...,  3.1640e-01,  1.4276e-03,  1.0463e-01]],

          [[-1.9333e-01, -2.3790e-02, -1.8478e-01,  ..., -6.0476e-02,  6.6580e-03,  1.7396e-01],
           [ 3.8446e-02, -6.8585e-02,  1.8205e-01,  ...,  2.9872e-02,  5.5316e-02,  1.1129e-01],
           [-8.4996e-03, -6.8807e-02, -1.2926e-01,  ..., -2.8666e-03,  1.0701e-01, -5.4722e-02],
           ...,
           [-1.0787e-01, -5.1398e-03,  2.2752e-01,  ..., -1.9846e-02, -1.0341e-01,  4.1114e-03],
           [ 1.4117e-02, -9.9711e-02,  1.1045e-01,  ...,  4.3007e-02,  3.6649e-02,  3.7427e-02],
           [-7.5399e-02,  2.6531e-01, -9.2936e-02,  ...,  2.9219e-01, -8.1848e-02, -3.1821e-02]],

          ...,

          [[ 4.4219e-05,  5.7690e-02, -1.0569e-01,  ...,  5.8110e-02,  1.5870e-02, -1.2692e-02],
           [-7.5560e-02, -8.2880e-02,  1.1041e-01,  ..., -1.1110e-01,  1.7158e-01,  9.2602e-02],
           [-1.9235e-01,  1.1049e-01,  6.9263e-02,  ...,  1.8956e-03,  1.0422e-01,  3.7345e-02],
           ...,
           [ 5.7292e-02,  1.2155e-01, -4.4879e-04,  ..., -3.2159e-03, -5.7022e-02,  7.1428e-02],
           [-6.5536e-02,  5.2000e-02, -1.1614e-01,  ..., -2.4017e-02,  5.4362e-02,  1.5543e-01],
           [-5.6441e-02,  2.7685e-01, -1.0732e-01,  ...,  8.2878e-03,  1.2838e-01, -1.5134e-02]],

          [[-7.5944e-02,  1.0851e-01,  1.7336e-01,  ...,  3.9967e-02,  3.1034e-02, -5.2872e-02],
           [-1.5726e-01,  1.6285e-02, -1.5322e-01,  ...,  5.0231e-02,  3.7195e-02, -1.2289e-01],
           [-1.0959e-01,  2.2796e-02,  7.4699e-02,  ...,  3.6581e-02, -9.6859e-02,  1.4374e-02],
           ...,
           [ 1.9728e-02, -5.2994e-02, -1.0111e-01,  ..., -1.1787e-01,  1.2975e-01, -6.5378e-03],
           [-8.6925e-02,  1.3311e-01,  1.5001e-01,  ..., -1.0849e-01,  4.2130e-02, -2.9890e-02],
           [-3.1217e-01,  1.7351e-01,  1.7177e-01,  ...,  1.1128e-01, -1.0272e-01, -2.2969e-02]],

          [[ 6.3340e-02, -1.9570e-01,  8.8426e-02,  ..., -2.2677e-03, -1.9932e-01,  1.7536e-01],
           [-6.9694e-02,  8.6373e-03,  4.4566e-02,  ..., -3.2263e-02, -1.0891e-02, -1.2731e-01],
           [-8.7922e-02, -4.2518e-02, -2.2084e-01,  ..., -1.3666e-01, -6.3402e-02,  2.5289e-02],
           ...,
           [-5.9229e-02,  7.5715e-02,  5.6888e-02,  ..., -7.9450e-02,  1.5765e-02, -5.9968e-02],
           [-1.9089e-02, -3.3602e-02, -7.5792e-02,  ...,  9.1326e-02, -9.2602e-02, -2.0123e-02],
           [ 2.0639e-01, -2.0313e-01, -1.2019e-01,  ..., -9.8024e-02, -7.9044e-02,  2.6031e-01]]],


         [[[ 1.8505e-01, -8.2891e-02,  1.3220e-03,  ..., -3.2965e-02, -2.4220e-01, -7.9849e-02],
           [ 2.3010e-02,  6.0594e-02,  1.1671e-01,  ...,  3.7633e-02,  1.5009e-01,  2.5681e-02],
           [-4.7493e-02,  1.6378e-02, -3.7301e-02,  ..., -2.5668e-01, -1.1043e-01,  4.3796e-02],
           ...,
           [ 2.2817e-02,  1.8035e-02,  1.5693e-01,  ...,  1.3603e-02,  8.0895e-02, -7.6434e-02],
           [ 5.2587e-02, -1.0572e-01, -6.5152e-02,  ..., -7.5184e-02,  5.9290e-02, -2.9095e-01],
           [-4.2389e-02, -2.0960e-01, -1.2672e-02,  ...,  5.4311e-02,  1.5073e-01, -1.5138e-01]],

          [[ 4.3849e-02,  1.6290e-02, -1.5036e-01,  ..., -1.2273e-01, -5.0591e-02,  8.0130e-02],
           [ 1.2681e-01, -5.0936e-02,  6.2976e-02,  ...,  6.9390e-02,  1.4028e-01, -7.7345e-02],
           [-6.7526e-02, -4.6386e-03,  9.5082e-03,  ...,  1.2703e-02,  1.1117e-01, -1.1803e-01],
           ...,
           [-1.0656e-03,  4.1111e-02, -1.3555e-02,  ...,  3.6089e-02, -3.2487e-02, -9.0938e-02],
           [-6.9036e-02, -2.9739e-01,  1.0823e-01,  ...,  4.1216e-02,  6.6867e-02,  7.2952e-02],
           [ 1.7455e-01, -1.6853e-01, -5.5430e-02,  ..., -1.4194e-01, -2.8837e-02, -8.2995e-02]],

          [[-5.0202e-03,  1.4765e-01,  5.0654e-02,  ...,  8.7406e-02, -1.2845e-01,  9.6455e-02],
           [ 4.9551e-02,  1.7674e-01, -4.2344e-02,  ...,  3.6934e-02,  1.0916e-01,  4.0092e-02],
           [ 3.7374e-02, -3.9215e-02, -9.7904e-02,  ..., -3.0723e-02, -1.6826e-01, -3.5522e-02],
           ...,
           [ 1.1615e-01,  9.8449e-02, -1.2563e-01,  ..., -1.8250e-01, -1.0976e-02,  5.1443e-02],
           [-3.4132e-02,  2.6991e-02, -3.7619e-02,  ...,  4.1965e-02,  2.0204e-01,  1.9771e-02],
           [-2.7500e-01,  2.9784e-02,  6.1271e-02,  ..., -7.5041e-02,  2.5563e-02, -8.5677e-02]],

          ...,

          [[ 2.7084e-02, -5.8116e-02,  4.8876e-02,  ..., -5.9634e-02,  9.5346e-02, -3.9631e-02],
           [ 3.7214e-02, -1.3125e-01, -9.1848e-02,  ..., -1.2619e-01, -1.5081e-02,  5.6378e-02],
           [-4.3672e-02,  1.2595e-01,  1.4620e-01,  ..., -1.1562e-01, -3.3295e-02,  1.2469e-01],
           ...,
           [ 1.2960e-01,  2.9321e-02, -1.1934e-01,  ...,  7.5403e-02,  1.8795e-01, -9.0119e-03],
           [-1.0921e-01, -1.3320e-01,  4.9889e-02,  ...,  1.6277e-01,  9.5479e-02,  1.1609e-01],
           [ 2.5049e-01, -3.6020e-01,  1.8755e-01,  ...,  6.7490e-02,  3.3680e-03, -1.3522e-01]],

          [[-5.5915e-02,  8.3644e-02, -9.7635e-02,  ...,  9.4437e-03, -1.9931e-01, -1.7101e-02],
           [-8.2127e-02,  3.7943e-02, -1.0292e-01,  ...,  7.5163e-03,  1.3568e-01,  1.4762e-02],
           [-4.9569e-02,  2.3263e-01, -6.4542e-02,  ..., -4.3722e-02, -3.8308e-02, -1.1405e-02],
           ...,
           [ 5.8215e-02, -1.3299e-01,  6.6755e-02,  ...,  7.8601e-02, -3.9378e-02,  5.8071e-02],
           [-8.1914e-02, -5.1991e-02,  8.4073e-02,  ...,  3.1983e-02,  2.1963e-01,  8.4083e-02],
           [-1.1340e-01, -7.8101e-02, -2.1556e-01,  ..., -7.0569e-03,  8.1984e-02,  1.6760e-01]],

          [[-4.5608e-02, -5.1491e-02,  5.7220e-03,  ..., -7.2507e-02,  1.6909e-01, -2.2204e-03],
           [-6.2245e-02, -1.4661e-01,  1.7564e-01,  ..., -1.0382e-01,  1.7797e-01, -6.6737e-02],
           [-9.8443e-02, -2.1764e-02,  1.9685e-02,  ..., -9.1763e-02,  2.3473e-01,  4.4529e-02],
           ...,
           [ 6.7547e-02, -2.0348e-02, -3.8798e-03,  ...,  7.2748e-02, -2.6449e-02, -5.8717e-03],
           [-8.4356e-03,  8.4878e-02, -5.7364e-02,  ...,  7.6244e-02,  4.3453e-02,  1.1527e-01],
           [ 7.0269e-02, -1.8536e-01,  3.9155e-02,  ..., -2.8571e-01,  1.4065e-01, -1.7117e-01]]]]])
DESIRED: (shape=torch.Size([2, 3, 16, 349526, 16]), dtype=torch.float32)
tensor([[[[[ 1.7678e-02, -1.1869e-01,  1.7209e-03,  ...,  2.0948e-02, -1.2922e-01, -1.9499e-02],
           [ 1.8190e-01,  1.5827e-01, -1.5013e-01,  ..., -1.1092e-01,  1.4093e-02, -8.5713e-02],
           [ 1.4221e-01, -4.0686e-02,  4.5202e-02,  ..., -1.4705e-02, -1.4522e-01,  2.2603e-02],
           ...,
           [ 1.1512e-01,  3.1642e-02, -4.1150e-02,  ...,  3.2517e-02, -8.2103e-02,  3.2869e-02],
           [ 6.6597e-03, -2.8933e-03, -7.2864e-02,  ..., -2.8429e-02,  1.0583e-01,  3.0074e-02],
           [ 2.3296e-02,  2.8040e-02,  1.0265e-01,  ..., -4.4482e-02, -4.3142e-02, -1.5405e-03]],

          [[-2.1772e-02,  4.5365e-02, -4.0567e-02,  ...,  4.1850e-02, -2.6265e-02, -1.0521e-01],
           [-1.0434e-02,  7.9878e-02, -7.1205e-03,  ...,  9.8341e-02,  5.5836e-02,  9.7038e-02],
           [ 1.1001e-01, -7.6013e-02, -7.1061e-02,  ..., -5.5704e-02, -4.0311e-02, -1.7760e-01],
           ...,
           [-5.0246e-02,  7.4168e-03,  2.3886e-02,  ...,  1.3964e-01, -4.8986e-02, -9.2333e-02],
           [ 1.4277e-01, -1.6291e-01,  7.2377e-02,  ..., -3.7339e-02,  8.6228e-02, -6.8943e-02],
           [-5.4848e-02, -3.5611e-02, -7.4544e-02,  ...,  6.1699e-02,  5.7380e-02, -6.3490e-02]],

          [[-1.3195e-01,  6.7783e-02, -1.2484e-01,  ..., -5.3518e-02,  4.9771e-02,  1.9543e-01],
           [ 1.4253e-02, -1.3843e-02,  1.7344e-01,  ...,  2.0529e-02,  5.8550e-02, -1.6507e-01],
           [ 6.7188e-02,  9.7987e-02,  1.9611e-01,  ...,  9.1967e-02,  2.0288e-01,  5.9768e-02],
           ...,
           [-8.6912e-02,  5.9631e-02,  4.3593e-02,  ...,  2.0400e-01, -9.3281e-06, -2.7839e-02],
           [ 1.6573e-01, -6.0532e-02,  1.2071e-02,  ...,  1.6880e-01,  1.8976e-02, -1.8158e-02],
           [ 9.4478e-03,  5.0392e-02, -1.1606e-01,  ...,  7.7718e-02,  4.2301e-02,  7.4551e-02]],

          ...,

          [[-2.7638e-01, -1.0444e-01, -3.3593e-02,  ...,  9.3273e-02,  5.3615e-02,  6.1194e-02],
           [-4.0035e-02,  3.1824e-03, -4.1431e-02,  ..., -1.5374e-01, -3.9844e-03,  1.2538e-01],
           [ 5.8797e-02, -1.9520e-02, -2.0996e-02,  ...,  1.7589e-02, -8.2646e-03, -8.3580e-02],
           ...,
           [ 1.1046e-01, -1.2869e-01,  5.9641e-02,  ...,  6.0303e-02, -3.3408e-02,  1.1570e-01],
           [ 1.4504e-01, -6.1918e-02,  8.2354e-02,  ...,  4.4248e-02, -3.5073e-02,  9.0225e-03],
           [ 5.2607e-02,  8.4256e-03,  1.9522e-02,  ..., -2.2069e-02,  3.3062e-02, -5.8082e-02]],

          [[-7.6248e-02,  5.0611e-04,  2.3682e-02,  ..., -6.5641e-02,  4.4012e-02, -1.8814e-02],
           [ 1.4566e-02, -8.1274e-02,  2.3463e-02,  ...,  6.7338e-02,  8.5098e-02,  6.0308e-03],
           [-1.5350e-02,  1.1780e-01,  1.3809e-01,  ..., -4.7296e-02, -2.0346e-02, -9.9974e-02],
           ...,
           [ 1.1688e-01, -5.8684e-02, -1.2735e-01,  ..., -5.0013e-02, -3.3897e-02,  8.7869e-02],
           [ 2.1602e-02,  7.2426e-03,  1.5684e-02,  ...,  8.2968e-02, -6.5913e-02, -2.8172e-02],
           [-2.6959e-02, -7.6272e-02, -1.4946e-02,  ...,  6.3955e-03,  4.1295e-02, -4.0016e-02]],

          [[-5.8085e-03, -1.6631e-01,  1.3059e-01,  ..., -6.6204e-02,  1.1470e-01,  1.2214e-01],
           [-2.3676e-02, -3.2635e-02, -5.0514e-02,  ...,  5.5042e-02, -5.0817e-02, -9.0572e-02],
           [ 4.0355e-02, -4.0910e-03, -2.0745e-01,  ..., -1.6287e-02,  1.4495e-03, -5.1589e-02],
           ...,
           [-1.4239e-01,  8.7322e-02, -1.0951e-02,  ...,  6.9901e-02, -6.7363e-02, -2.1616e-02],
           [ 1.0679e-01,  5.0165e-03, -1.1394e-01,  ...,  1.9622e-02,  1.2350e-01, -8.8123e-02],
           [-5.5857e-02, -9.9907e-02,  7.0052e-02,  ..., -2.3856e-02, -4.2583e-02, -7.1517e-03]]],


         [[[-3.0793e-02, -8.1815e-02,  3.9053e-02,  ..., -4.5166e-02, -1.0564e-01, -5.9908e-02],
           [-2.7364e-01,  1.5409e-02, -3.5829e-02,  ..., -3.4169e-02, -2.5662e-02,  1.3411e-01],
           [ 1.5539e-01, -1.8562e-02,  1.2002e-01,  ..., -1.1170e-02,  8.1806e-02,  1.0001e-02],
           ...,
           [ 9.9996e-02, -1.6132e-01, -3.3999e-03,  ...,  8.3698e-02,  6.8587e-02,  1.0060e-01],
           [-1.2652e-01,  9.8358e-02, -4.3663e-02,  ..., -5.9073e-02, -9.3428e-02, -2.3360e-02],
           [-1.7200e-02,  6.8129e-02, -6.1355e-02,  ..., -2.0335e-01, -2.7007e-02,  1.3424e-02]],

          [[ 3.1517e-02, -1.6612e-03, -6.7514e-03,  ...,  5.2971e-02,  2.5678e-02,  8.4137e-02],
           [-6.0259e-02,  6.4350e-02,  1.1563e-01,  ...,  2.2695e-02, -4.2524e-02, -2.2274e-01],
           [-1.3682e-01, -1.5328e-01,  1.0404e-02,  ...,  1.3784e-01, -9.4115e-02, -1.5754e-01],
           ...,
           [-4.7877e-02, -8.8103e-04, -1.2770e-01,  ...,  5.5901e-02,  1.4171e-02,  6.2368e-02],
           [-6.6318e-02,  9.2185e-02, -6.0715e-02,  ..., -5.6738e-02, -6.4205e-02,  5.6472e-02],
           [ 1.7293e-01, -2.8639e-02,  6.9269e-02,  ...,  8.9536e-02, -8.0467e-02, -6.2924e-02]],

          [[-1.3528e-01, -1.0640e-01,  8.3570e-02,  ...,  4.0866e-02,  1.4548e-01, -1.0115e-01],
           [-2.7363e-02,  9.4625e-02,  7.6939e-02,  ..., -1.5638e-01,  5.8590e-02,  2.0230e-02],
           [ 9.1995e-02, -8.6190e-02, -2.0597e-02,  ...,  8.9695e-02,  1.4686e-01,  2.9740e-01],
           ...,
           [-2.7519e-02,  1.9322e-01,  1.2674e-01,  ...,  1.5293e-01, -2.3302e-01,  8.2237e-03],
           [-1.0276e-01,  1.3271e-01,  1.0624e-01,  ...,  1.6120e-01,  1.7135e-01, -7.7796e-02],
           [-9.6655e-02, -1.9957e-02,  8.2975e-02,  ..., -1.8071e-01, -9.5896e-02,  3.3538e-02]],

          ...,

          [[ 3.7141e-02, -1.6830e-01, -2.6803e-02,  ...,  3.6332e-02,  2.9816e-02, -8.8611e-02],
           [-1.1456e-02, -8.0778e-02, -8.7805e-02,  ..., -4.4957e-02, -1.3485e-02,  7.7581e-02],
           [ 7.4461e-02, -7.9231e-05,  2.2535e-01,  ..., -1.9999e-02,  9.1316e-02, -8.3312e-02],
           ...,
           [-3.4507e-02,  4.6922e-02, -7.4580e-02,  ..., -9.9098e-02, -2.9617e-02, -4.1278e-02],
           [ 4.8504e-02, -1.3509e-02,  6.7890e-02,  ...,  8.5848e-02,  2.1691e-02,  1.5425e-01],
           [ 8.0498e-02,  1.8543e-01,  9.2522e-02,  ...,  3.5782e-02, -2.3990e-02, -1.8259e-01]],

          [[-4.6121e-02,  4.5275e-02, -1.1703e-01,  ..., -9.3450e-02,  7.3174e-02, -2.2679e-03],
           [-4.2467e-02,  1.0286e-01, -6.9883e-02,  ...,  7.1122e-03, -1.1983e-01,  6.6457e-02],
           [-1.6718e-01,  5.5855e-02,  1.9853e-01,  ...,  9.1035e-02,  4.4439e-02, -1.2860e-02],
           ...,
           [ 3.3807e-02,  9.1547e-02, -1.7815e-01,  ...,  5.8873e-03,  1.2014e-01,  1.0265e-01],
           [ 9.7783e-02, -3.2251e-02,  1.3523e-01,  ...,  1.4967e-01,  3.1653e-02,  1.8893e-02],
           [ 6.1422e-02,  1.3739e-01,  8.4625e-02,  ...,  7.0069e-03,  1.9265e-02,  4.0966e-02]],

          [[-2.2553e-03, -6.8780e-02, -1.2393e-01,  ..., -6.5132e-03, -1.0480e-02,  1.3866e-01],
           [-1.4786e-01, -9.7939e-02, -2.0115e-02,  ...,  1.4280e-01, -6.0578e-02, -3.6881e-02],
           [-5.4864e-02,  7.8702e-03, -4.0934e-02,  ..., -1.1767e-01,  2.1145e-03, -1.7000e-01],
           ...,
           [-7.7721e-02, -5.0249e-02, -6.0123e-02,  ..., -8.4866e-02, -7.6625e-02, -3.9783e-02],
           [-1.9231e-02,  3.1416e-01, -9.0982e-02,  ...,  2.1785e-02, -1.8539e-02,  1.2924e-01],
           [-1.3160e-03, -1.0862e-01,  5.7377e-03,  ...,  1.2693e-02,  4.1195e-02,  1.2853e-01]]],


         [[[ 3.4323e-02,  7.3804e-02,  9.3262e-02,  ...,  6.1913e-02, -8.7134e-02, -6.3999e-02],
           [ 4.4304e-02,  2.9380e-02,  7.6865e-03,  ...,  1.3464e-01, -2.0552e-02, -1.3955e-01],
           [-1.2601e-01, -1.3200e-01,  1.5847e-01,  ..., -8.7092e-02,  1.3441e-01,  2.7138e-02],
           ...,
           [ 7.1918e-02,  1.5671e-01,  2.7945e-01,  ..., -9.8597e-02,  1.8564e-02,  2.7031e-02],
           [-2.1564e-01, -1.2122e-01,  6.0555e-02,  ..., -1.2828e-01,  8.3378e-03, -2.7371e-02],
           [ 1.3736e-01, -3.5693e-02,  6.3901e-02,  ...,  6.1109e-02,  1.1558e-02,  1.0779e-01]],

          [[ 8.3758e-02,  3.4569e-02, -9.3018e-04,  ..., -1.7446e-01, -1.4403e-02, -8.7807e-02],
           [ 4.3753e-03, -7.1454e-02, -1.3344e-02,  ..., -1.1835e-01,  9.1923e-02, -1.3934e-02],
           [-1.0413e-01, -2.8933e-02,  1.9115e-02,  ..., -3.5910e-02, -6.3158e-02, -1.2236e-01],
           ...,
           [-1.8997e-01, -4.0607e-02,  3.1502e-02,  ..., -2.6670e-02, -6.4269e-02,  1.2755e-01],
           [ 1.7951e-01, -7.7850e-02,  7.1563e-02,  ...,  1.0359e-01, -1.3505e-01, -4.8131e-03],
           [ 9.8479e-03,  8.4120e-02,  3.8247e-02,  ...,  3.0819e-02,  8.2118e-02, -8.5125e-02]],

          [[ 5.2005e-02, -8.7354e-03, -2.0907e-01,  ..., -8.8093e-02, -7.3874e-02, -8.3908e-02],
           [ 7.4007e-02, -7.8830e-03,  1.0050e-01,  ...,  7.8965e-02,  1.7388e-01, -1.8402e-01],
           [-6.8659e-02,  1.1121e-01,  9.9473e-02,  ..., -1.7195e-01, -6.6126e-02, -5.3557e-02],
           ...,
           [ 6.7717e-02, -4.2867e-02, -6.9874e-02,  ..., -1.2003e-01, -2.2582e-04,  1.9686e-01],
           [ 2.0602e-01,  4.7224e-02, -7.7956e-02,  ..., -9.6627e-02, -8.7196e-02, -7.6171e-02],
           [ 3.6539e-02,  8.1997e-03,  7.9171e-02,  ...,  1.0871e-01, -7.3885e-02,  8.6238e-02]],

          ...,

          [[-6.8370e-02,  9.7779e-02,  3.2084e-02,  ...,  1.3662e-01, -1.2817e-01, -6.6525e-02],
           [-1.0421e-01,  4.9575e-02,  8.2023e-02,  ..., -1.2099e-01,  4.9644e-02, -1.6958e-02],
           [-5.7894e-02, -1.1338e-01,  5.7951e-02,  ...,  6.9661e-02,  1.0345e-01, -2.4140e-01],
           ...,
           [-1.6245e-01, -2.1996e-02, -1.1410e-01,  ...,  1.7381e-01, -7.4888e-02,  8.0187e-02],
           [ 4.6791e-02, -1.0249e-01,  1.7948e-01,  ...,  3.4041e-02, -7.1259e-02,  1.3903e-01],
           [-8.7317e-02,  4.8886e-03, -6.0517e-02,  ...,  3.9758e-02,  5.4588e-02, -2.4446e-02]],

          [[ 8.6413e-02,  1.0992e-01,  4.2992e-02,  ..., -1.3379e-01, -1.2311e-02,  2.7374e-01],
           [-6.1332e-02,  4.0560e-02, -7.4819e-02,  ...,  3.8633e-02,  1.0480e-02, -3.8337e-02],
           [-2.7061e-02, -1.5954e-02, -1.5008e-01,  ...,  5.9089e-02, -2.4241e-01, -2.7010e-03],
           ...,
           [ 7.1174e-02, -8.2015e-02, -1.9426e-01,  ..., -8.8654e-02,  8.8175e-02, -6.0884e-02],
           [-2.3830e-02,  2.1831e-01,  2.7088e-02,  ...,  1.2368e-01, -5.9341e-02, -2.9717e-02],
           [ 9.8506e-02, -4.4705e-02,  1.0968e-01,  ..., -2.1644e-02,  3.7747e-02, -1.0662e-01]],

          [[-1.0907e-01,  1.6808e-02,  4.6153e-02,  ...,  1.6254e-02, -2.1828e-02,  4.0576e-02],
           [ 1.6915e-01, -7.0257e-03,  8.3887e-03,  ..., -6.8072e-02, -3.8760e-02, -7.9600e-02],
           [-1.1647e-01, -5.5208e-02, -1.8755e-01,  ..., -1.4966e-03, -2.5315e-01, -9.0283e-02],
           ...,
           [ 9.2383e-02, -1.7990e-01, -1.0841e-02,  ..., -2.1225e-03,  5.5542e-02,  1.7308e-03],
           [-6.1809e-02, -1.4336e-01, -1.6371e-01,  ...,  1.2761e-01,  7.1046e-02,  1.1383e-01],
           [ 1.1666e-01,  1.2786e-01,  3.9327e-02,  ..., -1.1510e-01, -7.7611e-03,  8.6981e-02]]]],



        [[[[-1.3327e-02, -7.5995e-03, -1.1064e-01,  ...,  1.4703e-01, -1.0544e-01, -8.8224e-02],
           [-9.7992e-02, -9.8790e-02,  7.6756e-02,  ...,  7.7151e-02,  2.5536e-02,  7.9893e-02],
           [ 7.8480e-03,  1.9318e-01,  1.8886e-01,  ..., -6.6406e-02, -8.8224e-02,  7.0729e-02],
           ...,
           [ 1.0387e-01,  6.4673e-02, -1.1585e-01,  ...,  2.1255e-01,  1.3834e-01,  1.0606e-01],
           [ 1.2981e-01,  1.6105e-01, -3.6469e-02,  ...,  2.3592e-02,  7.2730e-02, -1.4212e-01],
           [ 1.2154e-02, -1.0281e-01, -2.7389e-02,  ...,  2.6727e-03, -1.3374e-01,  7.1823e-02]],

          [[-5.3494e-03,  5.7382e-02,  2.8021e-02,  ...,  8.6948e-02, -1.7400e-01,  5.0493e-02],
           [-7.1652e-02,  5.9668e-02,  3.0906e-02,  ..., -2.0324e-02, -1.5187e-02, -3.3846e-02],
           [ 1.1716e-01, -1.3411e-01,  1.2435e-01,  ...,  6.3969e-02,  2.3362e-02,  5.0942e-02],
           ...,
           [-6.2221e-02, -3.4528e-02, -1.6936e-02,  ...,  1.2658e-01, -8.3257e-02,  6.9013e-02],
           [-1.2824e-01, -9.1228e-02,  1.0784e-01,  ..., -4.3215e-04,  6.8006e-02,  1.0720e-01],
           [-3.7297e-02, -4.8843e-02,  4.2543e-03,  ..., -1.8887e-02, -9.0881e-02,  4.5665e-02]],

          [[-7.1148e-02,  1.2656e-01, -1.2811e-01,  ..., -1.6136e-03,  1.4728e-02,  3.9614e-02],
           [ 2.8608e-01, -1.7838e-01,  1.4163e-02,  ..., -8.6596e-02, -1.2421e-01, -2.4138e-01],
           [ 8.2303e-02, -1.8556e-01, -1.3375e-01,  ...,  3.7466e-02,  6.7215e-02,  6.2322e-02],
           ...,
           [ 2.0063e-02,  3.2728e-02,  1.0073e-01,  ..., -1.4855e-02, -1.7615e-01, -1.7525e-03],
           [-3.6733e-02,  2.0089e-01,  7.0277e-02,  ...,  3.5889e-02,  2.0179e-01, -2.9671e-02],
           [ 5.1386e-02,  1.2005e-01, -1.4622e-02,  ..., -1.1705e-01, -1.3551e-01,  1.0148e-02]],

          ...,

          [[ 1.3289e-01, -1.0309e-01, -3.1759e-02,  ...,  1.7048e-01, -1.8964e-02, -1.2934e-01],
           [ 8.3651e-04, -1.2118e-02,  4.8383e-02,  ...,  8.3546e-02,  3.1596e-03, -9.3150e-02],
           [ 1.3853e-02, -1.7523e-02,  1.0431e-01,  ...,  1.8272e-02, -1.0487e-01,  2.8880e-02],
           ...,
           [-6.2014e-02,  4.3244e-02,  1.7157e-01,  ...,  1.2507e-02,  1.1046e-01,  5.2350e-02],
           [-2.4355e-02, -3.0559e-02,  2.9041e-02,  ..., -6.5836e-02,  6.2233e-02,  1.8482e-01],
           [-7.9071e-02, -3.4261e-02,  3.5108e-02,  ..., -6.1720e-02, -1.0296e-01,  5.9495e-02]],

          [[-8.6372e-02, -3.5795e-02, -6.5172e-02,  ..., -2.6198e-01,  1.8766e-02,  7.5859e-02],
           [ 9.9940e-02,  4.0150e-02, -9.8056e-02,  ..., -1.7158e-01, -3.4062e-03, -2.6638e-02],
           [-5.5468e-02, -7.0994e-04, -7.5006e-02,  ..., -1.0616e-01,  1.7372e-02, -4.8893e-02],
           ...,
           [-1.4181e-01,  1.1548e-01,  5.5874e-02,  ...,  2.9942e-02, -1.8112e-02, -2.9444e-02],
           [ 3.9031e-02,  5.0681e-02,  5.8297e-02,  ...,  1.5017e-02,  7.6318e-02, -1.7010e-02],
           [ 1.0667e-02,  6.1897e-02, -8.4168e-02,  ..., -2.1743e-02, -4.0991e-02,  2.1014e-02]],

          [[ 1.6978e-02, -4.5087e-02,  8.1483e-02,  ..., -3.4949e-02, -7.6166e-02,  1.2526e-01],
           [ 1.1711e-01, -2.5587e-02, -3.6007e-02,  ..., -1.7148e-01,  1.1170e-02,  4.9115e-02],
           [-2.6843e-02,  4.2615e-02,  4.1452e-02,  ...,  1.1094e-01,  7.9744e-03, -9.2707e-02],
           ...,
           [ 2.5185e-02,  2.3065e-02, -1.1378e-01,  ..., -1.3932e-01, -6.4133e-02,  1.0574e-02],
           [-3.0315e-02,  3.8837e-02,  1.6509e-01,  ...,  1.2289e-01,  5.9516e-02,  4.5134e-02],
           [-2.8911e-02,  5.5351e-03,  1.0212e-01,  ..., -2.4960e-02, -2.7614e-02, -5.9342e-02]]],


         [[[-8.5187e-02,  6.7951e-02,  5.3837e-02,  ..., -7.2646e-02,  2.5646e-02,  6.2759e-02],
           [ 6.9788e-02, -8.1055e-02,  4.5650e-02,  ...,  1.2715e-01,  1.4196e-02, -1.5498e-01],
           [ 1.0499e-01, -1.5657e-02,  1.3404e-02,  ..., -7.7004e-02, -1.4072e-01, -1.9966e-03],
           ...,
           [-9.7883e-02,  5.0957e-02,  1.0122e-01,  ...,  1.4629e-01, -1.1889e-01, -3.2397e-02],
           [ 4.3204e-02,  1.5724e-01, -5.0146e-05,  ..., -4.0239e-02,  2.0330e-01, -1.8656e-02],
           [ 4.0169e-02, -3.9663e-02,  3.0913e-02,  ..., -1.2377e-01,  7.9213e-02, -5.5125e-03]],

          [[ 8.9421e-02,  8.8845e-02, -6.1184e-02,  ..., -2.7354e-02,  8.8321e-02,  1.2828e-01],
           [ 1.3511e-02, -5.0528e-02, -1.4633e-01,  ...,  3.6064e-02,  4.0775e-02,  1.3117e-01],
           [ 1.8151e-02, -1.1443e-01, -1.5861e-01,  ..., -2.4579e-02,  2.5034e-02, -2.1739e-02],
           ...,
           [ 1.2346e-02,  1.2971e-01,  3.8458e-02,  ...,  2.3587e-01,  1.1556e-01,  9.2041e-03],
           [ 1.0450e-01, -1.1313e-02, -8.5782e-02,  ...,  1.4086e-02, -3.6369e-02, -7.4709e-03],
           [ 8.8566e-02, -2.7712e-02, -3.7588e-02,  ...,  1.5820e-01,  7.1380e-04,  5.2313e-02]],

          [[-1.9333e-01, -2.3790e-02, -1.8478e-01,  ..., -6.0476e-02,  6.6580e-03,  1.7396e-01],
           [ 3.8446e-02, -6.8585e-02,  1.8205e-01,  ...,  2.9872e-02,  5.5316e-02,  1.1129e-01],
           [-8.4996e-03, -6.8807e-02, -1.2926e-01,  ..., -2.8666e-03,  1.0701e-01, -5.4722e-02],
           ...,
           [-1.0787e-01, -5.1398e-03,  2.2752e-01,  ..., -1.9846e-02, -1.0341e-01,  4.1114e-03],
           [ 1.4117e-02, -9.9711e-02,  1.1045e-01,  ...,  4.3007e-02,  3.6649e-02,  3.7427e-02],
           [-3.7699e-02,  1.3266e-01, -4.6468e-02,  ...,  1.4610e-01, -4.0924e-02, -1.5911e-02]],

          ...,

          [[ 4.4219e-05,  5.7690e-02, -1.0569e-01,  ...,  5.8110e-02,  1.5870e-02, -1.2692e-02],
           [-7.5560e-02, -8.2880e-02,  1.1041e-01,  ..., -1.1110e-01,  1.7158e-01,  9.2602e-02],
           [-1.9235e-01,  1.1049e-01,  6.9263e-02,  ...,  1.8956e-03,  1.0422e-01,  3.7345e-02],
           ...,
           [ 5.7292e-02,  1.2155e-01, -4.4879e-04,  ..., -3.2159e-03, -5.7022e-02,  7.1428e-02],
           [-6.5536e-02,  5.2000e-02, -1.1614e-01,  ..., -2.4017e-02,  5.4362e-02,  1.5543e-01],
           [-2.8220e-02,  1.3843e-01, -5.3662e-02,  ...,  4.1439e-03,  6.4188e-02, -7.5669e-03]],

          [[-7.5944e-02,  1.0851e-01,  1.7336e-01,  ...,  3.9967e-02,  3.1034e-02, -5.2872e-02],
           [-1.5726e-01,  1.6285e-02, -1.5322e-01,  ...,  5.0231e-02,  3.7195e-02, -1.2289e-01],
           [-1.0959e-01,  2.2796e-02,  7.4699e-02,  ...,  3.6581e-02, -9.6859e-02,  1.4374e-02],
           ...,
           [ 1.9728e-02, -5.2994e-02, -1.0111e-01,  ..., -1.1787e-01,  1.2975e-01, -6.5378e-03],
           [-8.6925e-02,  1.3311e-01,  1.5001e-01,  ..., -1.0849e-01,  4.2130e-02, -2.9890e-02],
           [-1.5608e-01,  8.6754e-02,  8.5887e-02,  ...,  5.5642e-02, -5.1362e-02, -1.1484e-02]],

          [[ 6.3340e-02, -1.9570e-01,  8.8426e-02,  ..., -2.2677e-03, -1.9932e-01,  1.7536e-01],
           [-6.9694e-02,  8.6373e-03,  4.4566e-02,  ..., -3.2263e-02, -1.0891e-02, -1.2731e-01],
           [-8.7922e-02, -4.2518e-02, -2.2084e-01,  ..., -1.3666e-01, -6.3402e-02,  2.5289e-02],
           ...,
           [-5.9229e-02,  7.5715e-02,  5.6888e-02,  ..., -7.9450e-02,  1.5765e-02, -5.9968e-02],
           [-1.9089e-02, -3.3602e-02, -7.5792e-02,  ...,  9.1326e-02, -9.2602e-02, -2.0123e-02],
           [ 1.0320e-01, -1.0157e-01, -6.0096e-02,  ..., -4.9012e-02, -3.9522e-02,  1.3015e-01]]],


         [[[ 1.8505e-01, -8.2891e-02,  1.3220e-03,  ..., -3.2965e-02, -2.4220e-01, -7.9849e-02],
           [ 2.3010e-02,  6.0594e-02,  1.1671e-01,  ...,  3.7633e-02,  1.5009e-01,  2.5681e-02],
           [-4.7493e-02,  1.6378e-02, -3.7301e-02,  ..., -2.5668e-01, -1.1043e-01,  4.3796e-02],
           ...,
           [ 2.2817e-02,  1.8035e-02,  1.5693e-01,  ...,  1.3603e-02,  8.0895e-02, -7.6434e-02],
           [ 5.2587e-02, -1.0572e-01, -6.5152e-02,  ..., -7.5184e-02,  5.9290e-02, -2.9095e-01],
           [-2.1194e-02, -1.0480e-01, -6.3359e-03,  ...,  2.7155e-02,  7.5363e-02, -7.5690e-02]],

          [[ 4.3849e-02,  1.6290e-02, -1.5036e-01,  ..., -1.2273e-01, -5.0591e-02,  8.0130e-02],
           [ 1.2681e-01, -5.0936e-02,  6.2976e-02,  ...,  6.9390e-02,  1.4028e-01, -7.7345e-02],
           [-6.7526e-02, -4.6386e-03,  9.5082e-03,  ...,  1.2703e-02,  1.1117e-01, -1.1803e-01],
           ...,
           [-1.0656e-03,  4.1111e-02, -1.3555e-02,  ...,  3.6089e-02, -3.2487e-02, -9.0938e-02],
           [-6.9036e-02, -2.9739e-01,  1.0823e-01,  ...,  4.1216e-02,  6.6867e-02,  7.2952e-02],
           [ 8.7275e-02, -8.4263e-02, -2.7715e-02,  ..., -7.0971e-02, -1.4418e-02, -4.1497e-02]],

          [[-5.0202e-03,  1.4765e-01,  5.0654e-02,  ...,  8.7406e-02, -1.2845e-01,  9.6455e-02],
           [ 4.9551e-02,  1.7674e-01, -4.2344e-02,  ...,  3.6934e-02,  1.0916e-01,  4.0092e-02],
           [ 3.7374e-02, -3.9215e-02, -9.7904e-02,  ..., -3.0723e-02, -1.6826e-01, -3.5522e-02],
           ...,
           [ 1.1615e-01,  9.8449e-02, -1.2563e-01,  ..., -1.8250e-01, -1.0976e-02,  5.1443e-02],
           [-3.4132e-02,  2.6991e-02, -3.7619e-02,  ...,  4.1965e-02,  2.0204e-01,  1.9771e-02],
           [-1.3750e-01,  1.4892e-02,  3.0636e-02,  ..., -3.7520e-02,  1.2781e-02, -4.2839e-02]],

          ...,

          [[ 2.7084e-02, -5.8116e-02,  4.8876e-02,  ..., -5.9634e-02,  9.5346e-02, -3.9631e-02],
           [ 3.7214e-02, -1.3125e-01, -9.1848e-02,  ..., -1.2619e-01, -1.5081e-02,  5.6378e-02],
           [-4.3672e-02,  1.2595e-01,  1.4620e-01,  ..., -1.1562e-01, -3.3295e-02,  1.2469e-01],
           ...,
           [ 1.2960e-01,  2.9321e-02, -1.1934e-01,  ...,  7.5403e-02,  1.8795e-01, -9.0119e-03],
           [-1.0921e-01, -1.3320e-01,  4.9889e-02,  ...,  1.6277e-01,  9.5479e-02,  1.1609e-01],
           [ 1.2524e-01, -1.8010e-01,  9.3777e-02,  ...,  3.3745e-02,  1.6840e-03, -6.7612e-02]],

          [[-5.5915e-02,  8.3644e-02, -9.7635e-02,  ...,  9.4437e-03, -1.9931e-01, -1.7101e-02],
           [-8.2127e-02,  3.7943e-02, -1.0292e-01,  ...,  7.5163e-03,  1.3568e-01,  1.4762e-02],
           [-4.9569e-02,  2.3263e-01, -6.4542e-02,  ..., -4.3722e-02, -3.8308e-02, -1.1405e-02],
           ...,
           [ 5.8215e-02, -1.3299e-01,  6.6755e-02,  ...,  7.8601e-02, -3.9378e-02,  5.8071e-02],
           [-8.1914e-02, -5.1991e-02,  8.4073e-02,  ...,  3.1983e-02,  2.1963e-01,  8.4083e-02],
           [-5.6702e-02, -3.9051e-02, -1.0778e-01,  ..., -3.5285e-03,  4.0992e-02,  8.3801e-02]],

          [[-4.5608e-02, -5.1491e-02,  5.7220e-03,  ..., -7.2507e-02,  1.6909e-01, -2.2204e-03],
           [-6.2245e-02, -1.4661e-01,  1.7564e-01,  ..., -1.0382e-01,  1.7797e-01, -6.6737e-02],
           [-9.8443e-02, -2.1764e-02,  1.9685e-02,  ..., -9.1763e-02,  2.3473e-01,  4.4529e-02],
           ...,
           [ 6.7547e-02, -2.0348e-02, -3.8798e-03,  ...,  7.2748e-02, -2.6449e-02, -5.8717e-03],
           [-8.4356e-03,  8.4878e-02, -5.7364e-02,  ...,  7.6244e-02,  4.3453e-02,  1.1527e-01],
           [ 3.5135e-02, -9.2681e-02,  1.9577e-02,  ..., -1.4285e-01,  7.0325e-02, -8.5585e-02]]]]])

2025-07-09 00:28:31.599221 GPU 7 67624 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 699051, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 699051, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:28:54.846824 GPU 4 66790 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 699051, 32],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 699051, 32],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:29:03.841081 GPU 5 66545 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 699051, 32],"float32"), kernel_size=2, stride=None, padding="SAME", ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[accuracy error] paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 699051, 32],"float32"), kernel_size=2, stride=None, padding="SAME", ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1388 / 536871936 (0.0%)
Greatest absolute difference: 0.2070731818675995 at index (1, 2, 14, 349525, 10) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 0, 349525, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 3, 16, 349526, 16]), dtype=torch.float32)
tensor([[[[[ 1.7678e-02, -1.1869e-01,  1.7209e-03,  ...,  2.0948e-02, -1.2922e-01, -1.9499e-02],
           [ 1.8190e-01,  1.5827e-01, -1.5013e-01,  ..., -1.1092e-01,  1.4093e-02, -8.5713e-02],
           [ 1.4221e-01, -4.0686e-02,  4.5202e-02,  ..., -1.4705e-02, -1.4522e-01,  2.2603e-02],
           ...,
           [ 1.1512e-01,  3.1642e-02, -4.1150e-02,  ...,  3.2517e-02, -8.2103e-02,  3.2869e-02],
           [ 6.6597e-03, -2.8933e-03, -7.2864e-02,  ..., -2.8429e-02,  1.0583e-01,  3.0074e-02],
           [ 4.6593e-02,  5.6080e-02,  2.0530e-01,  ..., -8.8963e-02, -8.6283e-02, -3.0811e-03]],

          [[-2.1772e-02,  4.5365e-02, -4.0567e-02,  ...,  4.1850e-02, -2.6265e-02, -1.0521e-01],
           [-1.0434e-02,  7.9878e-02, -7.1205e-03,  ...,  9.8341e-02,  5.5836e-02,  9.7038e-02],
           [ 1.1001e-01, -7.6013e-02, -7.1061e-02,  ..., -5.5704e-02, -4.0311e-02, -1.7760e-01],
           ...,
           [-5.0246e-02,  7.4168e-03,  2.3886e-02,  ...,  1.3964e-01, -4.8986e-02, -9.2333e-02],
           [ 1.4277e-01, -1.6291e-01,  7.2377e-02,  ..., -3.7339e-02,  8.6228e-02, -6.8943e-02],
           [-1.0970e-01, -7.1222e-02, -1.4909e-01,  ...,  1.2340e-01,  1.1476e-01, -1.2698e-01]],

          [[-1.3195e-01,  6.7783e-02, -1.2484e-01,  ..., -5.3518e-02,  4.9771e-02,  1.9543e-01],
           [ 1.4253e-02, -1.3843e-02,  1.7344e-01,  ...,  2.0529e-02,  5.8550e-02, -1.6507e-01],
           [ 6.7188e-02,  9.7987e-02,  1.9611e-01,  ...,  9.1967e-02,  2.0288e-01,  5.9768e-02],
           ...,
           [-8.6912e-02,  5.9631e-02,  4.3593e-02,  ...,  2.0400e-01, -9.3281e-06, -2.7839e-02],
           [ 1.6573e-01, -6.0532e-02,  1.2071e-02,  ...,  1.6880e-01,  1.8976e-02, -1.8158e-02],
           [ 1.8896e-02,  1.0078e-01, -2.3211e-01,  ...,  1.5544e-01,  8.4602e-02,  1.4910e-01]],

          ...,

          [[-2.7638e-01, -1.0444e-01, -3.3593e-02,  ...,  9.3273e-02,  5.3615e-02,  6.1194e-02],
           [-4.0035e-02,  3.1824e-03, -4.1431e-02,  ..., -1.5374e-01, -3.9844e-03,  1.2538e-01],
           [ 5.8797e-02, -1.9520e-02, -2.0996e-02,  ...,  1.7589e-02, -8.2646e-03, -8.3580e-02],
           ...,
           [ 1.1046e-01, -1.2869e-01,  5.9641e-02,  ...,  6.0303e-02, -3.3408e-02,  1.1570e-01],
           [ 1.4504e-01, -6.1918e-02,  8.2354e-02,  ...,  4.4248e-02, -3.5073e-02,  9.0225e-03],
           [ 1.0521e-01,  1.6851e-02,  3.9045e-02,  ..., -4.4138e-02,  6.6125e-02, -1.1616e-01]],

          [[-7.6248e-02,  5.0611e-04,  2.3682e-02,  ..., -6.5641e-02,  4.4012e-02, -1.8814e-02],
           [ 1.4566e-02, -8.1274e-02,  2.3463e-02,  ...,  6.7338e-02,  8.5098e-02,  6.0308e-03],
           [-1.5350e-02,  1.1780e-01,  1.3809e-01,  ..., -4.7296e-02, -2.0346e-02, -9.9974e-02],
           ...,
           [ 1.1688e-01, -5.8684e-02, -1.2735e-01,  ..., -5.0013e-02, -3.3897e-02,  8.7869e-02],
           [ 2.1602e-02,  7.2426e-03,  1.5684e-02,  ...,  8.2968e-02, -6.5913e-02, -2.8172e-02],
           [-5.3919e-02, -1.5254e-01, -2.9891e-02,  ...,  1.2791e-02,  8.2589e-02, -8.0033e-02]],

          [[-5.8085e-03, -1.6631e-01,  1.3059e-01,  ..., -6.6204e-02,  1.1470e-01,  1.2214e-01],
           [-2.3676e-02, -3.2635e-02, -5.0514e-02,  ...,  5.5042e-02, -5.0817e-02, -9.0572e-02],
           [ 4.0355e-02, -4.0910e-03, -2.0745e-01,  ..., -1.6287e-02,  1.4495e-03, -5.1589e-02],
           ...,
           [-1.4239e-01,  8.7322e-02, -1.0951e-02,  ...,  6.9901e-02, -6.7363e-02, -2.1616e-02],
           [ 1.0679e-01,  5.0165e-03, -1.1394e-01,  ...,  1.9622e-02,  1.2350e-01, -8.8123e-02],
           [-1.1171e-01, -1.9981e-01,  1.4010e-01,  ..., -4.7713e-02, -8.5166e-02, -1.4303e-02]]],


         [[[-3.0793e-02, -8.1815e-02,  3.9053e-02,  ..., -4.5166e-02, -1.0564e-01, -5.9908e-02],
           [-2.7364e-01,  1.5409e-02, -3.5829e-02,  ..., -3.4169e-02, -2.5662e-02,  1.3411e-01],
           [ 1.5539e-01, -1.8562e-02,  1.2002e-01,  ..., -1.1170e-02,  8.1806e-02,  1.0001e-02],
           ...,
           [ 9.9996e-02, -1.6132e-01, -3.3999e-03,  ...,  8.3698e-02,  6.8587e-02,  1.0060e-01],
           [-1.2652e-01,  9.8358e-02, -4.3663e-02,  ..., -5.9073e-02, -9.3428e-02, -2.3360e-02],
           [-3.4400e-02,  1.3626e-01, -1.2271e-01,  ..., -4.0671e-01, -5.4014e-02,  2.6849e-02]],

          [[ 3.1517e-02, -1.6612e-03, -6.7514e-03,  ...,  5.2971e-02,  2.5678e-02,  8.4137e-02],
           [-6.0259e-02,  6.4350e-02,  1.1563e-01,  ...,  2.2695e-02, -4.2524e-02, -2.2274e-01],
           [-1.3682e-01, -1.5328e-01,  1.0404e-02,  ...,  1.3784e-01, -9.4115e-02, -1.5754e-01],
           ...,
           [-4.7877e-02, -8.8103e-04, -1.2770e-01,  ...,  5.5901e-02,  1.4171e-02,  6.2368e-02],
           [-6.6318e-02,  9.2185e-02, -6.0715e-02,  ..., -5.6738e-02, -6.4205e-02,  5.6472e-02],
           [ 3.4586e-01, -5.7278e-02,  1.3854e-01,  ...,  1.7907e-01, -1.6093e-01, -1.2585e-01]],

          [[-1.3528e-01, -1.0640e-01,  8.3570e-02,  ...,  4.0866e-02,  1.4548e-01, -1.0115e-01],
           [-2.7363e-02,  9.4625e-02,  7.6939e-02,  ..., -1.5638e-01,  5.8590e-02,  2.0230e-02],
           [ 9.1995e-02, -8.6190e-02, -2.0597e-02,  ...,  8.9695e-02,  1.4686e-01,  2.9740e-01],
           ...,
           [-2.7519e-02,  1.9322e-01,  1.2674e-01,  ...,  1.5293e-01, -2.3302e-01,  8.2237e-03],
           [-1.0276e-01,  1.3271e-01,  1.0624e-01,  ...,  1.6120e-01,  1.7135e-01, -7.7796e-02],
           [-1.9331e-01, -3.9915e-02,  1.6595e-01,  ..., -3.6142e-01, -1.9179e-01,  6.7075e-02]],

          ...,

          [[ 3.7141e-02, -1.6830e-01, -2.6803e-02,  ...,  3.6332e-02,  2.9816e-02, -8.8611e-02],
           [-1.1456e-02, -8.0778e-02, -8.7805e-02,  ..., -4.4957e-02, -1.3485e-02,  7.7581e-02],
           [ 7.4461e-02, -7.9231e-05,  2.2535e-01,  ..., -1.9999e-02,  9.1316e-02, -8.3312e-02],
           ...,
           [-3.4507e-02,  4.6922e-02, -7.4580e-02,  ..., -9.9098e-02, -2.9617e-02, -4.1278e-02],
           [ 4.8504e-02, -1.3509e-02,  6.7890e-02,  ...,  8.5848e-02,  2.1691e-02,  1.5425e-01],
           [ 1.6100e-01,  3.7087e-01,  1.8504e-01,  ...,  7.1564e-02, -4.7979e-02, -3.6518e-01]],

          [[-4.6121e-02,  4.5275e-02, -1.1703e-01,  ..., -9.3450e-02,  7.3174e-02, -2.2679e-03],
           [-4.2467e-02,  1.0286e-01, -6.9883e-02,  ...,  7.1122e-03, -1.1983e-01,  6.6457e-02],
           [-1.6718e-01,  5.5855e-02,  1.9853e-01,  ...,  9.1035e-02,  4.4439e-02, -1.2860e-02],
           ...,
           [ 3.3807e-02,  9.1547e-02, -1.7815e-01,  ...,  5.8873e-03,  1.2014e-01,  1.0265e-01],
           [ 9.7783e-02, -3.2251e-02,  1.3523e-01,  ...,  1.4967e-01,  3.1653e-02,  1.8893e-02],
           [ 1.2284e-01,  2.7478e-01,  1.6925e-01,  ...,  1.4014e-02,  3.8529e-02,  8.1932e-02]],

          [[-2.2553e-03, -6.8780e-02, -1.2393e-01,  ..., -6.5132e-03, -1.0480e-02,  1.3866e-01],
           [-1.4786e-01, -9.7939e-02, -2.0115e-02,  ...,  1.4280e-01, -6.0578e-02, -3.6881e-02],
           [-5.4864e-02,  7.8702e-03, -4.0934e-02,  ..., -1.1767e-01,  2.1145e-03, -1.7000e-01],
           ...,
           [-7.7721e-02, -5.0249e-02, -6.0123e-02,  ..., -8.4866e-02, -7.6625e-02, -3.9783e-02],
           [-1.9231e-02,  3.1416e-01, -9.0982e-02,  ...,  2.1785e-02, -1.8539e-02,  1.2924e-01],
           [-2.6320e-03, -2.1724e-01,  1.1475e-02,  ...,  2.5385e-02,  8.2390e-02,  2.5705e-01]]],


         [[[ 3.4323e-02,  7.3804e-02,  9.3262e-02,  ...,  6.1913e-02, -8.7134e-02, -6.3999e-02],
           [ 4.4304e-02,  2.9380e-02,  7.6865e-03,  ...,  1.3464e-01, -2.0552e-02, -1.3955e-01],
           [-1.2601e-01, -1.3200e-01,  1.5847e-01,  ..., -8.7092e-02,  1.3441e-01,  2.7138e-02],
           ...,
           [ 7.1918e-02,  1.5671e-01,  2.7945e-01,  ..., -9.8597e-02,  1.8564e-02,  2.7031e-02],
           [-2.1564e-01, -1.2122e-01,  6.0555e-02,  ..., -1.2828e-01,  8.3378e-03, -2.7371e-02],
           [ 2.7472e-01, -7.1386e-02,  1.2780e-01,  ...,  1.2222e-01,  2.3115e-02,  2.1558e-01]],

          [[ 8.3758e-02,  3.4569e-02, -9.3018e-04,  ..., -1.7446e-01, -1.4403e-02, -8.7807e-02],
           [ 4.3753e-03, -7.1454e-02, -1.3344e-02,  ..., -1.1835e-01,  9.1923e-02, -1.3934e-02],
           [-1.0413e-01, -2.8933e-02,  1.9115e-02,  ..., -3.5910e-02, -6.3158e-02, -1.2236e-01],
           ...,
           [-1.8997e-01, -4.0607e-02,  3.1502e-02,  ..., -2.6670e-02, -6.4269e-02,  1.2755e-01],
           [ 1.7951e-01, -7.7850e-02,  7.1563e-02,  ...,  1.0359e-01, -1.3505e-01, -4.8131e-03],
           [ 1.9696e-02,  1.6824e-01,  7.6495e-02,  ...,  6.1638e-02,  1.6424e-01, -1.7025e-01]],

          [[ 5.2005e-02, -8.7354e-03, -2.0907e-01,  ..., -8.8093e-02, -7.3874e-02, -8.3908e-02],
           [ 7.4007e-02, -7.8830e-03,  1.0050e-01,  ...,  7.8965e-02,  1.7388e-01, -1.8402e-01],
           [-6.8659e-02,  1.1121e-01,  9.9473e-02,  ..., -1.7195e-01, -6.6126e-02, -5.3557e-02],
           ...,
           [ 6.7717e-02, -4.2867e-02, -6.9874e-02,  ..., -1.2003e-01, -2.2582e-04,  1.9686e-01],
           [ 2.0602e-01,  4.7224e-02, -7.7956e-02,  ..., -9.6627e-02, -8.7196e-02, -7.6171e-02],
           [ 7.3078e-02,  1.6399e-02,  1.5834e-01,  ...,  2.1742e-01, -1.4777e-01,  1.7248e-01]],

          ...,

          [[-6.8370e-02,  9.7779e-02,  3.2084e-02,  ...,  1.3662e-01, -1.2817e-01, -6.6525e-02],
           [-1.0421e-01,  4.9575e-02,  8.2023e-02,  ..., -1.2099e-01,  4.9644e-02, -1.6958e-02],
           [-5.7894e-02, -1.1338e-01,  5.7951e-02,  ...,  6.9661e-02,  1.0345e-01, -2.4140e-01],
           ...,
           [-1.6245e-01, -2.1996e-02, -1.1410e-01,  ...,  1.7381e-01, -7.4888e-02,  8.0187e-02],
           [ 4.6791e-02, -1.0249e-01,  1.7948e-01,  ...,  3.4041e-02, -7.1259e-02,  1.3903e-01],
           [-1.7463e-01,  9.7772e-03, -1.2103e-01,  ...,  7.9517e-02,  1.0918e-01, -4.8893e-02]],

          [[ 8.6413e-02,  1.0992e-01,  4.2992e-02,  ..., -1.3379e-01, -1.2311e-02,  2.7374e-01],
           [-6.1332e-02,  4.0560e-02, -7.4819e-02,  ...,  3.8633e-02,  1.0480e-02, -3.8337e-02],
           [-2.7061e-02, -1.5954e-02, -1.5008e-01,  ...,  5.9089e-02, -2.4241e-01, -2.7010e-03],
           ...,
           [ 7.1174e-02, -8.2015e-02, -1.9426e-01,  ..., -8.8654e-02,  8.8175e-02, -6.0884e-02],
           [-2.3830e-02,  2.1831e-01,  2.7088e-02,  ...,  1.2368e-01, -5.9341e-02, -2.9717e-02],
           [ 1.9701e-01, -8.9410e-02,  2.1935e-01,  ..., -4.3288e-02,  7.5494e-02, -2.1325e-01]],

          [[-1.0907e-01,  1.6808e-02,  4.6153e-02,  ...,  1.6254e-02, -2.1828e-02,  4.0576e-02],
           [ 1.6915e-01, -7.0257e-03,  8.3887e-03,  ..., -6.8072e-02, -3.8760e-02, -7.9600e-02],
           [-1.1647e-01, -5.5208e-02, -1.8755e-01,  ..., -1.4966e-03, -2.5315e-01, -9.0283e-02],
           ...,
           [ 9.2383e-02, -1.7990e-01, -1.0841e-02,  ..., -2.1225e-03,  5.5542e-02,  1.7308e-03],
           [-6.1809e-02, -1.4336e-01, -1.6371e-01,  ...,  1.2761e-01,  7.1046e-02,  1.1383e-01],
           [ 2.3332e-01,  2.5573e-01,  7.8653e-02,  ..., -2.3020e-01, -1.5522e-02,  1.7396e-01]]]],



        [[[[-1.3327e-02, -7.5995e-03, -1.1064e-01,  ...,  1.4703e-01, -1.0544e-01, -8.8224e-02],
           [-9.7992e-02, -9.8790e-02,  7.6756e-02,  ...,  7.7151e-02,  2.5536e-02,  7.9893e-02],
           [ 7.8480e-03,  1.9318e-01,  1.8886e-01,  ..., -6.6406e-02, -8.8224e-02,  7.0729e-02],
           ...,
           [ 1.0387e-01,  6.4673e-02, -1.1585e-01,  ...,  2.1255e-01,  1.3834e-01,  1.0606e-01],
           [ 1.2981e-01,  1.6105e-01, -3.6469e-02,  ...,  2.3592e-02,  7.2730e-02, -1.4212e-01],
           [ 2.4307e-02, -2.0563e-01, -5.4778e-02,  ...,  5.3453e-03, -2.6747e-01,  1.4365e-01]],

          [[-5.3494e-03,  5.7382e-02,  2.8021e-02,  ...,  8.6948e-02, -1.7400e-01,  5.0493e-02],
           [-7.1652e-02,  5.9668e-02,  3.0906e-02,  ..., -2.0324e-02, -1.5187e-02, -3.3846e-02],
           [ 1.1716e-01, -1.3411e-01,  1.2435e-01,  ...,  6.3969e-02,  2.3362e-02,  5.0942e-02],
           ...,
           [-6.2221e-02, -3.4528e-02, -1.6936e-02,  ...,  1.2658e-01, -8.3257e-02,  6.9013e-02],
           [-1.2824e-01, -9.1228e-02,  1.0784e-01,  ..., -4.3215e-04,  6.8006e-02,  1.0720e-01],
           [-7.4594e-02, -9.7687e-02,  8.5086e-03,  ..., -3.7773e-02, -1.8176e-01,  9.1329e-02]],

          [[-7.1148e-02,  1.2656e-01, -1.2811e-01,  ..., -1.6136e-03,  1.4728e-02,  3.9614e-02],
           [ 2.8608e-01, -1.7838e-01,  1.4163e-02,  ..., -8.6596e-02, -1.2421e-01, -2.4138e-01],
           [ 8.2303e-02, -1.8556e-01, -1.3375e-01,  ...,  3.7466e-02,  6.7215e-02,  6.2322e-02],
           ...,
           [ 2.0063e-02,  3.2728e-02,  1.0073e-01,  ..., -1.4855e-02, -1.7615e-01, -1.7525e-03],
           [-3.6733e-02,  2.0089e-01,  7.0277e-02,  ...,  3.5889e-02,  2.0179e-01, -2.9671e-02],
           [ 1.0277e-01,  2.4010e-01, -2.9245e-02,  ..., -2.3410e-01, -2.7102e-01,  2.0295e-02]],

          ...,

          [[ 1.3289e-01, -1.0309e-01, -3.1759e-02,  ...,  1.7048e-01, -1.8964e-02, -1.2934e-01],
           [ 8.3651e-04, -1.2118e-02,  4.8383e-02,  ...,  8.3546e-02,  3.1596e-03, -9.3150e-02],
           [ 1.3853e-02, -1.7523e-02,  1.0431e-01,  ...,  1.8272e-02, -1.0487e-01,  2.8880e-02],
           ...,
           [-6.2014e-02,  4.3244e-02,  1.7157e-01,  ...,  1.2507e-02,  1.1046e-01,  5.2350e-02],
           [-2.4355e-02, -3.0559e-02,  2.9041e-02,  ..., -6.5836e-02,  6.2233e-02,  1.8482e-01],
           [-1.5814e-01, -6.8522e-02,  7.0216e-02,  ..., -1.2344e-01, -2.0592e-01,  1.1899e-01]],

          [[-8.6372e-02, -3.5795e-02, -6.5172e-02,  ..., -2.6198e-01,  1.8766e-02,  7.5859e-02],
           [ 9.9940e-02,  4.0150e-02, -9.8056e-02,  ..., -1.7158e-01, -3.4062e-03, -2.6638e-02],
           [-5.5468e-02, -7.0994e-04, -7.5006e-02,  ..., -1.0616e-01,  1.7372e-02, -4.8893e-02],
           ...,
           [-1.4181e-01,  1.1548e-01,  5.5874e-02,  ...,  2.9942e-02, -1.8112e-02, -2.9444e-02],
           [ 3.9031e-02,  5.0681e-02,  5.8297e-02,  ...,  1.5017e-02,  7.6318e-02, -1.7010e-02],
           [ 2.1333e-02,  1.2379e-01, -1.6834e-01,  ..., -4.3487e-02, -8.1982e-02,  4.2028e-02]],

          [[ 1.6978e-02, -4.5087e-02,  8.1483e-02,  ..., -3.4949e-02, -7.6166e-02,  1.2526e-01],
           [ 1.1711e-01, -2.5587e-02, -3.6007e-02,  ..., -1.7148e-01,  1.1170e-02,  4.9115e-02],
           [-2.6843e-02,  4.2615e-02,  4.1452e-02,  ...,  1.1094e-01,  7.9744e-03, -9.2707e-02],
           ...,
           [ 2.5185e-02,  2.3065e-02, -1.1378e-01,  ..., -1.3932e-01, -6.4133e-02,  1.0574e-02],
           [-3.0315e-02,  3.8837e-02,  1.6509e-01,  ...,  1.2289e-01,  5.9516e-02,  4.5134e-02],
           [-5.7822e-02,  1.1070e-02,  2.0424e-01,  ..., -4.9920e-02, -5.5227e-02, -1.1868e-01]]],


         [[[-8.5187e-02,  6.7951e-02,  5.3837e-02,  ..., -7.2646e-02,  2.5646e-02,  6.2759e-02],
           [ 6.9788e-02, -8.1055e-02,  4.5650e-02,  ...,  1.2715e-01,  1.4196e-02, -1.5498e-01],
           [ 1.0499e-01, -1.5657e-02,  1.3404e-02,  ..., -7.7004e-02, -1.4072e-01, -1.9966e-03],
           ...,
           [-9.7883e-02,  5.0957e-02,  1.0122e-01,  ...,  1.4629e-01, -1.1889e-01, -3.2397e-02],
           [ 4.3204e-02,  1.5724e-01, -5.0146e-05,  ..., -4.0239e-02,  2.0330e-01, -1.8656e-02],
           [ 8.0338e-02, -7.9326e-02,  6.1825e-02,  ..., -2.4754e-01,  1.5843e-01, -1.1025e-02]],

          [[ 8.9421e-02,  8.8845e-02, -6.1184e-02,  ..., -2.7354e-02,  8.8321e-02,  1.2828e-01],
           [ 1.3511e-02, -5.0528e-02, -1.4633e-01,  ...,  3.6064e-02,  4.0775e-02,  1.3117e-01],
           [ 1.8151e-02, -1.1443e-01, -1.5861e-01,  ..., -2.4579e-02,  2.5034e-02, -2.1739e-02],
           ...,
           [ 1.2346e-02,  1.2971e-01,  3.8458e-02,  ...,  2.3587e-01,  1.1556e-01,  9.2041e-03],
           [ 1.0450e-01, -1.1313e-02, -8.5782e-02,  ...,  1.4086e-02, -3.6369e-02, -7.4709e-03],
           [ 1.7713e-01, -5.5425e-02, -7.5177e-02,  ...,  3.1640e-01,  1.4276e-03,  1.0463e-01]],

          [[-1.9333e-01, -2.3790e-02, -1.8478e-01,  ..., -6.0476e-02,  6.6580e-03,  1.7396e-01],
           [ 3.8446e-02, -6.8585e-02,  1.8205e-01,  ...,  2.9872e-02,  5.5316e-02,  1.1129e-01],
           [-8.4996e-03, -6.8807e-02, -1.2926e-01,  ..., -2.8666e-03,  1.0701e-01, -5.4722e-02],
           ...,
           [-1.0787e-01, -5.1398e-03,  2.2752e-01,  ..., -1.9846e-02, -1.0341e-01,  4.1114e-03],
           [ 1.4117e-02, -9.9711e-02,  1.1045e-01,  ...,  4.3007e-02,  3.6649e-02,  3.7427e-02],
           [-7.5399e-02,  2.6531e-01, -9.2936e-02,  ...,  2.9219e-01, -8.1848e-02, -3.1821e-02]],

          ...,

          [[ 4.4219e-05,  5.7690e-02, -1.0569e-01,  ...,  5.8110e-02,  1.5870e-02, -1.2692e-02],
           [-7.5560e-02, -8.2880e-02,  1.1041e-01,  ..., -1.1110e-01,  1.7158e-01,  9.2602e-02],
           [-1.9235e-01,  1.1049e-01,  6.9263e-02,  ...,  1.8956e-03,  1.0422e-01,  3.7345e-02],
           ...,
           [ 5.7292e-02,  1.2155e-01, -4.4879e-04,  ..., -3.2159e-03, -5.7022e-02,  7.1428e-02],
           [-6.5536e-02,  5.2000e-02, -1.1614e-01,  ..., -2.4017e-02,  5.4362e-02,  1.5543e-01],
           [-5.6441e-02,  2.7685e-01, -1.0732e-01,  ...,  8.2878e-03,  1.2838e-01, -1.5134e-02]],

          [[-7.5944e-02,  1.0851e-01,  1.7336e-01,  ...,  3.9967e-02,  3.1034e-02, -5.2872e-02],
           [-1.5726e-01,  1.6285e-02, -1.5322e-01,  ...,  5.0231e-02,  3.7195e-02, -1.2289e-01],
           [-1.0959e-01,  2.2796e-02,  7.4699e-02,  ...,  3.6581e-02, -9.6859e-02,  1.4374e-02],
           ...,
           [ 1.9728e-02, -5.2994e-02, -1.0111e-01,  ..., -1.1787e-01,  1.2975e-01, -6.5378e-03],
           [-8.6925e-02,  1.3311e-01,  1.5001e-01,  ..., -1.0849e-01,  4.2130e-02, -2.9890e-02],
           [-3.1217e-01,  1.7351e-01,  1.7177e-01,  ...,  1.1128e-01, -1.0272e-01, -2.2969e-02]],

          [[ 6.3340e-02, -1.9570e-01,  8.8426e-02,  ..., -2.2677e-03, -1.9932e-01,  1.7536e-01],
           [-6.9694e-02,  8.6373e-03,  4.4566e-02,  ..., -3.2263e-02, -1.0891e-02, -1.2731e-01],
           [-8.7922e-02, -4.2518e-02, -2.2084e-01,  ..., -1.3666e-01, -6.3402e-02,  2.5289e-02],
           ...,
           [-5.9229e-02,  7.5715e-02,  5.6888e-02,  ..., -7.9450e-02,  1.5765e-02, -5.9968e-02],
           [-1.9089e-02, -3.3602e-02, -7.5792e-02,  ...,  9.1326e-02, -9.2602e-02, -2.0123e-02],
           [ 2.0639e-01, -2.0313e-01, -1.2019e-01,  ..., -9.8024e-02, -7.9044e-02,  2.6031e-01]]],


         [[[ 1.8505e-01, -8.2891e-02,  1.3220e-03,  ..., -3.2965e-02, -2.4220e-01, -7.9849e-02],
           [ 2.3010e-02,  6.0594e-02,  1.1671e-01,  ...,  3.7633e-02,  1.5009e-01,  2.5681e-02],
           [-4.7493e-02,  1.6378e-02, -3.7301e-02,  ..., -2.5668e-01, -1.1043e-01,  4.3796e-02],
           ...,
           [ 2.2817e-02,  1.8035e-02,  1.5693e-01,  ...,  1.3603e-02,  8.0895e-02, -7.6434e-02],
           [ 5.2587e-02, -1.0572e-01, -6.5152e-02,  ..., -7.5184e-02,  5.9290e-02, -2.9095e-01],
           [-4.2389e-02, -2.0960e-01, -1.2672e-02,  ...,  5.4311e-02,  1.5073e-01, -1.5138e-01]],

          [[ 4.3849e-02,  1.6290e-02, -1.5036e-01,  ..., -1.2273e-01, -5.0591e-02,  8.0130e-02],
           [ 1.2681e-01, -5.0936e-02,  6.2976e-02,  ...,  6.9390e-02,  1.4028e-01, -7.7345e-02],
           [-6.7526e-02, -4.6386e-03,  9.5082e-03,  ...,  1.2703e-02,  1.1117e-01, -1.1803e-01],
           ...,
           [-1.0656e-03,  4.1111e-02, -1.3555e-02,  ...,  3.6089e-02, -3.2487e-02, -9.0938e-02],
           [-6.9036e-02, -2.9739e-01,  1.0823e-01,  ...,  4.1216e-02,  6.6867e-02,  7.2952e-02],
           [ 1.7455e-01, -1.6853e-01, -5.5430e-02,  ..., -1.4194e-01, -2.8837e-02, -8.2995e-02]],

          [[-5.0202e-03,  1.4765e-01,  5.0654e-02,  ...,  8.7406e-02, -1.2845e-01,  9.6455e-02],
           [ 4.9551e-02,  1.7674e-01, -4.2344e-02,  ...,  3.6934e-02,  1.0916e-01,  4.0092e-02],
           [ 3.7374e-02, -3.9215e-02, -9.7904e-02,  ..., -3.0723e-02, -1.6826e-01, -3.5522e-02],
           ...,
           [ 1.1615e-01,  9.8449e-02, -1.2563e-01,  ..., -1.8250e-01, -1.0976e-02,  5.1443e-02],
           [-3.4132e-02,  2.6991e-02, -3.7619e-02,  ...,  4.1965e-02,  2.0204e-01,  1.9771e-02],
           [-2.7500e-01,  2.9784e-02,  6.1271e-02,  ..., -7.5041e-02,  2.5563e-02, -8.5677e-02]],

          ...,

          [[ 2.7084e-02, -5.8116e-02,  4.8876e-02,  ..., -5.9634e-02,  9.5346e-02, -3.9631e-02],
           [ 3.7214e-02, -1.3125e-01, -9.1848e-02,  ..., -1.2619e-01, -1.5081e-02,  5.6378e-02],
           [-4.3672e-02,  1.2595e-01,  1.4620e-01,  ..., -1.1562e-01, -3.3295e-02,  1.2469e-01],
           ...,
           [ 1.2960e-01,  2.9321e-02, -1.1934e-01,  ...,  7.5403e-02,  1.8795e-01, -9.0119e-03],
           [-1.0921e-01, -1.3320e-01,  4.9889e-02,  ...,  1.6277e-01,  9.5479e-02,  1.1609e-01],
           [ 2.5049e-01, -3.6020e-01,  1.8755e-01,  ...,  6.7490e-02,  3.3680e-03, -1.3522e-01]],

          [[-5.5915e-02,  8.3644e-02, -9.7635e-02,  ...,  9.4437e-03, -1.9931e-01, -1.7101e-02],
           [-8.2127e-02,  3.7943e-02, -1.0292e-01,  ...,  7.5163e-03,  1.3568e-01,  1.4762e-02],
           [-4.9569e-02,  2.3263e-01, -6.4542e-02,  ..., -4.3722e-02, -3.8308e-02, -1.1405e-02],
           ...,
           [ 5.8215e-02, -1.3299e-01,  6.6755e-02,  ...,  7.8601e-02, -3.9378e-02,  5.8071e-02],
           [-8.1914e-02, -5.1991e-02,  8.4073e-02,  ...,  3.1983e-02,  2.1963e-01,  8.4083e-02],
           [-1.1340e-01, -7.8101e-02, -2.1556e-01,  ..., -7.0569e-03,  8.1984e-02,  1.6760e-01]],

          [[-4.5608e-02, -5.1491e-02,  5.7220e-03,  ..., -7.2507e-02,  1.6909e-01, -2.2204e-03],
           [-6.2245e-02, -1.4661e-01,  1.7564e-01,  ..., -1.0382e-01,  1.7797e-01, -6.6737e-02],
           [-9.8443e-02, -2.1764e-02,  1.9685e-02,  ..., -9.1763e-02,  2.3473e-01,  4.4529e-02],
           ...,
           [ 6.7547e-02, -2.0348e-02, -3.8798e-03,  ...,  7.2748e-02, -2.6449e-02, -5.8717e-03],
           [-8.4356e-03,  8.4878e-02, -5.7364e-02,  ...,  7.6244e-02,  4.3453e-02,  1.1527e-01],
           [ 7.0269e-02, -1.8536e-01,  3.9155e-02,  ..., -2.8571e-01,  1.4065e-01, -1.7117e-01]]]]])
DESIRED: (shape=torch.Size([2, 3, 16, 349526, 16]), dtype=torch.float32)
tensor([[[[[ 1.7678e-02, -1.1869e-01,  1.7209e-03,  ...,  2.0948e-02, -1.2922e-01, -1.9499e-02],
           [ 1.8190e-01,  1.5827e-01, -1.5013e-01,  ..., -1.1092e-01,  1.4093e-02, -8.5713e-02],
           [ 1.4221e-01, -4.0686e-02,  4.5202e-02,  ..., -1.4705e-02, -1.4522e-01,  2.2603e-02],
           ...,
           [ 1.1512e-01,  3.1642e-02, -4.1150e-02,  ...,  3.2517e-02, -8.2103e-02,  3.2869e-02],
           [ 6.6597e-03, -2.8933e-03, -7.2864e-02,  ..., -2.8429e-02,  1.0583e-01,  3.0074e-02],
           [ 2.3296e-02,  2.8040e-02,  1.0265e-01,  ..., -4.4482e-02, -4.3142e-02, -1.5405e-03]],

          [[-2.1772e-02,  4.5365e-02, -4.0567e-02,  ...,  4.1850e-02, -2.6265e-02, -1.0521e-01],
           [-1.0434e-02,  7.9878e-02, -7.1205e-03,  ...,  9.8341e-02,  5.5836e-02,  9.7038e-02],
           [ 1.1001e-01, -7.6013e-02, -7.1061e-02,  ..., -5.5704e-02, -4.0311e-02, -1.7760e-01],
           ...,
           [-5.0246e-02,  7.4168e-03,  2.3886e-02,  ...,  1.3964e-01, -4.8986e-02, -9.2333e-02],
           [ 1.4277e-01, -1.6291e-01,  7.2377e-02,  ..., -3.7339e-02,  8.6228e-02, -6.8943e-02],
           [-5.4848e-02, -3.5611e-02, -7.4544e-02,  ...,  6.1699e-02,  5.7380e-02, -6.3490e-02]],

          [[-1.3195e-01,  6.7783e-02, -1.2484e-01,  ..., -5.3518e-02,  4.9771e-02,  1.9543e-01],
           [ 1.4253e-02, -1.3843e-02,  1.7344e-01,  ...,  2.0529e-02,  5.8550e-02, -1.6507e-01],
           [ 6.7188e-02,  9.7987e-02,  1.9611e-01,  ...,  9.1967e-02,  2.0288e-01,  5.9768e-02],
           ...,
           [-8.6912e-02,  5.9631e-02,  4.3593e-02,  ...,  2.0400e-01, -9.3281e-06, -2.7839e-02],
           [ 1.6573e-01, -6.0532e-02,  1.2071e-02,  ...,  1.6880e-01,  1.8976e-02, -1.8158e-02],
           [ 9.4478e-03,  5.0392e-02, -1.1606e-01,  ...,  7.7718e-02,  4.2301e-02,  7.4551e-02]],

          ...,

          [[-2.7638e-01, -1.0444e-01, -3.3593e-02,  ...,  9.3273e-02,  5.3615e-02,  6.1194e-02],
           [-4.0035e-02,  3.1824e-03, -4.1431e-02,  ..., -1.5374e-01, -3.9844e-03,  1.2538e-01],
           [ 5.8797e-02, -1.9520e-02, -2.0996e-02,  ...,  1.7589e-02, -8.2646e-03, -8.3580e-02],
           ...,
           [ 1.1046e-01, -1.2869e-01,  5.9641e-02,  ...,  6.0303e-02, -3.3408e-02,  1.1570e-01],
           [ 1.4504e-01, -6.1918e-02,  8.2354e-02,  ...,  4.4248e-02, -3.5073e-02,  9.0225e-03],
           [ 5.2607e-02,  8.4256e-03,  1.9522e-02,  ..., -2.2069e-02,  3.3062e-02, -5.8082e-02]],

          [[-7.6248e-02,  5.0611e-04,  2.3682e-02,  ..., -6.5641e-02,  4.4012e-02, -1.8814e-02],
           [ 1.4566e-02, -8.1274e-02,  2.3463e-02,  ...,  6.7338e-02,  8.5098e-02,  6.0308e-03],
           [-1.5350e-02,  1.1780e-01,  1.3809e-01,  ..., -4.7296e-02, -2.0346e-02, -9.9974e-02],
           ...,
           [ 1.1688e-01, -5.8684e-02, -1.2735e-01,  ..., -5.0013e-02, -3.3897e-02,  8.7869e-02],
           [ 2.1602e-02,  7.2426e-03,  1.5684e-02,  ...,  8.2968e-02, -6.5913e-02, -2.8172e-02],
           [-2.6959e-02, -7.6272e-02, -1.4946e-02,  ...,  6.3955e-03,  4.1295e-02, -4.0016e-02]],

          [[-5.8085e-03, -1.6631e-01,  1.3059e-01,  ..., -6.6204e-02,  1.1470e-01,  1.2214e-01],
           [-2.3676e-02, -3.2635e-02, -5.0514e-02,  ...,  5.5042e-02, -5.0817e-02, -9.0572e-02],
           [ 4.0355e-02, -4.0910e-03, -2.0745e-01,  ..., -1.6287e-02,  1.4495e-03, -5.1589e-02],
           ...,
           [-1.4239e-01,  8.7322e-02, -1.0951e-02,  ...,  6.9901e-02, -6.7363e-02, -2.1616e-02],
           [ 1.0679e-01,  5.0165e-03, -1.1394e-01,  ...,  1.9622e-02,  1.2350e-01, -8.8123e-02],
           [-5.5857e-02, -9.9907e-02,  7.0052e-02,  ..., -2.3856e-02, -4.2583e-02, -7.1517e-03]]],


         [[[-3.0793e-02, -8.1815e-02,  3.9053e-02,  ..., -4.5166e-02, -1.0564e-01, -5.9908e-02],
           [-2.7364e-01,  1.5409e-02, -3.5829e-02,  ..., -3.4169e-02, -2.5662e-02,  1.3411e-01],
           [ 1.5539e-01, -1.8562e-02,  1.2002e-01,  ..., -1.1170e-02,  8.1806e-02,  1.0001e-02],
           ...,
           [ 9.9996e-02, -1.6132e-01, -3.3999e-03,  ...,  8.3698e-02,  6.8587e-02,  1.0060e-01],
           [-1.2652e-01,  9.8358e-02, -4.3663e-02,  ..., -5.9073e-02, -9.3428e-02, -2.3360e-02],
           [-1.7200e-02,  6.8129e-02, -6.1355e-02,  ..., -2.0335e-01, -2.7007e-02,  1.3424e-02]],

          [[ 3.1517e-02, -1.6612e-03, -6.7514e-03,  ...,  5.2971e-02,  2.5678e-02,  8.4137e-02],
           [-6.0259e-02,  6.4350e-02,  1.1563e-01,  ...,  2.2695e-02, -4.2524e-02, -2.2274e-01],
           [-1.3682e-01, -1.5328e-01,  1.0404e-02,  ...,  1.3784e-01, -9.4115e-02, -1.5754e-01],
           ...,
           [-4.7877e-02, -8.8103e-04, -1.2770e-01,  ...,  5.5901e-02,  1.4171e-02,  6.2368e-02],
           [-6.6318e-02,  9.2185e-02, -6.0715e-02,  ..., -5.6738e-02, -6.4205e-02,  5.6472e-02],
           [ 1.7293e-01, -2.8639e-02,  6.9269e-02,  ...,  8.9536e-02, -8.0467e-02, -6.2924e-02]],

          [[-1.3528e-01, -1.0640e-01,  8.3570e-02,  ...,  4.0866e-02,  1.4548e-01, -1.0115e-01],
           [-2.7363e-02,  9.4625e-02,  7.6939e-02,  ..., -1.5638e-01,  5.8590e-02,  2.0230e-02],
           [ 9.1995e-02, -8.6190e-02, -2.0597e-02,  ...,  8.9695e-02,  1.4686e-01,  2.9740e-01],
           ...,
           [-2.7519e-02,  1.9322e-01,  1.2674e-01,  ...,  1.5293e-01, -2.3302e-01,  8.2237e-03],
           [-1.0276e-01,  1.3271e-01,  1.0624e-01,  ...,  1.6120e-01,  1.7135e-01, -7.7796e-02],
           [-9.6655e-02, -1.9957e-02,  8.2975e-02,  ..., -1.8071e-01, -9.5896e-02,  3.3538e-02]],

          ...,

          [[ 3.7141e-02, -1.6830e-01, -2.6803e-02,  ...,  3.6332e-02,  2.9816e-02, -8.8611e-02],
           [-1.1456e-02, -8.0778e-02, -8.7805e-02,  ..., -4.4957e-02, -1.3485e-02,  7.7581e-02],
           [ 7.4461e-02, -7.9231e-05,  2.2535e-01,  ..., -1.9999e-02,  9.1316e-02, -8.3312e-02],
           ...,
           [-3.4507e-02,  4.6922e-02, -7.4580e-02,  ..., -9.9098e-02, -2.9617e-02, -4.1278e-02],
           [ 4.8504e-02, -1.3509e-02,  6.7890e-02,  ...,  8.5848e-02,  2.1691e-02,  1.5425e-01],
           [ 8.0498e-02,  1.8543e-01,  9.2522e-02,  ...,  3.5782e-02, -2.3990e-02, -1.8259e-01]],

          [[-4.6121e-02,  4.5275e-02, -1.1703e-01,  ..., -9.3450e-02,  7.3174e-02, -2.2679e-03],
           [-4.2467e-02,  1.0286e-01, -6.9883e-02,  ...,  7.1122e-03, -1.1983e-01,  6.6457e-02],
           [-1.6718e-01,  5.5855e-02,  1.9853e-01,  ...,  9.1035e-02,  4.4439e-02, -1.2860e-02],
           ...,
           [ 3.3807e-02,  9.1547e-02, -1.7815e-01,  ...,  5.8873e-03,  1.2014e-01,  1.0265e-01],
           [ 9.7783e-02, -3.2251e-02,  1.3523e-01,  ...,  1.4967e-01,  3.1653e-02,  1.8893e-02],
           [ 6.1422e-02,  1.3739e-01,  8.4625e-02,  ...,  7.0069e-03,  1.9265e-02,  4.0966e-02]],

          [[-2.2553e-03, -6.8780e-02, -1.2393e-01,  ..., -6.5132e-03, -1.0480e-02,  1.3866e-01],
           [-1.4786e-01, -9.7939e-02, -2.0115e-02,  ...,  1.4280e-01, -6.0578e-02, -3.6881e-02],
           [-5.4864e-02,  7.8702e-03, -4.0934e-02,  ..., -1.1767e-01,  2.1145e-03, -1.7000e-01],
           ...,
           [-7.7721e-02, -5.0249e-02, -6.0123e-02,  ..., -8.4866e-02, -7.6625e-02, -3.9783e-02],
           [-1.9231e-02,  3.1416e-01, -9.0982e-02,  ...,  2.1785e-02, -1.8539e-02,  1.2924e-01],
           [-1.3160e-03, -1.0862e-01,  5.7377e-03,  ...,  1.2693e-02,  4.1195e-02,  1.2853e-01]]],


         [[[ 3.4323e-02,  7.3804e-02,  9.3262e-02,  ...,  6.1913e-02, -8.7134e-02, -6.3999e-02],
           [ 4.4304e-02,  2.9380e-02,  7.6865e-03,  ...,  1.3464e-01, -2.0552e-02, -1.3955e-01],
           [-1.2601e-01, -1.3200e-01,  1.5847e-01,  ..., -8.7092e-02,  1.3441e-01,  2.7138e-02],
           ...,
           [ 7.1918e-02,  1.5671e-01,  2.7945e-01,  ..., -9.8597e-02,  1.8564e-02,  2.7031e-02],
           [-2.1564e-01, -1.2122e-01,  6.0555e-02,  ..., -1.2828e-01,  8.3378e-03, -2.7371e-02],
           [ 1.3736e-01, -3.5693e-02,  6.3901e-02,  ...,  6.1109e-02,  1.1558e-02,  1.0779e-01]],

          [[ 8.3758e-02,  3.4569e-02, -9.3018e-04,  ..., -1.7446e-01, -1.4403e-02, -8.7807e-02],
           [ 4.3753e-03, -7.1454e-02, -1.3344e-02,  ..., -1.1835e-01,  9.1923e-02, -1.3934e-02],
           [-1.0413e-01, -2.8933e-02,  1.9115e-02,  ..., -3.5910e-02, -6.3158e-02, -1.2236e-01],
           ...,
           [-1.8997e-01, -4.0607e-02,  3.1502e-02,  ..., -2.6670e-02, -6.4269e-02,  1.2755e-01],
           [ 1.7951e-01, -7.7850e-02,  7.1563e-02,  ...,  1.0359e-01, -1.3505e-01, -4.8131e-03],
           [ 9.8479e-03,  8.4120e-02,  3.8247e-02,  ...,  3.0819e-02,  8.2118e-02, -8.5125e-02]],

          [[ 5.2005e-02, -8.7354e-03, -2.0907e-01,  ..., -8.8093e-02, -7.3874e-02, -8.3908e-02],
           [ 7.4007e-02, -7.8830e-03,  1.0050e-01,  ...,  7.8965e-02,  1.7388e-01, -1.8402e-01],
           [-6.8659e-02,  1.1121e-01,  9.9473e-02,  ..., -1.7195e-01, -6.6126e-02, -5.3557e-02],
           ...,
           [ 6.7717e-02, -4.2867e-02, -6.9874e-02,  ..., -1.2003e-01, -2.2582e-04,  1.9686e-01],
           [ 2.0602e-01,  4.7224e-02, -7.7956e-02,  ..., -9.6627e-02, -8.7196e-02, -7.6171e-02],
           [ 3.6539e-02,  8.1997e-03,  7.9171e-02,  ...,  1.0871e-01, -7.3885e-02,  8.6238e-02]],

          ...,

          [[-6.8370e-02,  9.7779e-02,  3.2084e-02,  ...,  1.3662e-01, -1.2817e-01, -6.6525e-02],
           [-1.0421e-01,  4.9575e-02,  8.2023e-02,  ..., -1.2099e-01,  4.9644e-02, -1.6958e-02],
           [-5.7894e-02, -1.1338e-01,  5.7951e-02,  ...,  6.9661e-02,  1.0345e-01, -2.4140e-01],
           ...,
           [-1.6245e-01, -2.1996e-02, -1.1410e-01,  ...,  1.7381e-01, -7.4888e-02,  8.0187e-02],
           [ 4.6791e-02, -1.0249e-01,  1.7948e-01,  ...,  3.4041e-02, -7.1259e-02,  1.3903e-01],
           [-8.7317e-02,  4.8886e-03, -6.0517e-02,  ...,  3.9758e-02,  5.4588e-02, -2.4446e-02]],

          [[ 8.6413e-02,  1.0992e-01,  4.2992e-02,  ..., -1.3379e-01, -1.2311e-02,  2.7374e-01],
           [-6.1332e-02,  4.0560e-02, -7.4819e-02,  ...,  3.8633e-02,  1.0480e-02, -3.8337e-02],
           [-2.7061e-02, -1.5954e-02, -1.5008e-01,  ...,  5.9089e-02, -2.4241e-01, -2.7010e-03],
           ...,
           [ 7.1174e-02, -8.2015e-02, -1.9426e-01,  ..., -8.8654e-02,  8.8175e-02, -6.0884e-02],
           [-2.3830e-02,  2.1831e-01,  2.7088e-02,  ...,  1.2368e-01, -5.9341e-02, -2.9717e-02],
           [ 9.8506e-02, -4.4705e-02,  1.0968e-01,  ..., -2.1644e-02,  3.7747e-02, -1.0662e-01]],

          [[-1.0907e-01,  1.6808e-02,  4.6153e-02,  ...,  1.6254e-02, -2.1828e-02,  4.0576e-02],
           [ 1.6915e-01, -7.0257e-03,  8.3887e-03,  ..., -6.8072e-02, -3.8760e-02, -7.9600e-02],
           [-1.1647e-01, -5.5208e-02, -1.8755e-01,  ..., -1.4966e-03, -2.5315e-01, -9.0283e-02],
           ...,
           [ 9.2383e-02, -1.7990e-01, -1.0841e-02,  ..., -2.1225e-03,  5.5542e-02,  1.7308e-03],
           [-6.1809e-02, -1.4336e-01, -1.6371e-01,  ...,  1.2761e-01,  7.1046e-02,  1.1383e-01],
           [ 1.1666e-01,  1.2786e-01,  3.9327e-02,  ..., -1.1510e-01, -7.7611e-03,  8.6981e-02]]]],



        [[[[-1.3327e-02, -7.5995e-03, -1.1064e-01,  ...,  1.4703e-01, -1.0544e-01, -8.8224e-02],
           [-9.7992e-02, -9.8790e-02,  7.6756e-02,  ...,  7.7151e-02,  2.5536e-02,  7.9893e-02],
           [ 7.8480e-03,  1.9318e-01,  1.8886e-01,  ..., -6.6406e-02, -8.8224e-02,  7.0729e-02],
           ...,
           [ 1.0387e-01,  6.4673e-02, -1.1585e-01,  ...,  2.1255e-01,  1.3834e-01,  1.0606e-01],
           [ 1.2981e-01,  1.6105e-01, -3.6469e-02,  ...,  2.3592e-02,  7.2730e-02, -1.4212e-01],
           [ 1.2154e-02, -1.0281e-01, -2.7389e-02,  ...,  2.6727e-03, -1.3374e-01,  7.1823e-02]],

          [[-5.3494e-03,  5.7382e-02,  2.8021e-02,  ...,  8.6948e-02, -1.7400e-01,  5.0493e-02],
           [-7.1652e-02,  5.9668e-02,  3.0906e-02,  ..., -2.0324e-02, -1.5187e-02, -3.3846e-02],
           [ 1.1716e-01, -1.3411e-01,  1.2435e-01,  ...,  6.3969e-02,  2.3362e-02,  5.0942e-02],
           ...,
           [-6.2221e-02, -3.4528e-02, -1.6936e-02,  ...,  1.2658e-01, -8.3257e-02,  6.9013e-02],
           [-1.2824e-01, -9.1228e-02,  1.0784e-01,  ..., -4.3215e-04,  6.8006e-02,  1.0720e-01],
           [-3.7297e-02, -4.8843e-02,  4.2543e-03,  ..., -1.8887e-02, -9.0881e-02,  4.5665e-02]],

          [[-7.1148e-02,  1.2656e-01, -1.2811e-01,  ..., -1.6136e-03,  1.4728e-02,  3.9614e-02],
           [ 2.8608e-01, -1.7838e-01,  1.4163e-02,  ..., -8.6596e-02, -1.2421e-01, -2.4138e-01],
           [ 8.2303e-02, -1.8556e-01, -1.3375e-01,  ...,  3.7466e-02,  6.7215e-02,  6.2322e-02],
           ...,
           [ 2.0063e-02,  3.2728e-02,  1.0073e-01,  ..., -1.4855e-02, -1.7615e-01, -1.7525e-03],
           [-3.6733e-02,  2.0089e-01,  7.0277e-02,  ...,  3.5889e-02,  2.0179e-01, -2.9671e-02],
           [ 5.1386e-02,  1.2005e-01, -1.4622e-02,  ..., -1.1705e-01, -1.3551e-01,  1.0148e-02]],

          ...,

          [[ 1.3289e-01, -1.0309e-01, -3.1759e-02,  ...,  1.7048e-01, -1.8964e-02, -1.2934e-01],
           [ 8.3651e-04, -1.2118e-02,  4.8383e-02,  ...,  8.3546e-02,  3.1596e-03, -9.3150e-02],
           [ 1.3853e-02, -1.7523e-02,  1.0431e-01,  ...,  1.8272e-02, -1.0487e-01,  2.8880e-02],
           ...,
           [-6.2014e-02,  4.3244e-02,  1.7157e-01,  ...,  1.2507e-02,  1.1046e-01,  5.2350e-02],
           [-2.4355e-02, -3.0559e-02,  2.9041e-02,  ..., -6.5836e-02,  6.2233e-02,  1.8482e-01],
           [-7.9071e-02, -3.4261e-02,  3.5108e-02,  ..., -6.1720e-02, -1.0296e-01,  5.9495e-02]],

          [[-8.6372e-02, -3.5795e-02, -6.5172e-02,  ..., -2.6198e-01,  1.8766e-02,  7.5859e-02],
           [ 9.9940e-02,  4.0150e-02, -9.8056e-02,  ..., -1.7158e-01, -3.4062e-03, -2.6638e-02],
           [-5.5468e-02, -7.0994e-04, -7.5006e-02,  ..., -1.0616e-01,  1.7372e-02, -4.8893e-02],
           ...,
           [-1.4181e-01,  1.1548e-01,  5.5874e-02,  ...,  2.9942e-02, -1.8112e-02, -2.9444e-02],
           [ 3.9031e-02,  5.0681e-02,  5.8297e-02,  ...,  1.5017e-02,  7.6318e-02, -1.7010e-02],
           [ 1.0667e-02,  6.1897e-02, -8.4168e-02,  ..., -2.1743e-02, -4.0991e-02,  2.1014e-02]],

          [[ 1.6978e-02, -4.5087e-02,  8.1483e-02,  ..., -3.4949e-02, -7.6166e-02,  1.2526e-01],
           [ 1.1711e-01, -2.5587e-02, -3.6007e-02,  ..., -1.7148e-01,  1.1170e-02,  4.9115e-02],
           [-2.6843e-02,  4.2615e-02,  4.1452e-02,  ...,  1.1094e-01,  7.9744e-03, -9.2707e-02],
           ...,
           [ 2.5185e-02,  2.3065e-02, -1.1378e-01,  ..., -1.3932e-01, -6.4133e-02,  1.0574e-02],
           [-3.0315e-02,  3.8837e-02,  1.6509e-01,  ...,  1.2289e-01,  5.9516e-02,  4.5134e-02],
           [-2.8911e-02,  5.5351e-03,  1.0212e-01,  ..., -2.4960e-02, -2.7614e-02, -5.9342e-02]]],


         [[[-8.5187e-02,  6.7951e-02,  5.3837e-02,  ..., -7.2646e-02,  2.5646e-02,  6.2759e-02],
           [ 6.9788e-02, -8.1055e-02,  4.5650e-02,  ...,  1.2715e-01,  1.4196e-02, -1.5498e-01],
           [ 1.0499e-01, -1.5657e-02,  1.3404e-02,  ..., -7.7004e-02, -1.4072e-01, -1.9966e-03],
           ...,
           [-9.7883e-02,  5.0957e-02,  1.0122e-01,  ...,  1.4629e-01, -1.1889e-01, -3.2397e-02],
           [ 4.3204e-02,  1.5724e-01, -5.0146e-05,  ..., -4.0239e-02,  2.0330e-01, -1.8656e-02],
           [ 4.0169e-02, -3.9663e-02,  3.0913e-02,  ..., -1.2377e-01,  7.9213e-02, -5.5125e-03]],

          [[ 8.9421e-02,  8.8845e-02, -6.1184e-02,  ..., -2.7354e-02,  8.8321e-02,  1.2828e-01],
           [ 1.3511e-02, -5.0528e-02, -1.4633e-01,  ...,  3.6064e-02,  4.0775e-02,  1.3117e-01],
           [ 1.8151e-02, -1.1443e-01, -1.5861e-01,  ..., -2.4579e-02,  2.5034e-02, -2.1739e-02],
           ...,
           [ 1.2346e-02,  1.2971e-01,  3.8458e-02,  ...,  2.3587e-01,  1.1556e-01,  9.2041e-03],
           [ 1.0450e-01, -1.1313e-02, -8.5782e-02,  ...,  1.4086e-02, -3.6369e-02, -7.4709e-03],
           [ 8.8566e-02, -2.7712e-02, -3.7588e-02,  ...,  1.5820e-01,  7.1380e-04,  5.2313e-02]],

          [[-1.9333e-01, -2.3790e-02, -1.8478e-01,  ..., -6.0476e-02,  6.6580e-03,  1.7396e-01],
           [ 3.8446e-02, -6.8585e-02,  1.8205e-01,  ...,  2.9872e-02,  5.5316e-02,  1.1129e-01],
           [-8.4996e-03, -6.8807e-02, -1.2926e-01,  ..., -2.8666e-03,  1.0701e-01, -5.4722e-02],
           ...,
           [-1.0787e-01, -5.1398e-03,  2.2752e-01,  ..., -1.9846e-02, -1.0341e-01,  4.1114e-03],
           [ 1.4117e-02, -9.9711e-02,  1.1045e-01,  ...,  4.3007e-02,  3.6649e-02,  3.7427e-02],
           [-3.7699e-02,  1.3266e-01, -4.6468e-02,  ...,  1.4610e-01, -4.0924e-02, -1.5911e-02]],

          ...,

          [[ 4.4219e-05,  5.7690e-02, -1.0569e-01,  ...,  5.8110e-02,  1.5870e-02, -1.2692e-02],
           [-7.5560e-02, -8.2880e-02,  1.1041e-01,  ..., -1.1110e-01,  1.7158e-01,  9.2602e-02],
           [-1.9235e-01,  1.1049e-01,  6.9263e-02,  ...,  1.8956e-03,  1.0422e-01,  3.7345e-02],
           ...,
           [ 5.7292e-02,  1.2155e-01, -4.4879e-04,  ..., -3.2159e-03, -5.7022e-02,  7.1428e-02],
           [-6.5536e-02,  5.2000e-02, -1.1614e-01,  ..., -2.4017e-02,  5.4362e-02,  1.5543e-01],
           [-2.8220e-02,  1.3843e-01, -5.3662e-02,  ...,  4.1439e-03,  6.4188e-02, -7.5669e-03]],

          [[-7.5944e-02,  1.0851e-01,  1.7336e-01,  ...,  3.9967e-02,  3.1034e-02, -5.2872e-02],
           [-1.5726e-01,  1.6285e-02, -1.5322e-01,  ...,  5.0231e-02,  3.7195e-02, -1.2289e-01],
           [-1.0959e-01,  2.2796e-02,  7.4699e-02,  ...,  3.6581e-02, -9.6859e-02,  1.4374e-02],
           ...,
           [ 1.9728e-02, -5.2994e-02, -1.0111e-01,  ..., -1.1787e-01,  1.2975e-01, -6.5378e-03],
           [-8.6925e-02,  1.3311e-01,  1.5001e-01,  ..., -1.0849e-01,  4.2130e-02, -2.9890e-02],
           [-1.5608e-01,  8.6754e-02,  8.5887e-02,  ...,  5.5642e-02, -5.1362e-02, -1.1484e-02]],

          [[ 6.3340e-02, -1.9570e-01,  8.8426e-02,  ..., -2.2677e-03, -1.9932e-01,  1.7536e-01],
           [-6.9694e-02,  8.6373e-03,  4.4566e-02,  ..., -3.2263e-02, -1.0891e-02, -1.2731e-01],
           [-8.7922e-02, -4.2518e-02, -2.2084e-01,  ..., -1.3666e-01, -6.3402e-02,  2.5289e-02],
           ...,
           [-5.9229e-02,  7.5715e-02,  5.6888e-02,  ..., -7.9450e-02,  1.5765e-02, -5.9968e-02],
           [-1.9089e-02, -3.3602e-02, -7.5792e-02,  ...,  9.1326e-02, -9.2602e-02, -2.0123e-02],
           [ 1.0320e-01, -1.0157e-01, -6.0096e-02,  ..., -4.9012e-02, -3.9522e-02,  1.3015e-01]]],


         [[[ 1.8505e-01, -8.2891e-02,  1.3220e-03,  ..., -3.2965e-02, -2.4220e-01, -7.9849e-02],
           [ 2.3010e-02,  6.0594e-02,  1.1671e-01,  ...,  3.7633e-02,  1.5009e-01,  2.5681e-02],
           [-4.7493e-02,  1.6378e-02, -3.7301e-02,  ..., -2.5668e-01, -1.1043e-01,  4.3796e-02],
           ...,
           [ 2.2817e-02,  1.8035e-02,  1.5693e-01,  ...,  1.3603e-02,  8.0895e-02, -7.6434e-02],
           [ 5.2587e-02, -1.0572e-01, -6.5152e-02,  ..., -7.5184e-02,  5.9290e-02, -2.9095e-01],
           [-2.1194e-02, -1.0480e-01, -6.3359e-03,  ...,  2.7155e-02,  7.5363e-02, -7.5690e-02]],

          [[ 4.3849e-02,  1.6290e-02, -1.5036e-01,  ..., -1.2273e-01, -5.0591e-02,  8.0130e-02],
           [ 1.2681e-01, -5.0936e-02,  6.2976e-02,  ...,  6.9390e-02,  1.4028e-01, -7.7345e-02],
           [-6.7526e-02, -4.6386e-03,  9.5082e-03,  ...,  1.2703e-02,  1.1117e-01, -1.1803e-01],
           ...,
           [-1.0656e-03,  4.1111e-02, -1.3555e-02,  ...,  3.6089e-02, -3.2487e-02, -9.0938e-02],
           [-6.9036e-02, -2.9739e-01,  1.0823e-01,  ...,  4.1216e-02,  6.6867e-02,  7.2952e-02],
           [ 8.7275e-02, -8.4263e-02, -2.7715e-02,  ..., -7.0971e-02, -1.4418e-02, -4.1497e-02]],

          [[-5.0202e-03,  1.4765e-01,  5.0654e-02,  ...,  8.7406e-02, -1.2845e-01,  9.6455e-02],
           [ 4.9551e-02,  1.7674e-01, -4.2344e-02,  ...,  3.6934e-02,  1.0916e-01,  4.0092e-02],
           [ 3.7374e-02, -3.9215e-02, -9.7904e-02,  ..., -3.0723e-02, -1.6826e-01, -3.5522e-02],
           ...,
           [ 1.1615e-01,  9.8449e-02, -1.2563e-01,  ..., -1.8250e-01, -1.0976e-02,  5.1443e-02],
           [-3.4132e-02,  2.6991e-02, -3.7619e-02,  ...,  4.1965e-02,  2.0204e-01,  1.9771e-02],
           [-1.3750e-01,  1.4892e-02,  3.0636e-02,  ..., -3.7520e-02,  1.2781e-02, -4.2839e-02]],

          ...,

          [[ 2.7084e-02, -5.8116e-02,  4.8876e-02,  ..., -5.9634e-02,  9.5346e-02, -3.9631e-02],
           [ 3.7214e-02, -1.3125e-01, -9.1848e-02,  ..., -1.2619e-01, -1.5081e-02,  5.6378e-02],
           [-4.3672e-02,  1.2595e-01,  1.4620e-01,  ..., -1.1562e-01, -3.3295e-02,  1.2469e-01],
           ...,
           [ 1.2960e-01,  2.9321e-02, -1.1934e-01,  ...,  7.5403e-02,  1.8795e-01, -9.0119e-03],
           [-1.0921e-01, -1.3320e-01,  4.9889e-02,  ...,  1.6277e-01,  9.5479e-02,  1.1609e-01],
           [ 1.2524e-01, -1.8010e-01,  9.3777e-02,  ...,  3.3745e-02,  1.6840e-03, -6.7612e-02]],

          [[-5.5915e-02,  8.3644e-02, -9.7635e-02,  ...,  9.4437e-03, -1.9931e-01, -1.7101e-02],
           [-8.2127e-02,  3.7943e-02, -1.0292e-01,  ...,  7.5163e-03,  1.3568e-01,  1.4762e-02],
           [-4.9569e-02,  2.3263e-01, -6.4542e-02,  ..., -4.3722e-02, -3.8308e-02, -1.1405e-02],
           ...,
           [ 5.8215e-02, -1.3299e-01,  6.6755e-02,  ...,  7.8601e-02, -3.9378e-02,  5.8071e-02],
           [-8.1914e-02, -5.1991e-02,  8.4073e-02,  ...,  3.1983e-02,  2.1963e-01,  8.4083e-02],
           [-5.6702e-02, -3.9051e-02, -1.0778e-01,  ..., -3.5285e-03,  4.0992e-02,  8.3801e-02]],

          [[-4.5608e-02, -5.1491e-02,  5.7220e-03,  ..., -7.2507e-02,  1.6909e-01, -2.2204e-03],
           [-6.2245e-02, -1.4661e-01,  1.7564e-01,  ..., -1.0382e-01,  1.7797e-01, -6.6737e-02],
           [-9.8443e-02, -2.1764e-02,  1.9685e-02,  ..., -9.1763e-02,  2.3473e-01,  4.4529e-02],
           ...,
           [ 6.7547e-02, -2.0348e-02, -3.8798e-03,  ...,  7.2748e-02, -2.6449e-02, -5.8717e-03],
           [-8.4356e-03,  8.4878e-02, -5.7364e-02,  ...,  7.6244e-02,  4.3453e-02,  1.1527e-01],
           [ 3.5135e-02, -9.2681e-02,  1.9577e-02,  ..., -1.4285e-01,  7.0325e-02, -8.5585e-02]]]]])

2025-07-09 00:29:23.288254 GPU 4 66790 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:29:24.196249 GPU 2 66953 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:29:35.890122 GPU 5 66545 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:30:15.120369 GPU 3 68218 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:30:20.833129 GPU 7 67624 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:31:29.458942 GPU 4 68398 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:32:01.116964 GPU 5 68796 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:32:11.458591 GPU 2 66953 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:32:23.363340 GPU 7 68959 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:33:01.174839 GPU 2 69282 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:33:36.363178 GPU 4 69692 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:34:39.007756 GPU 7 70023 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 699051, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 699051, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:34:52.049232 GPU 5 70184 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 699051, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 699051, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:36:03.631266 GPU 4 70750 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 699051, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[accuracy error] paddle.nn.functional.avg_pool3d(Tensor([2, 3, 699051, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1371 / 536871936 (0.0%)
Greatest absolute difference: 0.1983405500650406 at index (1, 1, 349525, 9, 14) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 349525, 0, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 3, 349526, 16, 16]), dtype=torch.float32)
tensor([[[[[ 0.0780,  0.0891, -0.0555,  ...,  0.0325,  0.0180, -0.0511],
           [-0.1275,  0.0408, -0.0456,  ...,  0.0040,  0.2073, -0.1406],
           [ 0.0307,  0.0717, -0.2261,  ...,  0.0288, -0.0801,  0.0451],
           ...,
           [-0.1403, -0.0231, -0.0601,  ...,  0.0055, -0.0112, -0.0258],
           [-0.0711,  0.0776,  0.0062,  ..., -0.1650,  0.1135, -0.1196],
           [ 0.0508,  0.0180, -0.0352,  ...,  0.0643, -0.0694, -0.1785]],

          [[ 0.0419, -0.0923,  0.1867,  ..., -0.0928, -0.0037, -0.1205],
           [ 0.0090,  0.0989, -0.2047,  ...,  0.1084,  0.0689,  0.1049],
           [-0.1164,  0.0902,  0.1496,  ..., -0.0420, -0.1427,  0.0791],
           ...,
           [ 0.0470, -0.0623, -0.0129,  ..., -0.0943, -0.2484,  0.0648],
           [-0.0248,  0.0026, -0.0109,  ..., -0.2917,  0.0853, -0.0250],
           [ 0.1549,  0.1640, -0.0268,  ..., -0.0654, -0.1551, -0.0418]],

          [[ 0.0933, -0.0721,  0.1472,  ..., -0.0291,  0.0813, -0.1104],
           [ 0.1443,  0.1441, -0.0202,  ..., -0.1286, -0.0935,  0.0107],
           [ 0.1925,  0.0192, -0.1715,  ..., -0.0975, -0.0127, -0.0657],
           ...,
           [-0.1228, -0.1041, -0.1136,  ..., -0.1090, -0.0543,  0.1361],
           [ 0.0167, -0.1008,  0.0076,  ...,  0.0453,  0.0225,  0.0230],
           [ 0.0088, -0.1239,  0.1464,  ..., -0.1338,  0.0608, -0.0551]],

          ...,

          [[-0.0330, -0.0826,  0.1361,  ...,  0.0692, -0.0127,  0.0149],
           [-0.1122,  0.0718, -0.0512,  ...,  0.1085, -0.0206, -0.0501],
           [ 0.0100,  0.0975, -0.2099,  ..., -0.0264, -0.1056, -0.0308],
           ...,
           [-0.0748,  0.0976,  0.1628,  ..., -0.0438, -0.0420,  0.0541],
           [ 0.0994, -0.0352, -0.0558,  ..., -0.0048, -0.0256,  0.0263],
           [-0.0674, -0.1520, -0.0307,  ...,  0.1159, -0.1125, -0.0639]],

          [[ 0.0926,  0.0415, -0.0402,  ...,  0.1865, -0.1652, -0.0550],
           [ 0.1650, -0.1460,  0.0499,  ...,  0.0996,  0.0399, -0.0505],
           [ 0.0491, -0.0563,  0.1730,  ..., -0.0679,  0.2157,  0.0338],
           ...,
           [-0.0333,  0.0014, -0.1002,  ..., -0.0773,  0.1860, -0.0615],
           [-0.0424, -0.0451, -0.1715,  ..., -0.0617,  0.0044, -0.1555],
           [-0.0446, -0.1616,  0.0882,  ...,  0.0714, -0.0341,  0.0016]],

          [[ 0.2221,  0.0773,  0.0544,  ..., -0.1630,  0.0868,  0.1997],
           [ 0.2288, -0.0862,  0.0697,  ..., -0.1078, -0.2208, -0.0940],
           [-0.0762,  0.0758,  0.0348,  ..., -0.2117,  0.1054, -0.0069],
           ...,
           [-0.2163,  0.0143,  0.0346,  ...,  0.2891, -0.2277,  0.0625],
           [ 0.0891,  0.0269,  0.1116,  ...,  0.0273, -0.0447,  0.1451],
           [-0.1495, -0.0576,  0.0996,  ...,  0.1676,  0.0254,  0.0431]]],


         [[[-0.0630, -0.0868,  0.0940,  ..., -0.0275, -0.0895,  0.0189],
           [-0.1904, -0.0465,  0.1284,  ...,  0.0161,  0.0753,  0.1173],
           [ 0.1014, -0.0317,  0.0714,  ..., -0.1063, -0.0679,  0.0286],
           ...,
           [ 0.1704, -0.0118,  0.1073,  ..., -0.0852,  0.0016,  0.0142],
           [ 0.0702,  0.0341,  0.0217,  ...,  0.0035,  0.1370, -0.1828],
           [-0.1025,  0.0136,  0.0807,  ..., -0.1458,  0.0311, -0.0068]],

          [[ 0.0044, -0.0111,  0.1726,  ..., -0.0601,  0.1226,  0.0079],
           [-0.1451, -0.0263, -0.0196,  ...,  0.0725, -0.0629, -0.0257],
           [ 0.1604, -0.1358,  0.0293,  ..., -0.1070, -0.0386,  0.0354],
           ...,
           [-0.0540, -0.0302, -0.1129,  ..., -0.0715,  0.0248,  0.0636],
           [-0.0636,  0.0965, -0.0816,  ..., -0.0226, -0.0467, -0.0173],
           [ 0.0478,  0.2237,  0.0003,  ..., -0.0604, -0.1782,  0.0354]],

          [[-0.1377, -0.0356,  0.1022,  ...,  0.1478, -0.0740, -0.1573],
           [-0.1011,  0.1056,  0.0969,  ..., -0.0108, -0.0906,  0.0050],
           [-0.1317, -0.0241,  0.0368,  ..., -0.0745,  0.0193,  0.1939],
           ...,
           [-0.0528, -0.0402,  0.0929,  ..., -0.0588, -0.1536,  0.1100],
           [-0.0171,  0.0183, -0.0463,  ...,  0.0668, -0.0121,  0.0031],
           [ 0.1451, -0.0546,  0.0649,  ..., -0.0947, -0.0380, -0.0490]],

          ...,

          [[ 0.0783, -0.1174,  0.0458,  ...,  0.0819, -0.1842, -0.2585],
           [ 0.1510,  0.1361,  0.1862,  ..., -0.0587, -0.0515,  0.0403],
           [-0.0708,  0.1233,  0.1080,  ...,  0.1848,  0.1017,  0.0563],
           ...,
           [-0.0216, -0.0870, -0.0623,  ...,  0.0869,  0.0651,  0.1447],
           [ 0.0683,  0.0022, -0.0604,  ..., -0.0055, -0.1080,  0.0047],
           [-0.1648, -0.1302,  0.0668,  ..., -0.0360,  0.0201,  0.0398]],

          [[ 0.0681,  0.1934,  0.0686,  ..., -0.1696,  0.0348,  0.0268],
           [ 0.1359,  0.0396,  0.0156,  ..., -0.0170, -0.1314, -0.1083],
           [-0.1171, -0.1366,  0.1371,  ..., -0.0951, -0.0910,  0.0569],
           ...,
           [ 0.0493,  0.0992,  0.0404,  ..., -0.0443, -0.2002, -0.0119],
           [ 0.1401,  0.0501, -0.0500,  ..., -0.0713,  0.1272,  0.0891],
           [ 0.2473,  0.0903,  0.0278,  ...,  0.0080, -0.0625,  0.0243]],

          [[ 0.0492, -0.0022, -0.1543,  ...,  0.0962,  0.2521,  0.0343],
           [-0.0831,  0.0113,  0.1572,  ...,  0.0215,  0.1637, -0.0961],
           [ 0.1362,  0.0930, -0.0747,  ...,  0.0050,  0.0117, -0.0487],
           ...,
           [ 0.1072, -0.1589,  0.0308,  ..., -0.1522,  0.0211, -0.1561],
           [-0.0582, -0.1302,  0.2108,  ...,  0.1487, -0.1360, -0.1109],
           [ 0.1848, -0.1313,  0.2723,  ...,  0.0354,  0.0137,  0.0635]]],


         [[[-0.2514, -0.1600, -0.1405,  ..., -0.0412,  0.1162,  0.0116],
           [ 0.1183,  0.0944, -0.0391,  ..., -0.0101,  0.0263, -0.0342],
           [-0.1664, -0.1856, -0.1645,  ...,  0.0824,  0.0199, -0.1630],
           ...,
           [-0.0753, -0.1259, -0.1166,  ...,  0.0758, -0.0721,  0.1084],
           [-0.1099,  0.0758,  0.1760,  ..., -0.2146,  0.0027, -0.0196],
           [ 0.0263,  0.1340, -0.0303,  ...,  0.0412, -0.0635,  0.1110]],

          [[-0.0386, -0.0633,  0.1433,  ...,  0.0414, -0.0809,  0.1073],
           [ 0.0336, -0.1154,  0.1909,  ...,  0.0373,  0.0815,  0.0879],
           [ 0.0592, -0.1052,  0.0969,  ...,  0.1678,  0.1987,  0.0523],
           ...,
           [-0.1293, -0.1135, -0.1537,  ...,  0.0281, -0.2005, -0.0226],
           [ 0.1819,  0.0633,  0.1688,  ...,  0.0550, -0.0503, -0.1194],
           [ 0.0545,  0.0532, -0.1461,  ..., -0.0272,  0.0523, -0.0657]],

          [[-0.0299, -0.1025, -0.1364,  ..., -0.0793,  0.0178,  0.0798],
           [-0.1235,  0.1248,  0.0116,  ..., -0.1311, -0.0557,  0.0460],
           [-0.0053, -0.0241,  0.1579,  ...,  0.0491,  0.0191, -0.0038],
           ...,
           [-0.0045,  0.1174, -0.0886,  ..., -0.2524,  0.0281,  0.0760],
           [-0.1352,  0.1632,  0.1690,  ..., -0.0031, -0.1614,  0.1564],
           [-0.1302,  0.0764, -0.0706,  ...,  0.0916, -0.1423,  0.0036]],

          ...,

          [[-0.1003,  0.0173,  0.0143,  ..., -0.0678,  0.0748,  0.1546],
           [ 0.1596, -0.0104,  0.1903,  ..., -0.1737, -0.0543,  0.0054],
           [ 0.2950, -0.0683, -0.1377,  ..., -0.1256,  0.1307,  0.0896],
           ...,
           [ 0.0145, -0.0041, -0.0887,  ...,  0.0950, -0.0428,  0.0077],
           [ 0.1025,  0.0393,  0.0529,  ..., -0.2447,  0.1115,  0.0309],
           [ 0.0202,  0.0086,  0.0517,  ...,  0.0743, -0.1365,  0.0377]],

          [[-0.1012, -0.0894,  0.0578,  ...,  0.1113,  0.0116,  0.0361],
           [-0.0387,  0.1134, -0.0893,  ...,  0.1384,  0.0164,  0.0690],
           [ 0.0324,  0.1566, -0.0969,  ...,  0.0271, -0.0231,  0.2003],
           ...,
           [-0.0498,  0.0514, -0.0742,  ...,  0.0360, -0.0664, -0.0608],
           [-0.0053,  0.1373, -0.2098,  ..., -0.1530,  0.0176, -0.0980],
           [ 0.1092,  0.1741,  0.0686,  ..., -0.0134,  0.1467, -0.1869]],

          [[ 0.2118, -0.0513,  0.3112,  ..., -0.0372, -0.0769, -0.1200],
           [-0.0868, -0.1340, -0.1263,  ...,  0.2381,  0.0277, -0.0209],
           [ 0.1550, -0.0657,  0.0280,  ...,  0.0212,  0.0285,  0.0534],
           ...,
           [-0.1181, -0.1785,  0.0480,  ..., -0.2001, -0.1139,  0.0365],
           [-0.1884, -0.1098,  0.2461,  ...,  0.0961, -0.2974,  0.0248],
           [ 0.0081, -0.2892, -0.2222,  ..., -0.2509, -0.1140,  0.3113]]]],



        [[[[ 0.0495, -0.1202,  0.1194,  ...,  0.0924, -0.0056, -0.0381],
           [ 0.0961, -0.1296,  0.0349,  ...,  0.1641, -0.0291,  0.1164],
           [ 0.2226, -0.0841,  0.1567,  ...,  0.0468, -0.0632,  0.0363],
           ...,
           [-0.0678, -0.0640,  0.1840,  ..., -0.1006,  0.0039, -0.0244],
           [ 0.1181,  0.0563, -0.0495,  ...,  0.0237,  0.0727, -0.1364],
           [-0.0204, -0.1983, -0.0599,  ...,  0.0433,  0.0518,  0.0587]],

          [[-0.0072, -0.1127,  0.1111,  ...,  0.1479, -0.0617,  0.0409],
           [-0.0154,  0.1108, -0.0550,  ..., -0.1396, -0.1671, -0.0902],
           [ 0.0720,  0.0240, -0.0083,  ..., -0.0052,  0.0185, -0.0401],
           ...,
           [-0.2756,  0.0211, -0.1714,  ...,  0.0294,  0.0220,  0.0524],
           [ 0.0116, -0.0162, -0.0547,  ..., -0.0171,  0.0796,  0.1498],
           [-0.2295,  0.1426, -0.0233,  ..., -0.0806, -0.0199,  0.0199]],

          [[ 0.0244,  0.1667, -0.1533,  ...,  0.0653, -0.1137, -0.1163],
           [ 0.1073, -0.0290, -0.0459,  ..., -0.0605, -0.1220, -0.0219],
           [-0.1826, -0.0403, -0.0685,  ..., -0.1162, -0.0046,  0.0301],
           ...,
           [ 0.0063,  0.0397,  0.0173,  ..., -0.0822,  0.1532, -0.0090],
           [-0.1290, -0.0161,  0.1271,  ...,  0.0465, -0.1605, -0.1545],
           [-0.0018, -0.1680, -0.1572,  ..., -0.0193, -0.1265,  0.2089]],

          ...,

          [[ 0.0028,  0.0329,  0.0141,  ...,  0.0894, -0.2758, -0.0337],
           [ 0.1442, -0.1067,  0.0231,  ..., -0.0012, -0.0258, -0.0330],
           [ 0.0716,  0.0432,  0.1537,  ...,  0.1771,  0.0664, -0.0345],
           ...,
           [-0.1438,  0.0065, -0.0159,  ..., -0.1631,  0.1337, -0.0605],
           [ 0.0479, -0.1543, -0.0973,  ...,  0.0287,  0.1665, -0.0053],
           [ 0.0194,  0.1152, -0.0742,  ...,  0.1735, -0.0491, -0.0206]],

          [[ 0.0056,  0.0507, -0.0810,  ..., -0.1565, -0.0412, -0.1877],
           [ 0.1043, -0.0873, -0.0508,  ..., -0.0226,  0.2198, -0.1078],
           [ 0.1160,  0.0814,  0.2238,  ...,  0.1550,  0.1044, -0.1063],
           ...,
           [-0.0038,  0.0698,  0.0240,  ..., -0.0672,  0.0814, -0.1567],
           [ 0.0140,  0.2196, -0.0747,  ...,  0.0132, -0.2114,  0.0978],
           [-0.1977,  0.0641, -0.1169,  ...,  0.0003,  0.0222,  0.0753]],

          [[ 0.0749,  0.1507,  0.0629,  ...,  0.1944, -0.3055,  0.0421],
           [ 0.1129,  0.1004, -0.0237,  ...,  0.0168,  0.0370, -0.0295],
           [ 0.1543, -0.1961, -0.1241,  ..., -0.0146,  0.0880, -0.1655],
           ...,
           [ 0.0348,  0.0051,  0.1469,  ...,  0.0087, -0.1972,  0.1165],
           [ 0.2755,  0.3109,  0.0346,  ..., -0.0404,  0.2380,  0.1314],
           [ 0.0326,  0.0406, -0.0730,  ..., -0.0392, -0.0507, -0.0817]]],


         [[[ 0.0927, -0.1140,  0.1462,  ..., -0.2092, -0.0020, -0.1718],
           [ 0.0030, -0.0199,  0.1529,  ...,  0.1455,  0.0176, -0.0150],
           [ 0.0144, -0.0213, -0.0574,  ..., -0.1779, -0.0978,  0.2725],
           ...,
           [ 0.0226,  0.0461,  0.1101,  ...,  0.0583,  0.0769, -0.1069],
           [ 0.1175,  0.1287,  0.0456,  ...,  0.0392, -0.0387,  0.0793],
           [-0.1007, -0.0225, -0.0185,  ..., -0.0601, -0.0860, -0.0728]],

          [[-0.1272, -0.0331, -0.1397,  ..., -0.0413,  0.0091,  0.0232],
           [-0.0598, -0.1785,  0.1684,  ...,  0.0565, -0.0929,  0.2644],
           [-0.0391,  0.0484,  0.0326,  ..., -0.0709,  0.0966, -0.0492],
           ...,
           [ 0.0261, -0.0213,  0.0529,  ..., -0.0416,  0.0441, -0.0324],
           [ 0.0892,  0.0405, -0.0091,  ..., -0.0768, -0.1319,  0.0455],
           [-0.1196, -0.0283,  0.2782,  ..., -0.0044, -0.2185,  0.0627]],

          [[ 0.0565, -0.0408,  0.0713,  ...,  0.0319,  0.0077, -0.0903],
           [-0.2664,  0.0533,  0.0109,  ...,  0.0217, -0.1043,  0.0105],
           [-0.0745,  0.0362,  0.1280,  ...,  0.0462, -0.0439,  0.0980],
           ...,
           [-0.0867,  0.1903,  0.1050,  ..., -0.0181, -0.1115, -0.0187],
           [ 0.0961,  0.0934,  0.1224,  ..., -0.0496, -0.0556,  0.0303],
           [-0.2231,  0.1826, -0.0483,  ..., -0.2364, -0.0151, -0.0098]],

          ...,

          [[-0.0069,  0.0475, -0.1240,  ..., -0.0721, -0.0827,  0.0159],
           [ 0.0827, -0.0915, -0.0613,  ...,  0.0191,  0.0334,  0.0592],
           [-0.1274, -0.0180, -0.0283,  ...,  0.0709,  0.0142,  0.0272],
           ...,
           [-0.1165, -0.1182,  0.0665,  ..., -0.0383,  0.0014,  0.0892],
           [-0.0549, -0.0689, -0.1165,  ...,  0.0149, -0.1114,  0.0170],
           [-0.0584,  0.1048, -0.1234,  ..., -0.1143,  0.1103, -0.0630]],

          [[-0.0759,  0.0171,  0.1911,  ..., -0.0834, -0.2047,  0.0763],
           [ 0.0232,  0.1194,  0.1238,  ...,  0.1058,  0.0643, -0.0081],
           [-0.0744, -0.1177, -0.0979,  ...,  0.2378,  0.0684,  0.1171],
           ...,
           [-0.0427,  0.2122, -0.0489,  ...,  0.0787, -0.2252,  0.1283],
           [-0.1748,  0.0581, -0.0853,  ...,  0.1875,  0.1050,  0.0563],
           [-0.0800,  0.2069,  0.0712,  ..., -0.1412,  0.0843,  0.1058]],

          [[-0.1466,  0.2626, -0.1133,  ..., -0.0444,  0.0474, -0.2383],
           [ 0.0164, -0.0378, -0.0735,  ..., -0.0210, -0.0628, -0.3017],
           [ 0.0047,  0.0576,  0.0004,  ..., -0.0301,  0.1361,  0.0667],
           ...,
           [ 0.3135, -0.1077, -0.0435,  ..., -0.1671, -0.1239, -0.1314],
           [-0.0725,  0.0749,  0.0813,  ...,  0.2831, -0.0394,  0.0538],
           [-0.1289, -0.0972, -0.1162,  ...,  0.2840,  0.0167,  0.3223]]],


         [[[ 0.1396,  0.1143, -0.0863,  ...,  0.0394,  0.0156,  0.0353],
           [ 0.0892,  0.0734,  0.0228,  ...,  0.0135,  0.0716,  0.0333],
           [ 0.0004,  0.1338,  0.0282,  ..., -0.0678,  0.1556, -0.0255],
           ...,
           [-0.1068, -0.0730, -0.0880,  ...,  0.0033, -0.0197,  0.0110],
           [ 0.1549, -0.0849,  0.0027,  ...,  0.0571,  0.1091,  0.1388],
           [ 0.1906,  0.0947,  0.0884,  ...,  0.0305,  0.1945, -0.0123]],

          [[-0.0515, -0.0589, -0.2561,  ..., -0.1232,  0.0616,  0.0884],
           [-0.0775,  0.1048, -0.0029,  ...,  0.0925,  0.1092, -0.0191],
           [-0.0400,  0.0861,  0.1311,  ...,  0.0780,  0.1397,  0.0806],
           ...,
           [ 0.0769,  0.1494,  0.0938,  ...,  0.1591,  0.0544, -0.1134],
           [ 0.0179, -0.0593,  0.0274,  ...,  0.0163, -0.0559,  0.0501],
           [ 0.1977, -0.0190, -0.1452,  ..., -0.1277,  0.0213,  0.0282]],

          [[ 0.0759, -0.1430, -0.1447,  ...,  0.0266, -0.0389, -0.1658],
           [-0.0151,  0.1090, -0.0540,  ..., -0.0847, -0.0465, -0.0333],
           [-0.0946,  0.0034, -0.0598,  ..., -0.0392,  0.0352, -0.0049],
           ...,
           [-0.0454, -0.1452,  0.0322,  ...,  0.1467,  0.0497,  0.0663],
           [-0.0007, -0.2188,  0.2018,  ...,  0.0602,  0.0531, -0.1080],
           [ 0.0996,  0.0093, -0.0065,  ..., -0.0999,  0.1833, -0.1888]],

          ...,

          [[-0.0164, -0.1067,  0.0012,  ..., -0.0389,  0.1500, -0.0612],
           [ 0.1827,  0.1299,  0.1084,  ..., -0.0262, -0.0878, -0.1601],
           [-0.0473, -0.1420,  0.0474,  ..., -0.2082,  0.0399,  0.2435],
           ...,
           [ 0.0811, -0.0553,  0.1904,  ..., -0.0892, -0.0424, -0.0608],
           [ 0.0411,  0.0874, -0.0413,  ...,  0.1938,  0.2099,  0.1827],
           [-0.1597, -0.0907,  0.1163,  ..., -0.0486, -0.1983, -0.1295]],

          [[-0.0943, -0.0616,  0.0945,  ...,  0.1148,  0.0982,  0.1701],
           [-0.0334, -0.2247, -0.0476,  ...,  0.0357, -0.0358,  0.0266],
           [-0.0618,  0.1026, -0.0252,  ...,  0.0048, -0.0332, -0.0913],
           ...,
           [-0.0547,  0.0100, -0.0140,  ...,  0.0780,  0.0753, -0.0022],
           [ 0.0431, -0.1881, -0.0116,  ...,  0.1373, -0.0769,  0.1555],
           [-0.0109, -0.2449, -0.0731,  ...,  0.0568, -0.0717, -0.0124]],

          [[-0.0560, -0.0456, -0.0207,  ...,  0.1475, -0.0960, -0.0585],
           [-0.0073, -0.0468, -0.1371,  ...,  0.1188,  0.0369,  0.0628],
           [ 0.2454, -0.1291,  0.0340,  ...,  0.0814, -0.1700, -0.0954],
           ...,
           [-0.0365, -0.0534, -0.0469,  ..., -0.0172,  0.0613,  0.0613],
           [-0.0643,  0.1276, -0.2025,  ...,  0.0511,  0.1219, -0.1720],
           [ 0.0367, -0.2104,  0.0436,  ...,  0.0670,  0.2744, -0.2297]]]]])
DESIRED: (shape=torch.Size([2, 3, 349526, 16, 16]), dtype=torch.float32)
tensor([[[[[ 7.8050e-02,  8.9078e-02, -5.5465e-02,  ...,  3.2475e-02,  1.8011e-02, -5.1119e-02],
           [-1.2753e-01,  4.0808e-02, -4.5565e-02,  ...,  3.9992e-03,  2.0734e-01, -1.4057e-01],
           [ 3.0676e-02,  7.1696e-02, -2.2606e-01,  ...,  2.8755e-02, -8.0071e-02,  4.5088e-02],
           ...,
           [-1.4029e-01, -2.3066e-02, -6.0091e-02,  ...,  5.5066e-03, -1.1183e-02, -2.5849e-02],
           [-7.1095e-02,  7.7633e-02,  6.1799e-03,  ..., -1.6502e-01,  1.1346e-01, -1.1957e-01],
           [ 5.0804e-02,  1.8011e-02, -3.5226e-02,  ...,  6.4332e-02, -6.9432e-02, -1.7855e-01]],

          [[ 4.1855e-02, -9.2283e-02,  1.8667e-01,  ..., -9.2823e-02, -3.7448e-03, -1.2049e-01],
           [ 9.0026e-03,  9.8889e-02, -2.0468e-01,  ...,  1.0840e-01,  6.8894e-02,  1.0492e-01],
           [-1.1642e-01,  9.0192e-02,  1.4959e-01,  ..., -4.2024e-02, -1.4272e-01,  7.9135e-02],
           ...,
           [ 4.6972e-02, -6.2263e-02, -1.2924e-02,  ..., -9.4334e-02, -2.4843e-01,  6.4767e-02],
           [-2.4802e-02,  2.6356e-03, -1.0891e-02,  ..., -2.9169e-01,  8.5285e-02, -2.4951e-02],
           [ 1.5493e-01,  1.6395e-01, -2.6789e-02,  ..., -6.5442e-02, -1.5511e-01, -4.1769e-02]],

          [[ 9.3324e-02, -7.2084e-02,  1.4722e-01,  ..., -2.9091e-02,  8.1281e-02, -1.1036e-01],
           [ 1.4429e-01,  1.4407e-01, -2.0150e-02,  ..., -1.2857e-01, -9.3521e-02,  1.0708e-02],
           [ 1.9254e-01,  1.9154e-02, -1.7153e-01,  ..., -9.7538e-02, -1.2683e-02, -6.5684e-02],
           ...,
           [-1.2282e-01, -1.0412e-01, -1.1355e-01,  ..., -1.0904e-01, -5.4250e-02,  1.3611e-01],
           [ 1.6717e-02, -1.0076e-01,  7.6063e-03,  ...,  4.5323e-02,  2.2512e-02,  2.2986e-02],
           [ 8.7796e-03, -1.2386e-01,  1.4635e-01,  ..., -1.3383e-01,  6.0776e-02, -5.5058e-02]],

          ...,

          [[-3.3023e-02, -8.2589e-02,  1.3605e-01,  ...,  6.9185e-02, -1.2662e-02,  1.4947e-02],
           [-1.1218e-01,  7.1762e-02, -5.1176e-02,  ...,  1.0849e-01, -2.0613e-02, -5.0083e-02],
           [ 9.9687e-03,  9.7511e-02, -2.0994e-01,  ..., -2.6416e-02, -1.0559e-01, -3.0790e-02],
           ...,
           [-7.4771e-02,  9.7577e-02,  1.6281e-01,  ..., -4.3757e-02, -4.1963e-02,  5.4073e-02],
           [ 9.9366e-02, -3.5242e-02, -5.5753e-02,  ..., -4.8010e-03, -2.5600e-02,  2.6295e-02],
           [-6.7432e-02, -1.5201e-01, -3.0744e-02,  ...,  1.1588e-01, -1.1253e-01, -6.3856e-02]],

          [[ 9.2556e-02,  4.1478e-02, -4.0242e-02,  ...,  1.8653e-01, -1.6515e-01, -5.5011e-02],
           [ 1.6502e-01, -1.4599e-01,  4.9870e-02,  ...,  9.9610e-02,  3.9903e-02, -5.0456e-02],
           [ 4.9138e-02, -5.6340e-02,  1.7297e-01,  ..., -6.7869e-02,  2.1567e-01,  3.3785e-02],
           ...,
           [-3.3340e-02,  1.3748e-03, -1.0019e-01,  ..., -7.7260e-02,  1.8602e-01, -6.1472e-02],
           [-4.2378e-02, -4.5088e-02, -1.7147e-01,  ..., -6.1672e-02,  4.4375e-03, -1.5550e-01],
           [-4.4590e-02, -1.6163e-01,  8.8175e-02,  ...,  7.1389e-02, -3.4137e-02,  1.5711e-03]],

          [[ 1.1106e-01,  3.8663e-02,  2.7214e-02,  ..., -8.1479e-02,  4.3386e-02,  9.9828e-02],
           [ 1.1439e-01, -4.3084e-02,  3.4868e-02,  ..., -5.3912e-02, -1.1042e-01, -4.6989e-02],
           [-3.8108e-02,  3.7919e-02,  1.7389e-02,  ..., -1.0583e-01,  5.2701e-02, -3.4392e-03],
           ...,
           [-1.0815e-01,  7.1500e-03,  1.7278e-02,  ...,  1.4453e-01, -1.1384e-01,  3.1240e-02],
           [ 4.4549e-02,  1.3455e-02,  5.5798e-02,  ...,  1.3643e-02, -2.2353e-02,  7.2527e-02],
           [-7.4760e-02, -2.8793e-02,  4.9778e-02,  ...,  8.3809e-02,  1.2696e-02,  2.1528e-02]]],


         [[[-6.2976e-02, -8.6812e-02,  9.4001e-02,  ..., -2.7526e-02, -8.9475e-02,  1.8863e-02],
           [-1.9039e-01, -4.6462e-02,  1.2840e-01,  ...,  1.6054e-02,  7.5260e-02,  1.1731e-01],
           [ 1.0137e-01, -3.1690e-02,  7.1428e-02,  ..., -1.0631e-01, -6.7880e-02,  2.8603e-02],
           ...,
           [ 1.7040e-01, -1.1768e-02,  1.0732e-01,  ..., -8.5248e-02,  1.5991e-03,  1.4181e-02],
           [ 7.0163e-02,  3.4132e-02,  2.1725e-02,  ...,  3.5012e-03,  1.3699e-01, -1.8283e-01],
           [-1.0249e-01,  1.3593e-02,  8.0706e-02,  ..., -1.4577e-01,  3.1075e-02, -6.7727e-03]],

          [[ 4.4053e-03, -1.1130e-02,  1.7260e-01,  ..., -6.0065e-02,  1.2259e-01,  7.9404e-03],
           [-1.4507e-01, -2.6314e-02, -1.9564e-02,  ...,  7.2546e-02, -6.2915e-02, -2.5676e-02],
           [ 1.6045e-01, -1.3582e-01,  2.9256e-02,  ..., -1.0699e-01, -3.8605e-02,  3.5428e-02],
           ...,
           [-5.3971e-02, -3.0249e-02, -1.1287e-01,  ..., -7.1525e-02,  2.4839e-02,  6.3629e-02],
           [-6.3609e-02,  9.6523e-02, -8.1594e-02,  ..., -2.2556e-02, -4.6690e-02, -1.7259e-02],
           [ 4.7846e-02,  2.2367e-01,  3.2623e-04,  ..., -6.0445e-02, -1.7817e-01,  3.5354e-02]],

          [[-1.3773e-01, -3.5570e-02,  1.0222e-01,  ...,  1.4782e-01, -7.4033e-02, -1.5726e-01],
           [-1.0113e-01,  1.0562e-01,  9.6893e-02,  ..., -1.0816e-02, -9.0642e-02,  4.9503e-03],
           [-1.3168e-01, -2.4058e-02,  3.6830e-02,  ..., -7.4494e-02,  1.9291e-02,  1.9388e-01],
           ...,
           [-5.2774e-02, -4.0233e-02,  9.2910e-02,  ..., -5.8774e-02, -1.5357e-01,  1.1001e-01],
           [-1.7099e-02,  1.8250e-02, -4.6329e-02,  ...,  6.6825e-02, -1.2077e-02,  3.1235e-03],
           [ 1.4510e-01, -5.4642e-02,  6.4931e-02,  ..., -9.4708e-02, -3.8029e-02, -4.9007e-02]],

          ...,

          [[ 7.8344e-02, -1.1744e-01,  4.5762e-02,  ...,  8.1885e-02, -1.8423e-01, -2.5853e-01],
           [ 1.5104e-01,  1.3606e-01,  1.8624e-01,  ..., -5.8669e-02, -5.1531e-02,  4.0259e-02],
           [-7.0799e-02,  1.2330e-01,  1.0800e-01,  ...,  1.8479e-01,  1.0172e-01,  5.6292e-02],
           ...,
           [-2.1590e-02, -8.6972e-02, -6.2252e-02,  ...,  8.6910e-02,  6.5057e-02,  1.4466e-01],
           [ 6.8270e-02,  2.1628e-03, -6.0358e-02,  ..., -5.5047e-03, -1.0802e-01,  4.6739e-03],
           [-1.6478e-01, -1.3016e-01,  6.6783e-02,  ..., -3.6049e-02,  2.0054e-02,  3.9753e-02]],

          [[ 6.8068e-02,  1.9337e-01,  6.8574e-02,  ..., -1.6961e-01,  3.4825e-02,  2.6836e-02],
           [ 1.3590e-01,  3.9619e-02,  1.5557e-02,  ..., -1.7045e-02, -1.3137e-01, -1.0834e-01],
           [-1.1705e-01, -1.3664e-01,  1.3707e-01,  ..., -9.5097e-02, -9.1000e-02,  5.6898e-02],
           ...,
           [ 4.9291e-02,  9.9192e-02,  4.0430e-02,  ..., -4.4258e-02, -2.0023e-01, -1.1939e-02],
           [ 1.4013e-01,  5.0118e-02, -4.9968e-02,  ..., -7.1298e-02,  1.2720e-01,  8.9132e-02],
           [ 2.4730e-01,  9.0345e-02,  2.7826e-02,  ...,  7.9562e-03, -6.2486e-02,  2.4330e-02]],

          [[ 2.4601e-02, -1.0962e-03, -7.7167e-02,  ...,  4.8090e-02,  1.2605e-01,  1.7156e-02],
           [-4.1536e-02,  5.6286e-03,  7.8608e-02,  ...,  1.0727e-02,  8.1861e-02, -4.8044e-02],
           [ 6.8107e-02,  4.6520e-02, -3.7342e-02,  ...,  2.4793e-03,  5.8398e-03, -2.4349e-02],
           ...,
           [ 5.3613e-02, -7.9468e-02,  1.5399e-02,  ..., -7.6106e-02,  1.0546e-02, -7.8059e-02],
           [-2.9098e-02, -6.5092e-02,  1.0542e-01,  ...,  7.4333e-02, -6.7984e-02, -5.5431e-02],
           [ 9.2425e-02, -6.5663e-02,  1.3616e-01,  ...,  1.7706e-02,  6.8624e-03,  3.1742e-02]]],


         [[[-2.5137e-01, -1.6004e-01, -1.4045e-01,  ..., -4.1192e-02,  1.1616e-01,  1.1606e-02],
           [ 1.1826e-01,  9.4407e-02, -3.9061e-02,  ..., -1.0121e-02,  2.6305e-02, -3.4187e-02],
           [-1.6637e-01, -1.8555e-01, -1.6449e-01,  ...,  8.2417e-02,  1.9940e-02, -1.6298e-01],
           ...,
           [-7.5283e-02, -1.2586e-01, -1.1659e-01,  ...,  7.5816e-02, -7.2085e-02,  1.0836e-01],
           [-1.0992e-01,  7.5829e-02,  1.7596e-01,  ..., -2.1457e-01,  2.6796e-03, -1.9599e-02],
           [ 2.6276e-02,  1.3405e-01, -3.0332e-02,  ...,  4.1152e-02, -6.3545e-02,  1.1104e-01]],

          [[-3.8619e-02, -6.3297e-02,  1.4330e-01,  ...,  4.1437e-02, -8.0857e-02,  1.0733e-01],
           [ 3.3639e-02, -1.1544e-01,  1.9091e-01,  ...,  3.7294e-02,  8.1488e-02,  8.7895e-02],
           [ 5.9246e-02, -1.0519e-01,  9.6855e-02,  ...,  1.6777e-01,  1.9872e-01,  5.2302e-02],
           ...,
           [-1.2931e-01, -1.1355e-01, -1.5365e-01,  ...,  2.8062e-02, -2.0052e-01, -2.2637e-02],
           [ 1.8186e-01,  6.3276e-02,  1.6883e-01,  ...,  5.4982e-02, -5.0337e-02, -1.1940e-01],
           [ 5.4483e-02,  5.3153e-02, -1.4614e-01,  ..., -2.7174e-02,  5.2334e-02, -6.5714e-02]],

          [[-2.9943e-02, -1.0253e-01, -1.3645e-01,  ..., -7.9275e-02,  1.7843e-02,  7.9841e-02],
           [-1.2353e-01,  1.2483e-01,  1.1593e-02,  ..., -1.3113e-01, -5.5714e-02,  4.5987e-02],
           [-5.2960e-03, -2.4055e-02,  1.5794e-01,  ...,  4.9075e-02,  1.9109e-02, -3.8444e-03],
           ...,
           [-4.4710e-03,  1.1737e-01, -8.8650e-02,  ..., -2.5239e-01,  2.8058e-02,  7.5966e-02],
           [-1.3520e-01,  1.6320e-01,  1.6901e-01,  ..., -3.0954e-03, -1.6139e-01,  1.5636e-01],
           [-1.3019e-01,  7.6449e-02, -7.0593e-02,  ...,  9.1612e-02, -1.4228e-01,  3.6213e-03]],

          ...,

          [[-1.0034e-01,  1.7253e-02,  1.4296e-02,  ..., -6.7772e-02,  7.4818e-02,  1.5458e-01],
           [ 1.5958e-01, -1.0362e-02,  1.9028e-01,  ..., -1.7366e-01, -5.4262e-02,  5.4229e-03],
           [ 2.9495e-01, -6.8347e-02, -1.3774e-01,  ..., -1.2556e-01,  1.3071e-01,  8.9571e-02],
           ...,
           [ 1.4545e-02, -4.1154e-03, -8.8671e-02,  ...,  9.5013e-02, -4.2758e-02,  7.7481e-03],
           [ 1.0249e-01,  3.9284e-02,  5.2892e-02,  ..., -2.4471e-01,  1.1150e-01,  3.0941e-02],
           [ 2.0154e-02,  8.5756e-03,  5.1669e-02,  ...,  7.4264e-02, -1.3646e-01,  3.7713e-02]],

          [[-1.0119e-01, -8.9445e-02,  5.7797e-02,  ...,  1.1129e-01,  1.1560e-02,  3.6053e-02],
           [-3.8728e-02,  1.1337e-01, -8.9299e-02,  ...,  1.3843e-01,  1.6360e-02,  6.8982e-02],
           [ 3.2424e-02,  1.5661e-01, -9.6948e-02,  ...,  2.7097e-02, -2.3116e-02,  2.0026e-01],
           ...,
           [-4.9847e-02,  5.1408e-02, -7.4192e-02,  ...,  3.5990e-02, -6.6430e-02, -6.0827e-02],
           [-5.3493e-03,  1.3731e-01, -2.0977e-01,  ..., -1.5301e-01,  1.7563e-02, -9.7998e-02],
           [ 1.0924e-01,  1.7407e-01,  6.8582e-02,  ..., -1.3367e-02,  1.4670e-01, -1.8687e-01]],

          [[ 1.0590e-01, -2.5646e-02,  1.5558e-01,  ..., -1.8605e-02, -3.8436e-02, -5.9998e-02],
           [-4.3419e-02, -6.7019e-02, -6.3143e-02,  ...,  1.1907e-01,  1.3842e-02, -1.0465e-02],
           [ 7.7507e-02, -3.2860e-02,  1.3988e-02,  ...,  1.0598e-02,  1.4237e-02,  2.6707e-02],
           ...,
           [-5.9025e-02, -8.9239e-02,  2.4009e-02,  ..., -1.0003e-01, -5.6932e-02,  1.8226e-02],
           [-9.4219e-02, -5.4900e-02,  1.2306e-01,  ...,  4.8048e-02, -1.4872e-01,  1.2421e-02],
           [ 4.0638e-03, -1.4458e-01, -1.1111e-01,  ..., -1.2543e-01, -5.7009e-02,  1.5567e-01]]]],



        [[[[ 4.9479e-02, -1.2024e-01,  1.1941e-01,  ...,  9.2390e-02, -5.5908e-03, -3.8113e-02],
           [ 9.6079e-02, -1.2961e-01,  3.4854e-02,  ...,  1.6414e-01, -2.9110e-02,  1.1638e-01],
           [ 2.2255e-01, -8.4113e-02,  1.5673e-01,  ...,  4.6770e-02, -6.3248e-02,  3.6322e-02],
           ...,
           [-6.7842e-02, -6.3994e-02,  1.8403e-01,  ..., -1.0062e-01,  3.8798e-03, -2.4361e-02],
           [ 1.1814e-01,  5.6327e-02, -4.9480e-02,  ...,  2.3676e-02,  7.2667e-02, -1.3643e-01],
           [-2.0406e-02, -1.9831e-01, -5.9895e-02,  ...,  4.3266e-02,  5.1750e-02,  5.8714e-02]],

          [[-7.1710e-03, -1.1270e-01,  1.1113e-01,  ...,  1.4785e-01, -6.1713e-02,  4.0874e-02],
           [-1.5380e-02,  1.1079e-01, -5.5011e-02,  ..., -1.3959e-01, -1.6709e-01, -9.0185e-02],
           [ 7.2040e-02,  2.4002e-02, -8.3302e-03,  ..., -5.1623e-03,  1.8511e-02, -4.0077e-02],
           ...,
           [-2.7564e-01,  2.1107e-02, -1.7145e-01,  ...,  2.9391e-02,  2.1986e-02,  5.2382e-02],
           [ 1.1559e-02, -1.6240e-02, -5.4718e-02,  ..., -1.7096e-02,  7.9552e-02,  1.4976e-01],
           [-2.2953e-01,  1.4262e-01, -2.3251e-02,  ..., -8.0583e-02, -1.9912e-02,  1.9902e-02]],

          [[ 2.4438e-02,  1.6666e-01, -1.5331e-01,  ...,  6.5325e-02, -1.1366e-01, -1.1629e-01],
           [ 1.0733e-01, -2.9006e-02, -4.5896e-02,  ..., -6.0463e-02, -1.2201e-01, -2.1923e-02],
           [-1.8256e-01, -4.0270e-02, -6.8529e-02,  ..., -1.1619e-01, -4.6067e-03,  3.0066e-02],
           ...,
           [ 6.2903e-03,  3.9714e-02,  1.7300e-02,  ..., -8.2172e-02,  1.5318e-01, -8.9593e-03],
           [-1.2905e-01, -1.6068e-02,  1.2707e-01,  ...,  4.6463e-02, -1.6052e-01, -1.5450e-01],
           [-1.7904e-03, -1.6796e-01, -1.5720e-01,  ..., -1.9348e-02, -1.2647e-01,  2.0894e-01]],

          ...,

          [[ 2.8003e-03,  3.2853e-02,  1.4075e-02,  ...,  8.9401e-02, -2.7583e-01, -3.3725e-02],
           [ 1.4423e-01, -1.0672e-01,  2.3130e-02,  ..., -1.1733e-03, -2.5849e-02, -3.2961e-02],
           [ 7.1608e-02,  4.3221e-02,  1.5366e-01,  ...,  1.7713e-01,  6.6391e-02, -3.4493e-02],
           ...,
           [-1.4384e-01,  6.4884e-03, -1.5870e-02,  ..., -1.6315e-01,  1.3371e-01, -6.0458e-02],
           [ 4.7940e-02, -1.5425e-01, -9.7291e-02,  ...,  2.8684e-02,  1.6651e-01, -5.2837e-03],
           [ 1.9383e-02,  1.1520e-01, -7.4215e-02,  ...,  1.7351e-01, -4.9127e-02, -2.0613e-02]],

          [[ 5.6445e-03,  5.0734e-02, -8.0981e-02,  ..., -1.5653e-01, -4.1201e-02, -1.8769e-01],
           [ 1.0425e-01, -8.7267e-02, -5.0757e-02,  ..., -2.2610e-02,  2.1985e-01, -1.0785e-01],
           [ 1.1597e-01,  8.1362e-02,  2.2384e-01,  ...,  1.5502e-01,  1.0440e-01, -1.0629e-01],
           ...,
           [-3.8381e-03,  6.9779e-02,  2.3959e-02,  ..., -6.7214e-02,  8.1398e-02, -1.5665e-01],
           [ 1.3994e-02,  2.1955e-01, -7.4728e-02,  ...,  1.3176e-02, -2.1141e-01,  9.7762e-02],
           [-1.9766e-01,  6.4118e-02, -1.1691e-01,  ...,  3.3424e-04,  2.2248e-02,  7.5333e-02]],

          [[ 3.7439e-02,  7.5373e-02,  3.1448e-02,  ...,  9.7213e-02, -1.5274e-01,  2.1072e-02],
           [ 5.6471e-02,  5.0180e-02, -1.1843e-02,  ...,  8.3973e-03,  1.8481e-02, -1.4770e-02],
           [ 7.7144e-02, -9.8065e-02, -6.2051e-02,  ..., -7.3049e-03,  4.3985e-02, -8.2760e-02],
           ...,
           [ 1.7416e-02,  2.5467e-03,  7.3432e-02,  ...,  4.3597e-03, -9.8592e-02,  5.8249e-02],
           [ 1.3773e-01,  1.5546e-01,  1.7297e-02,  ..., -2.0189e-02,  1.1900e-01,  6.5719e-02],
           [ 1.6277e-02,  2.0287e-02, -3.6481e-02,  ..., -1.9600e-02, -2.5373e-02, -4.0846e-02]]],


         [[[ 9.2726e-02, -1.1399e-01,  1.4617e-01,  ..., -2.0922e-01, -1.9948e-03, -1.7182e-01],
           [ 2.9524e-03, -1.9855e-02,  1.5285e-01,  ...,  1.4551e-01,  1.7623e-02, -1.5013e-02],
           [ 1.4358e-02, -2.1286e-02, -5.7407e-02,  ..., -1.7788e-01, -9.7753e-02,  2.7251e-01],
           ...,
           [ 2.2624e-02,  4.6064e-02,  1.1007e-01,  ...,  5.8342e-02,  7.6900e-02, -1.0694e-01],
           [ 1.1755e-01,  1.2866e-01,  4.5644e-02,  ...,  3.9223e-02, -3.8653e-02,  7.9349e-02],
           [-1.0068e-01, -2.2474e-02, -1.8486e-02,  ..., -6.0086e-02, -8.5980e-02, -7.2841e-02]],

          [[-1.2723e-01, -3.3138e-02, -1.3975e-01,  ..., -4.1295e-02,  9.1083e-03,  2.3233e-02],
           [-5.9784e-02, -1.7853e-01,  1.6844e-01,  ...,  5.6460e-02, -9.2860e-02,  2.6440e-01],
           [-3.9084e-02,  4.8447e-02,  3.2649e-02,  ..., -7.0871e-02,  9.6554e-02, -4.9183e-02],
           ...,
           [ 2.6058e-02, -2.1307e-02,  5.2868e-02,  ..., -4.1602e-02,  4.4140e-02, -3.2434e-02],
           [ 8.9249e-02,  4.0452e-02, -9.0696e-03,  ..., -7.6812e-02, -1.3192e-01,  4.5488e-02],
           [-1.1958e-01, -2.8264e-02,  2.7817e-01,  ..., -4.4197e-03, -2.1852e-01,  6.2743e-02]],

          [[ 5.6544e-02, -4.0823e-02,  7.1283e-02,  ...,  3.1900e-02,  7.7130e-03, -9.0323e-02],
           [-2.6640e-01,  5.3334e-02,  1.0901e-02,  ...,  2.1655e-02, -1.0428e-01,  1.0456e-02],
           [-7.4492e-02,  3.6163e-02,  1.2804e-01,  ...,  4.6190e-02, -4.3939e-02,  9.8000e-02],
           ...,
           [-8.6682e-02,  1.9028e-01,  1.0499e-01,  ..., -1.8128e-02, -1.1148e-01, -1.8663e-02],
           [ 9.6069e-02,  9.3418e-02,  1.2242e-01,  ..., -4.9614e-02, -5.5555e-02,  3.0320e-02],
           [-2.2314e-01,  1.8255e-01, -4.8293e-02,  ..., -2.3645e-01, -1.5097e-02, -9.8190e-03]],

          ...,

          [[-6.8868e-03,  4.7469e-02, -1.2402e-01,  ..., -7.2076e-02, -8.2667e-02,  1.5854e-02],
           [ 8.2704e-02, -9.1504e-02, -6.1343e-02,  ...,  1.9143e-02,  3.3390e-02,  5.9150e-02],
           [-1.2735e-01, -1.7988e-02, -2.8300e-02,  ...,  7.0917e-02,  1.4233e-02,  2.7213e-02],
           ...,
           [-1.1651e-01, -1.1819e-01,  6.6483e-02,  ..., -3.8331e-02,  1.4125e-03,  8.9153e-02],
           [-5.4932e-02, -6.8875e-02, -1.1650e-01,  ...,  1.4876e-02, -1.1140e-01,  1.7018e-02],
           [-5.8416e-02,  1.0480e-01, -1.2339e-01,  ..., -1.1429e-01,  1.1029e-01, -6.2981e-02]],

          [[-7.5865e-02,  1.7099e-02,  1.9110e-01,  ..., -8.3350e-02, -2.0472e-01,  7.6263e-02],
           [ 2.3225e-02,  1.1944e-01,  1.2384e-01,  ...,  1.0583e-01,  6.4323e-02, -8.1213e-03],
           [-7.4442e-02, -1.1770e-01, -9.7853e-02,  ...,  2.3779e-01,  6.8417e-02,  1.1714e-01],
           ...,
           [-4.2736e-02,  2.1218e-01, -4.8883e-02,  ...,  7.8720e-02, -2.2520e-01,  1.2828e-01],
           [-1.7484e-01,  5.8112e-02, -8.5343e-02,  ...,  1.8746e-01,  1.0496e-01,  5.6290e-02],
           [-8.0010e-02,  2.0689e-01,  7.1166e-02,  ..., -1.4119e-01,  8.4252e-02,  1.0582e-01]],

          [[-7.3289e-02,  1.3130e-01, -5.6630e-02,  ..., -2.2203e-02,  2.3706e-02, -1.1913e-01],
           [ 8.1959e-03, -1.8902e-02, -3.6733e-02,  ..., -1.0477e-02, -3.1416e-02, -1.5087e-01],
           [ 2.3322e-03,  2.8819e-02,  2.0381e-04,  ..., -1.5060e-02,  6.8054e-02,  3.3361e-02],
           ...,
           [ 1.5675e-01, -5.3840e-02, -2.1747e-02,  ..., -8.3543e-02, -6.1943e-02, -6.5699e-02],
           [-3.6262e-02,  3.7457e-02,  4.0663e-02,  ...,  1.4156e-01, -1.9701e-02,  2.6879e-02],
           [-6.4447e-02, -4.8616e-02, -5.8091e-02,  ...,  1.4200e-01,  8.3533e-03,  1.6116e-01]]],


         [[[ 1.3960e-01,  1.1435e-01, -8.6273e-02,  ...,  3.9386e-02,  1.5579e-02,  3.5286e-02],
           [ 8.9155e-02,  7.3408e-02,  2.2774e-02,  ...,  1.3460e-02,  7.1611e-02,  3.3288e-02],
           [ 4.3412e-04,  1.3376e-01,  2.8157e-02,  ..., -6.7847e-02,  1.5561e-01, -2.5508e-02],
           ...,
           [-1.0684e-01, -7.3045e-02, -8.7974e-02,  ...,  3.2581e-03, -1.9732e-02,  1.1011e-02],
           [ 1.5485e-01, -8.4909e-02,  2.6662e-03,  ...,  5.7097e-02,  1.0911e-01,  1.3882e-01],
           [ 1.9063e-01,  9.4684e-02,  8.8377e-02,  ...,  3.0477e-02,  1.9453e-01, -1.2343e-02]],

          [[-5.1542e-02, -5.8934e-02, -2.5614e-01,  ..., -1.2323e-01,  6.1581e-02,  8.8427e-02],
           [-7.7451e-02,  1.0482e-01, -2.9115e-03,  ...,  9.2531e-02,  1.0922e-01, -1.9115e-02],
           [-4.0012e-02,  8.6145e-02,  1.3108e-01,  ...,  7.7982e-02,  1.3969e-01,  8.0564e-02],
           ...,
           [ 7.6876e-02,  1.4945e-01,  9.3836e-02,  ...,  1.5914e-01,  5.4412e-02, -1.1336e-01],
           [ 1.7871e-02, -5.9298e-02,  2.7369e-02,  ...,  1.6330e-02, -5.5912e-02,  5.0106e-02],
           [ 1.9769e-01, -1.9031e-02, -1.4525e-01,  ..., -1.2774e-01,  2.1266e-02,  2.8177e-02]],

          [[ 7.5874e-02, -1.4304e-01, -1.4470e-01,  ...,  2.6628e-02, -3.8856e-02, -1.6576e-01],
           [-1.5087e-02,  1.0904e-01, -5.3951e-02,  ..., -8.4747e-02, -4.6491e-02, -3.3346e-02],
           [-9.4628e-02,  3.4476e-03, -5.9785e-02,  ..., -3.9196e-02,  3.5183e-02, -4.8972e-03],
           ...,
           [-4.5382e-02, -1.4515e-01,  3.2151e-02,  ...,  1.4668e-01,  4.9729e-02,  6.6255e-02],
           [-6.5420e-04, -2.1877e-01,  2.0180e-01,  ...,  6.0162e-02,  5.3112e-02, -1.0800e-01],
           [ 9.9567e-02,  9.3137e-03, -6.5244e-03,  ..., -9.9934e-02,  1.8326e-01, -1.8880e-01]],

          ...,

          [[-1.6396e-02, -1.0665e-01,  1.2208e-03,  ..., -3.8902e-02,  1.4995e-01, -6.1154e-02],
           [ 1.8266e-01,  1.2992e-01,  1.0842e-01,  ..., -2.6161e-02, -8.7786e-02, -1.6010e-01],
           [-4.7322e-02, -1.4198e-01,  4.7394e-02,  ..., -2.0817e-01,  3.9870e-02,  2.4351e-01],
           ...,
           [ 8.1115e-02, -5.5260e-02,  1.9041e-01,  ..., -8.9211e-02, -4.2370e-02, -6.0787e-02],
           [ 4.1063e-02,  8.7441e-02, -4.1348e-02,  ...,  1.9379e-01,  2.0987e-01,  1.8273e-01],
           [-1.5969e-01, -9.0732e-02,  1.1633e-01,  ..., -4.8576e-02, -1.9827e-01, -1.2951e-01]],

          [[-9.4259e-02, -6.1569e-02,  9.4483e-02,  ...,  1.1483e-01,  9.8162e-02,  1.7014e-01],
           [-3.3393e-02, -2.2467e-01, -4.7647e-02,  ...,  3.5670e-02, -3.5821e-02,  2.6606e-02],
           [-6.1842e-02,  1.0256e-01, -2.5175e-02,  ...,  4.7749e-03, -3.3177e-02, -9.1323e-02],
           ...,
           [-5.4726e-02,  9.9870e-03, -1.4035e-02,  ...,  7.8040e-02,  7.5342e-02, -2.1645e-03],
           [ 4.3128e-02, -1.8811e-01, -1.1621e-02,  ...,  1.3730e-01, -7.6928e-02,  1.5547e-01],
           [-1.0874e-02, -2.4495e-01, -7.3124e-02,  ...,  5.6757e-02, -7.1673e-02, -1.2366e-02]],

          [[-2.7977e-02, -2.2801e-02, -1.0360e-02,  ...,  7.3751e-02, -4.8019e-02, -2.9272e-02],
           [-3.6727e-03, -2.3376e-02, -6.8562e-02,  ...,  5.9415e-02,  1.8443e-02,  3.1423e-02],
           [ 1.2272e-01, -6.4543e-02,  1.7023e-02,  ...,  4.0720e-02, -8.4984e-02, -4.7682e-02],
           ...,
           [-1.8266e-02, -2.6709e-02, -2.3440e-02,  ..., -8.6123e-03,  3.0666e-02,  3.0670e-02],
           [-3.2164e-02,  6.3797e-02, -1.0125e-01,  ...,  2.5567e-02,  6.0925e-02, -8.5984e-02],
           [ 1.8331e-02, -1.0522e-01,  2.1780e-02,  ...,  3.3483e-02,  1.3718e-01, -1.1484e-01]]]]])

2025-07-09 00:39:13.358505 GPU 5 72944 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 11184811],"float16"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:39:18.357958 GPU 7 73103 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 11184811],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 11184811],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:39:27.614436 GPU 3 73263 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:39:46.341753 GPU 4 73659 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:39:58.437347 GPU 2 73820 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:40:42.593387 GPU 5 73991 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:41:35.667598 GPU 7 73103 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:42:12.803621 GPU 3 74423 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:42:24.803325 GPU 2 73820 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:42:52.703305 GPU 4 73659 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:43:35.347883 GPU 2 73820 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:43:41.635139 GPU 5 73991 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:43:59.518439 GPU 4 73659 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:44:46.980978 GPU 2 73820 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 4194305, 8, 8, 8],"float16"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:44:53.326230 GPU 5 73991 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 5592406, 8, 8, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:45:11.107335 GPU 4 73659 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 65536, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 65536, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:45:39.056374 GPU 5 75499 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 65536, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 65536, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:46:10.507354 GPU 7 75664 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 65536, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 65536, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:46:16.167885 GPU 2 75823 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 65536, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[0,0,0,0,0,0,], divisor_override=8, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 65536, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[0,0,0,0,0,0,], divisor_override=8, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:46:47.134894 GPU 4 73659 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 65536, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 65536, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:47:18.127607 GPU 4 73659 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 8, 8, 5592406, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NDHWC", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([2, 8, 8, 5592406, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NDHWC", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:47:23.153851 GPU 5 75499 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 8, 8, 8, 2097153],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=False, exclusive=False, divisor_override=None, data_format="NDHWC", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:48:00.677744 GPU 2 75823 test begin: paddle.nn.functional.avg_pool3d(Tensor([2796203, 3, 8, 8, 8],"float16"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:48:02.379418 GPU 7 75664 test begin: paddle.nn.functional.avg_pool3d(Tensor([3, 1, 127827, 1600, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([3, 1, 127827, 1600, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:50:36.925683 GPU 7 77488 test begin: paddle.nn.functional.avg_pool3d(Tensor([3, 1, 7, 127827, 1600],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([3, 1, 7, 127827, 1600],"float32"), kernel_size=tuple(5,1,1,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:51:01.604637 GPU 2 77650 test begin: paddle.nn.functional.avg_pool3d(Tensor([3, 1, 7, 3, 68174085],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([3, 1, 7, 3, 68174085],"float32"), kernel_size=tuple(5,1,1,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:51:10.291708 GPU 4 77808 test begin: paddle.nn.functional.avg_pool3d(Tensor([3, 1, 7, 40, 5113057],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([3, 1, 7, 40, 5113057],"float32"), kernel_size=tuple(5,1,1,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:52:44.920785 GPU 7 77488 test begin: paddle.nn.functional.avg_pool3d(Tensor([3, 127827, 40, 40, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([3, 127827, 40, 40, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:53:15.947158 GPU 2 77650 test begin: paddle.nn.functional.avg_pool3d(Tensor([3, 127827, 7, 40, 40],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([3, 127827, 7, 40, 40],"float32"), kernel_size=tuple(5,1,1,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:53:26.833595 GPU 4 77808 test begin: paddle.nn.functional.avg_pool3d(Tensor([3, 42609, 3, 1600, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([3, 42609, 3, 1600, 7],"float32"), kernel_size=tuple(1,1,5,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:53:31.158119 GPU 7 77488 test begin: paddle.nn.functional.avg_pool3d(Tensor([3, 42609, 7, 3, 1600],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([3, 42609, 7, 3, 1600],"float32"), kernel_size=tuple(5,1,1,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:54:15.385751 GPU 7 77488 test begin: paddle.nn.functional.avg_pool3d(Tensor([383480, 1, 7, 40, 40],"float32"), kernel_size=tuple(5,1,1,), stride=1, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([383480, 1, 7, 40, 40],"float32"), kernel_size=tuple(5,1,1,), stride=1, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:54:16.141808 GPU 2 77650 test begin: paddle.nn.functional.avg_pool3d(Tensor([43691, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([43691, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:54:21.478832 GPU 4 77808 test begin: paddle.nn.functional.avg_pool3d(Tensor([43691, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([43691, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:54:50.039875 GPU 2 77650 test begin: paddle.nn.functional.avg_pool3d(Tensor([43691, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([43691, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=0, divisor_override=8, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:54:54.127517 GPU 4 77808 test begin: paddle.nn.functional.avg_pool3d(Tensor([43691, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[0,0,0,0,0,0,], divisor_override=8, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([43691, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding=list[0,0,0,0,0,0,], divisor_override=8, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:54:59.048027 GPU 7 77488 test begin: paddle.nn.functional.avg_pool3d(Tensor([43691, 3, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, )
[cuda error] backward paddle.nn.functional.avg_pool3d(Tensor([43691, 3, 32, 32, 32],"float32"), kernel_size=2, stride=None, padding="SAME", ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCDHW", name=None, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:55:24.385436 GPU 2 77650 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([10700, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([10700, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:55:26.538511 GPU 4 77808 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([10700, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([10700, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:55:28.363797 GPU 7 77488 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:55:47.697298 GPU 5 78499 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:55:48.648720 GPU 2 77650 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:55:50.428591 GPU 4 77808 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:56:03.088282 GPU 3 78656 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:57:25.415390 GPU 7 78815 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:57:53.364163 GPU 4 78972 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:57:53.914751 GPU 2 79053 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:58:47.199273 GPU 5 78499 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], exclusive=True, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], exclusive=True, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 00:59:27.510228 GPU 3 79545 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 3, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:59:48.490044 GPU 7 79703 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([1398102, 8, 8, 8, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NDHWC", exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 00:59:56.535777 GPU 5 78499 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, exclusive=True, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, exclusive=True, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:00:01.065102 GPU 4 79862 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:00:04.592884 GPU 2 80019 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:01:07.546426 GPU 5 78499 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:01:44.535096 GPU 3 80181 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:01:52.458560 GPU 7 80339 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:02:13.344498 GPU 5 78499 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], exclusive=False, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], exclusive=False, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:02:27.621174 GPU 2 80499 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:02:49.019627 GPU 4 79862 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:03:40.734910 GPU 4 80666 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:04:08.224766 GPU 3 80824 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 2097153, 8, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:04:15.409292 GPU 5 80981 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 11184811, 8, 8],"float16"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:04:29.773972 GPU 7 81140 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 11184811, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 11184811, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:05:07.346207 GPU 2 81299 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:05:50.708517 GPU 4 81461 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:05:55.451913 GPU 5 81619 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:06:35.087994 GPU 3 81784 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:06:35.181295 GPU 7 81140 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:07:21.775017 GPU 2 81944 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:08:15.150983 GPU 5 82105 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:08:36.186091 GPU 7 82262 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:09:01.302868 GPU 4 81461 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], exclusive=True, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], exclusive=True, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:09:37.282161 GPU 2 82587 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:10:08.967957 GPU 4 81461 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), exclusive=True, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 5592406, 8, 8],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), exclusive=True, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:13:47.011505 GPU 4 84888 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 11184811],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 11184811],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:13:52.485694 GPU 5 85116 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=2, padding=1, exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:13:59.000125 GPU 2 85274 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:13:59.077687 GPU 7 85338 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], exclusive=False, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[1,2,3,], padding=list[0,0,0,], exclusive=False, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:15:13.130540 GPU 3 85595 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:16:01.156132 GPU 4 84888 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], ceil_mode=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:16:10.480229 GPU 5 85762 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=1, exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:16:38.025018 GPU 7 85338 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], exclusive=False, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[0,0,0,], exclusive=False, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:16:57.981733 GPU 2 85274 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], exclusive=True, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,0,0,], exclusive=True, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:17:46.008330 GPU 7 85338 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], exclusive=True, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,1,1,], exclusive=True, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:18:04.441797 GPU 4 85943 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], exclusive=True, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=list[1,0,0,], exclusive=True, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:18:06.135338 GPU 2 85274 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), exclusive=True, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 3, 8, 8, 5592406],"float64"), kernel_size=list[3,3,3,], stride=tuple(3,2,1,), padding=tuple(1,0,0,), exclusive=True, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:18:38.958591 GPU 5 86101 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 4194304, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 4194304, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:18:57.367580 GPU 7 85338 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 4194305, 8, 8, 8],"float16"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:18:58.463545 GPU 3 86259 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 5592406, 8, 8, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NDHWC", exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:19:56.780232 GPU 2 86417 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 8, 8, 5592406, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NDHWC", exclusive=False, )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2, 8, 8, 5592406, 3],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NDHWC", exclusive=False, ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:20:25.922568 GPU 7 86577 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2, 8, 8, 8, 2097153],"float64"), kernel_size=list[3,3,3,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NDHWC", exclusive=False, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:20:36.497789 GPU 4 85943 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2796203, 3, 8, 8, 8],"float16"), kernel_size=list[3,3,3,], stride=list[3,2,1,], padding=list[1,2,1,], exclusive=True, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 01:20:59.408068 GPU 5 86101 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([2796203, 3, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([2796203, 3, 8, 8, 8],"float32"), kernel_size=list[3,3,3,], stride=list[1,1,1,], padding=list[0,0,0,], ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:21:12.927681 GPU 3 86751 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 9363],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 9363],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:21:48.687689 GPU 5 86101 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 9363, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 9363, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:22:04.432358 GPU 4 86908 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 5350, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 5350, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:22:16.761821 GPU 5 86101 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 9363],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 9363],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:22:37.322264 GPU 7 87068 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 9363, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 9363, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:22:38.779822 GPU 2 86417 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 42800, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 42800, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:22:39.431561 GPU 5 86101 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2739138, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([8, 2739138, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:23:03.069258 GPU 5 86101 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 342393, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[cuda error] backward paddle.nn.functional.avg_pool3d(x=Tensor([8, 342393, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 
 (External) CUDA error(701), too many resources requested for launch. 
  [Hint: 'cudaErrorLaunchOutOfResources'. This indicates that a launch did not occur because it did not have appropriate resources. Although this error is similar tocudaErrorInvalidConfiguration, this error usually indicates that the user has attempted to pass too many arguments to the device kernel, or the kernellaunch specifies too many threads for the kernel's register count.] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1391)


2025-07-09 01:31:54.194290 GPU 7 89559 test begin: paddle.nn.functional.batch_norm(Tensor([119304648, 4, 3, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=False, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([119304648, 4, 3, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=False, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=False, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:32:10.327395 GPU 4 89716 test begin: paddle.nn.functional.batch_norm(Tensor([119304648, 4, 3, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=True, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=True, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([119304648, 4, 3, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=True, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=True, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:35:40.752864 GPU 5 90664 test begin: paddle.nn.functional.batch_norm(Tensor([1320430, 6, 12, 24],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), training=False, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([1320430, 6, 12, 24],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), training=False, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=False, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:37:03.915367 GPU 5 90664 test begin: paddle.nn.functional.batch_norm(Tensor([16, 16, 16, 1048576],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=False, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([16, 16, 16, 1048576],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=False, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=True, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:37:26.276844 GPU 7 91297 test begin: paddle.nn.functional.batch_norm(Tensor([16, 16, 16, 1048576],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=True, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([16, 16, 16, 1048576],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=True, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=False, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:39:22.046433 GPU 3 91990 test begin: paddle.nn.functional.batch_norm(Tensor([16, 16, 2097152, 8],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=False, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([16, 16, 2097152, 8],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=False, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=True, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:40:04.974189 GPU 7 91297 test begin: paddle.nn.functional.batch_norm(Tensor([16, 16, 2097152, 8],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=True, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([16, 16, 2097152, 8],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=True, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=False, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:43:59.865261 GPU 5 93508 test begin: paddle.nn.functional.batch_norm(Tensor([2, 1, 2, 570425345],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([2, 1, 2, 570425345],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:44:02.243349 GPU 2 93595 test begin: paddle.nn.functional.batch_norm(Tensor([2, 1, 380283564, 3],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([2, 1, 380283564, 3],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:48:02.977155 GPU 2 94705 test begin: paddle.nn.functional.batch_norm(Tensor([2, 190141782, 2, 3],"float32"), Tensor([190141782],"float32"), Tensor([190141782],"float32"), Tensor([190141782],"float32"), Tensor([190141782],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([2, 190141782, 2, 3],"float32"), Tensor([190141782],"float32"), Tensor([190141782],"float32"), Tensor([190141782],"float32"), Tensor([190141782],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:48:06.288688 GPU 4 94932 test begin: paddle.nn.functional.batch_norm(Tensor([2, 238609295, 3, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=False, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([2, 238609295, 3, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=False, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=False, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:48:11.443314 GPU 3 95089 test begin: paddle.nn.functional.batch_norm(Tensor([2, 238609295, 3, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=True, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=True, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([2, 238609295, 3, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=True, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=True, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:52:12.865259 GPU 2 95724 test begin: paddle.nn.functional.batch_norm(Tensor([2, 4, 178956971, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=False, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([2, 4, 178956971, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=False, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=False, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:52:19.835608 GPU 3 95881 test begin: paddle.nn.functional.batch_norm(Tensor([2, 4, 178956971, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=True, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=True, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([2, 4, 178956971, 3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), Tensor([3],"float16"), training=True, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=True, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:56:37.628956 GPU 2 96986 test begin: paddle.nn.functional.batch_norm(Tensor([2097152, 16, 16, 8],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=False, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([2097152, 16, 16, 8],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=False, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=True, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 01:56:38.408426 GPU 7 97067 test begin: paddle.nn.functional.batch_norm(Tensor([2097152, 16, 16, 8],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=True, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([2097152, 16, 16, 8],"float16"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), Tensor([16],"float32"), training=True, momentum=0.1, epsilon=1e-05, data_format="NCHW", use_global_stats=False, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 02:07:49.627913 GPU 5 101661 test begin: paddle.nn.functional.batch_norm(Tensor([380283564, 1, 2, 3],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([380283564, 1, 2, 3],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 02:08:33.330176 GPU 7 101821 test begin: paddle.nn.functional.batch_norm(Tensor([4, 1980644, 12, 24],"float32"), Tensor([1980644],"float32"), Tensor([1980644],"float32"), Tensor([1980644],"float32"), Tensor([1980644],"float32"), training=False, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([4, 1980644, 12, 24],"float32"), Tensor([1980644],"float32"), Tensor([1980644],"float32"), Tensor([1980644],"float32"), Tensor([1980644],"float32"), training=False, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=False, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 02:13:15.480968 GPU 4 103560 test begin: paddle.nn.functional.batch_norm(Tensor([4, 6, 12, 7922575],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), training=False, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([4, 6, 12, 7922575],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), training=False, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=False, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 02:17:05.442826 GPU 5 104511 test begin: paddle.nn.functional.batch_norm(Tensor([4, 6, 3961288, 24],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), training=False, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.batch_norm(Tensor([4, 6, 3961288, 24],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), training=False, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=False, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/gpu/batch_norm_kernel.cu:692)


2025-07-09 02:59:18.405855 GPU 7 119068 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([1, 1, 4294967297],"float16"), label=Tensor([1, 1, 4294967297],"float16"), weight=None, reduction="mean", name=None, )
[accuracy error] backward  paddle.nn.functional.binary_cross_entropy(Tensor([1, 1, 4294967297],"float16"), label=Tensor([1, 1, 4294967297],"float16"), weight=None, reduction="mean", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1063165 / 4294967297 (0.0%)
Greatest absolute difference: nan at index (0, 0, 9396388) (up to 0.01 allowed)
Greatest relative difference: nan at index (0, 0, 1528) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 1, 4294967297]), dtype=torch.float16)
tensor([[[-0., -0., 0.,  ..., 0., -0., 0.]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([1, 1, 4294967297]), dtype=torch.float16)
tensor([[[-0., -0., 0.,  ..., 0., -0., 0.]]], dtype=torch.float16)

2025-07-09 02:59:23.532910 GPU 4 119225 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([1, 2147483649, 2],"float16"), label=Tensor([1, 2147483649, 2],"float16"), weight=None, reduction="mean", name=None, )
[accuracy error] backward  paddle.nn.functional.binary_cross_entropy(Tensor([1, 2147483649, 2],"float16"), label=Tensor([1, 2147483649, 2],"float16"), weight=None, reduction="mean", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1064935 / 4294967298 (0.0%)
Greatest absolute difference: nan at index (0, 122562, 0) (up to 0.01 allowed)
Greatest relative difference: nan at index (0, 415, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 2147483649, 2]), dtype=torch.float16)
tensor([[[0., 0.],
         [-0., 0.],
         [0., 0.],
         ...,
         [-0., -0.],
         [0., -0.],
         [-0., 0.]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([1, 2147483649, 2]), dtype=torch.float16)
tensor([[[0., 0.],
         [-0., 0.],
         [0., 0.],
         ...,
         [-0., -0.],
         [0., -0.],
         [-0., 0.]]], dtype=torch.float16)

2025-07-09 02:59:58.983346 GPU 3 119384 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([143165577, 30],"float16"), Tensor([143165577, 30],"float16"), None, "mean", None, )
[accuracy error] backward  paddle.nn.functional.binary_cross_entropy(Tensor([143165577, 30],"float16"), Tensor([143165577, 30],"float16"), None, "mean", None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1054600 / 4294967310 (0.0%)
Greatest absolute difference: nan at index (151916, 8) (up to 0.01 allowed)
Greatest relative difference: nan at index (105, 28) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([143165577, 30]), dtype=torch.float16)
tensor([[-0., -0., 0.,  ..., 0., -0., -0.],
        [-0., 0., 0.,  ..., 0., 0., -0.],
        [0., -0., 0.,  ..., -0., 0., 0.],
        ...,
        [-0., -0., -0.,  ..., 0., -0., 0.],
        [0., -0., -0.,  ..., 0., 0., 0.],
        [0., 0., -0.,  ..., -0., 0., 0.]], dtype=torch.float16)
DESIRED: (shape=torch.Size([143165577, 30]), dtype=torch.float16)
tensor([[-0., -0., 0.,  ..., 0., -0., -0.],
        [-0., 0., 0.,  ..., 0., 0., -0.],
        [0., -0., 0.,  ..., -0., 0., 0.],
        ...,
        [-0., -0., -0.,  ..., 0., -0., 0.],
        [0., -0., -0.,  ..., 0., 0., 0.],
        [0., 0., -0.,  ..., -0., 0., 0.]], dtype=torch.float16)

2025-07-09 03:01:11.849208 GPU 2 119541 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([143165577, 30],"float16"), Tensor([143165577, 30],"float16"), reduction="mean", )
[accuracy error] backward  paddle.nn.functional.binary_cross_entropy(Tensor([143165577, 30],"float16"), Tensor([143165577, 30],"float16"), reduction="mean", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1053442 / 4294967310 (0.0%)
Greatest absolute difference: nan at index (80139, 8) (up to 0.01 allowed)
Greatest relative difference: nan at index (196, 10) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([143165577, 30]), dtype=torch.float16)
tensor([[-0., 0., 0.,  ..., 0., 0., 0.],
        [0., -0., 0.,  ..., 0., -0., 0.],
        [-0., 0., -0.,  ..., 0., -0., -0.],
        ...,
        [-0., -0., -0.,  ..., -0., 0., -0.],
        [0., -0., -0.,  ..., 0., -0., -0.],
        [0., -0., -0.,  ..., 0., -0., 0.]], dtype=torch.float16)
DESIRED: (shape=torch.Size([143165577, 30]), dtype=torch.float16)
tensor([[-0., 0., 0.,  ..., 0., 0., 0.],
        [0., -0., 0.,  ..., 0., -0., 0.],
        [-0., 0., -0.,  ..., 0., -0., -0.],
        ...,
        [-0., -0., -0.,  ..., -0., 0., -0.],
        [0., -0., -0.,  ..., 0., -0., -0.],
        [0., -0., -0.,  ..., 0., -0., 0.]], dtype=torch.float16)

2025-07-09 03:01:21.977153 GPU 5 119698 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([20, 214748365],"float16"), Tensor([20, 214748365],"float16"), None, "mean", None, )
[accuracy error] backward  paddle.nn.functional.binary_cross_entropy(Tensor([20, 214748365],"float16"), Tensor([20, 214748365],"float16"), None, "mean", None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1057119 / 4294967300 (0.0%)
Greatest absolute difference: nan at index (0, 11296535) (up to 0.01 allowed)
Greatest relative difference: nan at index (0, 254) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([20, 214748365]), dtype=torch.float16)
tensor([[0., -0., 0.,  ..., -0., 0., -0.],
        [0., -0., 0.,  ..., -0., -0., -0.],
        [-0., 0., 0.,  ..., -0., 0., -0.],
        ...,
        [0., -0., 0.,  ..., -0., 0., -0.],
        [0., -0., -0.,  ..., -0., -0., 0.],
        [-0., 0., -0.,  ..., 0., 0., -0.]], dtype=torch.float16)
DESIRED: (shape=torch.Size([20, 214748365]), dtype=torch.float16)
tensor([[0., -0., 0.,  ..., -0., 0., -0.],
        [0., -0., 0.,  ..., -0., -0., -0.],
        [-0., 0., 0.,  ..., -0., 0., -0.],
        ...,
        [0., -0., 0.,  ..., -0., 0., -0.],
        [0., -0., -0.,  ..., -0., -0., 0.],
        [-0., 0., -0.,  ..., 0., 0., -0.]], dtype=torch.float16)

2025-07-09 03:04:42.769576 GPU 7 119068 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([20, 214748365],"float16"), Tensor([20, 214748365],"float16"), reduction="mean", )
[accuracy error] backward  paddle.nn.functional.binary_cross_entropy(Tensor([20, 214748365],"float16"), Tensor([20, 214748365],"float16"), reduction="mean", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1064668 / 4294967300 (0.0%)
Greatest absolute difference: nan at index (0, 14725679) (up to 0.01 allowed)
Greatest relative difference: nan at index (0, 868) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([20, 214748365]), dtype=torch.float16)
tensor([[0., 0., -0.,  ..., 0., -0., -0.],
        [-0., -0., 0.,  ..., 0., -0., 0.],
        [-0., 0., 0.,  ..., -0., 0., -0.],
        ...,
        [0., 0., -0.,  ..., -0., 0., -0.],
        [0., -0., 0.,  ..., -0., -0., 0.],
        [0., -0., 0.,  ..., 0., -0., 0.]], dtype=torch.float16)
DESIRED: (shape=torch.Size([20, 214748365]), dtype=torch.float16)
tensor([[0., 0., -0.,  ..., 0., -0., -0.],
        [-0., -0., 0.,  ..., 0., -0., 0.],
        [-0., 0., 0.,  ..., -0., 0., -0.],
        ...,
        [0., 0., -0.,  ..., -0., 0., -0.],
        [0., -0., 0.,  ..., -0., -0., 0.],
        [0., -0., 0.,  ..., 0., -0., 0.]], dtype=torch.float16)

2025-07-09 03:04:46.963525 GPU 4 119225 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([2147483649, 1, 2],"float16"), label=Tensor([2147483649, 1, 2],"float16"), weight=None, reduction="mean", name=None, )
[accuracy error] backward  paddle.nn.functional.binary_cross_entropy(Tensor([2147483649, 1, 2],"float16"), label=Tensor([2147483649, 1, 2],"float16"), weight=None, reduction="mean", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1065327 / 4294967298 (0.0%)
Greatest absolute difference: nan at index (6044303, 0, 0) (up to 0.01 allowed)
Greatest relative difference: nan at index (3723, 0, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2147483649, 1, 2]), dtype=torch.float16)
tensor([[[0., 0.]],

        [[-0., 0.]],

        [[-0., -0.]],

        ...,

        [[-0., 0.]],

        [[0., -0.]],

        [[0., -0.]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2147483649, 1, 2]), dtype=torch.float16)
tensor([[[0., 0.]],

        [[-0., 0.]],

        [[-0., -0.]],

        ...,

        [[-0., 0.]],

        [[0., -0.]],

        [[0., -0.]]], dtype=torch.float16)

2025-07-09 03:05:21.244363 GPU 3 119384 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([2147483649, 2],"float16"), Tensor([2147483649, 2],"float16"), None, "mean", None, )
[accuracy error] backward  paddle.nn.functional.binary_cross_entropy(Tensor([2147483649, 2],"float16"), Tensor([2147483649, 2],"float16"), None, "mean", None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1053718 / 4294967298 (0.0%)
Greatest absolute difference: nan at index (10676865, 1) (up to 0.01 allowed)
Greatest relative difference: nan at index (4582, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2147483649, 2]), dtype=torch.float16)
tensor([[-0., 0.],
        [0., -0.],
        [-0., -0.],
        ...,
        [0., -0.],
        [-0., -0.],
        [0., 0.]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2147483649, 2]), dtype=torch.float16)
tensor([[-0., 0.],
        [0., -0.],
        [-0., -0.],
        ...,
        [0., -0.],
        [-0., -0.],
        [0., 0.]], dtype=torch.float16)

2025-07-09 03:06:16.005712 GPU 5 119698 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), None, "mean", None, )
[accuracy error] backward  paddle.nn.functional.binary_cross_entropy(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), None, "mean", None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1060702 / 4294967300 (0.0%)
Greatest absolute difference: nan at index (0, 18288079) (up to 0.01 allowed)
Greatest relative difference: nan at index (0, 15800) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([5, 858993460]), dtype=torch.float16)
tensor([[0., -0., 0.,  ..., -0., 0., -0.],
        [0., -0., -0.,  ..., -0., -0., 0.],
        [0., -0., -0.,  ..., -0., -0., 0.],
        [0., -0., -0.,  ..., -0., 0., 0.],
        [-0., 0., -0.,  ..., 0., -0., 0.]], dtype=torch.float16)
DESIRED: (shape=torch.Size([5, 858993460]), dtype=torch.float16)
tensor([[0., -0., 0.,  ..., -0., 0., -0.],
        [0., -0., -0.,  ..., -0., -0., 0.],
        [0., -0., -0.,  ..., -0., -0., 0.],
        [0., -0., -0.,  ..., -0., 0., 0.],
        [-0., 0., -0.,  ..., 0., -0., 0.]], dtype=torch.float16)

2025-07-09 03:29:45.226044 GPU 7 125995 test begin: paddle.nn.functional.conv1d(Tensor([107375, 400, 100],"float32"), Tensor([256, 100, 3],"float32"), bias=Tensor([256],"float32"), padding=1, stride=list[1,], dilation=list[1,], groups=4, data_format="NCL", )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv1d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752003062 (unix time) try "date -d @1752003062" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 125995 (TID 0x7f5a1cfd9740) from PID 24 ***]


2025-07-09 03:54:12.332830 GPU 3 131550 test begin: paddle.nn.functional.conv1d(Tensor([2632, 64, 25500],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool)
1   torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool)
2   torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&)
3   torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&)
4   torch::autograd::generated::ConvolutionBackward0::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&)
5   at::_ops::convolution_backward::call(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, std::array<bool, 3ul>)
6   at::_ops::convolution_backward::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, std::array<bool, 3ul>)
7   at::native::convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, std::array<bool, 3ul>)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752004538 (unix time) try "date -d @1752004538" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 131550 (TID 0x7f0b08151700) from PID 24 ***]


2025-07-09 04:06:06.690426 GPU 2 135293 test begin: paddle.nn.functional.conv1d(Tensor([74899, 32, 1792],"float32"), Tensor([1, 32, 7],"float32"), bias=Tensor([1],"float32"), padding=3, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv1d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752005236 (unix time) try "date -d @1752005236" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 135293 (TID 0x7fafcefe1740) from PID 24 ***]


2025-07-09 04:07:16.012690 GPU 4 135769 test begin: paddle.nn.functional.conv1d(Tensor([74899, 32, 1792],"float32"), Tensor([32, 32, 11],"float32"), bias=Tensor([32],"float32"), padding=5, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv1d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752005314 (unix time) try "date -d @1752005314" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 135769 (TID 0x7f78524cd740) from PID 24 ***]


2025-07-09 04:08:58.375220 GPU 3 136562 test begin: paddle.nn.functional.conv1d(Tensor([74899, 32, 1792],"float32"), Tensor([32, 32, 7],"float32"), bias=Tensor([32],"float32"), padding=3, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv1d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752005427 (unix time) try "date -d @1752005427" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 136562 (TID 0x7f2d6430b740) from PID 24 ***]


2025-07-09 04:10:59.123851 GPU 4 137356 test begin: paddle.nn.functional.conv1d(Tensor([8192, 256, 2048],"float32"), Tensor([20, 256, 5],"float32"), bias=None, padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv1d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752005542 (unix time) try "date -d @1752005542" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 137356 (TID 0x7f018dde1740) from PID 24 ***]


2025-07-09 04:11:11.720315 GPU 3 137515 test begin: paddle.nn.functional.conv1d(Tensor([8192, 256, 2048],"float32"), Tensor([256, 256, 5],"float32"), bias=None, padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv1d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752005548 (unix time) try "date -d @1752005548" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 137515 (TID 0x7f3fc1e85740) from PID 24 ***]


2025-07-09 04:17:30.953035 GPU 4 137992 test begin: paddle.nn.functional.conv1d(x=Tensor([2, 3, 715827883],"float32"), weight=Tensor([1, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 04:23:54.941499 GPU 4 141793 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 1, 1431655765],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=1, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation

2025-07-09 04:23:56.523566 GPU 5 141319 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 1, 1431655765],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation

2025-07-09 04:24:34.580942 GPU 2 141565 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 2, 715827883],"float32"), bias=Tensor([2],"float32"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 2, 715827883],"float32"), bias=Tensor([2],"float32"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 04:25:41.425552 GPU 3 142112 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 477218589, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 477218589, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 04:26:06.477009 GPU 5 142270 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 477218589, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv1d_transpose(Tensor([2, 3, 2],"float32"), Tensor([3, 477218589, 3],"float32"), bias=Tensor([1],"float32"), output_size=None, output_padding=0, padding=list[1,], stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 04:29:03.988898 GPU 2 142994 test begin: paddle.nn.functional.conv2d(Tensor([1, 1024, 116509, 36],"float32"), Tensor([256, 1024, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 04:29:55.480590 GPU 7 143309 test begin: paddle.nn.functional.conv2d(Tensor([1, 1024, 12, 349526],"float32"), Tensor([256, 1024, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 04:30:30.568078 GPU 2 143468 test begin: paddle.nn.functional.conv2d(Tensor([1, 1024, 123362, 34],"float32"), Tensor([256, 1024, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 04:31:26.051691 GPU 7 143785 test begin: paddle.nn.functional.conv2d(Tensor([1, 1024, 161320, 26],"float32"), Tensor([256, 1024, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 04:36:45.784679 GPU 2 145374 test begin: paddle.nn.functional.conv2d(Tensor([1, 11184811, 12, 32],"float32"), Tensor([256, 11184811, 1, 1],"float32"), None, list[1,1,], 0, list[1,1,], 1, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 05:48:07.036544 GPU 4 151666 test begin: paddle.nn.functional.conv2d(Tensor([1, 3, 224, 6391321],"float32"), Tensor([3, 3, 3, 3],"float32"), Tensor([3],"float32"), list[3,2,], 0, list[1,1,], 1, "NCHW", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 05:49:06.950927 GPU 5 151892 test begin: paddle.nn.functional.conv2d(Tensor([1, 3, 224, 6391321],"float32"), Tensor([3, 3, 3, 3],"float32"), Tensor([3],"float32"), list[4,3,], 0, list[1,1,], 1, "NCHW", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 05:49:35.993712 GPU 2 152053 test begin: paddle.nn.functional.conv2d(Tensor([1, 3, 6391321, 224],"float32"), Tensor([3, 3, 3, 3],"float32"), Tensor([3],"float32"), list[3,2,], 0, list[1,1,], 1, "NCHW", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 05:49:52.700081 GPU 4 152210 test begin: paddle.nn.functional.conv2d(Tensor([1, 3, 6391321, 224],"float32"), Tensor([3, 3, 3, 3],"float32"), Tensor([3],"float32"), list[4,3,], 0, list[1,1,], 1, "NCHW", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 07:05:54.847960 GPU 5 157715 test begin: paddle.nn.functional.conv2d(Tensor([1009, 256, 129, 129],"float32"), Tensor([1009, 256, 3, 3],"float32"), bias=None, stride=2, padding=0, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool)
1   torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool)
2   torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&)
3   torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&)
4   torch::autograd::generated::ConvolutionBackward0::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&)
5   at::_ops::convolution_backward::call(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, std::array<bool, 3ul>)
6   at::_ops::convolution_backward::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, std::array<bool, 3ul>)
7   at::native::convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, std::array<bool, 3ul>)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752015966 (unix time) try "date -d @1752015966" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 157715 (TID 0x7f208e5b6700) from PID 24 ***]


2025-07-09 07:06:51.761908 GPU 5 157876 test begin: paddle.nn.functional.conv2d(Tensor([1009, 256, 129, 129],"float32"), Tensor([512, 256, 3, 3],"float32"), bias=None, stride=2, padding=0, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool)
1   torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool)
2   torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&)
3   torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&)
4   torch::autograd::generated::ConvolutionBackward0::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&)
5   at::_ops::convolution_backward::call(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, std::array<bool, 3ul>)
6   at::_ops::convolution_backward::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, std::array<bool, 3ul>)
7   at::native::convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, std::array<bool, 3ul>)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752016086 (unix time) try "date -d @1752016086" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 157876 (TID 0x7faad5bb5700) from PID 24 ***]


2025-07-09 07:16:19.080249 GPU 5 159148 test begin: paddle.nn.functional.conv2d(Tensor([1024, 256, 128, 128],"float32"), Tensor([256, 256, 3, 3],"float32"), bias=None, stride=1, padding=1, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752016581 (unix time) try "date -d @1752016581" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 159148 (TID 0x7fce33901740) from PID 24 ***]


2025-07-09 07:16:51.038998 GPU 3 159466 test begin: paddle.nn.functional.conv2d(Tensor([1024, 256, 128, 128],"float32"), Tensor([256, 256, 3, 3],"float32"), padding=1, groups=1, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752016693 (unix time) try "date -d @1752016693" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 159466 (TID 0x7f7772103740) from PID 24 ***]


2025-07-09 07:17:04.939603 GPU 5 159623 test begin: paddle.nn.functional.conv2d(Tensor([1024, 256, 128, 128],"float32"), Tensor([3, 256, 1, 1],"float32"), padding=0, groups=1, )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool)
1   torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool)
2   torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&)
3   torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&)
4   torch::autograd::generated::ConvolutionBackward0::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&)
5   at::_ops::convolution_backward::call(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, std::array<bool, 3ul>)
6   at::_ops::convolution_backward::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, std::array<bool, 3ul>)
7   at::native::convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, std::array<bool, 3ul>)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752016695 (unix time) try "date -d @1752016695" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 159623 (TID 0x7fb325bb5700) from PID 24 ***]


2025-07-09 07:18:56.932794 GPU 3 159940 test begin: paddle.nn.functional.conv2d(Tensor([1024, 4096, 32, 32],"float32"), Tensor([1024, 512, 3, 3],"float32"), padding=1, groups=8, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752016808 (unix time) try "date -d @1752016808" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 159940 (TID 0x7f284b3ee740) from PID 24 ***]


2025-07-09 07:24:02.081165 GPU 3 160257 test begin: paddle.nn.functional.conv2d(Tensor([128, 2048, 128, 128],"float32"), Tensor([128, 256, 3, 3],"float32"), padding=1, groups=8, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752017047 (unix time) try "date -d @1752017047" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 160257 (TID 0x7ff020ec9740) from PID 24 ***]


2025-07-09 07:24:19.918927 GPU 7 160418 test begin: paddle.nn.functional.conv2d(Tensor([128, 2048, 128, 128],"float32"), Tensor([2048, 256, 3, 3],"float32"), padding=1, groups=8, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752017064 (unix time) try "date -d @1752017064" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 160418 (TID 0x7ffb675a0740) from PID 24 ***]


2025-07-09 07:27:47.356075 GPU 5 161050 test begin: paddle.nn.functional.conv2d(Tensor([24, 21179, 67, 67],"float32"), Tensor([1, 21179, 4, 4],"float32"), )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752017269 (unix time) try "date -d @1752017269" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 161050 (TID 0x7f9b00661740) from PID 24 ***]


2025-07-09 07:27:51.437806 GPU 3 161212 test begin: paddle.nn.functional.conv2d(Tensor([273, 128, 256, 256],"float32"), Tensor([128, 128, 3, 3],"float32"), bias=None, stride=1, padding=1, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752017274 (unix time) try "date -d @1752017274" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 161212 (TID 0x7f940ce3d740) from PID 24 ***]


2025-07-09 07:27:55.844740 GPU 7 161370 test begin: paddle.nn.functional.conv2d(Tensor([273, 128, 256, 256],"float32"), Tensor([273, 128, 3, 3],"float32"), bias=None, stride=1, padding=1, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752017280 (unix time) try "date -d @1752017280" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 161370 (TID 0x7f58b4e2c740) from PID 24 ***]


2025-07-09 07:27:59.562223 GPU 3 161529 test begin: paddle.nn.functional.conv2d(Tensor([35, 1024, 256, 256],"float32"), Tensor([1024, 128, 3, 3],"float32"), padding=1, groups=8, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv2d::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
3   at::native::conv2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752017354 (unix time) try "date -d @1752017354" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 161529 (TID 0x7fc4fb01f740) from PID 24 ***]


2025-07-09 07:37:47.739388 GPU 4 2644 test begin: paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 128, 262144],"float32"), Tensor([128, 3, 4, 4],"float32"), bias=Tensor([3],"float32"), padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[paddle error] paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 128, 262144],"float32"), Tensor([128, 3, 4, 4],"float32"), bias=Tensor([3],"float32"), padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 07:38:58.156209 GPU 2 3195 test begin: paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 262144, 128],"float32"), Tensor([128, 3, 4, 4],"float32"), bias=Tensor([3],"float32"), padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[paddle error] paddle.nn.functional.conv2d_transpose(Tensor([1, 128, 262144, 128],"float32"), Tensor([128, 3, 4, 4],"float32"), bias=Tensor([3],"float32"), padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 07:50:36.390955 GPU 5 9209 test begin: paddle.nn.functional.conv2d_transpose(Tensor([10, 256, 14, 14],"float32"), Tensor([256, 256, 32768, 2],"float32"), bias=Tensor([256],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv2d_transpose(Tensor([10, 256, 14, 14],"float32"), Tensor([256, 256, 32768, 2],"float32"), bias=Tensor([256],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 07:50:57.493151 GPU 2 9368 test begin: paddle.nn.functional.conv2d_transpose(Tensor([1024, 256, 128, 128],"float32"), Tensor([256, 128, 3, 3],"float32"), padding=0, stride=2, groups=1, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv_transpose2d_input::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>)
3   at::native::conv_transpose2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution_transpose::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution_transpose(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752018727 (unix time) try "date -d @1752018727" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 9368 (TID 0x7fb7ebacb740) from PID 24 ***]


2025-07-09 07:51:13.095112 GPU 7 9683 test begin: paddle.nn.functional.conv2d_transpose(Tensor([10496, 16, 172, 79],"float32"), Tensor([16, 8, 5, 5],"float32"), bias=Tensor([8],"float32"), padding=2, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv_transpose2d_input::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>)
3   at::native::conv_transpose2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution_transpose::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution_transpose(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752018718 (unix time) try "date -d @1752018718" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 9683 (TID 0x7f48d0762740) from PID 24 ***]


2025-07-09 07:55:33.026934 GPU 2 9845 test begin: paddle.nn.functional.conv2d_transpose(Tensor([119304648, 3, 3, 2],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding=list[1,1,], data_format="NHWC", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv2d_transpose(Tensor([119304648, 3, 3, 2],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding=list[1,1,], data_format="NHWC", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 07:56:54.722466 GPU 3 10639 test begin: paddle.nn.functional.conv2d_transpose(Tensor([128, 2048, 128, 128],"float32"), Tensor([2048, 128, 3, 3],"float32"), padding=0, stride=2, groups=8, )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv_transpose2d_input::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>)
3   at::native::conv_transpose2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution_transpose::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution_transpose(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752019095 (unix time) try "date -d @1752019095" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 10639 (TID 0x7f476d693740) from PID 24 ***]


2025-07-09 07:59:43.714298 GPU 4 10482 test begin: paddle.nn.functional.conv2d_transpose(Tensor([178956971, 3, 2, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv2d_transpose(Tensor([178956971, 3, 2, 2],"float64"), Tensor([3, 1, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 08:03:18.707483 GPU 3 12160 test begin: paddle.nn.functional.conv2d_transpose(Tensor([2, 178956971, 3, 2],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding=list[1,1,], data_format="NHWC", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv2d_transpose(Tensor([2, 178956971, 3, 2],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding=list[1,1,], data_format="NHWC", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 08:05:46.933177 GPU 4 12796 test begin: paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 178956971, 3],"float64"), Tensor([2, 2, 178956971, 1],"float64"), groups=1, padding="SAME", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)

2025-07-09 08:05:51.710938 GPU 2 12954 test begin: paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 178956971, 3],"float64"), Tensor([2, 2, 178956971, 1],"float64"), groups=1, padding="VALID", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)

2025-07-09 08:26:51.064859 GPU 5 14856 test begin: paddle.nn.functional.conv2d_transpose(Tensor([64, 16, 172, 12955],"float32"), Tensor([16, 8, 5, 5],"float32"), bias=Tensor([8],"float32"), padding=2, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv_transpose2d_input::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>)
3   at::native::conv_transpose2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution_transpose::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution_transpose(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752020847 (unix time) try "date -d @1752020847" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 14856 (TID 0x7fbf22087740) from PID 24 ***]


2025-07-09 08:28:08.982153 GPU 5 15014 test begin: paddle.nn.functional.conv2d_transpose(Tensor([64, 16, 28206, 79],"float32"), Tensor([16, 8, 5, 5],"float32"), bias=Tensor([8],"float32"), padding=2, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function(torch::PythonArgs&, _object*, _object*, _object*, _object*, char const*, char const*)
1   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
2   at::_ops::conv_transpose2d_input::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>)
3   at::native::conv_transpose2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>)
4   at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
5   at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt)
6   at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long)
7   at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool)
8   at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool)
9   at::_ops::cudnn_convolution_transpose::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool)
10  at::native::cudnn_convolution_transpose(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752020947 (unix time) try "date -d @1752020947" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x18) received by PID 15014 (TID 0x7f160ed79740) from PID 24 ***]


2025-07-09 08:35:56.838344 GPU 4 15418 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 2048, 16, 16385],"float16"), Tensor([2048, 128, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv2d_transpose(Tensor([8, 2048, 16, 16385],"float16"), Tensor([2048, 128, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 08:39:06.672659 GPU 4 15418 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 2048, 16385, 16],"float16"), Tensor([2048, 128, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv2d_transpose(Tensor([8, 2048, 16385, 16],"float16"), Tensor([2048, 128, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 08:42:26.046979 GPU 2 16525 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8193, 2048, 16, 16],"float16"), Tensor([2048, 128, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv2d_transpose(Tensor([8193, 2048, 16, 16],"float16"), Tensor([2048, 128, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 08:56:27.047592 GPU 2 19218 test begin: paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 44739243],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 08:56:42.979877 GPU 4 18898 test begin: paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 4, 44739243],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=list[2,2,1,], padding=1, dilation=2, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 09:02:02.700551 GPU 7 21207 test begin: paddle.nn.functional.conv3d(x=Tensor([2, 3, 4, 44739243, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 09:04:08.847869 GPU 3 22791 test begin: paddle.nn.functional.conv3d(x=Tensor([2, 3, 44739243, 4, 4],"float32"), weight=Tensor([1, 3, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=2, padding=0, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 09:10:28.490094 GPU 5 24534 test begin: paddle.nn.functional.conv3d_transpose(Tensor([1398102, 6, 8, 8, 8],"float32"), Tensor([6, 8, 1, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([1398102, 6, 8, 8, 8],"float32"), Tensor([6, 8, 1, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:11:24.340762 GPU 5 24534 test begin: paddle.nn.functional.conv3d_transpose(Tensor([178956971, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([178956971, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:11:28.194170 GPU 2 25013 test begin: paddle.nn.functional.conv3d_transpose(Tensor([178956971, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([178956971, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:12:43.752365 GPU 4 26123 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float32"), Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 2, 2, 2, 3],"float32"), Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NDHWC", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:21:35.209010 GPU 7 29921 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 159072863, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 159072863, 3, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:22:27.833015 GPU 5 30324 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([1],"float32"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation

2025-07-09 09:22:58.602781 GPU 3 30553 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation

2025-07-09 09:24:01.713675 GPU 2 30941 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([3],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:25:22.348913 GPU 5 31330 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 159072863],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 1, 3, 3, 159072863],"float32"), bias=Tensor([1],"float32"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:29:55.960816 GPU 4 31807 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 12, 12, 12, 828505],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 12, 12, 12, 828505],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:39:02.695593 GPU 4 34912 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 89478486],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 89478486],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:39:49.780734 GPU 7 35233 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 89478486],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 89478486],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:41:25.012984 GPU 5 35550 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 89478486, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 89478486, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:42:04.034760 GPU 4 35707 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 89478486, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 89478486, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:45:56.912726 GPU 7 36501 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 89478486, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 89478486, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:46:08.176852 GPU 5 36659 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 89478486, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 89478486, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 09:48:07.104483 GPU 7 36501 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 5592406, 8, 8],"float32"), Tensor([6, 8, 1, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 09:50:00.586020 GPU 4 38406 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 5592406, 8],"float32"), Tensor([6, 8, 1, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 09:51:34.720520 GPU 5 40012 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 5592406],"float32"), Tensor([6, 8, 1, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)


2025-07-09 10:07:48.865658 GPU 5 51697 test begin: paddle.nn.functional.conv3d_transpose(Tensor([89478486, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([89478486, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=1, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:08:46.004195 GPU 4 52357 test begin: paddle.nn.functional.conv3d_transpose(Tensor([89478486, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(Tensor([89478486, 3, 2, 2, 2],"float64"), Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), padding=list[1,0,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:09:28.528956 GPU 2 53081 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([178956971, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([178956971, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:10:00.319539 GPU 5 51697 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([178956971, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([178956971, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:11:45.384892 GPU 3 54256 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float32"), weight=Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, data_format="NDHWC", dilation=1, )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([2, 2, 2, 2, 3],"float32"), weight=Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, data_format="NDHWC", dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:20:24.214096 GPU 3 54256 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 159072863, 3, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation

2025-07-09 10:20:51.857324 GPU 5 61243 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=1, dilation=1, )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation

2025-07-09 10:20:56.192404 GPU 4 61411 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation

2025-07-09 10:22:10.988574 GPU 5 62776 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 159072863, 3],"float32"), bias=Tensor([3],"float32"), stride=1, padding=list[1,0,1,], groups=3, dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:22:28.790327 GPU 4 63119 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 159072863],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, )
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 2],"float32"), weight=Tensor([3, 1, 3, 3, 159072863],"float32"), bias=Tensor([1],"float32"), stride=1, padding=list[1,0,1,], dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:36:23.795213 GPU 5 71093 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 89478486],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 89478486],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:36:54.422317 GPU 7 70887 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 89478486],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 2, 89478486],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:41:07.649135 GPU 7 72347 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 89478486, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 89478486, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:41:15.551670 GPU 5 72507 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 89478486, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 2, 89478486, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:44:10.708653 GPU 7 72347 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 89478486, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([2, 3, 89478486, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:45:42.692557 GPU 2 74965 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([89478486, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([89478486, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=1, dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:45:52.267605 GPU 3 75137 test begin: paddle.nn.functional.conv3d_transpose(x=Tensor([89478486, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/utils/_device.py:78: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return func(*args, **kwargs)
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
Could not load library libcudnn_cnn_train.so.8. Error: /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8: symbol _ZTIN10cask_cudnn14BaseKernelInfoE version libcudnn_cnn_infer.so.8 not defined in file libcudnn_cnn_infer.so.8 with link time reference
GET was unable to find an engine to execute this computation
[paddle error] paddle.nn.functional.conv3d_transpose(x=Tensor([89478486, 3, 2, 2, 2],"float64"), weight=Tensor([3, 1, 3, 3, 3],"float64"), bias=Tensor([1],"float64"), stride=1, padding=list[1,0,1,], dilation=1, ) 
 (External) CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. 
  [Hint: 'CUDNN_STATUS_NOT_SUPPORTED'.  The functionality requested is not presently supported by cuDNN.  ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/backends/gpu/cuda/cudnn_desc.h:158)


2025-07-09 10:48:38.883181 GPU 2 74965 test begin: paddle.nn.functional.cosine_similarity(Tensor([5, 1, 858993460],"float16"), Tensor([1, 858993460],"float16"), axis=0, eps=1e-08, )
[accuracy error] paddle.nn.functional.cosine_similarity(Tensor([5, 1, 858993460],"float16"), Tensor([1, 858993460],"float16"), axis=0, eps=1e-08, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 757178 / 858993460 (0.1%)
Greatest absolute difference: nan at index (0, 1307966) (up to 0.01 allowed)
Greatest relative difference: nan at index (0, 1307966) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 858993460]), dtype=torch.float16)
tensor([[-0.0457,  0.1132, -0.7920,  ...,  0.2661, -0.1050, -0.0362]], dtype=torch.float16)
DESIRED: (shape=torch.Size([1, 858993460]), dtype=torch.float16)
tensor([[-0.0458,  0.1130, -0.7925,  ...,  0.2661, -0.1049, -0.0363]], dtype=torch.float16)

2025-07-09 10:49:41.381814 GPU 7 76887 test begin: paddle.nn.functional.ctc_loss(Tensor([40, 8611, 6625],"float32"), Tensor([8611, 25],"int32"), Tensor([8611],"int64"), Tensor([8611],"int64"), 0, "none", norm_by_times=False, )
One of the differentiated Tensors does not require grad
[paddle error] paddle.nn.functional.ctc_loss(Tensor([40, 8611, 6625],"float32"), Tensor([8611, 25],"int32"), Tensor([8611],"int64"), Tensor([8611],"int64"), 0, "none", norm_by_times=False, ) 
 (PreconditionNotMet) warp-ctc [version 2] Error in get_workspace_size: cuda memcpy or memset failed
  [Hint: Expected CTC_STATUS_SUCCESS == status, but received CTC_STATUS_SUCCESS:0 != status:1.] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/kernels/impl/warpctc_kernel_impl.h:200)


2025-07-09 10:51:24.740580 GPU 3 75137 test begin: paddle.nn.functional.flashmask_attention(Tensor([2731, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), startend_row_indices=Tensor([1, 1, 2048, 1],"int32"), causal=True, )
[paddle_to_torch] Unsupported API paddle.nn.functional.flashmask_attention: Rule for paddle.nn.functional.flashmask_attention is not implemented

2025-07-09 10:51:24.833680 GPU 3 75137 test begin: paddle.nn.functional.flashmask_attention(Tensor([2731, 2048, 8, 96],"float16"), Tensor([2731, 2048, 8, 96],"float16"), Tensor([2731, 2048, 8, 96],"float16"), startend_row_indices=Tensor([2731, 1, 2048, 1],"int32"), causal=True, )
[paddle_to_torch] Unsupported API paddle.nn.functional.flashmask_attention: Rule for paddle.nn.functional.flashmask_attention is not implemented

2025-07-09 10:52:36.029029 GPU 7 76887 test begin: paddle.nn.functional.interpolate(Tensor([1, 100, 55925, 768],"float32"), size=list[384,384,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029586 (unix time) try "date -d @1752029586" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12c57) received by PID 76887 (TID 0x7fac5dbec740) from PID 76887 ***]


2025-07-09 10:53:25.277684 GPU 2 78061 test begin: paddle.nn.functional.interpolate(Tensor([1, 100, 58356, 736],"float32"), size=list[368,368,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029627 (unix time) try "date -d @1752029627" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x130ed) received by PID 78061 (TID 0x7f072cc92740) from PID 78061 ***]


2025-07-09 10:53:29.106550 GPU 5 78247 test begin: paddle.nn.functional.interpolate(Tensor([1, 100, 61009, 704],"float32"), size=list[352,352,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029630 (unix time) try "date -d @1752029630" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x131a7) received by PID 78247 (TID 0x7f736f554740) from PID 78247 ***]


2025-07-09 10:53:47.525727 GPU 3 78594 test begin: paddle.nn.functional.interpolate(Tensor([1, 100, 704, 61009],"float32"), size=list[352,352,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029650 (unix time) try "date -d @1752029650" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13302) received by PID 78594 (TID 0x7f7716844740) from PID 78594 ***]


2025-07-09 10:53:50.923356 GPU 4 78434 test begin: paddle.nn.functional.interpolate(Tensor([1, 100, 736, 58356],"float32"), size=list[368,368,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029655 (unix time) try "date -d @1752029655" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13262) received by PID 78434 (TID 0x7f22ffcfe740) from PID 78434 ***]


2025-07-09 10:53:54.724658 GPU 2 79240 test begin: paddle.nn.functional.interpolate(Tensor([1, 100, 768, 55925],"float32"), size=list[384,384,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029735 (unix time) try "date -d @1752029735" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13588) received by PID 79240 (TID 0x7fadef318740) from PID 79240 ***]


2025-07-09 10:54:02.057294 GPU 7 79556 test begin: paddle.nn.functional.interpolate(Tensor([1, 1048576, 64, 64],"float32"), size=tuple(16,16,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029739 (unix time) try "date -d @1752029739" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x136c4) received by PID 79556 (TID 0x7f6f44a05740) from PID 79556 ***]


2025-07-09 10:56:00.126438 GPU 4 79731 test begin: paddle.nn.functional.interpolate(Tensor([1, 1101274, 50, 78],"float32"), size=list[24,24,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029791 (unix time) try "date -d @1752029791" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13773) received by PID 79731 (TID 0x7f2f68c56740) from PID 79731 ***]


2025-07-09 10:56:18.523260 GPU 2 81117 test begin: paddle.nn.functional.interpolate(Tensor([1, 1130255, 50, 76],"float32"), size=list[24,24,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029898 (unix time) try "date -d @1752029898" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13cdd) received by PID 81117 (TID 0x7faf47a74740) from PID 81117 ***]


2025-07-09 10:56:32.590687 GPU 3 79984 test begin: paddle.nn.functional.interpolate(Tensor([1, 1130255, 76, 50],"float32"), size=list[24,24,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029827 (unix time) try "date -d @1752029827" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13870) received by PID 79984 (TID 0x7f3f9551d740) from PID 79984 ***]


2025-07-09 10:57:13.966952 GPU 3 81516 test begin: paddle.nn.functional.interpolate(Tensor([1, 1263226, 50, 68],"float32"), size=list[24,24,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029939 (unix time) try "date -d @1752029939" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13e6c) received by PID 81516 (TID 0x7f915fa53740) from PID 81516 ***]


2025-07-09 10:57:15.756972 GPU 4 81604 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 1, 33554432],"float32"), list[128,128,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 1, 33554432],"float32"), list[128,128,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2045056 / 2097152 (97.5%)
Greatest absolute difference: 0.989540696144104 at index (0, 86, 0, 62) (up to 0.01 allowed)
Greatest relative difference: 4145.6708984375 at index (0, 124, 0, 125) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.3920, -0.1841, -0.3901,  ..., -0.1872,  0.2200, -0.2637],
          [-0.3920, -0.1841, -0.3901,  ..., -0.1872,  0.2200, -0.2637],
          [-0.3920, -0.1841, -0.3901,  ..., -0.1872,  0.2200, -0.2637],
          ...,
          [-0.3920, -0.1841, -0.3901,  ..., -0.1872,  0.2200, -0.2637],
          [-0.3920, -0.1841, -0.3901,  ..., -0.1872,  0.2200, -0.2637],
          [-0.3920, -0.1841, -0.3901,  ..., -0.1872,  0.2200, -0.2637]],

         [[ 0.4266,  0.3361,  0.2559,  ...,  0.1245,  0.4798,  0.3763],
          [ 0.4266,  0.3361,  0.2559,  ...,  0.1245,  0.4798,  0.3763],
          [ 0.4266,  0.3361,  0.2559,  ...,  0.1245,  0.4798,  0.3763],
          ...,
          [ 0.4266,  0.3361,  0.2559,  ...,  0.1245,  0.4798,  0.3763],
          [ 0.4266,  0.3361,  0.2559,  ...,  0.1245,  0.4798,  0.3763],
          [ 0.4266,  0.3361,  0.2559,  ...,  0.1245,  0.4798,  0.3763]],

         [[ 0.0653, -0.1173,  0.0067,  ..., -0.0216, -0.4263,  0.1301],
          [ 0.0653, -0.1173,  0.0067,  ..., -0.0216, -0.4263,  0.1301],
          [ 0.0653, -0.1173,  0.0067,  ..., -0.0216, -0.4263,  0.1301],
          ...,
          [ 0.0653, -0.1173,  0.0067,  ..., -0.0216, -0.4263,  0.1301],
          [ 0.0653, -0.1173,  0.0067,  ..., -0.0216, -0.4263,  0.1301],
          [ 0.0653, -0.1173,  0.0067,  ..., -0.0216, -0.4263,  0.1301]],

         ...,

         [[-0.1845,  0.2919, -0.3558,  ...,  0.2883,  0.3262, -0.3959],
          [-0.1845,  0.2919, -0.3558,  ...,  0.2883,  0.3262, -0.3959],
          [-0.1845,  0.2919, -0.3558,  ...,  0.2883,  0.3262, -0.3959],
          ...,
          [-0.1845,  0.2919, -0.3558,  ...,  0.2883,  0.3262, -0.3959],
          [-0.1845,  0.2919, -0.3558,  ...,  0.2883,  0.3262, -0.3959],
          [-0.1845,  0.2919, -0.3558,  ...,  0.2883,  0.3262, -0.3959]],

         [[ 0.4028, -0.2872, -0.1857,  ..., -0.0943, -0.2123, -0.2535],
          [ 0.4028, -0.2872, -0.1857,  ..., -0.0943, -0.2123, -0.2535],
          [ 0.4028, -0.2872, -0.1857,  ..., -0.0943, -0.2123, -0.2535],
          ...,
          [ 0.4028, -0.2872, -0.1857,  ..., -0.0943, -0.2123, -0.2535],
          [ 0.4028, -0.2872, -0.1857,  ..., -0.0943, -0.2123, -0.2535],
          [ 0.4028, -0.2872, -0.1857,  ..., -0.0943, -0.2123, -0.2535]],

         [[-0.4457, -0.1830,  0.2351,  ..., -0.1204, -0.2473,  0.1767],
          [-0.4457, -0.1830,  0.2351,  ..., -0.1204, -0.2473,  0.1767],
          [-0.4457, -0.1830,  0.2351,  ..., -0.1204, -0.2473,  0.1767],
          ...,
          [-0.4457, -0.1830,  0.2351,  ..., -0.1204, -0.2473,  0.1767],
          [-0.4457, -0.1830,  0.2351,  ..., -0.1204, -0.2473,  0.1767],
          [-0.4457, -0.1830,  0.2351,  ..., -0.1204, -0.2473,  0.1767]]]])
DESIRED: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[ 0.0102,  0.0667, -0.0269,  ...,  0.4598, -0.4759,  0.3969],
          [ 0.0102,  0.0667, -0.0269,  ...,  0.4598, -0.4759,  0.3969],
          [ 0.0102,  0.0667, -0.0269,  ...,  0.4598, -0.4759,  0.3969],
          ...,
          [ 0.0102,  0.0667, -0.0269,  ...,  0.4598, -0.4759,  0.3969],
          [ 0.0102,  0.0667, -0.0269,  ...,  0.4598, -0.4759,  0.3969],
          [ 0.0102,  0.0667, -0.0269,  ...,  0.4598, -0.4759,  0.3969]],

         [[ 0.3309,  0.3394, -0.1750,  ..., -0.3150, -0.4260,  0.2426],
          [ 0.3309,  0.3394, -0.1750,  ..., -0.3150, -0.4260,  0.2426],
          [ 0.3309,  0.3394, -0.1750,  ..., -0.3150, -0.4260,  0.2426],
          ...,
          [ 0.3309,  0.3394, -0.1750,  ..., -0.3150, -0.4260,  0.2426],
          [ 0.3309,  0.3394, -0.1750,  ..., -0.3150, -0.4260,  0.2426],
          [ 0.3309,  0.3394, -0.1750,  ..., -0.3150, -0.4260,  0.2426]],

         [[-0.1960, -0.0403,  0.4208,  ..., -0.3633, -0.4703, -0.1361],
          [-0.1960, -0.0403,  0.4208,  ..., -0.3633, -0.4703, -0.1361],
          [-0.1960, -0.0403,  0.4208,  ..., -0.3633, -0.4703, -0.1361],
          ...,
          [-0.1960, -0.0403,  0.4208,  ..., -0.3633, -0.4703, -0.1361],
          [-0.1960, -0.0403,  0.4208,  ..., -0.3633, -0.4703, -0.1361],
          [-0.1960, -0.0403,  0.4208,  ..., -0.3633, -0.4703, -0.1361]],

         ...,

         [[-0.0439, -0.1064, -0.0255,  ..., -0.4053,  0.3879,  0.4171],
          [-0.0439, -0.1064, -0.0255,  ..., -0.4053,  0.3879,  0.4171],
          [-0.0439, -0.1064, -0.0255,  ..., -0.4053,  0.3879,  0.4171],
          ...,
          [-0.0439, -0.1064, -0.0255,  ..., -0.4053,  0.3879,  0.4171],
          [-0.0439, -0.1064, -0.0255,  ..., -0.4053,  0.3879,  0.4171],
          [-0.0439, -0.1064, -0.0255,  ..., -0.4053,  0.3879,  0.4171]],

         [[-0.0363, -0.3530, -0.3808,  ..., -0.3322,  0.1071, -0.1953],
          [-0.0363, -0.3530, -0.3808,  ..., -0.3322,  0.1071, -0.1953],
          [-0.0363, -0.3530, -0.3808,  ..., -0.3322,  0.1071, -0.1953],
          ...,
          [-0.0363, -0.3530, -0.3808,  ..., -0.3322,  0.1071, -0.1953],
          [-0.0363, -0.3530, -0.3808,  ..., -0.3322,  0.1071, -0.1953],
          [-0.0363, -0.3530, -0.3808,  ..., -0.3322,  0.1071, -0.1953]],

         [[ 0.1306, -0.2368,  0.2759,  ..., -0.0985, -0.1196,  0.4693],
          [ 0.1306, -0.2368,  0.2759,  ..., -0.0985, -0.1196,  0.4693],
          [ 0.1306, -0.2368,  0.2759,  ..., -0.0985, -0.1196,  0.4693],
          ...,
          [ 0.1306, -0.2368,  0.2759,  ..., -0.0985, -0.1196,  0.4693],
          [ 0.1306, -0.2368,  0.2759,  ..., -0.0985, -0.1196,  0.4693],
          [ 0.1306, -0.2368,  0.2759,  ..., -0.0985, -0.1196,  0.4693]]]])

2025-07-09 10:58:49.481771 GPU 4 81604 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 1, 33554432],"float32"), list[16,32,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752029955 (unix time) try "date -d @1752029955" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13ec4) received by PID 81604 (TID 0x7f51d1c97740) from PID 81604 ***]


2025-07-09 10:59:01.476153 GPU 2 82273 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 1048576, 32],"float32"), list[128,128,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 128, 1048576, 32],"float32"), list[128,128,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1014366 / 4294967296 (0.0%)
Greatest absolute difference: 0.8666619062423706 at index (0, 127, 905215, 14) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 4095, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 1048576, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 128, 1048576, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 10:59:03.382121 GPU 5 79328 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 1048576, 32],"float32"), list[16,32,], mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030033 (unix time) try "date -d @1752030033" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x135e0) received by PID 79328 (TID 0x7f020659c740) from PID 79328 ***]


2025-07-09 10:59:22.420611 GPU 4 82726 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 1048576, 32],"float32"), list[32,64,], mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030088 (unix time) try "date -d @1752030088" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14326) received by PID 82726 (TID 0x7f107dfac740) from PID 82726 ***]


2025-07-09 10:59:44.813513 GPU 3 82890 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 11184811, 3],"float32"), list[128,128,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 11184811, 3],"float32"), list[128,128,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2035862 / 2097152 (97.1%)
Greatest absolute difference: 0.9980419874191284 at index (0, 21, 105, 0) (up to 0.01 allowed)
Greatest relative difference: 1677742.0 at index (0, 92, 14, 61) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.0524, -0.0591, -0.0658,  ..., -0.3683, -0.3665, -0.3647],
          [ 0.0298,  0.0264,  0.0230,  ...,  0.1933,  0.1995,  0.2056],
          [ 0.0972,  0.0980,  0.0987,  ...,  0.1145,  0.1140,  0.1135],
          ...,
          [-0.1484, -0.1503, -0.1522,  ...,  0.2886,  0.2977,  0.3068],
          [ 0.2457,  0.2456,  0.2456,  ...,  0.3199,  0.3212,  0.3224],
          [-0.3433, -0.3356, -0.3278,  ...,  0.1075,  0.1068,  0.1062]],

         [[ 0.1846,  0.1842,  0.1839,  ...,  0.4522,  0.4569,  0.4616],
          [ 0.2982,  0.2944,  0.2905,  ...,  0.2969,  0.3008,  0.3048],
          [-0.4531, -0.4413, -0.4296,  ...,  0.1757,  0.1738,  0.1718],
          ...,
          [ 0.3408,  0.3363,  0.3317,  ..., -0.4603, -0.4687, -0.4770],
          [ 0.3102,  0.3006,  0.2911,  ...,  0.0527,  0.0584,  0.0640],
          [-0.0542, -0.0515, -0.0488,  ..., -0.3083, -0.3152, -0.3221]],

         [[-0.1009, -0.0995, -0.0981,  ..., -0.3884, -0.3944, -0.4005],
          [-0.2805, -0.2717, -0.2629,  ..., -0.0910, -0.0969, -0.1029],
          [-0.2263, -0.2198, -0.2132,  ..., -0.0502, -0.0542, -0.0581],
          ...,
          [ 0.0272,  0.0333,  0.0393,  ..., -0.4341, -0.4478, -0.4616],
          [-0.4478, -0.4331, -0.4184,  ...,  0.0225,  0.0149,  0.0074],
          [-0.2353, -0.2390, -0.2428,  ..., -0.4683, -0.4682, -0.4681]],

         ...,

         [[ 0.0261,  0.0198,  0.0136,  ..., -0.2411, -0.2390, -0.2368],
          [ 0.1314,  0.1264,  0.1213,  ..., -0.1316, -0.1307, -0.1297],
          [-0.1826, -0.1856, -0.1886,  ..., -0.4257, -0.4266, -0.4275],
          ...,
          [ 0.0216,  0.0287,  0.0359,  ..., -0.3819, -0.3958, -0.4098],
          [ 0.2874,  0.2757,  0.2640,  ..., -0.2025, -0.1984, -0.1942],
          [-0.2425, -0.2324, -0.2223,  ..., -0.0436, -0.0508, -0.0580]],

         [[ 0.1947,  0.1876,  0.1806,  ...,  0.0793,  0.0847,  0.0901],
          [-0.1174, -0.1190, -0.1205,  ..., -0.3047, -0.3062, -0.3077],
          [-0.1182, -0.1130, -0.1078,  ..., -0.0881, -0.0930, -0.0979],
          ...,
          [-0.3045, -0.3042, -0.3038,  ..., -0.3556, -0.3567, -0.3579],
          [-0.4286, -0.4247, -0.4208,  ..., -0.2682, -0.2696, -0.2710],
          [ 0.2114,  0.2081,  0.2048,  ...,  0.3254,  0.3306,  0.3359]],

         [[ 0.3827,  0.3713,  0.3599,  ..., -0.0246, -0.0194, -0.0143],
          [ 0.0242,  0.0288,  0.0335,  ..., -0.4649, -0.4776, -0.4904],
          [-0.3543, -0.3494, -0.3446,  ..., -0.2674, -0.2709, -0.2745],
          ...,
          [ 0.0234,  0.0215,  0.0197,  ..., -0.2671, -0.2699, -0.2727],
          [ 0.0476,  0.0532,  0.0589,  ...,  0.4298,  0.4302,  0.4306],
          [ 0.0166,  0.0197,  0.0228,  ..., -0.3688, -0.3782, -0.3877]]]])
DESIRED: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.0695, -0.0695, -0.0695,  ...,  0.1795,  0.1795,  0.1795],
          [ 0.0824,  0.0824,  0.0824,  ..., -0.1827, -0.1827, -0.1827],
          [-0.3597, -0.3597, -0.3597,  ...,  0.2350,  0.2350,  0.2350],
          ...,
          [-0.4437, -0.4437, -0.4437,  ...,  0.1923,  0.1923,  0.1923],
          [-0.4697, -0.4697, -0.4697,  ...,  0.1482,  0.1482,  0.1482],
          [ 0.4179,  0.4179,  0.4179,  ...,  0.0191,  0.0191,  0.0191]],

         [[ 0.0198,  0.0198,  0.0198,  ...,  0.1808,  0.1808,  0.1808],
          [ 0.4128,  0.4128,  0.4128,  ..., -0.0767, -0.0767, -0.0767],
          [-0.3634, -0.3634, -0.3634,  ..., -0.4019, -0.4019, -0.4019],
          ...,
          [-0.3864, -0.3864, -0.3864,  ...,  0.2202,  0.2202,  0.2202],
          [-0.4497, -0.4497, -0.4497,  ...,  0.4319,  0.4319,  0.4319],
          [ 0.2550,  0.2550,  0.2550,  ...,  0.2617,  0.2617,  0.2617]],

         [[ 0.0292,  0.0292,  0.0292,  ..., -0.1977, -0.1977, -0.1977],
          [-0.4386, -0.4386, -0.4386,  ...,  0.1857,  0.1857,  0.1857],
          [ 0.0490,  0.0490,  0.0490,  ..., -0.2052, -0.2052, -0.2052],
          ...,
          [ 0.4232,  0.4232,  0.4232,  ..., -0.4771, -0.4771, -0.4771],
          [-0.2084, -0.2084, -0.2084,  ..., -0.3837, -0.3837, -0.3837],
          [ 0.3032,  0.3032,  0.3032,  ..., -0.3030, -0.3030, -0.3030]],

         ...,

         [[-0.1328, -0.1328, -0.1328,  ..., -0.3640, -0.3640, -0.3640],
          [-0.3749, -0.3749, -0.3749,  ..., -0.1711, -0.1711, -0.1711],
          [-0.0044, -0.0044, -0.0044,  ...,  0.1855,  0.1855,  0.1855],
          ...,
          [ 0.1136,  0.1136,  0.1136,  ...,  0.4979,  0.4979,  0.4979],
          [-0.3213, -0.3213, -0.3213,  ...,  0.2083,  0.2083,  0.2083],
          [-0.2030, -0.2030, -0.2030,  ..., -0.2218, -0.2218, -0.2218]],

         [[-0.3950, -0.3950, -0.3950,  ..., -0.3475, -0.3475, -0.3475],
          [ 0.0494,  0.0494,  0.0494,  ..., -0.0772, -0.0772, -0.0772],
          [ 0.3839,  0.3839,  0.3839,  ...,  0.1101,  0.1101,  0.1101],
          ...,
          [-0.2731, -0.2731, -0.2731,  ..., -0.3051, -0.3051, -0.3051],
          [ 0.4692,  0.4692,  0.4692,  ..., -0.1034, -0.1034, -0.1034],
          [ 0.3016,  0.3016,  0.3016,  ...,  0.4005,  0.4005,  0.4005]],

         [[-0.2435, -0.2435, -0.2435,  ..., -0.3897, -0.3897, -0.3897],
          [ 0.0644,  0.0644,  0.0644,  ...,  0.2404,  0.2404,  0.2404],
          [ 0.2037,  0.2037,  0.2037,  ..., -0.1403, -0.1403, -0.1403],
          ...,
          [ 0.3133,  0.3133,  0.3133,  ...,  0.4500,  0.4500,  0.4500],
          [-0.3603, -0.3603, -0.3603,  ..., -0.2153, -0.2153, -0.2153],
          [-0.4468, -0.4468, -0.4468,  ...,  0.4691,  0.4691,  0.4691]]]])

2025-07-09 11:00:08.202814 GPU 7 80703 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 128, 139265],"float32"), list[256,256,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 128, 139265],"float32"), list[256,256,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 8120233 / 8388608 (96.8%)
Greatest absolute difference: 0.9654344320297241 at index (0, 83, 255, 135) (up to 0.01 allowed)
Greatest relative difference: 1557173.5 at index (0, 38, 46, 47) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 256, 256]), dtype=torch.float32)
tensor([[[[ 0.2328, -0.1502,  0.0181,  ...,  0.1186,  0.0654, -0.3768],
          [ 0.0538, -0.0304, -0.1869,  ...,  0.0218, -0.1287, -0.4275],
          [-0.1251,  0.0894, -0.3919,  ..., -0.0750, -0.3228, -0.4783],
          ...,
          [ 0.2603, -0.2569,  0.2631,  ..., -0.2105,  0.3947, -0.2599],
          [-0.1022,  0.0579,  0.0272,  ...,  0.0144,  0.1387,  0.0256],
          [-0.4646,  0.3727, -0.2087,  ...,  0.2394, -0.1174,  0.3112]],

         [[ 0.3168, -0.1944, -0.3005,  ..., -0.1873, -0.3722,  0.2120],
          [ 0.1349, -0.2073, -0.0183,  ..., -0.1772, -0.0101, -0.0053],
          [-0.0471, -0.2202,  0.2639,  ..., -0.1670,  0.3521, -0.2227],
          ...,
          [-0.2152,  0.2178, -0.0959,  ...,  0.4748,  0.0852,  0.1239],
          [-0.0311,  0.1154,  0.0737,  ...,  0.3316, -0.0210, -0.0022],
          [ 0.1530,  0.0129,  0.2433,  ...,  0.1883, -0.1272, -0.1283]],

         [[-0.3329, -0.4041,  0.0080,  ..., -0.0932,  0.3278,  0.3187],
          [-0.0312, -0.1549, -0.1502,  ...,  0.1072,  0.0711,  0.2643],
          [ 0.2704,  0.0943, -0.3085,  ...,  0.3076, -0.1855,  0.2099],
          ...,
          [-0.2709,  0.2259, -0.2535,  ...,  0.3787, -0.4015,  0.2244],
          [ 0.0381, -0.0642, -0.2443,  ...,  0.3759, -0.3451,  0.2161],
          [ 0.3472, -0.3542, -0.2351,  ...,  0.3732, -0.2887,  0.2078]],

         ...,

         [[ 0.4230,  0.2673,  0.0582,  ...,  0.0053, -0.1624, -0.4855],
          [ 0.0203,  0.0787, -0.0906,  ...,  0.1020, -0.1706, -0.4598],
          [-0.3824, -0.1099, -0.2394,  ...,  0.1987, -0.1788, -0.4342],
          ...,
          [ 0.1828, -0.0067,  0.0565,  ..., -0.3930, -0.2745, -0.3269],
          [-0.0543,  0.0156, -0.1019,  ..., -0.3534, -0.0948, -0.1643],
          [-0.2914,  0.0379, -0.2603,  ..., -0.3137,  0.0849, -0.0017]],

         [[ 0.3471, -0.2781, -0.2448,  ..., -0.2238, -0.4660, -0.1326],
          [ 0.1275, -0.1876, -0.2633,  ..., -0.1653, -0.2400, -0.2334],
          [-0.0921, -0.0971, -0.2817,  ..., -0.1069, -0.0139, -0.3342],
          ...,
          [ 0.2370,  0.3575, -0.4025,  ..., -0.0305,  0.3371,  0.2658],
          [ 0.0381,  0.2097,  0.0300,  ..., -0.1135,  0.0546, -0.0041],
          [-0.1608,  0.0620,  0.4625,  ..., -0.1965, -0.2279, -0.2740]],

         [[-0.1566,  0.1531, -0.0185,  ..., -0.0850, -0.3976,  0.2654],
          [-0.1282,  0.2948, -0.0228,  ...,  0.0748, -0.0527, -0.0556],
          [-0.0998,  0.4364, -0.0272,  ...,  0.2346,  0.2923, -0.3765],
          ...,
          [-0.3966,  0.0690,  0.1178,  ...,  0.0034,  0.0244,  0.2262],
          [-0.4346, -0.0527, -0.0013,  ...,  0.0923,  0.0147,  0.2086],
          [-0.4726, -0.1744, -0.1204,  ...,  0.1811,  0.0050,  0.1909]]]])
DESIRED: (shape=torch.Size([1, 128, 256, 256]), dtype=torch.float32)
tensor([[[[-0.0085,  0.1646, -0.1060,  ..., -0.3766, -0.2561, -0.0364],
          [ 0.0879,  0.0532, -0.1346,  ..., -0.3204, -0.1719, -0.0515],
          [ 0.2808, -0.1696, -0.1917,  ..., -0.2082, -0.0035, -0.0816],
          ...,
          [-0.1484, -0.2272,  0.0251,  ..., -0.0894,  0.0156,  0.0790],
          [ 0.2284,  0.1745,  0.1248,  ..., -0.1716, -0.1088, -0.1553],
          [ 0.4169,  0.3754,  0.1746,  ..., -0.2126, -0.1711, -0.2725]],

         [[-0.0576, -0.2228,  0.3455,  ...,  0.2240, -0.2828,  0.1950],
          [ 0.0141, -0.2431,  0.2974,  ...,  0.1321, -0.1790,  0.0912],
          [ 0.1575, -0.2838,  0.2012,  ..., -0.0518,  0.0288, -0.1163],
          ...,
          [ 0.2154,  0.1803, -0.1058,  ..., -0.0149, -0.1215, -0.2696],
          [ 0.1078, -0.0099, -0.1743,  ..., -0.0398, -0.0783, -0.0778],
          [ 0.0540, -0.1049, -0.2086,  ..., -0.0523, -0.0567,  0.0181]],

         [[-0.0282, -0.3312,  0.2147,  ..., -0.2696,  0.3959,  0.0123],
          [-0.0132, -0.2261,  0.2494,  ..., -0.2143,  0.2782,  0.0525],
          [ 0.0167, -0.0158,  0.3188,  ..., -0.1038,  0.0427,  0.1329],
          ...,
          [ 0.0055,  0.0281,  0.1590,  ...,  0.2401, -0.1657,  0.1279],
          [-0.2255,  0.0826,  0.0774,  ...,  0.1912, -0.1739, -0.0134],
          [-0.3411,  0.1098,  0.0366,  ...,  0.1668, -0.1781, -0.0841]],

         ...,

         [[-0.2408,  0.2858, -0.1637,  ...,  0.3061, -0.3476, -0.0910],
          [-0.1790,  0.1633, -0.1029,  ...,  0.1707, -0.1580, -0.1164],
          [-0.0554, -0.0816,  0.0187,  ..., -0.1000,  0.2212, -0.1672],
          ...,
          [ 0.0690,  0.0187,  0.0128,  ...,  0.0021,  0.0645, -0.0904],
          [ 0.1750, -0.1206,  0.2436,  ...,  0.0120,  0.0871,  0.1055],
          [ 0.2280, -0.1903,  0.3590,  ...,  0.0170,  0.0983,  0.2034]],

         [[-0.2562,  0.1000,  0.1492,  ...,  0.4285, -0.0647,  0.0663],
          [-0.1772,  0.1307,  0.2248,  ...,  0.3016, -0.0852,  0.0957],
          [-0.0194,  0.1919,  0.3760,  ...,  0.0477, -0.1262,  0.1546],
          ...,
          [ 0.2028,  0.0885,  0.3320,  ...,  0.0406,  0.0798,  0.0306],
          [ 0.1114,  0.1779,  0.1839,  ...,  0.0546,  0.1968,  0.0417],
          [ 0.0657,  0.2226,  0.1098,  ...,  0.0615,  0.2554,  0.0473]],

         [[-0.0654,  0.0093,  0.3218,  ..., -0.1737,  0.0526,  0.1388],
          [-0.0145,  0.0369,  0.2436,  ..., -0.0961,  0.1153,  0.0279],
          [ 0.0874,  0.0921,  0.0874,  ...,  0.0591,  0.2408, -0.1938],
          ...,
          [-0.0461, -0.2951,  0.1648,  ..., -0.0950, -0.0450, -0.0588],
          [-0.1134, -0.1271,  0.0607,  ..., -0.1609,  0.1311, -0.0817],
          [-0.1470, -0.0431,  0.0086,  ..., -0.1939,  0.2192, -0.0932]]]])

2025-07-09 11:00:17.834478 GPU 6 83098 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 128, 262144],"float32"), list[256,256,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 128, 262144],"float32"), list[256,256,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 8107351 / 8388608 (96.6%)
Greatest absolute difference: 0.973635196685791 at index (0, 26, 0, 119) (up to 0.01 allowed)
Greatest relative difference: 3714416.75 at index (0, 18, 122, 69) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 256, 256]), dtype=torch.float32)
tensor([[[[ 9.3729e-03,  1.7950e-01, -2.1777e-01,  ..., -1.3702e-02, -3.3633e-01, -2.7286e-02],
          [-3.2618e-02,  2.1870e-01, -3.2959e-01,  ...,  1.0780e-01, -3.6053e-01,  3.8250e-03],
          [-7.4609e-02,  2.5791e-01, -4.4141e-01,  ...,  2.2930e-01, -3.8472e-01,  3.4936e-02],
          ...,
          [ 3.5903e-02, -2.1834e-01,  3.4866e-01,  ...,  4.0701e-01,  1.7289e-01,  1.5396e-01],
          [ 2.5420e-01,  6.2473e-02,  1.8937e-01,  ...,  2.6504e-02,  3.2199e-01, -5.4552e-02],
          [ 4.7249e-01,  3.4328e-01,  3.0078e-02,  ..., -3.5400e-01,  4.7108e-01, -2.6306e-01]],

         [[-1.5835e-01, -4.2985e-01,  1.7938e-01,  ..., -5.0334e-03,  2.1611e-01,  4.4821e-01],
          [-1.5589e-01, -1.1316e-01,  1.2356e-01,  ..., -2.0858e-01,  2.9563e-01,  3.8705e-01],
          [-1.5343e-01,  2.0353e-01,  6.7735e-02,  ..., -4.1213e-01,  3.7515e-01,  3.2589e-01],
          ...,
          [ 2.4100e-02, -2.9861e-01,  3.6175e-01,  ..., -7.4059e-02,  1.8726e-01,  6.1751e-02],
          [ 2.2335e-01,  6.5206e-02,  3.3699e-02,  ..., -2.4961e-01,  6.9987e-02,  2.4415e-01],
          [ 4.2259e-01,  4.2902e-01, -2.9435e-01,  ..., -4.2517e-01, -4.7286e-02,  4.2655e-01]],

         [[-3.7550e-01, -8.2652e-02, -2.5054e-01,  ...,  1.9028e-01, -1.4624e-01,  1.0272e-01],
          [-4.7789e-02, -2.7892e-01,  4.3099e-02,  ..., -7.2281e-02, -3.0391e-01,  2.2440e-01],
          [ 2.7992e-01, -4.7518e-01,  3.3674e-01,  ..., -3.3484e-01, -4.6158e-01,  3.4608e-01],
          ...,
          [ 2.9681e-02,  4.5433e-01,  3.6310e-01,  ..., -2.3690e-01, -1.7336e-01,  9.3214e-02],
          [-1.9694e-01,  3.9727e-01,  1.7846e-01,  ..., -2.7329e-01,  8.4862e-02,  1.6023e-01],
          [-4.2355e-01,  3.4021e-01, -6.1896e-03,  ..., -3.0968e-01,  3.4308e-01,  2.2725e-01]],

         ...,

         [[ 1.8797e-01,  3.0880e-01,  3.8110e-01,  ...,  3.2609e-01,  4.7732e-01,  3.4947e-01],
          [-3.5050e-02,  2.4437e-01, -2.9524e-04,  ...,  3.7643e-01,  2.6282e-01, -7.0682e-02],
          [-2.5807e-01,  1.7994e-01, -3.8169e-01,  ...,  4.2678e-01,  4.8321e-02, -4.9083e-01],
          ...,
          [-4.6876e-01, -2.4650e-01,  4.5476e-01,  ...,  5.4134e-02, -4.6820e-01, -2.3818e-01],
          [-1.6169e-01,  9.0969e-02,  2.0060e-01,  ..., -3.5302e-02, -2.4709e-01, -1.0224e-01],
          [ 1.4538e-01,  4.2844e-01, -5.3560e-02,  ..., -1.2474e-01, -2.5984e-02,  3.3692e-02]],

         [[ 4.2277e-01, -3.6002e-01, -6.3197e-04,  ...,  4.3525e-01, -1.6576e-03,  1.4229e-02],
          [ 1.5337e-01, -1.9345e-01,  1.7346e-01,  ...,  1.2417e-01, -1.5806e-01,  1.4692e-02],
          [-1.1603e-01, -2.6893e-02,  3.4755e-01,  ..., -1.8691e-01, -3.1447e-01,  1.5154e-02],
          ...,
          [ 4.3777e-01, -2.8646e-01, -4.8426e-01,  ..., -4.1353e-01, -1.1066e-01, -2.3907e-01],
          [ 4.5688e-01, -1.7645e-01, -3.5963e-01,  ..., -3.7751e-01, -2.0732e-01,  6.0974e-02],
          [ 4.7600e-01, -6.6448e-02, -2.3500e-01,  ..., -3.4149e-01, -3.0398e-01,  3.6101e-01]],

         [[-4.3266e-01,  2.0311e-01,  2.9132e-01,  ..., -5.7898e-02, -4.6463e-01,  3.4882e-01],
          [-7.7835e-02,  3.1713e-01,  1.1526e-03,  ...,  1.8338e-01, -5.0509e-03,  1.6697e-01],
          [ 2.7699e-01,  4.3115e-01, -2.8901e-01,  ...,  4.2466e-01,  4.5453e-01, -1.4882e-02],
          ...,
          [-4.0077e-01, -2.0981e-01, -1.8360e-01,  ..., -1.1395e-01, -1.8790e-01, -4.6369e-01],
          [-4.0764e-01,  1.2949e-01, -8.9862e-02,  ..., -9.0029e-02, -3.2815e-01, -2.4861e-01],
          [-4.1451e-01,  4.6879e-01,  3.8794e-03,  ..., -6.6104e-02, -4.6840e-01, -3.3531e-02]]]])
DESIRED: (shape=torch.Size([1, 128, 256, 256]), dtype=torch.float32)
tensor([[[[-0.0113, -0.1865,  0.3480,  ..., -0.0629,  0.4320,  0.1356],
          [-0.0169, -0.2396,  0.3159,  ..., -0.0200,  0.2760,  0.0978],
          [-0.0281, -0.3458,  0.2515,  ...,  0.0658, -0.0359,  0.0221],
          ...,
          [ 0.0077,  0.0250, -0.2356,  ...,  0.0153,  0.0579, -0.3076],
          [-0.1151,  0.2968,  0.1236,  ...,  0.0170, -0.0438, -0.3101],
          [-0.1764,  0.4327,  0.3032,  ...,  0.0178, -0.0946, -0.3113]],

         [[-0.3393, -0.0173, -0.3116,  ..., -0.1650, -0.2946,  0.1469],
          [-0.2514,  0.0033, -0.2068,  ..., -0.1468, -0.3162,  0.0721],
          [-0.0755,  0.0447,  0.0027,  ..., -0.1104, -0.3593, -0.0775],
          ...,
          [-0.0318, -0.1622,  0.1642,  ..., -0.0346, -0.0140, -0.0855],
          [-0.0089,  0.0751, -0.0289,  ...,  0.0828, -0.2635, -0.0262],
          [ 0.0025,  0.1938, -0.1255,  ...,  0.1415, -0.3882,  0.0035]],

         [[-0.1958,  0.1778,  0.1420,  ..., -0.0430,  0.0652, -0.3918],
          [-0.1233,  0.1636,  0.0976,  ..., -0.0081, -0.0363, -0.3426],
          [ 0.0217,  0.1351,  0.0090,  ...,  0.0616, -0.2393, -0.2441],
          ...,
          [-0.1442, -0.1513, -0.0192,  ...,  0.1273,  0.2059,  0.1542],
          [-0.1785, -0.0278, -0.2373,  ..., -0.0906,  0.0918,  0.0074],
          [-0.1957,  0.0340, -0.3464,  ..., -0.1996,  0.0348, -0.0661]],

         ...,

         [[ 0.0649, -0.0252,  0.1127,  ...,  0.4356,  0.0161,  0.2152],
          [ 0.0651, -0.0180,  0.0747,  ...,  0.3092, -0.0097,  0.1593],
          [ 0.0655, -0.0037, -0.0011,  ...,  0.0563, -0.0613,  0.0477],
          ...,
          [ 0.4215, -0.1246,  0.2097,  ...,  0.0953,  0.0533,  0.0315],
          [ 0.3477, -0.0761,  0.0021,  ...,  0.0574,  0.1491,  0.1337],
          [ 0.3108, -0.0519, -0.1017,  ...,  0.0384,  0.1970,  0.1847]],

         [[ 0.0712,  0.2181, -0.2608,  ..., -0.2215, -0.0284, -0.3569],
          [ 0.1470,  0.1044, -0.1906,  ..., -0.1630, -0.0095, -0.1691],
          [ 0.2985, -0.1229, -0.0503,  ..., -0.0459,  0.0282,  0.2065],
          ...,
          [ 0.2597,  0.2446,  0.2682,  ...,  0.3598,  0.1769,  0.1745],
          [ 0.0764,  0.1038, -0.0092,  ...,  0.2256,  0.0518,  0.1650],
          [-0.0153,  0.0334, -0.1479,  ...,  0.1585, -0.0107,  0.1602]],

         [[-0.0289,  0.2098,  0.3138,  ...,  0.3758,  0.1246,  0.2152],
          [-0.0521,  0.1056,  0.2876,  ...,  0.2739, -0.0118,  0.1353],
          [-0.0985, -0.1028,  0.2350,  ...,  0.0701, -0.2847, -0.0245],
          ...,
          [ 0.0586, -0.0506, -0.3231,  ...,  0.1395,  0.2690,  0.1220],
          [ 0.2669, -0.1802, -0.1111,  ...,  0.0910,  0.0282,  0.0373],
          [ 0.3710, -0.2451, -0.0051,  ...,  0.0667, -0.0923, -0.0050]]]])

2025-07-09 11:00:33.597260 GPU 7 80703 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 139265, 128],"float32"), list[256,256,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 139265, 128],"float32"), list[256,256,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 8120651 / 8388608 (96.8%)
Greatest absolute difference: 0.9800946712493896 at index (0, 58, 49, 0) (up to 0.01 allowed)
Greatest relative difference: 4439005.0 at index (0, 19, 63, 134) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 256, 256]), dtype=torch.float32)
tensor([[[[ 2.3275e-01,  2.6661e-01,  3.0047e-01,  ..., -1.3429e-01, -1.2914e-01, -1.2399e-01],
          [-4.2120e-01, -6.7289e-02,  2.8662e-01,  ...,  6.1085e-02,  1.0940e-01,  1.5772e-01],
          [ 3.9039e-01,  8.5432e-02, -2.1952e-01,  ...,  1.8974e-01,  2.3370e-01,  2.7766e-01],
          ...,
          [-1.2402e-01,  7.5171e-02,  2.7436e-01,  ..., -1.0280e-01,  1.1262e-02,  1.2533e-01],
          [-4.2587e-01, -7.5831e-02,  2.7421e-01,  ...,  3.7179e-01, -4.5829e-03, -3.8096e-01],
          [-4.6686e-01, -3.8789e-01, -3.0892e-01,  ...,  2.2081e-01,  2.6598e-01,  3.1115e-01]],

         [[ 3.1680e-01,  1.2002e-01, -7.6756e-02,  ..., -2.4319e-01, -2.2490e-01, -2.0662e-01],
          [ 1.1976e-01, -6.2148e-02, -2.4405e-01,  ...,  2.7391e-01,  2.1391e-01,  1.5392e-01],
          [-3.4817e-01, -2.9042e-01, -2.3266e-01,  ..., -1.7749e-01,  4.1434e-02,  2.6036e-01],
          ...,
          [ 3.8994e-01,  3.9790e-01,  4.0587e-01,  ..., -3.5455e-02,  8.0861e-02,  1.9718e-01],
          [ 2.1936e-01,  2.8687e-01,  3.5439e-01,  ..., -1.8462e-01, -2.8980e-01, -3.9498e-01],
          [-3.3255e-01, -3.4872e-01, -3.6488e-01,  ..., -1.2561e-01, -1.2696e-01, -1.2830e-01]],

         [[-3.3287e-01,  7.8311e-02,  4.8950e-01,  ..., -1.4319e-01, -1.9409e-01, -2.4499e-01],
          [ 1.1334e-01, -8.3748e-03, -1.3009e-01,  ..., -2.5107e-01, -7.5296e-03,  2.3601e-01],
          [ 4.5796e-02,  1.2923e-01,  2.1266e-01,  ...,  3.5843e-02,  8.9952e-02,  1.4406e-01],
          ...,
          [ 1.1016e-01, -6.3949e-02, -2.3806e-01,  ...,  3.8224e-01,  1.2624e-01, -1.2975e-01],
          [ 3.5384e-01,  2.2732e-01,  1.0081e-01,  ...,  4.0346e-01,  3.2296e-01,  2.4246e-01],
          [-3.1686e-01, -1.1305e-02,  2.9425e-01,  ...,  2.7293e-01,  2.4039e-01,  2.0785e-01]],

         ...,

         [[ 4.2303e-01,  4.4634e-01,  4.6966e-01,  ..., -5.5898e-02,  1.6231e-01,  3.8052e-01],
          [-5.2587e-02, -1.0232e-01, -1.5205e-01,  ...,  9.2414e-02, -8.6860e-02, -2.6613e-01],
          [-1.3582e-01,  1.6064e-02,  1.6794e-01,  ...,  3.2799e-01,  1.9116e-01,  5.4328e-02],
          ...,
          [ 1.5774e-01,  7.5733e-02, -6.2786e-03,  ..., -1.3464e-01, -1.2142e-02,  1.1036e-01],
          [-3.7454e-02, -2.3669e-01, -4.3594e-01,  ...,  1.8961e-01,  9.4055e-02, -1.5021e-03],
          [-1.0681e-01,  1.1378e-01,  3.3436e-01,  ...,  3.9703e-01,  1.9769e-01, -1.6585e-03]],

         [[ 3.4713e-01,  3.5898e-01,  3.7082e-01,  ...,  6.9533e-02, -1.0993e-01, -2.8939e-01],
          [-2.3845e-01, -1.1415e-02,  2.1562e-01,  ..., -2.7936e-01, -3.4551e-01, -4.1165e-01],
          [-1.5992e-01, -2.3636e-01, -3.1279e-01,  ...,  2.4587e-01,  1.1378e-01, -1.8315e-02],
          ...,
          [ 1.8801e-01,  1.8969e-01,  1.9137e-01,  ...,  5.8553e-02, -2.5451e-02, -1.0946e-01],
          [ 2.5893e-01,  1.3290e-01,  6.8645e-03,  ...,  2.2727e-02,  9.6633e-02,  1.7054e-01],
          [ 9.6781e-02, -1.0982e-01, -3.1641e-01,  ...,  4.2054e-01,  7.3270e-02, -2.7400e-01]],

         [[-1.5661e-01, -1.5684e-01, -1.5707e-01,  ..., -3.7592e-01, -8.9118e-02,  1.9768e-01],
          [-9.2413e-02,  2.0515e-02,  1.3344e-01,  ...,  8.6560e-02, -1.1587e-01, -3.1830e-01],
          [-1.6633e-01,  3.3378e-02,  2.3308e-01,  ...,  2.1554e-01,  1.3043e-02, -1.8945e-01],
          ...,
          [-2.1943e-01, -1.2179e-01, -2.4141e-02,  ...,  7.5012e-02,  3.7700e-02,  3.8757e-04],
          [ 1.4647e-01,  1.9847e-01,  2.5047e-01,  ...,  1.9284e-01,  2.0673e-01,  2.2062e-01],
          [ 4.6498e-01,  4.0219e-01,  3.3939e-01,  ...,  1.8774e-01,  1.8933e-01,  1.9091e-01]]]])
DESIRED: (shape=torch.Size([1, 128, 256, 256]), dtype=torch.float32)
tensor([[[[ 1.6050e-01,  1.4401e-01,  1.1104e-01,  ...,  1.6681e-01, -1.8813e-02, -1.1162e-01],
          [-4.4571e-02, -2.0777e-02,  2.6812e-02,  ...,  1.8836e-01,  1.8451e-01,  1.8258e-01],
          [ 2.8182e-01,  1.6903e-01, -5.6559e-02,  ..., -1.4506e-01, -1.0440e-01, -8.4070e-02],
          ...,
          [-5.2418e-02, -8.5624e-02, -1.5204e-01,  ..., -7.2084e-02,  5.4473e-02,  1.1775e-01],
          [ 1.8296e-01,  8.5785e-02, -1.0857e-01,  ..., -1.2366e-01, -6.0586e-02, -2.9047e-02],
          [-2.5398e-01, -1.4572e-01,  7.0802e-02,  ...,  1.4712e-01,  9.9069e-04, -7.2075e-02]],

         [[-9.0997e-02, -1.3452e-01, -2.2158e-01,  ..., -2.4838e-01, -1.4385e-01, -9.1581e-02],
          [ 2.2889e-01,  1.6109e-01,  2.5496e-02,  ..., -2.1627e-02, -2.6491e-01, -3.8655e-01],
          [ 1.6909e-01,  2.2287e-01,  3.3042e-01,  ..., -1.0359e-01,  1.8787e-01,  3.3360e-01],
          ...,
          [-2.0533e-01, -1.7367e-01, -1.1035e-01,  ...,  1.6916e-02, -2.1386e-02, -4.0537e-02],
          [ 2.1006e-03,  6.3402e-02,  1.8600e-01,  ...,  1.0330e-01,  1.3531e-01,  1.5131e-01],
          [-1.4641e-01, -1.9239e-01, -2.8435e-01,  ...,  1.0536e-01,  1.9077e-01,  2.3347e-01]],

         [[ 3.3957e-01,  2.6183e-01,  1.0636e-01,  ...,  2.6072e-01,  6.7945e-02, -2.8442e-02],
          [-2.8002e-01, -2.4968e-01, -1.8899e-01,  ..., -2.4033e-01, -2.9525e-01, -3.2271e-01],
          [ 6.9891e-02, -4.6332e-03, -1.5368e-01,  ..., -3.3998e-01, -2.0163e-01, -1.3246e-01],
          ...,
          [ 1.0354e-01,  8.8537e-02,  5.8541e-02,  ...,  1.9776e-02,  1.5370e-01,  2.2067e-01],
          [-1.4248e-02,  3.4148e-02,  1.3094e-01,  ..., -7.7035e-02,  7.7979e-03,  5.0214e-02],
          [ 1.3926e-01,  1.4367e-01,  1.5249e-01,  ..., -1.9247e-01, -9.4501e-02, -4.5518e-02]],

         ...,

         [[ 3.4260e-01,  3.0851e-01,  2.4034e-01,  ..., -4.3241e-03, -9.6246e-02, -1.4221e-01],
          [-3.9596e-02, -6.2096e-02, -1.0710e-01,  ..., -8.6751e-02,  1.9070e-01,  3.2942e-01],
          [-4.2734e-01, -2.5045e-01,  1.0334e-01,  ...,  1.0679e-01,  6.5547e-02,  4.4926e-02],
          ...,
          [ 1.6928e-01,  4.9299e-02, -1.9066e-01,  ..., -1.6004e-01, -1.9549e-01, -2.1322e-01],
          [-2.3523e-01, -1.7275e-01, -4.7788e-02,  ..., -1.1133e-01, -1.5727e-01, -1.8023e-01],
          [ 9.3309e-02,  6.2110e-02, -2.8722e-04,  ...,  1.5100e-01,  1.0753e-01,  8.5803e-02]],

         [[ 6.3269e-02, -5.0886e-02, -2.7919e-01,  ...,  4.9432e-02, -5.0741e-02, -1.0083e-01],
          [-1.5214e-01, -1.6725e-01, -1.9746e-01,  ...,  1.1515e-03, -1.6537e-01, -2.4863e-01],
          [ 6.2232e-02,  2.4663e-02, -5.0475e-02,  ..., -2.0994e-01, -2.5032e-01, -2.7051e-01],
          ...,
          [ 1.6332e-01,  1.3824e-01,  8.8084e-02,  ..., -3.2399e-01, -1.6097e-01, -7.9464e-02],
          [ 3.5017e-03,  2.6195e-02,  7.1582e-02,  ..., -2.2925e-02,  2.0994e-01,  3.2637e-01],
          [ 1.6758e-02, -2.7548e-02, -1.1616e-01,  ...,  2.3221e-01,  1.3671e-01,  8.8955e-02]],

         [[-5.3715e-02, -7.5714e-02, -1.1971e-01,  ...,  2.8430e-01,  3.5822e-01,  3.9518e-01],
          [ 3.4261e-03, -1.0653e-02, -3.8811e-02,  ..., -1.5476e-01, -2.4744e-02,  4.0264e-02],
          [ 2.8739e-02,  1.4587e-02, -1.3717e-02,  ...,  2.7919e-01,  1.3539e-01,  6.3492e-02],
          ...,
          [-5.7532e-02, -3.4811e-02,  1.0632e-02,  ..., -2.7594e-01,  5.2824e-02,  2.1721e-01],
          [-9.1307e-02, -5.2493e-04,  1.8104e-01,  ...,  5.3776e-02, -6.1286e-02, -1.1882e-01],
          [ 2.3551e-01,  1.4829e-01, -2.6146e-02,  ..., -7.3702e-02, -1.6543e-01, -2.1129e-01]]]])

2025-07-09 11:00:44.232899 GPU 7 80703 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 16, 2097152],"float32"), list[16,32,], mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030070 (unix time) try "date -d @1752030070" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13b3f) received by PID 80703 (TID 0x7f7ea111a740) from PID 80703 ***]


2025-07-09 11:01:16.653547 GPU 7 83486 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 16, 2097152],"float32"), list[32,64,], mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030168 (unix time) try "date -d @1752030168" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1461e) received by PID 83486 (TID 0x7f58a671f740) from PID 83486 ***]


2025-07-09 11:01:24.005614 GPU 3 82890 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 16777216, 2],"float32"), list[128,128,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 16777216, 2],"float32"), list[128,128,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2034918 / 2097152 (97.0%)
Greatest absolute difference: 0.9902858138084412 at index (0, 42, 111, 0) (up to 0.01 allowed)
Greatest relative difference: 5302270.0 at index (0, 4, 21, 44) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.0524, -0.0558, -0.0591,  ..., -0.4723, -0.4756, -0.4790],
          [-0.4315, -0.4263, -0.4210,  ...,  0.2196,  0.2248,  0.2300],
          [ 0.4343,  0.4320,  0.4296,  ...,  0.1438,  0.1415,  0.1392],
          ...,
          [ 0.0157,  0.0154,  0.0151,  ..., -0.0240, -0.0243, -0.0246],
          [-0.1143, -0.1153, -0.1163,  ..., -0.2390, -0.2400, -0.2410],
          [-0.3433, -0.3395, -0.3356,  ...,  0.1404,  0.1443,  0.1481]],

         [[ 0.1062,  0.1068,  0.1074,  ...,  0.1833,  0.1839,  0.1846],
          [ 0.3687,  0.3654,  0.3621,  ..., -0.0471, -0.0505, -0.0538],
          [-0.0337, -0.0333, -0.0330,  ...,  0.0085,  0.0089,  0.0092],
          ...,
          [-0.4770, -0.4750, -0.4729,  ..., -0.2193, -0.2172, -0.2152],
          [-0.1369, -0.1350, -0.1332,  ...,  0.0941,  0.0960,  0.0978],
          [-0.2072, -0.2060, -0.2048,  ..., -0.0566, -0.0554, -0.0542]],

         [[ 0.1172,  0.1137,  0.1103,  ..., -0.3152, -0.3186, -0.3221],
          [-0.3500, -0.3493, -0.3486,  ..., -0.2645, -0.2638, -0.2632],
          [ 0.2400,  0.2409,  0.2418,  ...,  0.3536,  0.3545,  0.3554],
          ...,
          [ 0.4121,  0.4053,  0.3984,  ..., -0.4478, -0.4547, -0.4616],
          [ 0.0074,  0.0046,  0.0018,  ..., -0.3433, -0.3461, -0.3489],
          [ 0.2237,  0.2229,  0.2220,  ...,  0.1199,  0.1191,  0.1182]],

         ...,

         [[ 0.1279,  0.1267,  0.1255,  ..., -0.0238, -0.0250, -0.0263],
          [-0.0906, -0.0912, -0.0919,  ..., -0.1689, -0.1695, -0.1701],
          [ 0.3973,  0.3907,  0.3841,  ..., -0.4265, -0.4331, -0.4397],
          ...,
          [ 0.4548,  0.4534,  0.4520,  ...,  0.2812,  0.2798,  0.2784],
          [-0.2608, -0.2597, -0.2587,  ..., -0.1248, -0.1238, -0.1227],
          [ 0.4278,  0.4275,  0.4273,  ...,  0.4005,  0.4003,  0.4001]],

         [[-0.2323, -0.2304, -0.2284,  ...,  0.0137,  0.0156,  0.0176],
          [-0.3211, -0.3216, -0.3222,  ..., -0.3893, -0.3898, -0.3904],
          [-0.2617, -0.2605, -0.2593,  ..., -0.1080, -0.1068, -0.1056],
          ...,
          [-0.2244, -0.2264, -0.2283,  ..., -0.4720, -0.4740, -0.4759],
          [-0.4889, -0.4835, -0.4781,  ...,  0.1840,  0.1893,  0.1947],
          [ 0.3904,  0.3853,  0.3801,  ..., -0.2539, -0.2591, -0.2642]],

         [[-0.2408, -0.2412, -0.2417,  ..., -0.2974, -0.2978, -0.2983],
          [-0.4253, -0.4257, -0.4261,  ..., -0.4743, -0.4747, -0.4751],
          [ 0.0303,  0.0292,  0.0281,  ..., -0.1104, -0.1116, -0.1127],
          ...,
          [ 0.3697,  0.3640,  0.3584,  ..., -0.3348, -0.3404, -0.3461],
          [ 0.0774,  0.0781,  0.0788,  ...,  0.1662,  0.1669,  0.1676],
          [-0.0371, -0.0337, -0.0302,  ...,  0.3976,  0.4011,  0.4046]]]])
DESIRED: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[ 0.0506,  0.0506,  0.0506,  ..., -0.0638, -0.0638, -0.0638],
          [ 0.3239,  0.3239,  0.3239,  ..., -0.0942, -0.0942, -0.0942],
          [-0.1618, -0.1618, -0.1618,  ..., -0.0359, -0.0359, -0.0359],
          ...,
          [ 0.1590,  0.1590,  0.1590,  ...,  0.1923,  0.1923,  0.1923],
          [ 0.1482,  0.1482,  0.1482,  ..., -0.3684, -0.3684, -0.3684],
          [ 0.4179,  0.4179,  0.4179,  ...,  0.3674,  0.3674,  0.3674]],

         [[-0.2877, -0.2877, -0.2877,  ...,  0.0526,  0.0526,  0.0526],
          [ 0.2147,  0.2147,  0.2147,  ...,  0.2154,  0.2154,  0.2154],
          [-0.1558, -0.1558, -0.1558,  ..., -0.1745, -0.1745, -0.1745],
          ...,
          [-0.3864, -0.3864, -0.3864,  ..., -0.4666, -0.4666, -0.4666],
          [-0.3333, -0.3333, -0.3333,  ...,  0.4319,  0.4319,  0.4319],
          [ 0.1845,  0.1845,  0.1845,  ...,  0.2550,  0.2550,  0.2550]],

         [[-0.1547, -0.1547, -0.1547,  ...,  0.1353,  0.1353,  0.1353],
          [-0.0519, -0.0519, -0.0519,  ..., -0.0521, -0.0521, -0.0521],
          [ 0.2386,  0.2386,  0.2386,  ...,  0.0978,  0.0978,  0.0978],
          ...,
          [ 0.4513,  0.4513,  0.4513,  ...,  0.4232,  0.4232,  0.4232],
          [-0.2084, -0.2084, -0.2084,  ...,  0.2250,  0.2250,  0.2250],
          [-0.3292, -0.3292, -0.3292,  ..., -0.4502, -0.4502, -0.4502]],

         ...,

         [[-0.3567, -0.3567, -0.3567,  ...,  0.2985,  0.2985,  0.2985],
          [-0.0068, -0.0068, -0.0068,  ..., -0.3049, -0.3049, -0.3049],
          [ 0.0073,  0.0073,  0.0073,  ..., -0.0666, -0.0666, -0.0666],
          ...,
          [-0.2219, -0.2219, -0.2219,  ..., -0.3003, -0.3003, -0.3003],
          [-0.1804, -0.1804, -0.1804,  ...,  0.1314,  0.1314,  0.1314],
          [ 0.1460,  0.1460,  0.1460,  ..., -0.4093, -0.4093, -0.4093]],

         [[-0.0761, -0.0761, -0.0761,  ..., -0.2977, -0.2977, -0.2977],
          [-0.0671, -0.0671, -0.0671,  ...,  0.1073,  0.1073,  0.1073],
          [ 0.0980,  0.0980,  0.0980,  ...,  0.0058,  0.0058,  0.0058],
          ...,
          [ 0.4446,  0.4446,  0.4446,  ...,  0.4671,  0.4671,  0.4671],
          [-0.3294, -0.3294, -0.3294,  ..., -0.4523, -0.4523, -0.4523],
          [ 0.2496,  0.2496,  0.2496,  ..., -0.4812, -0.4812, -0.4812]],

         [[-0.1330, -0.1330, -0.1330,  ...,  0.2265,  0.2265,  0.2265],
          [-0.1854, -0.1854, -0.1854,  ...,  0.2814,  0.2814,  0.2814],
          [ 0.2304,  0.2304,  0.2304,  ..., -0.0440, -0.0440, -0.0440],
          ...,
          [ 0.2661,  0.2661,  0.2661,  ..., -0.0445, -0.0445, -0.0445],
          [-0.2678, -0.2678, -0.2678,  ...,  0.4400,  0.4400,  0.4400],
          [-0.0149, -0.0149, -0.0149,  ..., -0.0297, -0.0297, -0.0297]]]])

2025-07-09 11:01:28.535907 GPU 5 83943 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 16777216, 2],"float32"), list[16,32,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 128, 16777216, 2],"float32"), list[16,32,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 6053 / 4294967296 (0.0%)
Greatest absolute difference: 3.7479727268218994 at index (0, 71, 15204352, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 524287, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 16777216, 2]), dtype=torch.float32)
tensor([[[[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]],

         [[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]],

         [[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]],

         ...,

         [[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]],

         [[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]],

         [[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]]]])
DESIRED: (shape=torch.Size([1, 128, 16777216, 2]), dtype=torch.float32)
tensor([[[[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]],

         [[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]],

         [[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]],

         ...,

         [[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]],

         [[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]],

         [[0., 0.],
          [0., 0.],
          [0., 0.],
          ...,
          [0., 0.],
          [0., 0.],
          [0., 0.]]]])

2025-07-09 11:01:34.382212 GPU 4 84108 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 2, 16777216],"float32"), list[128,128,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 2, 16777216],"float32"), list[128,128,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2036233 / 2097152 (97.1%)
Greatest absolute difference: 0.9948508143424988 at index (0, 8, 0, 122) (up to 0.01 allowed)
Greatest relative difference: 233939.8125 at index (0, 118, 38, 39) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.0281, -0.1115,  0.4435,  ...,  0.3970, -0.1991, -0.1860],
          [-0.0296, -0.1131,  0.4371,  ...,  0.3920, -0.1939, -0.1844],
          [-0.0311, -0.1148,  0.4307,  ...,  0.3869, -0.1886, -0.1829],
          ...,
          [-0.2135, -0.3143, -0.3508,  ..., -0.2333,  0.4563,  0.0050],
          [-0.2150, -0.3159, -0.3571,  ..., -0.2383,  0.4616,  0.0065],
          [-0.2165, -0.3176, -0.3635,  ..., -0.2433,  0.4668,  0.0081]],

         [[-0.3690, -0.3621, -0.2300,  ..., -0.3974,  0.4619,  0.1720],
          [-0.3661, -0.3564, -0.2280,  ..., -0.3923,  0.4585,  0.1739],
          [-0.3632, -0.3506, -0.2260,  ..., -0.3873,  0.4550,  0.1758],
          ...,
          [-0.0086,  0.3556,  0.0192,  ...,  0.2366,  0.0287,  0.4113],
          [-0.0057,  0.3614,  0.0211,  ...,  0.2416,  0.0252,  0.4132],
          [-0.0028,  0.3671,  0.0231,  ...,  0.2467,  0.0217,  0.4151]],

         [[-0.1160, -0.2877,  0.2646,  ...,  0.0649, -0.1509,  0.1199],
          [-0.1117, -0.2870,  0.2597,  ...,  0.0620, -0.1525,  0.1176],
          [-0.1075, -0.2862,  0.2549,  ...,  0.0592, -0.1541,  0.1154],
          ...,
          [ 0.4150, -0.1947, -0.3400,  ..., -0.2956, -0.3546, -0.1607],
          [ 0.4192, -0.1940, -0.3448,  ..., -0.2985, -0.3563, -0.1630],
          [ 0.4235, -0.1933, -0.3497,  ..., -0.3014, -0.3579, -0.1652]],

         ...,

         [[-0.4666, -0.0882, -0.3312,  ..., -0.1911, -0.1643,  0.2033],
          [-0.4666, -0.0897, -0.3269,  ..., -0.1900, -0.1669,  0.2043],
          [-0.4667, -0.0912, -0.3226,  ..., -0.1889, -0.1695,  0.2052],
          ...,
          [-0.4706, -0.2813,  0.2084,  ..., -0.0480, -0.4920,  0.3228],
          [-0.4707, -0.2828,  0.2127,  ..., -0.0469, -0.4946,  0.3238],
          [-0.4707, -0.2844,  0.2171,  ..., -0.0457, -0.4973,  0.3248]],

         [[ 0.3833,  0.3508, -0.4011,  ...,  0.4618, -0.2949,  0.1603],
          [ 0.3810,  0.3503, -0.3991,  ...,  0.4591, -0.2946,  0.1610],
          [ 0.3786,  0.3498, -0.3970,  ...,  0.4564, -0.2942,  0.1616],
          ...,
          [ 0.0925,  0.2935, -0.1420,  ...,  0.1193, -0.2473,  0.2437],
          [ 0.0901,  0.2930, -0.1400,  ...,  0.1166, -0.2469,  0.2443],
          [ 0.0878,  0.2925, -0.1379,  ...,  0.1139, -0.2465,  0.2450]],

         [[-0.3476, -0.0709, -0.0433,  ..., -0.2262, -0.0168, -0.4823],
          [-0.3481, -0.0720, -0.0445,  ..., -0.2208, -0.0178, -0.4818],
          [-0.3486, -0.0732, -0.0457,  ..., -0.2154, -0.0187, -0.4814],
          ...,
          [-0.4098, -0.2093, -0.1948,  ...,  0.4505, -0.1352, -0.4231],
          [-0.4103, -0.2104, -0.1961,  ...,  0.4559, -0.1361, -0.4226],
          [-0.4108, -0.2115, -0.1973,  ...,  0.4613, -0.1371, -0.4222]]]])
DESIRED: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[ 0.2840, -0.0657,  0.0423,  ...,  0.3842,  0.2024, -0.0678],
          [ 0.2840, -0.0657,  0.0423,  ...,  0.3842,  0.2024, -0.0678],
          [ 0.2840, -0.0657,  0.0423,  ...,  0.3842,  0.2024, -0.0678],
          ...,
          [-0.0176, -0.0893,  0.1522,  ...,  0.0515,  0.0271,  0.4366],
          [-0.0176, -0.0893,  0.1522,  ...,  0.0515,  0.0271,  0.4366],
          [-0.0176, -0.0893,  0.1522,  ...,  0.0515,  0.0271,  0.4366]],

         [[ 0.4958,  0.2009,  0.1015,  ..., -0.3136, -0.2720, -0.1476],
          [ 0.4958,  0.2009,  0.1015,  ..., -0.3136, -0.2720, -0.1476],
          [ 0.4958,  0.2009,  0.1015,  ..., -0.3136, -0.2720, -0.1476],
          ...,
          [-0.1397,  0.2758, -0.1401,  ...,  0.3516, -0.3378, -0.3357],
          [-0.1397,  0.2758, -0.1401,  ...,  0.3516, -0.3378, -0.3357],
          [-0.1397,  0.2758, -0.1401,  ...,  0.3516, -0.3378, -0.3357]],

         [[-0.0420,  0.4892,  0.1046,  ...,  0.4020, -0.4664, -0.3106],
          [-0.0420,  0.4892,  0.1046,  ...,  0.4020, -0.4664, -0.3106],
          [-0.0420,  0.4892,  0.1046,  ...,  0.4020, -0.4664, -0.3106],
          ...,
          [-0.1384,  0.1492,  0.0529,  ...,  0.0822,  0.1611,  0.1348],
          [-0.1384,  0.1492,  0.0529,  ...,  0.0822,  0.1611,  0.1348],
          [-0.1384,  0.1492,  0.0529,  ...,  0.0822,  0.1611,  0.1348]],

         ...,

         [[ 0.2595, -0.1365,  0.2642,  ...,  0.3211,  0.4951,  0.4564],
          [ 0.2595, -0.1365,  0.2642,  ...,  0.3211,  0.4951,  0.4564],
          [ 0.2595, -0.1365,  0.2642,  ...,  0.3211,  0.4951,  0.4564],
          ...,
          [ 0.2087,  0.0539, -0.0269,  ...,  0.4207, -0.1002,  0.3145],
          [ 0.2087,  0.0539, -0.0269,  ...,  0.4207, -0.1002,  0.3145],
          [ 0.2087,  0.0539, -0.0269,  ...,  0.4207, -0.1002,  0.3145]],

         [[ 0.4172, -0.1988, -0.1050,  ...,  0.2230, -0.4805, -0.4974],
          [ 0.4172, -0.1988, -0.1050,  ...,  0.2230, -0.4805, -0.4974],
          [ 0.4172, -0.1988, -0.1050,  ...,  0.2230, -0.4805, -0.4974],
          ...,
          [ 0.0396, -0.2497,  0.2727,  ...,  0.1742, -0.0467, -0.3013],
          [ 0.0396, -0.2497,  0.2727,  ...,  0.1742, -0.0467, -0.3013],
          [ 0.0396, -0.2497,  0.2727,  ...,  0.1742, -0.0467, -0.3013]],

         [[ 0.3825, -0.1105, -0.0067,  ...,  0.4313, -0.0061, -0.2868],
          [ 0.3825, -0.1105, -0.0067,  ...,  0.4313, -0.0061, -0.2868],
          [ 0.3825, -0.1105, -0.0067,  ...,  0.4313, -0.0061, -0.2868],
          ...,
          [ 0.2331,  0.1321, -0.0877,  ...,  0.0153,  0.0296,  0.2081],
          [ 0.2331,  0.1321, -0.0877,  ...,  0.0153,  0.0296,  0.2081],
          [ 0.2331,  0.1321, -0.0877,  ...,  0.0153,  0.0296,  0.2081]]]])

2025-07-09 11:01:46.206862 GPU 3 82890 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 2, 16777216],"float32"), list[16,32,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030127 (unix time) try "date -d @1752030127" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x143ca) received by PID 82890 (TID 0x7efd27d90740) from PID 82890 ***]


2025-07-09 11:02:12.883808 GPU 2 82273 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 2, 8912897],"float32"), list[128,128,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 2, 8912897],"float32"), list[128,128,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2036635 / 2097152 (97.1%)
Greatest absolute difference: 0.9844222068786621 at index (0, 109, 127, 124) (up to 0.01 allowed)
Greatest relative difference: 1540658.875 at index (0, 29, 32, 118) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.0957,  0.4440,  0.1441,  ..., -0.4151,  0.2163, -0.1991],
          [-0.0912,  0.4406,  0.1437,  ..., -0.4132,  0.2127, -0.2001],
          [-0.0866,  0.4371,  0.1433,  ..., -0.4113,  0.2092, -0.2011],
          ...,
          [ 0.4723,  0.0121,  0.0923,  ..., -0.1757, -0.2254, -0.3198],
          [ 0.4769,  0.0087,  0.0919,  ..., -0.1738, -0.2289, -0.3208],
          [ 0.4814,  0.0052,  0.0915,  ..., -0.1719, -0.2325, -0.3217]],

         [[-0.1787,  0.3078,  0.0831,  ...,  0.0975,  0.0263,  0.3083],
          [-0.1749,  0.3051,  0.0852,  ...,  0.0940,  0.0251,  0.3093],
          [-0.1712,  0.3024,  0.0874,  ...,  0.0906,  0.0239,  0.3103],
          ...,
          [ 0.2890, -0.0318,  0.3510,  ..., -0.3321, -0.1243,  0.4323],
          [ 0.2927, -0.0346,  0.3531,  ..., -0.3355, -0.1255,  0.4333],
          [ 0.2964, -0.0373,  0.3552,  ..., -0.3390, -0.1267,  0.4343]],

         [[-0.1503,  0.0607,  0.0423,  ..., -0.4308, -0.0597, -0.1297],
          [-0.1491,  0.0579,  0.0420,  ..., -0.4292, -0.0564, -0.1258],
          [-0.1480,  0.0550,  0.0418,  ..., -0.4277, -0.0531, -0.1219],
          ...,
          [-0.0057, -0.2951,  0.0162,  ..., -0.2422,  0.3575,  0.3567],
          [-0.0045, -0.2980,  0.0160,  ..., -0.2407,  0.3608,  0.3606],
          [-0.0034, -0.3008,  0.0158,  ..., -0.2392,  0.3642,  0.3645]],

         ...,

         [[ 0.1262,  0.0617,  0.0821,  ..., -0.1233,  0.3767,  0.1181],
          [ 0.1221,  0.0646,  0.0821,  ..., -0.1201,  0.3751,  0.1202],
          [ 0.1180,  0.0675,  0.0821,  ..., -0.1169,  0.3735,  0.1223],
          ...,
          [-0.3863,  0.4233,  0.0839,  ...,  0.2760,  0.1793,  0.3836],
          [-0.3904,  0.4262,  0.0839,  ...,  0.2792,  0.1777,  0.3857],
          [-0.3945,  0.4291,  0.0839,  ...,  0.2824,  0.1762,  0.3878]],

         [[ 0.4267,  0.3639, -0.3546,  ..., -0.1354,  0.0558,  0.1411],
          [ 0.4249,  0.3595, -0.3540,  ..., -0.1377,  0.0586,  0.1405],
          [ 0.4230,  0.3551, -0.3534,  ..., -0.1399,  0.0614,  0.1400],
          ...,
          [ 0.1910, -0.1842, -0.2831,  ..., -0.4155,  0.4015,  0.0719],
          [ 0.1891, -0.1886, -0.2826,  ..., -0.4178,  0.4042,  0.0713],
          [ 0.1872, -0.1930, -0.2820,  ..., -0.4200,  0.4070,  0.0708]],

         [[ 0.2468, -0.0846, -0.1835,  ..., -0.1536,  0.1720, -0.0264],
          [ 0.2457, -0.0809, -0.1822,  ..., -0.1495,  0.1743, -0.0285],
          [ 0.2446, -0.0771, -0.1809,  ..., -0.1454,  0.1766, -0.0306],
          ...,
          [ 0.1118,  0.3864, -0.0232,  ...,  0.3579,  0.4574, -0.2870],
          [ 0.1107,  0.3902, -0.0219,  ...,  0.3620,  0.4597, -0.2890],
          [ 0.1096,  0.3939, -0.0206,  ...,  0.3661,  0.4620, -0.2911]]]])
DESIRED: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[ 0.0675,  0.0260, -0.2409,  ...,  0.1493,  0.0210, -0.0502],
          [ 0.0675,  0.0260, -0.2409,  ...,  0.1493,  0.0210, -0.0502],
          [ 0.0675,  0.0260, -0.2409,  ...,  0.1493,  0.0210, -0.0502],
          ...,
          [ 0.1039,  0.3109,  0.1126,  ...,  0.1216, -0.0052, -0.3894],
          [ 0.1039,  0.3109,  0.1126,  ...,  0.1216, -0.0052, -0.3894],
          [ 0.1039,  0.3109,  0.1126,  ...,  0.1216, -0.0052, -0.3894]],

         [[-0.1780,  0.3236, -0.0240,  ..., -0.3477,  0.1818, -0.1634],
          [-0.1780,  0.3236, -0.0240,  ..., -0.3477,  0.1818, -0.1634],
          [-0.1780,  0.3236, -0.0240,  ..., -0.3477,  0.1818, -0.1634],
          ...,
          [-0.1481, -0.1118, -0.1197,  ..., -0.0507,  0.3349, -0.4104],
          [-0.1481, -0.1118, -0.1197,  ..., -0.0507,  0.3349, -0.4104],
          [-0.1481, -0.1118, -0.1197,  ..., -0.0507,  0.3349, -0.4104]],

         [[-0.0989,  0.0056,  0.0043,  ...,  0.0918,  0.4293,  0.3292],
          [-0.0989,  0.0056,  0.0043,  ...,  0.0918,  0.4293,  0.3292],
          [-0.0989,  0.0056,  0.0043,  ...,  0.0918,  0.4293,  0.3292],
          ...,
          [-0.0738,  0.1819,  0.2028,  ...,  0.1845,  0.2957, -0.1630],
          [-0.0738,  0.1819,  0.2028,  ...,  0.1845,  0.2957, -0.1630],
          [-0.0738,  0.1819,  0.2028,  ...,  0.1845,  0.2957, -0.1630]],

         ...,

         [[ 0.0565,  0.1515,  0.0277,  ...,  0.3617,  0.0559, -0.1798],
          [ 0.0565,  0.1515,  0.0277,  ...,  0.3617,  0.0559, -0.1798],
          [ 0.0565,  0.1515,  0.0277,  ...,  0.3617,  0.0559, -0.1798],
          ...,
          [-0.3578,  0.1506,  0.0588,  ..., -0.4095,  0.1832, -0.3815],
          [-0.3578,  0.1506,  0.0588,  ..., -0.4095,  0.1832, -0.3815],
          [-0.3578,  0.1506,  0.0588,  ..., -0.4095,  0.1832, -0.3815]],

         [[-0.1607, -0.1318,  0.1845,  ...,  0.3810,  0.2892,  0.1973],
          [-0.1607, -0.1318,  0.1845,  ...,  0.3810,  0.2892,  0.1973],
          [-0.1607, -0.1318,  0.1845,  ...,  0.3810,  0.2892,  0.1973],
          ...,
          [-0.1195,  0.3179,  0.4279,  ..., -0.1139, -0.4466, -0.1806],
          [-0.1195,  0.3179,  0.4279,  ..., -0.1139, -0.4466, -0.1806],
          [-0.1195,  0.3179,  0.4279,  ..., -0.1139, -0.4466, -0.1806]],

         [[-0.0468,  0.4430,  0.1577,  ..., -0.1757, -0.2649, -0.2882],
          [-0.0468,  0.4430,  0.1577,  ..., -0.1757, -0.2649, -0.2882],
          [-0.0468,  0.4430,  0.1577,  ..., -0.1757, -0.2649, -0.2882],
          ...,
          [-0.2100,  0.2358, -0.2321,  ...,  0.1031, -0.2457,  0.2540],
          [-0.2100,  0.2358, -0.2321,  ...,  0.1031, -0.2457,  0.2540],
          [-0.2100,  0.2358, -0.2321,  ...,  0.1031, -0.2457,  0.2540]]]])

2025-07-09 11:02:26.172112 GPU 2 82273 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 262144, 128],"float32"), list[256,256,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 262144, 128],"float32"), list[256,256,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 8106004 / 8388608 (96.6%)
Greatest absolute difference: 0.9564828872680664 at index (0, 60, 95, 255) (up to 0.01 allowed)
Greatest relative difference: 2061564.5 at index (0, 78, 171, 87) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 256, 256]), dtype=torch.float32)
tensor([[[[-0.0957, -0.2906, -0.4855,  ...,  0.3116, -0.0064, -0.3245],
          [-0.2495, -0.0674,  0.1147,  ...,  0.4247,  0.3274,  0.2301],
          [ 0.0511,  0.0985,  0.1459,  ...,  0.3686,  0.1331, -0.1024],
          ...,
          [ 0.1820,  0.2734,  0.3648,  ...,  0.3429,  0.3663,  0.3897],
          [-0.1831, -0.0499,  0.0833,  ..., -0.0259, -0.2087, -0.3915],
          [-0.1147, -0.2596, -0.4044,  ..., -0.4013, -0.0582,  0.2849]],

         [[ 0.0792, -0.0030, -0.0852,  ..., -0.0016, -0.0324, -0.0633],
          [ 0.2528, -0.0565, -0.3657,  ...,  0.0818,  0.1505,  0.2191],
          [ 0.2046,  0.3146,  0.4245,  ..., -0.4162, -0.0906,  0.2350],
          ...,
          [ 0.4181,  0.4034,  0.3888,  ...,  0.2591,  0.3133,  0.3676],
          [-0.0485, -0.1999, -0.3513,  ..., -0.2267, -0.0909,  0.0449],
          [-0.0465, -0.1942, -0.3419,  ...,  0.1425, -0.1467, -0.4360]],

         [[-0.3309, -0.3855, -0.4401,  ...,  0.1477,  0.0102, -0.1272],
          [-0.4364, -0.3620, -0.2876,  ...,  0.0564, -0.0664, -0.1892],
          [ 0.4312,  0.0953, -0.2406,  ..., -0.0960,  0.1166,  0.3291],
          ...,
          [ 0.2837, -0.0268, -0.3373,  ..., -0.4515, -0.1547,  0.1420],
          [-0.4439, -0.4302, -0.4166,  ..., -0.2651, -0.0224,  0.2204],
          [-0.1604, -0.1523, -0.1441,  ...,  0.0063, -0.0890, -0.1843]],

         ...,

         [[-0.2008,  0.0799,  0.3606,  ..., -0.1291, -0.0316,  0.0659],
          [-0.3565, -0.1814, -0.0063,  ...,  0.2167, -0.0540, -0.3247],
          [-0.0989,  0.1902,  0.4793,  ...,  0.2910,  0.0425, -0.2060],
          ...,
          [ 0.1938, -0.0503, -0.2945,  ..., -0.0112,  0.1210,  0.2533],
          [-0.2845, -0.3164, -0.3482,  ...,  0.4675,  0.3838,  0.3001],
          [ 0.2343,  0.1965,  0.1587,  ...,  0.3584,  0.2229,  0.0875]],

         [[ 0.0507, -0.0136, -0.0779,  ..., -0.0949, -0.0587, -0.0225],
          [ 0.3994,  0.0771, -0.2452,  ...,  0.1682, -0.0693, -0.3069],
          [-0.1426, -0.0430,  0.0566,  ...,  0.2686,  0.3619,  0.4552],
          ...,
          [ 0.4590,  0.0327, -0.3936,  ...,  0.1891,  0.2579,  0.3268],
          [ 0.2420,  0.3210,  0.4000,  ...,  0.1717, -0.0839, -0.3394],
          [-0.3850, -0.2042, -0.0233,  ...,  0.3206,  0.0327, -0.2552]],

         [[-0.2914,  0.0635,  0.4185,  ...,  0.2638, -0.1084, -0.4805],
          [-0.2632,  0.0947,  0.4526,  ...,  0.3376,  0.2854,  0.2331],
          [-0.1367, -0.3048, -0.4728,  ..., -0.1504,  0.1421,  0.4346],
          ...,
          [ 0.1563, -0.1353, -0.4269,  ..., -0.0595, -0.1349, -0.2102],
          [-0.0576, -0.0734, -0.0891,  ...,  0.3712, -0.0379, -0.4469],
          [ 0.0773, -0.1806, -0.4384,  ...,  0.3867,  0.2241,  0.0615]]]])
DESIRED: (shape=torch.Size([1, 128, 256, 256]), dtype=torch.float32)
tensor([[[[-0.2010, -0.2221, -0.2642,  ..., -0.2086, -0.0530,  0.0248],
          [ 0.2339,  0.1762,  0.0607,  ..., -0.0988, -0.0494, -0.0247],
          [ 0.2952,  0.2876,  0.2725,  ..., -0.0169, -0.0705, -0.0972],
          ...,
          [-0.3764, -0.3108, -0.1797,  ..., -0.1439, -0.0671, -0.0287],
          [-0.1567, -0.0673,  0.1117,  ..., -0.0295, -0.0574, -0.0714],
          [ 0.0434,  0.0963,  0.2020,  ...,  0.2256,  0.1035,  0.0424]],

         [[-0.2802, -0.1990, -0.0365,  ..., -0.0671,  0.0378,  0.0903],
          [ 0.1843,  0.0438, -0.2372,  ..., -0.1296,  0.0684,  0.1675],
          [ 0.0702,  0.0348, -0.0360,  ..., -0.1272, -0.2625, -0.3302],
          ...,
          [-0.2718, -0.1190,  0.1866,  ...,  0.1903,  0.1053,  0.0627],
          [-0.1302, -0.0563,  0.0915,  ..., -0.2162, -0.1730, -0.1514],
          [-0.1769, -0.1776, -0.1790,  ..., -0.2601, -0.2094, -0.1841]],

         [[ 0.0259,  0.0904,  0.2194,  ..., -0.0726,  0.1613,  0.2782],
          [ 0.1486,  0.1110,  0.0356,  ..., -0.2462, -0.0243,  0.0867],
          [-0.1776, -0.1259, -0.0226,  ..., -0.3798, -0.2170, -0.1355],
          ...,
          [-0.1257, -0.1314, -0.1428,  ..., -0.1052, -0.0958, -0.0912],
          [ 0.0553,  0.0031, -0.1014,  ...,  0.0048, -0.1543, -0.2339],
          [ 0.0913,  0.0491, -0.0355,  ..., -0.0405, -0.1801, -0.2499]],

         ...,

         [[ 0.1463,  0.0635, -0.1020,  ..., -0.0128, -0.0497, -0.0682],
          [-0.0927,  0.0086,  0.2112,  ..., -0.0242,  0.0333,  0.0621],
          [-0.2802, -0.1959, -0.0273,  ...,  0.0329,  0.0231,  0.0183],
          ...,
          [ 0.1379,  0.0854, -0.0196,  ...,  0.3340,  0.0906, -0.0310],
          [-0.1960, -0.1173,  0.0402,  ...,  0.2496,  0.2732,  0.2850],
          [ 0.1809,  0.1753,  0.1642,  ...,  0.2481,  0.2863,  0.3055]],

         [[ 0.0675,  0.0666,  0.0650,  ..., -0.0122, -0.1748, -0.2561],
          [-0.0573, -0.0137,  0.0734,  ..., -0.2520, -0.2684, -0.2766],
          [ 0.2108,  0.1388, -0.0051,  ..., -0.0257,  0.1088,  0.1760],
          ...,
          [-0.2234, -0.2004, -0.1545,  ..., -0.0456,  0.1271,  0.2135],
          [-0.2584, -0.2060, -0.1011,  ..., -0.2014, -0.1594, -0.1385],
          [-0.2395, -0.2068, -0.1414,  ...,  0.1180,  0.0479,  0.0128]],

         [[ 0.2140,  0.1921,  0.1483,  ..., -0.0063,  0.2484,  0.3758],
          [ 0.1163,  0.0024, -0.2254,  ...,  0.2223,  0.2051,  0.1965],
          [-0.0259,  0.0225,  0.1194,  ...,  0.2609,  0.2084,  0.1821],
          ...,
          [-0.0035, -0.0373, -0.1047,  ..., -0.0145, -0.1187, -0.1707],
          [ 0.3249,  0.1538, -0.1883,  ..., -0.1324, -0.1445, -0.1505],
          [-0.2352, -0.2123, -0.1665,  ..., -0.0246, -0.0660, -0.0867]]]])

2025-07-09 11:02:52.063705 GPU 2 82273 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 3, 11184811],"float32"), list[128,128,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 3, 11184811],"float32"), list[128,128,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2036016 / 2097152 (97.1%)
Greatest absolute difference: 0.9983305335044861 at index (0, 72, 0, 99) (up to 0.01 allowed)
Greatest relative difference: 874319.6875 at index (0, 124, 63, 100) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.0957,  0.0811,  0.1632,  ...,  0.4893,  0.2656,  0.3716],
          [-0.0950,  0.0833,  0.1570,  ...,  0.4882,  0.2619,  0.3666],
          [-0.0944,  0.0855,  0.1508,  ...,  0.4870,  0.2582,  0.3617],
          ...,
          [-0.2076,  0.1409,  0.0011,  ...,  0.0925, -0.0420,  0.0785],
          [-0.2101,  0.1396,  0.0049,  ...,  0.0872, -0.0432,  0.0789],
          [-0.2126,  0.1383,  0.0087,  ...,  0.0819, -0.0443,  0.0792]],

         [[-0.0858,  0.0386, -0.0843,  ..., -0.2317,  0.1500, -0.0024],
          [-0.0773,  0.0368, -0.0814,  ..., -0.2220,  0.1504, -0.0063],
          [-0.0687,  0.0350, -0.0784,  ..., -0.2123,  0.1508, -0.0101],
          ...,
          [-0.3369,  0.2892, -0.3817,  ...,  0.4788,  0.1893, -0.4345],
          [-0.3498,  0.2952, -0.3895,  ...,  0.4803,  0.1895, -0.4375],
          [-0.3627,  0.3011, -0.3974,  ...,  0.4818,  0.1898, -0.4405]],

         [[-0.2003,  0.2058,  0.2027,  ..., -0.3216, -0.0730, -0.3552],
          [-0.2004,  0.2036,  0.1950,  ..., -0.3215, -0.0688, -0.3557],
          [-0.2005,  0.2015,  0.1874,  ..., -0.3214, -0.0645, -0.3563],
          ...,
          [ 0.3136, -0.2399,  0.0338,  ...,  0.3312,  0.2230, -0.4188],
          [ 0.3221, -0.2450,  0.0389,  ...,  0.3418,  0.2234, -0.4193],
          [ 0.3305, -0.2500,  0.0440,  ...,  0.3523,  0.2238, -0.4198]],

         ...,

         [[ 0.3096, -0.1663, -0.3207,  ...,  0.2103,  0.4910, -0.4658],
          [ 0.3080, -0.1658, -0.3136,  ...,  0.2121,  0.4874, -0.4578],
          [ 0.3064, -0.1654, -0.3064,  ...,  0.2139,  0.4839, -0.4499],
          ...,
          [-0.2083, -0.0446, -0.1357,  ..., -0.0920, -0.3683, -0.0448],
          [-0.2151, -0.0431, -0.1400,  ..., -0.0987, -0.3786, -0.0462],
          [-0.2219, -0.0416, -0.1444,  ..., -0.1055, -0.3889, -0.0475]],

         [[-0.0952, -0.2832,  0.4559,  ...,  0.2277,  0.2186, -0.1918],
          [-0.0962, -0.2785,  0.4446,  ...,  0.2199,  0.2151, -0.1945],
          [-0.0973, -0.2738,  0.4333,  ...,  0.2120,  0.2116, -0.1973],
          ...,
          [-0.2058, -0.1008,  0.2556,  ...,  0.1604, -0.1356,  0.2468],
          [-0.2065, -0.1027,  0.2640,  ...,  0.1675, -0.1378,  0.2568],
          [-0.2073, -0.1046,  0.2724,  ...,  0.1745, -0.1399,  0.2667]],

         [[-0.4805,  0.1030,  0.1670,  ...,  0.1387,  0.1136,  0.0753],
          [-0.4663,  0.1037,  0.1663,  ...,  0.1333,  0.1067,  0.0712],
          [-0.4520,  0.1044,  0.1656,  ...,  0.1279,  0.0999,  0.0671],
          ...,
          [ 0.3090,  0.2193,  0.0515,  ...,  0.0485, -0.0971,  0.2794],
          [ 0.3071,  0.2204,  0.0503,  ...,  0.0526, -0.0935,  0.2869],
          [ 0.3052,  0.2216,  0.0492,  ...,  0.0567, -0.0898,  0.2945]]]])
DESIRED: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[ 3.4923e-01, -1.1054e-01, -4.1946e-01,  ..., -1.5570e-01, -4.2823e-01, -3.9903e-01],
          [ 3.4923e-01, -1.1054e-01, -4.1946e-01,  ..., -1.5570e-01, -4.2823e-01, -3.9903e-01],
          [ 3.4923e-01, -1.1054e-01, -4.1946e-01,  ..., -1.5570e-01, -4.2823e-01, -3.9903e-01],
          ...,
          [-1.8792e-01, -2.4582e-01,  7.9363e-02,  ..., -1.5714e-01,  2.3324e-01,  8.5687e-02],
          [-1.8792e-01, -2.4582e-01,  7.9363e-02,  ..., -1.5714e-01,  2.3324e-01,  8.5687e-02],
          [-1.8792e-01, -2.4582e-01,  7.9363e-02,  ..., -1.5714e-01,  2.3324e-01,  8.5687e-02]],

         [[-5.8892e-02, -3.4152e-02,  3.6929e-01,  ...,  4.4631e-02,  4.5557e-01, -3.5732e-01],
          [-5.8892e-02, -3.4152e-02,  3.6929e-01,  ...,  4.4631e-02,  4.5557e-01, -3.5732e-01],
          [-5.8892e-02, -3.4152e-02,  3.6929e-01,  ...,  4.4631e-02,  4.5557e-01, -3.5732e-01],
          ...,
          [-2.2904e-02,  1.4857e-04, -4.6352e-01,  ..., -3.6426e-01,  3.3768e-01,  2.7003e-01],
          [-2.2904e-02,  1.4857e-04, -4.6352e-01,  ..., -3.6426e-01,  3.3768e-01,  2.7003e-01],
          [-2.2904e-02,  1.4857e-04, -4.6352e-01,  ..., -3.6426e-01,  3.3768e-01,  2.7003e-01]],

         [[ 4.7868e-01,  6.8982e-02,  1.8586e-01,  ...,  1.6275e-01, -4.7188e-01, -3.4142e-01],
          [ 4.7868e-01,  6.8982e-02,  1.8586e-01,  ...,  1.6275e-01, -4.7188e-01, -3.4142e-01],
          [ 4.7868e-01,  6.8982e-02,  1.8586e-01,  ...,  1.6275e-01, -4.7188e-01, -3.4142e-01],
          ...,
          [-1.2798e-01, -1.2518e-01,  2.3373e-01,  ..., -2.1581e-01,  3.2958e-01,  2.1956e-01],
          [-1.2798e-01, -1.2518e-01,  2.3373e-01,  ..., -2.1581e-01,  3.2958e-01,  2.1956e-01],
          [-1.2798e-01, -1.2518e-01,  2.3373e-01,  ..., -2.1581e-01,  3.2958e-01,  2.1956e-01]],

         ...,

         [[-5.5494e-02,  4.3452e-02, -1.7811e-01,  ...,  4.8776e-01,  3.8819e-01, -9.2260e-02],
          [-5.5494e-02,  4.3452e-02, -1.7811e-01,  ...,  4.8776e-01,  3.8819e-01, -9.2260e-02],
          [-5.5494e-02,  4.3452e-02, -1.7811e-01,  ...,  4.8776e-01,  3.8819e-01, -9.2260e-02],
          ...,
          [-2.0831e-01,  2.5122e-01, -3.5086e-01,  ...,  1.2344e-01,  1.7889e-01, -4.7144e-01],
          [-2.0831e-01,  2.5122e-01, -3.5086e-01,  ...,  1.2344e-01,  1.7889e-01, -4.7144e-01],
          [-2.0831e-01,  2.5122e-01, -3.5086e-01,  ...,  1.2344e-01,  1.7889e-01, -4.7144e-01]],

         [[-4.1812e-01,  1.0962e-02,  2.3423e-01,  ...,  3.5108e-01,  2.0903e-02,  2.1968e-01],
          [-4.1812e-01,  1.0962e-02,  2.3423e-01,  ...,  3.5108e-01,  2.0903e-02,  2.1968e-01],
          [-4.1812e-01,  1.0962e-02,  2.3423e-01,  ...,  3.5108e-01,  2.0903e-02,  2.1968e-01],
          ...,
          [ 2.1668e-01,  2.1091e-01, -3.1326e-02,  ...,  1.8430e-01, -4.1991e-01,  7.6443e-02],
          [ 2.1668e-01,  2.1091e-01, -3.1326e-02,  ...,  1.8430e-01, -4.1991e-01,  7.6443e-02],
          [ 2.1668e-01,  2.1091e-01, -3.1326e-02,  ...,  1.8430e-01, -4.1991e-01,  7.6443e-02]],

         [[-4.3855e-03,  3.7364e-01,  2.6877e-01,  ..., -8.4603e-02,  1.8641e-01,  3.2864e-01],
          [-4.3855e-03,  3.7364e-01,  2.6877e-01,  ..., -8.4603e-02,  1.8641e-01,  3.2864e-01],
          [-4.3855e-03,  3.7364e-01,  2.6877e-01,  ..., -8.4603e-02,  1.8641e-01,  3.2864e-01],
          ...,
          [-1.8347e-01, -1.7079e-01, -5.9301e-02,  ...,  1.0646e-01, -2.8216e-01, -8.3669e-02],
          [-1.8347e-01, -1.7079e-01, -5.9301e-02,  ...,  1.0646e-01, -2.8216e-01, -8.3669e-02],
          [-1.8347e-01, -1.7079e-01, -5.9301e-02,  ...,  1.0646e-01, -2.8216e-01, -8.3669e-02]]]])

2025-07-09 11:02:53.742184 GPU 7 84302 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 32, 1048576],"float32"), list[128,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030264 (unix time) try "date -d @1752030264" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1494e) received by PID 84302 (TID 0x7f9d73287740) from PID 84302 ***]


2025-07-09 11:02:57.505195 GPU 3 84460 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 33554432, 1],"float32"), list[128,128,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 33554432, 1],"float32"), list[128,128,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2046464 / 2097152 (97.6%)
Greatest absolute difference: 0.9873965382575989 at index (0, 97, 105, 0) (up to 0.01 allowed)
Greatest relative difference: 7215.87255859375 at index (0, 3, 90, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.2168, -0.2168, -0.2168,  ..., -0.2168, -0.2168, -0.2168],
          [ 0.0176,  0.0176,  0.0176,  ...,  0.0176,  0.0176,  0.0176],
          [-0.4060, -0.4060, -0.4060,  ..., -0.4060, -0.4060, -0.4060],
          ...,
          [ 0.3200,  0.3200,  0.3200,  ...,  0.3200,  0.3200,  0.3200],
          [-0.4858, -0.4858, -0.4858,  ..., -0.4858, -0.4858, -0.4858],
          [ 0.4911,  0.4911,  0.4911,  ...,  0.4911,  0.4911,  0.4911]],

         [[ 0.2432,  0.2432,  0.2432,  ...,  0.2432,  0.2432,  0.2432],
          [ 0.3472,  0.3472,  0.3472,  ...,  0.3472,  0.3472,  0.3472],
          [ 0.3398,  0.3398,  0.3398,  ...,  0.3398,  0.3398,  0.3398],
          ...,
          [-0.1519, -0.1519, -0.1519,  ..., -0.1519, -0.1519, -0.1519],
          [-0.2198, -0.2198, -0.2198,  ..., -0.2198, -0.2198, -0.2198],
          [ 0.1270,  0.1270,  0.1270,  ...,  0.1270,  0.1270,  0.1270]],

         [[-0.1154, -0.1154, -0.1154,  ..., -0.1154, -0.1154, -0.1154],
          [ 0.1730,  0.1730,  0.1730,  ...,  0.1730,  0.1730,  0.1730],
          [-0.2374, -0.2374, -0.2374,  ..., -0.2374, -0.2374, -0.2374],
          ...,
          [-0.3864, -0.3864, -0.3864,  ..., -0.3864, -0.3864, -0.3864],
          [ 0.3265,  0.3265,  0.3265,  ...,  0.3265,  0.3265,  0.3265],
          [ 0.0429,  0.0429,  0.0429,  ...,  0.0429,  0.0429,  0.0429]],

         ...,

         [[ 0.2193,  0.2193,  0.2193,  ...,  0.2193,  0.2193,  0.2193],
          [-0.1531, -0.1531, -0.1531,  ..., -0.1531, -0.1531, -0.1531],
          [ 0.2399,  0.2399,  0.2399,  ...,  0.2399,  0.2399,  0.2399],
          ...,
          [ 0.2928,  0.2928,  0.2928,  ...,  0.2928,  0.2928,  0.2928],
          [-0.0667, -0.0667, -0.0667,  ..., -0.0667, -0.0667, -0.0667],
          [-0.1635, -0.1635, -0.1635,  ..., -0.1635, -0.1635, -0.1635]],

         [[ 0.2438,  0.2438,  0.2438,  ...,  0.2438,  0.2438,  0.2438],
          [ 0.2781,  0.2781,  0.2781,  ...,  0.2781,  0.2781,  0.2781],
          [-0.1890, -0.1890, -0.1890,  ..., -0.1890, -0.1890, -0.1890],
          ...,
          [-0.1017, -0.1017, -0.1017,  ..., -0.1017, -0.1017, -0.1017],
          [ 0.0880,  0.0880,  0.0880,  ...,  0.0880,  0.0880,  0.0880],
          [-0.4955, -0.4955, -0.4955,  ..., -0.4955, -0.4955, -0.4955]],

         [[ 0.1057,  0.1057,  0.1057,  ...,  0.1057,  0.1057,  0.1057],
          [ 0.1524,  0.1524,  0.1524,  ...,  0.1524,  0.1524,  0.1524],
          [ 0.0515,  0.0515,  0.0515,  ...,  0.0515,  0.0515,  0.0515],
          ...,
          [-0.0527, -0.0527, -0.0527,  ..., -0.0527, -0.0527, -0.0527],
          [ 0.1862,  0.1862,  0.1862,  ...,  0.1862,  0.1862,  0.1862],
          [ 0.3617,  0.3617,  0.3617,  ...,  0.3617,  0.3617,  0.3617]]]])
DESIRED: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[ 0.2658,  0.2658,  0.2658,  ...,  0.2658,  0.2658,  0.2658],
          [-0.2426, -0.2426, -0.2426,  ..., -0.2426, -0.2426, -0.2426],
          [ 0.2057,  0.2057,  0.2057,  ...,  0.2057,  0.2057,  0.2057],
          ...,
          [-0.3644, -0.3644, -0.3644,  ..., -0.3644, -0.3644, -0.3644],
          [-0.3231, -0.3231, -0.3231,  ..., -0.3231, -0.3231, -0.3231],
          [ 0.0236,  0.0236,  0.0236,  ...,  0.0236,  0.0236,  0.0236]],

         [[ 0.4142,  0.4142,  0.4142,  ...,  0.4142,  0.4142,  0.4142],
          [-0.1920, -0.1920, -0.1920,  ..., -0.1920, -0.1920, -0.1920],
          [ 0.2334,  0.2334,  0.2334,  ...,  0.2334,  0.2334,  0.2334],
          ...,
          [ 0.3016,  0.3016,  0.3016,  ...,  0.3016,  0.3016,  0.3016],
          [ 0.4947,  0.4947,  0.4947,  ...,  0.4947,  0.4947,  0.4947],
          [-0.4705, -0.4705, -0.4705,  ..., -0.4705, -0.4705, -0.4705]],

         [[ 0.2834,  0.2834,  0.2834,  ...,  0.2834,  0.2834,  0.2834],
          [-0.2636, -0.2636, -0.2636,  ..., -0.2636, -0.2636, -0.2636],
          [ 0.1594,  0.1594,  0.1594,  ...,  0.1594,  0.1594,  0.1594],
          ...,
          [-0.0066, -0.0066, -0.0066,  ..., -0.0066, -0.0066, -0.0066],
          [ 0.4938,  0.4938,  0.4938,  ...,  0.4938,  0.4938,  0.4938],
          [ 0.3931,  0.3931,  0.3931,  ...,  0.3931,  0.3931,  0.3931]],

         ...,

         [[ 0.0576,  0.0576,  0.0576,  ...,  0.0576,  0.0576,  0.0576],
          [-0.3373, -0.3373, -0.3373,  ..., -0.3373, -0.3373, -0.3373],
          [-0.0994, -0.0994, -0.0994,  ..., -0.0994, -0.0994, -0.0994],
          ...,
          [-0.3851, -0.3851, -0.3851,  ..., -0.3851, -0.3851, -0.3851],
          [-0.2326, -0.2326, -0.2326,  ..., -0.2326, -0.2326, -0.2326],
          [-0.3856, -0.3856, -0.3856,  ..., -0.3856, -0.3856, -0.3856]],

         [[ 0.2294,  0.2294,  0.2294,  ...,  0.2294,  0.2294,  0.2294],
          [ 0.4491,  0.4491,  0.4491,  ...,  0.4491,  0.4491,  0.4491],
          [ 0.0545,  0.0545,  0.0545,  ...,  0.0545,  0.0545,  0.0545],
          ...,
          [ 0.2513,  0.2513,  0.2513,  ...,  0.2513,  0.2513,  0.2513],
          [-0.1032, -0.1032, -0.1032,  ..., -0.1032, -0.1032, -0.1032],
          [-0.3625, -0.3625, -0.3625,  ..., -0.3625, -0.3625, -0.3625]],

         [[ 0.2809,  0.2809,  0.2809,  ...,  0.2809,  0.2809,  0.2809],
          [ 0.1417,  0.1417,  0.1417,  ...,  0.1417,  0.1417,  0.1417],
          [-0.2174, -0.2174, -0.2174,  ..., -0.2174, -0.2174, -0.2174],
          ...,
          [ 0.3675,  0.3675,  0.3675,  ...,  0.3675,  0.3675,  0.3675],
          [ 0.1155,  0.1155,  0.1155,  ...,  0.1155,  0.1155,  0.1155],
          [ 0.0567,  0.0567,  0.0567,  ...,  0.0567,  0.0567,  0.0567]]]])

2025-07-09 11:03:01.628192 GPU 4 84108 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 33554432, 1],"float32"), list[16,32,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 128, 33554432, 1],"float32"), list[16,32,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2533 / 4294967296 (0.0%)
Greatest absolute difference: 5.53117561340332 at index (0, 50, 15728640, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 1048575, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 33554432, 1]), dtype=torch.float32)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]])
DESIRED: (shape=torch.Size([1, 128, 33554432, 1]), dtype=torch.float32)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]])

2025-07-09 11:03:13.587412 GPU 2 82273 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 364723, 92],"float32"), size=list[92,92,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030214 (unix time) try "date -d @1752030214" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14161) received by PID 82273 (TID 0x7f3bcb64f740) from PID 82273 ***]


2025-07-09 11:04:24.310105 GPU 2 85026 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 4, 8388608],"float32"), list[16,32,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030374 (unix time) try "date -d @1752030374" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14c22) received by PID 85026 (TID 0x7f7d0367e740) from PID 85026 ***]


2025-07-09 11:04:26.041404 GPU 4 84108 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 5592406, 6],"float32"), list[128,128,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 5592406, 6],"float32"), list[128,128,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2033065 / 2097152 (96.9%)
Greatest absolute difference: 0.9679530262947083 at index (0, 34, 54, 0) (up to 0.01 allowed)
Greatest relative difference: 3598678.25 at index (0, 30, 113, 110) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.0281, -0.0110,  0.0061,  ..., -0.0148, -0.0186, -0.0223],
          [-0.3881, -0.3801, -0.3722,  ...,  0.2020,  0.2159,  0.2297],
          [ 0.4646,  0.4355,  0.4064,  ...,  0.1913,  0.2103,  0.2294],
          ...,
          [ 0.2396,  0.2305,  0.2213,  ...,  0.0230,  0.0263,  0.0295],
          [ 0.0843,  0.0880,  0.0916,  ..., -0.3348, -0.3387, -0.3426],
          [ 0.3878,  0.3728,  0.3579,  ..., -0.1708, -0.1818, -0.1928]],

         [[ 0.1129,  0.1136,  0.1144,  ..., -0.3344, -0.3537, -0.3729],
          [-0.1062, -0.1062, -0.1062,  ...,  0.1433,  0.1450,  0.1467],
          [ 0.1195,  0.1115,  0.1035,  ...,  0.2001,  0.1926,  0.1850],
          ...,
          [-0.3296, -0.3209, -0.3122,  ...,  0.3730,  0.3807,  0.3884],
          [ 0.2467,  0.2316,  0.2164,  ...,  0.1728,  0.2001,  0.2273],
          [-0.0059, -0.0142, -0.0225,  ..., -0.2012, -0.2140, -0.2269]],

         [[-0.3022, -0.2790, -0.2559,  ..., -0.3740, -0.3887, -0.4034],
          [-0.3207, -0.3107, -0.3007,  ...,  0.0948,  0.0873,  0.0797],
          [-0.2353, -0.2354, -0.2356,  ..., -0.0394, -0.0379, -0.0365],
          ...,
          [ 0.3090,  0.3148,  0.3207,  ...,  0.3407,  0.3519,  0.3630],
          [ 0.2912,  0.2715,  0.2518,  ...,  0.2550,  0.2656,  0.2761],
          [-0.3400, -0.3082, -0.2764,  ..., -0.1982, -0.2113, -0.2245]],

         ...,

         [[-0.0097, -0.0023,  0.0051,  ...,  0.2512,  0.2830,  0.3149],
          [-0.2733, -0.2676, -0.2619,  ...,  0.0755,  0.0831,  0.0907],
          [-0.1954, -0.1939, -0.1924,  ..., -0.1892, -0.1833, -0.1775],
          ...,
          [ 0.0266,  0.0269,  0.0273,  ..., -0.0674, -0.0730, -0.0786],
          [ 0.1318,  0.1415,  0.1513,  ...,  0.0111,  0.0101,  0.0092],
          [ 0.2092,  0.2128,  0.2165,  ..., -0.3850, -0.4064, -0.4279]],

         [[ 0.2996,  0.3009,  0.3023,  ...,  0.2031,  0.1973,  0.1916],
          [-0.2857, -0.2868, -0.2880,  ..., -0.1678, -0.1667, -0.1657],
          [ 0.1368,  0.1191,  0.1014,  ..., -0.0776, -0.0762, -0.0748],
          ...,
          [ 0.4335,  0.4108,  0.3881,  ...,  0.3166,  0.3191,  0.3216],
          [-0.4365, -0.4230, -0.4095,  ..., -0.2316, -0.2460, -0.2604],
          [ 0.4959,  0.4627,  0.4296,  ..., -0.0399, -0.0262, -0.0125]],

         [[ 0.4535,  0.4372,  0.4209,  ...,  0.0301,  0.0183,  0.0064],
          [ 0.1890,  0.1882,  0.1873,  ...,  0.3784,  0.3987,  0.4190],
          [-0.4126, -0.3902, -0.3679,  ...,  0.1287,  0.1409,  0.1531],
          ...,
          [-0.0969, -0.1066, -0.1163,  ..., -0.0417, -0.0373, -0.0329],
          [-0.0366, -0.0317, -0.0267,  ...,  0.1096,  0.1353,  0.1610],
          [ 0.2281,  0.2069,  0.1857,  ...,  0.4286,  0.4378,  0.4469]]]])
DESIRED: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.1846, -0.1846, -0.1846,  ...,  0.3066,  0.3066,  0.3066],
          [ 0.3024,  0.3024,  0.3024,  ...,  0.0409,  0.0409,  0.0409],
          [-0.2532, -0.2532, -0.2532,  ...,  0.2968,  0.2968,  0.2968],
          ...,
          [-0.3209, -0.3209, -0.3209,  ..., -0.3545, -0.3545, -0.3545],
          [ 0.0767,  0.0767,  0.0767,  ..., -0.2174, -0.2174, -0.2174],
          [ 0.0170,  0.0170,  0.0170,  ..., -0.0720, -0.0720, -0.0720]],

         [[ 0.1109,  0.1109,  0.1109,  ...,  0.4129,  0.4129,  0.4129],
          [ 0.2200,  0.2200,  0.2200,  ...,  0.0750,  0.0750,  0.0750],
          [ 0.1923,  0.1923,  0.1923,  ...,  0.1728,  0.1728,  0.1728],
          ...,
          [-0.1345, -0.1345, -0.1345,  ..., -0.4960, -0.4960, -0.4960],
          [ 0.0409,  0.0409,  0.0409,  ..., -0.0931, -0.0931, -0.0931],
          [-0.2972, -0.2972, -0.2972,  ..., -0.2175, -0.2175, -0.2175]],

         [[ 0.1705,  0.1705,  0.1705,  ...,  0.3293,  0.3293,  0.3293],
          [ 0.1185,  0.1185,  0.1185,  ...,  0.2058,  0.2058,  0.2058],
          [-0.0700, -0.0700, -0.0700,  ..., -0.4503, -0.4503, -0.4503],
          ...,
          [ 0.4211,  0.4211,  0.4211,  ..., -0.4687, -0.4687, -0.4687],
          [-0.0926, -0.0926, -0.0926,  ...,  0.1978,  0.1978,  0.1978],
          [ 0.2325,  0.2325,  0.2325,  ...,  0.0452,  0.0452,  0.0452]],

         ...,

         [[-0.3706, -0.3706, -0.3706,  ..., -0.0728, -0.0728, -0.0728],
          [-0.1670, -0.1670, -0.1670,  ..., -0.0208, -0.0208, -0.0208],
          [ 0.4701,  0.4701,  0.4701,  ...,  0.3056,  0.3056,  0.3056],
          ...,
          [-0.1451, -0.1451, -0.1451,  ..., -0.3450, -0.3450, -0.3450],
          [-0.0331, -0.0331, -0.0331,  ...,  0.1464,  0.1464,  0.1464],
          [-0.2647, -0.2647, -0.2647,  ..., -0.3430, -0.3430, -0.3430]],

         [[ 0.0343,  0.0343,  0.0343,  ..., -0.4768, -0.4768, -0.4768],
          [ 0.1159,  0.1159,  0.1159,  ...,  0.1155,  0.1155,  0.1155],
          [ 0.1521,  0.1521,  0.1521,  ...,  0.1331,  0.1331,  0.1331],
          ...,
          [-0.2300, -0.2300, -0.2300,  ...,  0.3458,  0.3458,  0.3458],
          [ 0.3264,  0.3264,  0.3264,  ...,  0.0206,  0.0206,  0.0206],
          [-0.4446, -0.4446, -0.4446,  ...,  0.3712,  0.3712,  0.3712]],

         [[-0.3215, -0.3215, -0.3215,  ...,  0.0943,  0.0943,  0.0943],
          [ 0.1741,  0.1741,  0.1741,  ...,  0.0933,  0.0933,  0.0933],
          [-0.1005, -0.1005, -0.1005,  ..., -0.1567, -0.1567, -0.1567],
          ...,
          [-0.1495, -0.1495, -0.1495,  ..., -0.3163, -0.3163, -0.3163],
          [ 0.0777,  0.0777,  0.0777,  ..., -0.3001, -0.3001, -0.3001],
          [ 0.1873,  0.1873,  0.1873,  ...,  0.4045,  0.4045,  0.4045]]]])

2025-07-09 11:04:47.424700 GPU 3 84460 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 6, 5592406],"float32"), list[128,128,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 6, 5592406],"float32"), list[128,128,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2033235 / 2097152 (97.0%)
Greatest absolute difference: 0.982319712638855 at index (0, 58, 127, 76) (up to 0.01 allowed)
Greatest relative difference: 4147790.75 at index (0, 77, 62, 38) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.2168,  0.1337,  0.0564,  ..., -0.1678, -0.4992, -0.4756],
          [-0.1965,  0.1165,  0.0625,  ..., -0.1736, -0.4680, -0.4553],
          [-0.1763,  0.0992,  0.0687,  ..., -0.1794, -0.4367, -0.4350],
          ...,
          [ 0.0510, -0.2264,  0.0608,  ...,  0.2878, -0.1755, -0.2423],
          [ 0.0437, -0.2380,  0.0615,  ...,  0.3068, -0.1755, -0.2637],
          [ 0.0365, -0.2496,  0.0621,  ...,  0.3259, -0.1756, -0.2850]],

         [[ 0.3833, -0.2871, -0.2685,  ..., -0.3148,  0.3655,  0.2381],
          [ 0.3783, -0.2690, -0.2489,  ..., -0.2977,  0.3507,  0.2184],
          [ 0.3733, -0.2509, -0.2293,  ..., -0.2806,  0.3359,  0.1987],
          ...,
          [-0.0043, -0.0165, -0.1280,  ..., -0.0208, -0.4200,  0.1088],
          [ 0.0055, -0.0172, -0.1405,  ..., -0.0158, -0.4431,  0.1084],
          [ 0.0154, -0.0179, -0.1530,  ..., -0.0108, -0.4663,  0.1079]],

         [[-0.2830, -0.1734, -0.2014,  ...,  0.1975, -0.1511, -0.0267],
          [-0.2570, -0.1641, -0.1886,  ...,  0.1911, -0.1549, -0.0161],
          [-0.2310, -0.1547, -0.1758,  ...,  0.1847, -0.1588, -0.0056],
          ...,
          [ 0.0110,  0.2014, -0.3320,  ..., -0.1096,  0.0674, -0.1438],
          [-0.0078,  0.2027, -0.3613,  ..., -0.1212,  0.0743, -0.1529],
          [-0.0266,  0.2039, -0.3907,  ..., -0.1328,  0.0813, -0.1619]],

         ...,

         [[-0.3710, -0.2196, -0.0191,  ...,  0.0049,  0.1423, -0.3262],
          [-0.3452, -0.1958, -0.0201,  ...,  0.0034,  0.1462, -0.3180],
          [-0.3194, -0.1721, -0.0211,  ...,  0.0018,  0.1501, -0.3099],
          ...,
          [-0.4578,  0.1561, -0.2068,  ..., -0.2322,  0.0448, -0.0821],
          [-0.4611,  0.1518, -0.2168,  ..., -0.2296,  0.0584, -0.0799],
          [-0.4644,  0.1474, -0.2268,  ..., -0.2270,  0.0719, -0.0778]],

         [[-0.0758, -0.0121,  0.1438,  ...,  0.0898, -0.4004,  0.0401],
          [-0.0685, -0.0236,  0.1438,  ...,  0.0901, -0.3700,  0.0309],
          [-0.0612, -0.0352,  0.1437,  ...,  0.0904, -0.3396,  0.0217],
          ...,
          [ 0.1930, -0.0557, -0.0686,  ...,  0.1848, -0.0030,  0.0759],
          [ 0.1858, -0.0516, -0.0684,  ...,  0.1818,  0.0160,  0.0896],
          [ 0.1786, -0.0475, -0.0682,  ...,  0.1787,  0.0349,  0.1032]],

         [[-0.1878,  0.2599, -0.0520,  ..., -0.2867, -0.4505,  0.2242],
          [-0.1811,  0.2478, -0.0441,  ..., -0.2860, -0.4368,  0.2138],
          [-0.1744,  0.2357, -0.0362,  ..., -0.2853, -0.4232,  0.2033],
          ...,
          [-0.0845, -0.2010,  0.3034,  ...,  0.1820, -0.4105, -0.4319],
          [-0.1040, -0.2028,  0.3140,  ...,  0.1895, -0.4400, -0.4589],
          [-0.1236, -0.2046,  0.3246,  ...,  0.1970, -0.4695, -0.4859]]]])
DESIRED: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[ 0.3304,  0.0952,  0.1043,  ..., -0.1737,  0.4025,  0.4972],
          [ 0.3304,  0.0952,  0.1043,  ..., -0.1737,  0.4025,  0.4972],
          [ 0.3304,  0.0952,  0.1043,  ..., -0.1737,  0.4025,  0.4972],
          ...,
          [-0.3636, -0.2916, -0.2693,  ..., -0.4324, -0.2903, -0.4336],
          [-0.3636, -0.2916, -0.2693,  ..., -0.4324, -0.2903, -0.4336],
          [-0.3636, -0.2916, -0.2693,  ..., -0.4324, -0.2903, -0.4336]],

         [[-0.3323,  0.1707,  0.0963,  ...,  0.0421,  0.0179,  0.1861],
          [-0.3323,  0.1707,  0.0963,  ...,  0.0421,  0.0179,  0.1861],
          [-0.3323,  0.1707,  0.0963,  ...,  0.0421,  0.0179,  0.1861],
          ...,
          [-0.2053, -0.0754,  0.0830,  ..., -0.2089, -0.0216, -0.0694],
          [-0.2053, -0.0754,  0.0830,  ..., -0.2089, -0.0216, -0.0694],
          [-0.2053, -0.0754,  0.0830,  ..., -0.2089, -0.0216, -0.0694]],

         [[ 0.4364, -0.0991, -0.2007,  ..., -0.1767, -0.3476, -0.0447],
          [ 0.4364, -0.0991, -0.2007,  ..., -0.1767, -0.3476, -0.0447],
          [ 0.4364, -0.0991, -0.2007,  ..., -0.1767, -0.3476, -0.0447],
          ...,
          [ 0.1702, -0.0104,  0.1563,  ...,  0.2260,  0.2087,  0.4371],
          [ 0.1702, -0.0104,  0.1563,  ...,  0.2260,  0.2087,  0.4371],
          [ 0.1702, -0.0104,  0.1563,  ...,  0.2260,  0.2087,  0.4371]],

         ...,

         [[ 0.3400, -0.0233, -0.3879,  ...,  0.2920, -0.2466, -0.1936],
          [ 0.3400, -0.0233, -0.3879,  ...,  0.2920, -0.2466, -0.1936],
          [ 0.3400, -0.0233, -0.3879,  ...,  0.2920, -0.2466, -0.1936],
          ...,
          [ 0.2595,  0.1615, -0.0706,  ...,  0.3300, -0.3724,  0.0754],
          [ 0.2595,  0.1615, -0.0706,  ...,  0.3300, -0.3724,  0.0754],
          [ 0.2595,  0.1615, -0.0706,  ...,  0.3300, -0.3724,  0.0754]],

         [[ 0.0048, -0.0752, -0.1158,  ...,  0.0067,  0.1371, -0.3647],
          [ 0.0048, -0.0752, -0.1158,  ...,  0.0067,  0.1371, -0.3647],
          [ 0.0048, -0.0752, -0.1158,  ...,  0.0067,  0.1371, -0.3647],
          ...,
          [ 0.2453,  0.0077,  0.3433,  ..., -0.2897, -0.0313,  0.1787],
          [ 0.2453,  0.0077,  0.3433,  ..., -0.2897, -0.0313,  0.1787],
          [ 0.2453,  0.0077,  0.3433,  ..., -0.2897, -0.0313,  0.1787]],

         [[ 0.3684,  0.0536,  0.0417,  ...,  0.2683,  0.1308,  0.0818],
          [ 0.3684,  0.0536,  0.0417,  ...,  0.2683,  0.1308,  0.0818],
          [ 0.3684,  0.0536,  0.0417,  ...,  0.2683,  0.1308,  0.0818],
          ...,
          [-0.3498,  0.0723,  0.2552,  ..., -0.4923, -0.2484,  0.4381],
          [-0.3498,  0.0723,  0.2552,  ..., -0.4923, -0.2484,  0.4381],
          [-0.3498,  0.0723,  0.2552,  ..., -0.4923, -0.2484,  0.4381]]]])

2025-07-09 11:04:49.476518 GPU 5 83943 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 8388608, 4],"float32"), list[16,32,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 128, 8388608, 4],"float32"), list[16,32,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 16012 / 4294967296 (0.0%)
Greatest absolute difference: 1.486576795578003 at index (0, 56, 7602175, 3) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 262143, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 8388608, 4]), dtype=torch.float32)
tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 128, 8388608, 4]), dtype=torch.float32)
tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]]])

2025-07-09 11:04:52.816160 GPU 4 84108 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 8912897, 2],"float32"), list[128,128,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 128, 8912897, 2],"float32"), list[128,128,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2035449 / 2097152 (97.1%)
Greatest absolute difference: 0.9947686791419983 at index (0, 8, 122, 0) (up to 0.01 allowed)
Greatest relative difference: 325231.71875 at index (0, 81, 28, 53) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[-0.0281, -0.0247, -0.0213,  ...,  0.3996,  0.4030,  0.4064],
          [ 0.2816,  0.2762,  0.2709,  ..., -0.3859, -0.3912, -0.3966],
          [-0.3001, -0.2971, -0.2941,  ...,  0.0715,  0.0745,  0.0775],
          ...,
          [ 0.1552,  0.1505,  0.1457,  ..., -0.4385, -0.4433, -0.4480],
          [ 0.4444,  0.4382,  0.4319,  ..., -0.3368, -0.3431, -0.3493],
          [-0.1349, -0.1368, -0.1388,  ..., -0.3784, -0.3803, -0.3822]],

         [[-0.1838, -0.1802, -0.1765,  ...,  0.2717,  0.2753,  0.2790],
          [ 0.3477,  0.3454,  0.3431,  ...,  0.0582,  0.0559,  0.0536],
          [-0.0306, -0.0266, -0.0227,  ...,  0.4613,  0.4652,  0.4691],
          ...,
          [-0.4042, -0.4003, -0.3964,  ...,  0.0885,  0.0924,  0.0964],
          [ 0.1635,  0.1661,  0.1688,  ...,  0.4916,  0.4942,  0.4968],
          [-0.4719, -0.4643, -0.4567,  ...,  0.4823,  0.4899,  0.4976]],

         [[-0.2930, -0.2901, -0.2872,  ...,  0.0659,  0.0688,  0.0716],
          [ 0.3385,  0.3371,  0.3357,  ...,  0.1663,  0.1650,  0.1636],
          [-0.2346, -0.2324, -0.2303,  ...,  0.0301,  0.0322,  0.0344],
          ...,
          [-0.1183, -0.1155, -0.1127,  ...,  0.2351,  0.2379,  0.2408],
          [-0.4592, -0.4555, -0.4517,  ...,  0.0044,  0.0081,  0.0118],
          [-0.2977, -0.2953, -0.2930,  ..., -0.0038, -0.0015,  0.0009]],

         ...,

         [[ 0.2672,  0.2619,  0.2565,  ..., -0.4000, -0.4054, -0.4107],
          [-0.1636, -0.1643, -0.1649,  ..., -0.2468, -0.2475, -0.2481],
          [ 0.2878,  0.2868,  0.2858,  ...,  0.1585,  0.1574,  0.1564],
          ...,
          [ 0.4848,  0.4803,  0.4757,  ..., -0.0821, -0.0866, -0.0912],
          [ 0.1317,  0.1268,  0.1218,  ..., -0.4887, -0.4937, -0.4987],
          [-0.4875, -0.4863, -0.4851,  ..., -0.3366, -0.3354, -0.3342]],

         [[ 0.2139,  0.2086,  0.2034,  ..., -0.4398, -0.4451, -0.4503],
          [-0.2961, -0.2930, -0.2900,  ...,  0.0841,  0.0871,  0.0901],
          [-0.0116, -0.0117, -0.0118,  ..., -0.0260, -0.0261, -0.0262],
          ...,
          [ 0.0674,  0.0633,  0.0591,  ..., -0.4504, -0.4545, -0.4587],
          [ 0.3533,  0.3513,  0.3492,  ...,  0.0957,  0.0937,  0.0916],
          [ 0.3607,  0.3608,  0.3608,  ...,  0.3708,  0.3709,  0.3710]],

         [[-0.4051, -0.3998, -0.3946,  ...,  0.2513,  0.2566,  0.2618],
          [ 0.2556,  0.2530,  0.2505,  ..., -0.0651, -0.0676, -0.0702],
          [-0.1957, -0.1946, -0.1935,  ..., -0.0565, -0.0554, -0.0543],
          ...,
          [-0.0235, -0.0224, -0.0213,  ...,  0.1154,  0.1165,  0.1176],
          [-0.4588, -0.4515, -0.4442,  ...,  0.4558,  0.4631,  0.4704],
          [-0.0091, -0.0067, -0.0043,  ...,  0.2921,  0.2945,  0.2969]]]])
DESIRED: (shape=torch.Size([1, 128, 128, 128]), dtype=torch.float32)
tensor([[[[ 3.5790e-02,  3.5790e-02,  3.5790e-02,  ...,  2.1500e-01,  2.1500e-01,  2.1500e-01],
          [ 9.2522e-02,  9.2522e-02,  9.2522e-02,  ...,  9.6605e-03,  9.6605e-03,  9.6605e-03],
          [-9.5344e-02, -9.5344e-02, -9.5344e-02,  ..., -9.3988e-02, -9.3988e-02, -9.3988e-02],
          ...,
          [ 3.4547e-01,  3.4547e-01,  3.4547e-01,  ..., -8.6361e-02, -8.6361e-02, -8.6361e-02],
          [ 9.2491e-02,  9.2491e-02,  9.2491e-02,  ..., -3.8887e-01, -3.8887e-01, -3.8887e-01],
          [-2.3613e-01, -2.3613e-01, -2.3613e-01,  ...,  2.3540e-01,  2.3540e-01,  2.3540e-01]],

         [[-2.4042e-01, -2.4042e-01, -2.4042e-01,  ...,  1.2040e-02,  1.2040e-02,  1.2040e-02],
          [-1.5224e-01, -1.5224e-01, -1.5224e-01,  ..., -1.3180e-01, -1.3180e-01, -1.3180e-01],
          [-3.3762e-02, -3.3762e-02, -3.3762e-02,  ...,  1.2028e-01,  1.2028e-01,  1.2028e-01],
          ...,
          [ 2.6987e-01,  2.6987e-01,  2.6987e-01,  ..., -1.5877e-03, -1.5877e-03, -1.5877e-03],
          [ 3.2212e-01,  3.2212e-01,  3.2212e-01,  ..., -1.7006e-01, -1.7006e-01, -1.7006e-01],
          [ 1.1534e-01,  1.1534e-01,  1.1534e-01,  ...,  4.9474e-01,  4.9474e-01,  4.9474e-01]],

         [[ 2.8069e-01,  2.8069e-01,  2.8069e-01,  ...,  9.2284e-02,  9.2284e-02,  9.2284e-02],
          [ 3.7898e-03,  3.7898e-03,  3.7898e-03,  ...,  1.2367e-01,  1.2367e-01,  1.2367e-01],
          [-1.7741e-02, -1.7741e-02, -1.7741e-02,  ..., -3.7626e-01, -3.7626e-01, -3.7626e-01],
          ...,
          [-3.8558e-01, -3.8558e-01, -3.8558e-01,  ..., -8.3033e-02, -8.3033e-02, -8.3033e-02],
          [-4.0163e-01, -4.0163e-01, -4.0163e-01,  ..., -2.3744e-01, -2.3744e-01, -2.3744e-01],
          [-4.0602e-01, -4.0602e-01, -4.0602e-01,  ...,  5.5414e-02,  5.5414e-02,  5.5414e-02]],

         ...,

         [[ 2.3958e-05,  2.3958e-05,  2.3958e-05,  ...,  1.7156e-01,  1.7156e-01,  1.7156e-01],
          [ 8.5255e-02,  8.5255e-02,  8.5255e-02,  ...,  5.9044e-02,  5.9044e-02,  5.9044e-02],
          [ 1.9943e-01,  1.9943e-01,  1.9943e-01,  ...,  4.3823e-02,  4.3823e-02,  4.3823e-02],
          ...,
          [-4.6371e-01, -4.6371e-01, -4.6371e-01,  ...,  1.4603e-01,  1.4603e-01,  1.4603e-01],
          [-4.0719e-02, -4.0719e-02, -4.0719e-02,  ...,  2.3348e-02,  2.3348e-02,  2.3348e-02],
          [-3.0462e-02, -3.0462e-02, -3.0462e-02,  ...,  6.3499e-02,  6.3499e-02,  6.3499e-02]],

         [[-6.5402e-02, -6.5402e-02, -6.5402e-02,  ..., -6.9513e-02, -6.9513e-02, -6.9513e-02],
          [-2.5758e-01, -2.5758e-01, -2.5758e-01,  ..., -3.9030e-01, -3.9030e-01, -3.9030e-01],
          [ 2.2873e-01,  2.2873e-01,  2.2873e-01,  ...,  1.0997e-02,  1.0997e-02,  1.0997e-02],
          ...,
          [-2.1115e-01, -2.1115e-01, -2.1115e-01,  ...,  4.7557e-01,  4.7557e-01,  4.7557e-01],
          [-2.8997e-01, -2.8997e-01, -2.8997e-01,  ...,  2.1955e-01,  2.1955e-01,  2.1955e-01],
          [ 6.2407e-02,  6.2407e-02,  6.2407e-02,  ..., -5.5938e-02, -5.5938e-02, -5.5938e-02]],

         [[ 3.3311e-01,  3.3311e-01,  3.3311e-01,  ..., -2.7549e-02, -2.7549e-02, -2.7549e-02],
          [ 4.8779e-01,  4.8779e-01,  4.8779e-01,  ...,  4.0941e-01,  4.0941e-01,  4.0941e-01],
          [ 4.0911e-01,  4.0911e-01,  4.0911e-01,  ..., -1.2072e-01, -1.2072e-01, -1.2072e-01],
          ...,
          [-4.6949e-01, -4.6949e-01, -4.6949e-01,  ..., -1.5806e-01, -1.5806e-01, -1.5806e-01],
          [ 2.1900e-01,  2.1900e-01,  2.1900e-01,  ...,  3.6896e-01,  3.6896e-01,  3.6896e-01],
          [-1.4472e-01, -1.4472e-01, -1.4472e-01,  ...,  3.9756e-01,  3.9756e-01,  3.9756e-01]]]])

2025-07-09 11:05:05.669614 GPU 4 84108 test begin: paddle.nn.functional.interpolate(Tensor([1, 128, 92, 364723],"float32"), size=list[92,92,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030329 (unix time) try "date -d @1752030329" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1488c) received by PID 84108 (TID 0x7fc0f8844740) from PID 84108 ***]


2025-07-09 11:05:29.011075 GPU 3 84460 test begin: paddle.nn.functional.interpolate(Tensor([1, 14514, 544, 544],"float32"), size=list[272,272,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030364 (unix time) try "date -d @1752030364" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x149ec) received by PID 84460 (TID 0x7f751643c740) from PID 84460 ***]


2025-07-09 11:06:58.791525 GPU 6 83098 test begin: paddle.nn.functional.interpolate(Tensor([1, 17825793, 128],"float32"), size=list[64,], mode="linear", align_mode=1, align_corners=False, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 17825793, 128],"float32"), size=list[64,], mode="linear", align_mode=1, align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1009257225 / 1140850752 (88.5%)
Greatest absolute difference: 0.9997016787528992 at index (0, 11445183, 63) (up to 0.01 allowed)
Greatest relative difference: 504246944.0 at index (0, 11457083, 27) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 17825793, 64]), dtype=torch.float32)
tensor([[[ 0.0094,  0.2308, -0.2937,  ..., -0.2310,  0.1009,  0.0151],
         [-0.0061, -0.4370,  0.2541,  ..., -0.4188, -0.1483, -0.1874],
         [-0.1879, -0.3962,  0.4134,  ...,  0.1219,  0.3106, -0.2053],
         ...,
         [-0.0282, -0.2847,  0.4120,  ...,  0.4976, -0.3851,  0.4756],
         [ 0.4262, -0.4488, -0.3930,  ..., -0.2861, -0.4855,  0.1837],
         [-0.2180, -0.4103, -0.3116,  ...,  0.0202,  0.1633, -0.4203]]])
DESIRED: (shape=torch.Size([1, 17825793, 64]), dtype=torch.float32)
tensor([[[ 0.0094,  0.2257, -0.2899,  ...,  0.3029,  0.0775,  0.2121],
         [-0.0061, -0.4358,  0.2510,  ...,  0.4140, -0.2588, -0.0544],
         [-0.1879, -0.3970,  0.4057,  ...,  0.0556,  0.0929,  0.2730],
         ...,
         [-0.0282, -0.2871,  0.3870,  ...,  0.1322, -0.3690, -0.2128],
         [ 0.4262, -0.4338, -0.3725,  ..., -0.4209,  0.1392,  0.3881],
         [-0.2180, -0.3998, -0.3120,  ..., -0.1965,  0.4895, -0.1190]]])

2025-07-09 11:07:04.135537 GPU 5 86245 test begin: paddle.nn.functional.interpolate(Tensor([1, 17825793, 128],"float32"), size=list[64,], scale_factor=None, mode="linear", align_corners=False, align_mode=1, data_format="NCW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 17825793, 128],"float32"), size=list[64,], scale_factor=None, mode="linear", align_corners=False, align_mode=1, data_format="NCW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1009254576 / 1140850752 (88.5%)
Greatest absolute difference: 0.9998685121536255 at index (0, 3096281, 63) (up to 0.01 allowed)
Greatest relative difference: 1243469696.0 at index (0, 15047858, 35) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 17825793, 64]), dtype=torch.float32)
tensor([[[ 0.0755,  0.2858, -0.4501,  ..., -0.2361, -0.0479,  0.0744],
         [-0.2717, -0.1031,  0.0291,  ...,  0.0031, -0.4109, -0.1108],
         [-0.0979, -0.2319, -0.2948,  ...,  0.4203,  0.0465,  0.3343],
         ...,
         [ 0.1063, -0.0609,  0.4227,  ...,  0.1591, -0.4563, -0.2260],
         [ 0.1283,  0.0067, -0.4865,  ...,  0.3067, -0.0948,  0.4855],
         [ 0.1494, -0.2916, -0.1541,  ..., -0.1001, -0.3908, -0.1653]]])
DESIRED: (shape=torch.Size([1, 17825793, 64]), dtype=torch.float32)
tensor([[[ 0.0755,  0.2839, -0.4377,  ...,  0.2957,  0.2041,  0.2282],
         [-0.2717, -0.0938,  0.0333,  ...,  0.1503, -0.2681,  0.3879],
         [-0.0979, -0.2337, -0.2939,  ..., -0.3282, -0.4331, -0.1762],
         ...,
         [ 0.1063, -0.0653,  0.4226,  ..., -0.2125,  0.0751, -0.1384],
         [ 0.1283,  0.0071, -0.4741,  ..., -0.4434, -0.3491,  0.1338],
         [ 0.1494, -0.2921, -0.1573,  ...,  0.0109, -0.2646,  0.4557]]])

2025-07-09 11:07:52.520843 GPU 6 83098 test begin: paddle.nn.functional.interpolate(Tensor([1, 19, 128, 1766023],"float32"), list[1024,1024,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030496 (unix time) try "date -d @1752030496" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1449a) received by PID 83098 (TID 0x7feeeae78740) from PID 83098 ***]


2025-07-09 11:07:59.615916 GPU 7 86707 test begin: paddle.nn.functional.interpolate(Tensor([1, 19, 128, 1766023],"float32"), list[512,1024,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030572 (unix time) try "date -d @1752030572" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x152b3) received by PID 86707 (TID 0x7f8f125b2740) from PID 86707 ***]


2025-07-09 11:08:47.924189 GPU 4 87140 test begin: paddle.nn.functional.interpolate(Tensor([1, 19, 1766023, 128],"float32"), list[1024,1024,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 19, 1766023, 128],"float32"), list[1024,1024,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4597083 / 4294967936 (0.1%)
Greatest absolute difference: 2.8455216884613037 at index (0, 0, 1461625, 22) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 861, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 19, 1766023, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 19, 1766023, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:09:00.553316 GPU 6 87316 test begin: paddle.nn.functional.interpolate(Tensor([1, 19, 1766023, 128],"float32"), list[512,1024,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 19, 1766023, 128],"float32"), list[512,1024,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2296931 / 4294967936 (0.1%)
Greatest absolute difference: 2.7260982990264893 at index (0, 4, 784707, 92) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 1724, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 19, 1766023, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 19, 1766023, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:09:03.883059 GPU 3 85739 test begin: paddle.nn.functional.interpolate(Tensor([1, 19, 256, 883012],"float32"), list[1024,1024,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030569 (unix time) try "date -d @1752030569" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14eeb) received by PID 85739 (TID 0x7ffb1484f740) from PID 85739 ***]


2025-07-09 11:09:38.717002 GPU 7 88178 test begin: paddle.nn.functional.interpolate(Tensor([1, 19, 883012, 256],"float32"), list[1024,1024,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 19, 883012, 256],"float32"), list[1024,1024,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 9131637 / 4294970368 (0.2%)
Greatest absolute difference: 1.7010124921798706 at index (0, 3, 475567, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 430, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 19, 883012, 256]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 19, 883012, 256]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:10:13.935848 GPU 2 85992 test begin: paddle.nn.functional.interpolate(Tensor([1, 19, 883012, 256],"float32"), list[512,1024,], mode="bilinear", align_corners=False, data_format="NCHW", )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 19, 883012, 256],"float32"), list[512,1024,], mode="bilinear", align_corners=False, data_format="NCHW", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4565223 / 4294970368 (0.1%)
Greatest absolute difference: 1.7347922325134277 at index (0, 17, 126760, 206) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 861, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 19, 883012, 256]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 19, 883012, 256]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:10:15.301189 GPU 3 88508 test begin: paddle.nn.functional.interpolate(Tensor([1, 19, 883012, 256],"float32"), size=list[1024,1024,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 19, 883012, 256],"float32"), size=list[1024,1024,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 9129703 / 4294970368 (0.2%)
Greatest absolute difference: 1.6614826917648315 at index (0, 17, 816182, 245) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 430, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 19, 883012, 256]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 19, 883012, 256]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:10:42.390150 GPU 5 88776 test begin: paddle.nn.functional.interpolate(Tensor([1, 192, 1398102, 16],"float32"), size=tuple(24,24,), mode="bicubic", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030734 (unix time) try "date -d @1752030734" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15ac8) received by PID 88776 (TID 0x7f5e33f13740) from PID 88776 ***]


2025-07-09 11:11:33.410516 GPU 4 87140 test begin: paddle.nn.functional.interpolate(Tensor([1, 192, 16, 1398102],"float32"), size=tuple(24,24,), mode="bicubic", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030717 (unix time) try "date -d @1752030717" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15464) received by PID 87140 (TID 0x7f6dcce42740) from PID 87140 ***]


2025-07-09 11:11:46.232330 GPU 2 85992 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 124, 17318417],"float32"), list[496,512,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030729 (unix time) try "date -d @1752030729" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14fe8) received by PID 85992 (TID 0x7ff04a3fb740) from PID 85992 ***]


2025-07-09 11:11:52.554708 GPU 6 89272 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 124, 17318417],"float32"), size=list[496,512,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030806 (unix time) try "date -d @1752030806" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15cb8) received by PID 89272 (TID 0x7fd312093740) from PID 89272 ***]


2025-07-09 11:12:16.391067 GPU 2 89442 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 128, 16777216],"float16"), list[512,512,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030845 (unix time) try "date -d @1752030845" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15d62) received by PID 89442 (TID 0x7fb4e6410740) from PID 89442 ***]


2025-07-09 11:12:22.629765 GPU 5 89607 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 128, 16777216],"float32"), list[512,512,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030832 (unix time) try "date -d @1752030832" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15e07) received by PID 89607 (TID 0x7f9425459740) from PID 89607 ***]


2025-07-09 11:12:24.533516 GPU 7 88178 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 128, 16777216],"float32"), size=list[512,512,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752030773 (unix time) try "date -d @1752030773" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15872) received by PID 88178 (TID 0x7f87296f2740) from PID 88178 ***]


2025-07-09 11:13:08.054114 GPU 3 88508 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 16777216, 128],"float32"), list[496,512,], mode="bilinear", align_corners=False, data_format="NCHW", )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 2, 16777216, 128],"float32"), list[496,512,], mode="bilinear", align_corners=False, data_format="NCHW", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 165475 / 4294967296 (0.0%)
Greatest absolute difference: 1.6481282711029053 at index (0, 0, 11788023, 68) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 16912, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 2, 16777216, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 2, 16777216, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:13:32.086312 GPU 6 90240 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 16777216, 128],"float32"), list[512,512,], mode="bilinear", align_corners=False, data_format="NCHW", )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 2, 16777216, 128],"float32"), list[512,512,], mode="bilinear", align_corners=False, data_format="NCHW", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 191104 / 4294967296 (0.0%)
Greatest absolute difference: 1.5982340574264526 at index (0, 0, 16367616, 87) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 16383, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 2, 16777216, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 2, 16777216, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:13:40.017144 GPU 7 90400 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 16777216, 128],"float32"), size=list[496,512,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 2, 16777216, 128],"float32"), size=list[496,512,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 165434 / 4294967296 (0.0%)
Greatest absolute difference: 1.6832412481307983 at index (0, 1, 11585073, 84) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 16912, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 2, 16777216, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 2, 16777216, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:14:10.821098 GPU 2 90579 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 16777216, 128],"float32"), size=list[512,512,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 2, 16777216, 128],"float32"), size=list[512,512,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 191279 / 4294967296 (0.0%)
Greatest absolute difference: 1.635614037513733 at index (0, 0, 14893056, 8) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 16383, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 2, 16777216, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 2, 16777216, 128]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:14:55.305863 GPU 3 88508 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 33554432, 64],"float32"), list[496,512,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 2, 33554432, 64],"float32"), list[496,512,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 72650 / 4294967296 (0.0%)
Greatest absolute difference: 2.724281072616577 at index (0, 1, 18096392, 30) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 33824, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 2, 33554432, 64]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 2, 33554432, 64]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:15:50.040759 GPU 4 89769 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 33554432, 64],"float32"), list[512,512,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 2, 33554432, 64],"float32"), list[512,512,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 80534 / 4294967296 (0.0%)
Greatest absolute difference: 2.6818723678588867 at index (0, 0, 25985024, 47) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 32767, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 2, 33554432, 64]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 2, 33554432, 64]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:16:27.458331 GPU 5 90752 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 544, 3947581],"float32"), size=list[272,272,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031011 (unix time) try "date -d @1752031011" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16280) received by PID 90752 (TID 0x7fd08da44740) from PID 90752 ***]


2025-07-09 11:16:35.091462 GPU 6 90240 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 62, 34636834],"float32"), list[496,512,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031022 (unix time) try "date -d @1752031022" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16080) received by PID 90240 (TID 0x7f9f9446b740) from PID 90240 ***]


2025-07-09 11:16:35.118333 GPU 7 90400 test begin: paddle.nn.functional.interpolate(Tensor([1, 2, 64, 33554432],"float32"), list[512,512,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031022 (unix time) try "date -d @1752031022" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16120) received by PID 90400 (TID 0x7f21c1ead740) from PID 90400 ***]


2025-07-09 11:16:40.368593 GPU 3 88508 test begin: paddle.nn.functional.interpolate(Tensor([1, 200, 39476, 544],"float32"), size=list[272,272,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031026 (unix time) try "date -d @1752031026" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x159bc) received by PID 88508 (TID 0x7f3ee4ee0740) from PID 88508 ***]


2025-07-09 11:17:03.735997 GPU 2 90579 test begin: paddle.nn.functional.interpolate(Tensor([1, 200, 544, 39476],"float32"), size=list[272,272,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031049 (unix time) try "date -d @1752031049" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x161d3) received by PID 90579 (TID 0x7f3456e6e740) from PID 90579 ***]


2025-07-09 11:17:10.186738 GPU 6 91546 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 1, 16777216],"float16"), list[64,64,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031124 (unix time) try "date -d @1752031124" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1659a) received by PID 91546 (TID 0x7f8765602740) from PID 91546 ***]


2025-07-09 11:17:10.186886 GPU 7 91610 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 1, 16777216],"float32"), list[62,64,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031118 (unix time) try "date -d @1752031118" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x165da) received by PID 91610 (TID 0x7fc93f7f7740) from PID 91610 ***]


2025-07-09 11:17:12.996299 GPU 3 91721 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 1, 16777216],"float32"), list[64,128,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031137 (unix time) try "date -d @1752031137" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16649) received by PID 91721 (TID 0x7fec58c72740) from PID 91721 ***]


2025-07-09 11:17:36.524636 GPU 2 92314 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 1, 16777216],"float32"), list[64,64,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031147 (unix time) try "date -d @1752031147" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1689a) received by PID 92314 (TID 0x7f269a2ba740) from PID 92314 ***]


2025-07-09 11:17:42.347077 GPU 5 92472 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 1, 16777217],"float16"), list[64,64,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031154 (unix time) try "date -d @1752031154" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16938) received by PID 92472 (TID 0x7fc2d212e740) from PID 92472 ***]


2025-07-09 11:18:27.997085 GPU 4 89769 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 1048576, 16],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 1048576, 16],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 947862 / 4294967296 (0.0%)
Greatest absolute difference: 2.7177248001098633 at index (0, 26, 147984, 3) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 4227, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 1048576, 16]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 256, 1048576, 16]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:18:49.191518 GPU 6 92719 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 1048576, 16],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 1048576, 16],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1023996 / 4294967296 (0.0%)
Greatest absolute difference: 1.4205819368362427 at index (0, 103, 397311, 15) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 4095, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 1048576, 16]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 256, 1048576, 16]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:19:03.268442 GPU 3 92889 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 128, 131072],"float32"), size=list[256,256,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031256 (unix time) try "date -d @1752031256" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16ad9) received by PID 92889 (TID 0x7f26e122f740) from PID 92889 ***]


2025-07-09 11:19:14.432207 GPU 2 93063 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 1290556, 13],"float32"), size=list[38,25,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031294 (unix time) try "date -d @1752031294" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16b87) received by PID 93063 (TID 0x7f34a15bb740) from PID 93063 ***]


2025-07-09 11:19:20.340239 GPU 5 93557 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 13, 1290556],"float32"), size=list[25,34,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031268 (unix time) try "date -d @1752031268" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16d75) received by PID 93557 (TID 0x7f755c824740) from PID 93557 ***]


2025-07-09 11:19:27.846149 GPU 7 93728 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 13, 1290556],"float32"), size=list[25,38,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031259 (unix time) try "date -d @1752031259" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16e20) received by PID 93728 (TID 0x7f0739657740) from PID 93728 ***]


2025-07-09 11:20:13.563956 GPU 4 89769 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 13, 1290556],"float32"), size=list[25,39,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031245 (unix time) try "date -d @1752031245" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15ea9) received by PID 89769 (TID 0x7f3a4b741740) from PID 89769 ***]


2025-07-09 11:21:02.535164 GPU 3 94488 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 131072, 128],"float32"), list[128,256,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031367 (unix time) try "date -d @1752031367" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17118) received by PID 94488 (TID 0x7f30db3b4740) from PID 94488 ***]


2025-07-09 11:21:06.225836 GPU 7 94647 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 131072, 128],"float32"), size=list[256,256,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031384 (unix time) try "date -d @1752031384" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x171b7) received by PID 94647 (TID 0x7f02fedc4740) from PID 94647 ***]


2025-07-09 11:21:25.235652 GPU 5 95155 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 16, 1048576],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031386 (unix time) try "date -d @1752031386" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x173b3) received by PID 95155 (TID 0x7f829c9ab740) from PID 95155 ***]


2025-07-09 11:21:35.251531 GPU 4 95314 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 16, 1048576],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031385 (unix time) try "date -d @1752031385" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17452) received by PID 95314 (TID 0x7f39ce4f6740) from PID 95314 ***]


2025-07-09 11:22:20.326147 GPU 6 92719 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 16777216, 1],"float16"), list[64,64,], mode="bilinear", align_corners=False, data_format="NCHW", )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 16777216, 1],"float16"), list[64,64,], mode="bilinear", align_corners=False, data_format="NCHW", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 24451 / 4294967296 (0.0%)
Greatest absolute difference: 10.6484375 at index (0, 68, 13762560, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 131071, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 16777216, 1]), dtype=torch.float16)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([1, 256, 16777216, 1]), dtype=torch.float16)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]], dtype=torch.float16)

2025-07-09 11:22:26.873088 GPU 2 95520 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 16777216, 1],"float32"), list[62,64,], mode="bilinear", align_corners=False, data_format="NCHW", )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 16777216, 1],"float32"), list[62,64,], mode="bilinear", align_corners=False, data_format="NCHW", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 20606 / 4294967296 (0.0%)
Greatest absolute difference: 8.783085823059082 at index (0, 113, 10147509, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 135299, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 16777216, 1]), dtype=torch.float32)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]])
DESIRED: (shape=torch.Size([1, 256, 16777216, 1]), dtype=torch.float32)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]])

2025-07-09 11:23:10.922361 GPU 7 95774 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 16777216, 1],"float32"), list[64,128,], mode="bilinear", align_corners=False, data_format="NCHW", )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 16777216, 1],"float32"), list[64,128,], mode="bilinear", align_corners=False, data_format="NCHW", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 24469 / 4294967296 (0.0%)
Greatest absolute difference: 14.858907699584961 at index (0, 136, 15335424, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 131071, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 16777216, 1]), dtype=torch.float32)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]])
DESIRED: (shape=torch.Size([1, 256, 16777216, 1]), dtype=torch.float32)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]])

2025-07-09 11:23:12.043670 GPU 4 95872 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 16777216, 1],"float32"), list[64,64,], mode="bilinear", align_corners=False, data_format="NCHW", )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 16777216, 1],"float32"), list[64,64,], mode="bilinear", align_corners=False, data_format="NCHW", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 24438 / 4294967296 (0.0%)
Greatest absolute difference: 9.8621244430542 at index (0, 231, 15597568, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 131071, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 16777216, 1]), dtype=torch.float32)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]])
DESIRED: (shape=torch.Size([1, 256, 16777216, 1]), dtype=torch.float32)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]])

2025-07-09 11:23:13.732114 GPU 5 95975 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 16777217, 1],"float16"), list[64,64,], mode="bilinear", align_corners=False, data_format="NCHW", )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 16777217, 1],"float16"), list[64,64,], mode="bilinear", align_corners=False, data_format="NCHW", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 24447 / 4294967552 (0.0%)
Greatest absolute difference: 8.921875 at index (0, 42, 9830400, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 131071, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 16777217, 1]), dtype=torch.float16)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([1, 256, 16777217, 1]), dtype=torch.float16)
tensor([[[[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         ...,

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]],

         [[0.],
          [0.],
          [0.],
          ...,
          [0.],
          [0.],
          [0.]]]], dtype=torch.float16)

2025-07-09 11:23:36.447887 GPU 3 96827 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 19, 883012],"float32"), size=list[38,25,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031535 (unix time) try "date -d @1752031535" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17a3b) received by PID 96827 (TID 0x7f574d5b2740) from PID 96827 ***]


2025-07-09 11:25:22.272406 GPU 2 95520 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 190651, 88],"float32"), size=list[88,88,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031554 (unix time) try "date -d @1752031554" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17520) received by PID 95520 (TID 0x7fcf8e2b6740) from PID 95520 ***]


2025-07-09 11:25:28.998139 GPU 6 92719 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 200, 83887],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031577 (unix time) try "date -d @1752031577" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16a2f) received by PID 92719 (TID 0x7f1117821740) from PID 92719 ***]


2025-07-09 11:26:02.038990 GPU 2 97588 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 262144, 64],"float16"), list[128,128,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031658 (unix time) try "date -d @1752031658" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17d34) received by PID 97588 (TID 0x7f10aa1e7740) from PID 97588 ***]


2025-07-09 11:26:08.064541 GPU 4 95872 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 262144, 64],"float32"), list[124,128,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031586 (unix time) try "date -d @1752031586" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17680) received by PID 95872 (TID 0x7fe8f1583740) from PID 95872 ***]


2025-07-09 11:26:25.110553 GPU 6 97831 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 262144, 64],"float32"), list[128,128,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031689 (unix time) try "date -d @1752031689" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17e27) received by PID 97831 (TID 0x7f4e2ada2740) from PID 97831 ***]


2025-07-09 11:26:29.010701 GPU 3 97989 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 262144, 64],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031680 (unix time) try "date -d @1752031680" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17ec5) received by PID 97989 (TID 0x7f9668d3e740) from PID 97989 ***]


2025-07-09 11:26:33.050761 GPU 4 98147 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 262144, 64],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031701 (unix time) try "date -d @1752031701" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17f63) received by PID 98147 (TID 0x7fdd1aac1740) from PID 98147 ***]


2025-07-09 11:26:42.784627 GPU 5 95975 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 262144, 64],"float32"), size=list[256,256,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 262144, 64],"float32"), size=list[256,256,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 8110976 / 4294967296 (0.2%)
Greatest absolute difference: 0.910532534122467 at index (0, 45, 117247, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 511, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 262144, 64]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 256, 262144, 64]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:26:47.952586 GPU 7 95774 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 304, 55189],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031648 (unix time) try "date -d @1752031648" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1761e) received by PID 95774 (TID 0x7f7cb6d83740) from PID 95774 ***]


2025-07-09 11:27:43.786764 GPU 2 99277 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 31, 541201],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031763 (unix time) try "date -d @1752031763" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x183cd) received by PID 99277 (TID 0x7f3f47278740) from PID 99277 ***]


2025-07-09 11:28:05.769277 GPU 3 99571 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 32, 524288],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031792 (unix time) try "date -d @1752031792" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x184f3) received by PID 99571 (TID 0x7ff18aa3f740) from PID 99571 ***]


2025-07-09 11:28:14.546828 GPU 7 99770 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 32, 524288],"float32"), size=list[256,256,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031802 (unix time) try "date -d @1752031802" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x185ba) received by PID 99770 (TID 0x7f3009c16740) from PID 99770 ***]


2025-07-09 11:28:15.772191 GPU 6 99858 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 524288, 32],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 524288, 32],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1864312 / 4294967296 (0.0%)
Greatest absolute difference: 1.6565872430801392 at index (0, 35, 310767, 24) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 2113, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 524288, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 256, 524288, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:29:09.849435 GPU 4 100304 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 524288, 32],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 524288, 32],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2027794 / 4294967296 (0.0%)
Greatest absolute difference: 0.8876190185546875 at index (0, 11, 518143, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 2047, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 524288, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 256, 524288, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:29:21.732266 GPU 5 95975 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 524288, 32],"float32"), size=list[256,256,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 256, 524288, 32],"float32"), size=list[256,256,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4095200 / 4294967296 (0.1%)
Greatest absolute difference: 1.5592552423477173 at index (0, 106, 289791, 26) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 1023, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 256, 524288, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 256, 524288, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 11:29:59.598835 GPU 3 101224 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 53774, 312],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031920 (unix time) try "date -d @1752031920" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18b68) received by PID 101224 (TID 0x7fbede3c4740) from PID 101224 ***]


2025-07-09 11:30:07.666341 GPU 7 101445 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 55189, 304],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031932 (unix time) try "date -d @1752031932" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18c45) received by PID 101445 (TID 0x7efc633e9740) from PID 101445 ***]


2025-07-09 11:30:10.603189 GPU 2 101533 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 61681, 272],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031933 (unix time) try "date -d @1752031933" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18c9d) received by PID 101533 (TID 0x7f702dee3740) from PID 101533 ***]


2025-07-09 11:31:02.973177 GPU 6 99858 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 62, 270601],"float32"), list[124,128,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031889 (unix time) try "date -d @1752031889" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18612) received by PID 99858 (TID 0x7f5b75b83740) from PID 99858 ***]


2025-07-09 11:31:04.439376 GPU 5 95975 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 62, 270601],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031890 (unix time) try "date -d @1752031890" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x176e7) received by PID 95975 (TID 0x7fe766098740) from PID 95975 ***]


2025-07-09 11:31:37.825539 GPU 5 102094 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 64, 262144],"float16"), list[128,128,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031992 (unix time) try "date -d @1752031992" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18ece) received by PID 102094 (TID 0x7fce85170740) from PID 102094 ***]


2025-07-09 11:32:01.597236 GPU 4 100304 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 64, 262144],"float32"), list[128,128,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752031945 (unix time) try "date -d @1752031945" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x187d0) received by PID 100304 (TID 0x7f9e0a67c740) from PID 100304 ***]


2025-07-09 11:32:07.587291 GPU 3 102420 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 64, 262144],"float32"), list[128,256,], mode="bilinear", align_corners=False, data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032027 (unix time) try "date -d @1752032027" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19014) received by PID 102420 (TID 0x7fa365bab740) from PID 102420 ***]


2025-07-09 11:32:10.405940 GPU 6 102508 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 64, 262144],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032038 (unix time) try "date -d @1752032038" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1906c) received by PID 102508 (TID 0x7f74ec759740) from PID 102508 ***]


2025-07-09 11:32:18.338374 GPU 7 102736 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 64, 262144],"float32"), size=list[256,256,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032035 (unix time) try "date -d @1752032035" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19150) received by PID 102736 (TID 0x7f4ed7ef6740) from PID 102736 ***]


2025-07-09 11:32:20.825103 GPU 2 102824 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 838861, 20],"float32"), size=list[25,39,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032044 (unix time) try "date -d @1752032044" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x191a8) received by PID 102824 (TID 0x7f059bea8740) from PID 102824 ***]


2025-07-09 11:33:09.160421 GPU 4 103302 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 83887, 200],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032099 (unix time) try "date -d @1752032099" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19386) received by PID 103302 (TID 0x7f1a698b5740) from PID 103302 ***]


2025-07-09 11:33:53.520167 GPU 3 104165 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 88, 190651],"float32"), size=list[88,88,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032121 (unix time) try "date -d @1752032121" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x196e5) received by PID 104165 (TID 0x7fbba68ce740) from PID 104165 ***]


2025-07-09 11:34:02.121076 GPU 7 104495 test begin: paddle.nn.functional.interpolate(Tensor([1, 256, 986896, 17],"float32"), size=list[25,34,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032143 (unix time) try "date -d @1752032143" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1982f) received by PID 104495 (TID 0x7f9c3fbbf740) from PID 104495 ***]


2025-07-09 11:34:05.386199 GPU 6 104646 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 100, 166472],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032147 (unix time) try "date -d @1752032147" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x198c6) received by PID 104646 (TID 0x7f95901d4740) from PID 104646 ***]


2025-07-09 11:34:10.765189 GPU 2 104818 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 100, 166472],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032153 (unix time) try "date -d @1752032153" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19972) received by PID 104818 (TID 0x7f02bcca8740) from PID 104818 ***]


2025-07-09 11:35:27.289483 GPU 3 105625 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 106713, 156],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032216 (unix time) try "date -d @1752032216" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19c99) received by PID 105625 (TID 0x7f464afe1740) from PID 105625 ***]


2025-07-09 11:35:43.775078 GPU 4 105796 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 106713, 156],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032246 (unix time) try "date -d @1752032246" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19d44) received by PID 105796 (TID 0x7f8f935c7740) from PID 105796 ***]


2025-07-09 11:35:45.848706 GPU 5 105884 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 109521, 152],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032251 (unix time) try "date -d @1752032251" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19d9c) received by PID 105884 (TID 0x7f8aa3e8d740) from PID 105884 ***]


2025-07-09 11:35:53.291185 GPU 6 106234 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 109521, 152],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032252 (unix time) try "date -d @1752032252" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19efa) received by PID 106234 (TID 0x7fdbd9e06740) from PID 106234 ***]


2025-07-09 11:35:59.029596 GPU 2 106421 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 122406, 136],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032263 (unix time) try "date -d @1752032263" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19fb5) received by PID 106421 (TID 0x7fa2ccd3c740) from PID 106421 ***]


2025-07-09 11:36:33.285589 GPU 7 106662 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 122406, 136],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032290 (unix time) try "date -d @1752032290" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a0a6) received by PID 106662 (TID 0x7f2100fa6740) from PID 106662 ***]


2025-07-09 11:37:32.727261 GPU 4 107562 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 152, 109521],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032350 (unix time) try "date -d @1752032350" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a42a) received by PID 107562 (TID 0x7f754963b740) from PID 107562 ***]


2025-07-09 11:37:38.669906 GPU 5 107797 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 152, 109521],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032356 (unix time) try "date -d @1752032356" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a515) received by PID 107797 (TID 0x7fe8ec605740) from PID 107797 ***]


2025-07-09 11:37:39.212309 GPU 6 107883 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 166472, 100],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032362 (unix time) try "date -d @1752032362" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a56b) received by PID 107883 (TID 0x7fba70ec8740) from PID 107883 ***]


2025-07-09 11:37:50.447921 GPU 2 108121 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 166472, 100],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032373 (unix time) try "date -d @1752032373" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a659) received by PID 108121 (TID 0x7f61408c9740) from PID 108121 ***]


2025-07-09 11:37:50.551295 GPU 3 108186 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 213426, 78],"float32"), size=list[24,24,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032378 (unix time) try "date -d @1752032378" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a69a) received by PID 108186 (TID 0x7f78dfb13740) from PID 108186 ***]


2025-07-09 11:38:58.040048 GPU 7 108806 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 219042, 76],"float32"), size=list[24,24,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032424 (unix time) try "date -d @1752032424" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a906) received by PID 108806 (TID 0x7f4b803f2740) from PID 108806 ***]


2025-07-09 11:39:22.560108 GPU 5 109255 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 244812, 68],"float32"), size=list[24,24,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032451 (unix time) try "date -d @1752032451" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1aac7) received by PID 109255 (TID 0x7f4518d58740) from PID 109255 ***]


2025-07-09 11:39:28.476512 GPU 6 109420 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 25, 665887],"float32"), size=list[12,12,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032461 (unix time) try "date -d @1752032461" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ab6c) received by PID 109420 (TID 0x7fe3f99a2740) from PID 109420 ***]


2025-07-09 11:39:43.983400 GPU 3 109594 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 25, 665887],"float32"), size=list[16,16,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032474 (unix time) try "date -d @1752032474" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ac1a) received by PID 109594 (TID 0x7f9d07888740) from PID 109594 ***]


2025-07-09 11:39:45.450655 GPU 2 109682 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 332944, 50],"float32"), size=list[24,24,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032479 (unix time) try "date -d @1752032479" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ac72) received by PID 109682 (TID 0x7efc17590740) from PID 109682 ***]


2025-07-09 11:39:56.523844 GPU 4 109911 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 38, 438084],"float32"), size=list[12,12,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032482 (unix time) try "date -d @1752032482" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ad57) received by PID 109911 (TID 0x7f51d1dbf740) from PID 109911 ***]


2025-07-09 11:40:58.854335 GPU 5 110353 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 38, 438084],"float32"), size=list[16,16,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032544 (unix time) try "date -d @1752032544" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1af11) received by PID 110353 (TID 0x7f1274992740) from PID 110353 ***]


2025-07-09 11:41:07.274485 GPU 6 110533 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 426851, 39],"float32"), size=list[12,12,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032561 (unix time) try "date -d @1752032561" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1afc5) received by PID 110533 (TID 0x7f3ef1bd0740) from PID 110533 ***]


2025-07-09 11:41:11.010980 GPU 7 110705 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 426851, 39],"float32"), size=list[16,16,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032572 (unix time) try "date -d @1752032572" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b071) received by PID 110705 (TID 0x7f154a526740) from PID 110705 ***]


2025-07-09 11:41:25.425290 GPU 2 111176 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 438084, 38],"float32"), size=list[12,12,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032577 (unix time) try "date -d @1752032577" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b248) received by PID 111176 (TID 0x7f79c1045740) from PID 111176 ***]


2025-07-09 11:41:28.898032 GPU 4 111362 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 438084, 38],"float32"), size=list[16,16,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032592 (unix time) try "date -d @1752032592" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b302) received by PID 111362 (TID 0x7fb630c59740) from PID 111362 ***]


2025-07-09 11:42:06.566131 GPU 3 111549 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 489623, 34],"float32"), size=list[12,12,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032614 (unix time) try "date -d @1752032614" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b3bd) received by PID 111549 (TID 0x7f89cfbe9740) from PID 111549 ***]


2025-07-09 11:42:47.270646 GPU 6 111735 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 489623, 34],"float32"), size=list[16,16,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032667 (unix time) try "date -d @1752032667" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b477) received by PID 111735 (TID 0x7ff028dba740) from PID 111735 ***]


2025-07-09 11:42:57.872555 GPU 7 111903 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 50, 332944],"float32"), size=list[24,24,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032665 (unix time) try "date -d @1752032665" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b51f) received by PID 111903 (TID 0x7fcdeecae740) from PID 111903 ***]


2025-07-09 11:43:03.360378 GPU 2 112068 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 665887, 25],"float32"), size=list[12,12,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032680 (unix time) try "date -d @1752032680" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b5c4) received by PID 112068 (TID 0x7ff14e9ab740) from PID 112068 ***]


2025-07-09 11:43:16.054907 GPU 5 112236 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 665887, 25],"float32"), size=list[16,16,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032698 (unix time) try "date -d @1752032698" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b66c) received by PID 112236 (TID 0x7f541d2d5740) from PID 112236 ***]


2025-07-09 11:43:49.642030 GPU 3 112674 test begin: paddle.nn.functional.interpolate(Tensor([1, 258, 76, 219042],"float32"), size=list[24,24,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032716 (unix time) try "date -d @1752032716" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b822) received by PID 112674 (TID 0x7f6ea2f87740) from PID 112674 ***]


2025-07-09 11:43:59.807080 GPU 4 112833 test begin: paddle.nn.functional.interpolate(Tensor([1, 275319, 100, 156],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032752 (unix time) try "date -d @1752032752" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b8c1) received by PID 112833 (TID 0x7fa4c16ce740) from PID 112833 ***]


2025-07-09 11:44:33.073321 GPU 6 113017 test begin: paddle.nn.functional.interpolate(Tensor([1, 275319, 100, 156],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032780 (unix time) try "date -d @1752032780" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b979) received by PID 113017 (TID 0x7f2d4b5bd740) from PID 113017 ***]


2025-07-09 11:44:46.007118 GPU 2 113374 test begin: paddle.nn.functional.interpolate(Tensor([1, 282564, 100, 152],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032804 (unix time) try "date -d @1752032804" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bade) received by PID 113374 (TID 0x7ff3cdd2f740) from PID 113374 ***]


2025-07-09 11:45:05.188500 GPU 5 113543 test begin: paddle.nn.functional.interpolate(Tensor([1, 282564, 100, 152],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032858 (unix time) try "date -d @1752032858" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bb87) received by PID 113543 (TID 0x7fa148c29740) from PID 113543 ***]


2025-07-09 11:45:09.908634 GPU 7 113701 test begin: paddle.nn.functional.interpolate(Tensor([1, 282564, 152, 100],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032841 (unix time) try "date -d @1752032841" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bc25) received by PID 113701 (TID 0x7fcb9daf4740) from PID 113701 ***]


2025-07-09 11:45:23.387569 GPU 3 114186 test begin: paddle.nn.functional.interpolate(Tensor([1, 282564, 152, 100],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752032870 (unix time) try "date -d @1752032870" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1be0a) received by PID 114186 (TID 0x7fcf80694740) from PID 114186 ***]


2025-07-09 11:47:32.035140 GPU 7 115035 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 181, 7909701],"float32"), size=tuple(280,280,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033004 (unix time) try "date -d @1752033004" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c15b) received by PID 115035 (TID 0x7f8995b0c740) from PID 115035 ***]


2025-07-09 11:47:35.866635 GPU 2 115123 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 182, 7866241],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033010 (unix time) try "date -d @1752033010" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c1b3) received by PID 115123 (TID 0x7fce2dcb3740) from PID 115123 ***]


2025-07-09 11:47:49.188125 GPU 5 115367 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 200, 7158279],"float32"), size=tuple(280,280,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033024 (unix time) try "date -d @1752033024" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c2a7) received by PID 115367 (TID 0x7f9ad6254740) from PID 115367 ***]


2025-07-09 11:48:42.867748 GPU 3 115665 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 212, 6753094],"float32"), size=tuple(280,280,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033060 (unix time) try "date -d @1752033060" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c3d1) received by PID 115665 (TID 0x7f2f39de1740) from PID 115665 ***]


2025-07-09 11:48:45.984436 GPU 6 115752 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 22369622, 64],"float32"), size=tuple(16,16,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033072 (unix time) try "date -d @1752033072" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c428) received by PID 115752 (TID 0x7f2b3769a740) from PID 115752 ***]


2025-07-09 11:48:48.861378 GPU 4 115843 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 22369622, 64],"float32"), size=tuple(64,64,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033075 (unix time) try "date -d @1752033075" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c483) received by PID 115843 (TID 0x7f6e9b79f740) from PID 115843 ***]


2025-07-09 11:50:26.287363 GPU 2 116709 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 227, 6306854],"float32"), size=tuple(280,280,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033176 (unix time) try "date -d @1752033176" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c7e5) received by PID 116709 (TID 0x7f81e71df740) from PID 116709 ***]


2025-07-09 11:51:07.718969 GPU 7 116958 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 235, 6092153],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033214 (unix time) try "date -d @1752033214" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c8de) received by PID 116958 (TID 0x7f75f718f740) from PID 116958 ***]


2025-07-09 11:51:07.754560 GPU 5 116894 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 250, 5726624],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033214 (unix time) try "date -d @1752033214" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c89e) received by PID 116894 (TID 0x7f9e95dc3740) from PID 116894 ***]


2025-07-09 11:51:27.331331 GPU 6 117479 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 256, 5592406],"float32"), size=tuple(180,160,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033234 (unix time) try "date -d @1752033234" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1cae7) received by PID 117479 (TID 0x7f2a3c61b740) from PID 117479 ***]


2025-07-09 11:51:27.850901 GPU 4 117560 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 28, 51130564],"float32"), size=tuple(46,40,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033234 (unix time) try "date -d @1752033234" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1cb38) received by PID 117560 (TID 0x7fa9309c2740) from PID 117560 ***]


2025-07-09 11:52:15.069711 GPU 3 117803 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 280, 5113057],"float32"), size=tuple(256,200,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033272 (unix time) try "date -d @1752033272" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1cc2b) received by PID 117803 (TID 0x7f34c05b8740) from PID 117803 ***]


2025-07-09 11:53:42.472882 GPU 7 118283 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 280, 5113057],"float32"), size=tuple(300,375,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033328 (unix time) try "date -d @1752033328" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ce0b) received by PID 118283 (TID 0x7f481f2ee740) from PID 118283 ***]


2025-07-09 11:54:00.125311 GPU 5 118450 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 374, 3827957],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033348 (unix time) try "date -d @1752033348" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ceb2) received by PID 118450 (TID 0x7fbf1674e740) from PID 118450 ***]


2025-07-09 11:54:02.601488 GPU 6 118538 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 4090446, 350],"float32"), size=tuple(300,375,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033342 (unix time) try "date -d @1752033342" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1cf0a) received by PID 118538 (TID 0x7f46cce8c740) from PID 118538 ***]


2025-07-09 11:54:03.082963 GPU 2 118619 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 4820390, 297],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033337 (unix time) try "date -d @1752033337" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1cf5b) received by PID 118619 (TID 0x7f9a9bc19740) from PID 118619 ***]


2025-07-09 11:54:38.187954 GPU 3 118940 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 5113057, 280],"float32"), size=tuple(256,200,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033366 (unix time) try "date -d @1752033366" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d09c) received by PID 118940 (TID 0x7f7f2ceb7740) from PID 118940 ***]


2025-07-09 11:54:47.133838 GPU 4 119099 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 5187159, 276],"float32"), size=tuple(280,280,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033390 (unix time) try "date -d @1752033390" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d13b) received by PID 119099 (TID 0x7f06a3b60740) from PID 119099 ***]


2025-07-09 11:55:43.182388 GPU 2 119565 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 5225022, 274],"float32"), size=tuple(280,280,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033432 (unix time) try "date -d @1752033432" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d30d) received by PID 119565 (TID 0x7f2a38301740) from PID 119565 ***]


2025-07-09 11:55:47.994025 GPU 6 119731 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 5506369, 260],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033453 (unix time) try "date -d @1752033453" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d3b3) received by PID 119731 (TID 0x7ff94373d740) from PID 119731 ***]


2025-07-09 11:56:12.327466 GPU 3 119909 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 5658719, 253],"float32"), size=tuple(280,280,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033474 (unix time) try "date -d @1752033474" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d465) received by PID 119909 (TID 0x7f748d749740) from PID 119909 ***]


2025-07-09 11:56:13.824162 GPU 5 119997 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 59652324, 24],"float32"), size=tuple(46,40,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033468 (unix time) try "date -d @1752033468" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d4bd) received by PID 119997 (TID 0x7f43c921d740) from PID 119997 ***]


2025-07-09 11:56:15.259261 GPU 7 120085 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 64, 22369622],"float32"), size=tuple(16,16,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033460 (unix time) try "date -d @1752033460" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d515) received by PID 120085 (TID 0x7f47efc00740) from PID 120085 ***]


2025-07-09 11:57:17.554106 GPU 2 120411 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 64, 22369622],"float32"), size=tuple(64,64,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033538 (unix time) try "date -d @1752033538" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d65b) received by PID 120411 (TID 0x7f74cce0d740) from PID 120411 ***]


2025-07-09 11:57:21.334466 GPU 4 120838 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 6817409, 210],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033537 (unix time) try "date -d @1752033537" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d806) received by PID 120838 (TID 0x7f0988bb3740) from PID 120838 ***]


2025-07-09 11:57:45.668227 GPU 7 121012 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 7017921, 204],"float32"), size=tuple(280,280,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033552 (unix time) try "date -d @1752033552" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d8b4) received by PID 121012 (TID 0x7fef5584e740) from PID 121012 ***]


2025-07-09 11:57:54.350706 GPU 5 121170 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 7158279, 200],"float32"), size=tuple(180,160,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033569 (unix time) try "date -d @1752033569" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d952) received by PID 121170 (TID 0x7f3f64e0a740) from PID 121170 ***]


2025-07-09 11:58:11.011391 GPU 3 121338 test begin: paddle.nn.functional.interpolate(Tensor([1, 3, 8892272, 161],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033575 (unix time) try "date -d @1752033575" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d9fa) received by PID 121338 (TID 0x7fa1c3e25740) from PID 121338 ***]


2025-07-09 11:58:19.520424 GPU 6 121496 test begin: paddle.nn.functional.interpolate(Tensor([1, 315807, 100, 136],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033607 (unix time) try "date -d @1752033607" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1da98) received by PID 121496 (TID 0x7f54ae515740) from PID 121496 ***]


2025-07-09 11:59:03.000402 GPU 2 121675 test begin: paddle.nn.functional.interpolate(Tensor([1, 315807, 100, 136],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033643 (unix time) try "date -d @1752033643" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1db4b) received by PID 121675 (TID 0x7f743dd61740) from PID 121675 ***]


2025-07-09 11:59:17.720849 GPU 7 121845 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 1, 134217728],"float32"), list[32,32,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 32, 1, 134217728],"float32"), list[32,32,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 32160 / 32768 (98.1%)
Greatest absolute difference: 0.9633033871650696 at index (0, 10, 0, 7) (up to 0.01 allowed)
Greatest relative difference: 1369.65869140625 at index (0, 19, 0, 25) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[ 3.3200e-01, -2.0242e-01,  4.3792e-01,  ..., -1.0208e-01, -3.3559e-04, -4.6978e-01],
          [ 3.3200e-01, -2.0242e-01,  4.3792e-01,  ..., -1.0208e-01, -3.3559e-04, -4.6978e-01],
          [ 3.3200e-01, -2.0242e-01,  4.3792e-01,  ..., -1.0208e-01, -3.3559e-04, -4.6978e-01],
          ...,
          [ 3.3200e-01, -2.0242e-01,  4.3792e-01,  ..., -1.0208e-01, -3.3559e-04, -4.6978e-01],
          [ 3.3200e-01, -2.0242e-01,  4.3792e-01,  ..., -1.0208e-01, -3.3559e-04, -4.6978e-01],
          [ 3.3200e-01, -2.0242e-01,  4.3792e-01,  ..., -1.0208e-01, -3.3559e-04, -4.6978e-01]],

         [[ 4.1144e-01,  2.5479e-01,  2.8543e-01,  ..., -1.6015e-01,  6.6869e-02,  9.5972e-02],
          [ 4.1144e-01,  2.5479e-01,  2.8543e-01,  ..., -1.6015e-01,  6.6869e-02,  9.5972e-02],
          [ 4.1144e-01,  2.5479e-01,  2.8543e-01,  ..., -1.6015e-01,  6.6869e-02,  9.5972e-02],
          ...,
          [ 4.1144e-01,  2.5479e-01,  2.8543e-01,  ..., -1.6015e-01,  6.6869e-02,  9.5972e-02],
          [ 4.1144e-01,  2.5479e-01,  2.8543e-01,  ..., -1.6015e-01,  6.6869e-02,  9.5972e-02],
          [ 4.1144e-01,  2.5479e-01,  2.8543e-01,  ..., -1.6015e-01,  6.6869e-02,  9.5972e-02]],

         [[ 4.7860e-01, -1.5842e-01, -8.9428e-02,  ...,  4.2788e-01,  4.4421e-01, -1.7867e-01],
          [ 4.7860e-01, -1.5842e-01, -8.9428e-02,  ...,  4.2788e-01,  4.4421e-01, -1.7867e-01],
          [ 4.7860e-01, -1.5842e-01, -8.9428e-02,  ...,  4.2788e-01,  4.4421e-01, -1.7867e-01],
          ...,
          [ 4.7860e-01, -1.5842e-01, -8.9428e-02,  ...,  4.2788e-01,  4.4421e-01, -1.7867e-01],
          [ 4.7860e-01, -1.5842e-01, -8.9428e-02,  ...,  4.2788e-01,  4.4421e-01, -1.7867e-01],
          [ 4.7860e-01, -1.5842e-01, -8.9428e-02,  ...,  4.2788e-01,  4.4421e-01, -1.7867e-01]],

         ...,

         [[ 2.4526e-01,  1.5395e-02, -1.2028e-01,  ...,  2.0387e-01, -2.6767e-01, -4.4102e-01],
          [ 2.4526e-01,  1.5395e-02, -1.2028e-01,  ...,  2.0387e-01, -2.6767e-01, -4.4102e-01],
          [ 2.4526e-01,  1.5395e-02, -1.2028e-01,  ...,  2.0387e-01, -2.6767e-01, -4.4102e-01],
          ...,
          [ 2.4526e-01,  1.5395e-02, -1.2028e-01,  ...,  2.0387e-01, -2.6767e-01, -4.4102e-01],
          [ 2.4526e-01,  1.5395e-02, -1.2028e-01,  ...,  2.0387e-01, -2.6767e-01, -4.4102e-01],
          [ 2.4526e-01,  1.5395e-02, -1.2028e-01,  ...,  2.0387e-01, -2.6767e-01, -4.4102e-01]],

         [[-2.4146e-01, -3.6653e-01,  2.8147e-01,  ...,  3.0117e-01, -4.3456e-01,  2.7439e-01],
          [-2.4146e-01, -3.6653e-01,  2.8147e-01,  ...,  3.0117e-01, -4.3456e-01,  2.7439e-01],
          [-2.4146e-01, -3.6653e-01,  2.8147e-01,  ...,  3.0117e-01, -4.3456e-01,  2.7439e-01],
          ...,
          [-2.4146e-01, -3.6653e-01,  2.8147e-01,  ...,  3.0117e-01, -4.3456e-01,  2.7439e-01],
          [-2.4146e-01, -3.6653e-01,  2.8147e-01,  ...,  3.0117e-01, -4.3456e-01,  2.7439e-01],
          [-2.4146e-01, -3.6653e-01,  2.8147e-01,  ...,  3.0117e-01, -4.3456e-01,  2.7439e-01]],

         [[ 5.5588e-02,  1.9471e-02, -1.9064e-01,  ..., -1.2364e-01, -4.3272e-01, -4.7987e-01],
          [ 5.5588e-02,  1.9471e-02, -1.9064e-01,  ..., -1.2364e-01, -4.3272e-01, -4.7987e-01],
          [ 5.5588e-02,  1.9471e-02, -1.9064e-01,  ..., -1.2364e-01, -4.3272e-01, -4.7987e-01],
          ...,
          [ 5.5588e-02,  1.9471e-02, -1.9064e-01,  ..., -1.2364e-01, -4.3272e-01, -4.7987e-01],
          [ 5.5588e-02,  1.9471e-02, -1.9064e-01,  ..., -1.2364e-01, -4.3272e-01, -4.7987e-01],
          [ 5.5588e-02,  1.9471e-02, -1.9064e-01,  ..., -1.2364e-01, -4.3272e-01, -4.7987e-01]]]])
DESIRED: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[-0.0411,  0.2359,  0.4890,  ..., -0.2429, -0.3491, -0.3711],
          [-0.0411,  0.2359,  0.4890,  ..., -0.2429, -0.3491, -0.3711],
          [-0.0411,  0.2359,  0.4890,  ..., -0.2429, -0.3491, -0.3711],
          ...,
          [-0.0411,  0.2359,  0.4890,  ..., -0.2429, -0.3491, -0.3711],
          [-0.0411,  0.2359,  0.4890,  ..., -0.2429, -0.3491, -0.3711],
          [-0.0411,  0.2359,  0.4890,  ..., -0.2429, -0.3491, -0.3711]],

         [[-0.0286,  0.1950,  0.3993,  ..., -0.3271,  0.1936,  0.4609],
          [-0.0286,  0.1950,  0.3993,  ..., -0.3271,  0.1936,  0.4609],
          [-0.0286,  0.1950,  0.3993,  ..., -0.3271,  0.1936,  0.4609],
          ...,
          [-0.0286,  0.1950,  0.3993,  ..., -0.3271,  0.1936,  0.4609],
          [-0.0286,  0.1950,  0.3993,  ..., -0.3271,  0.1936,  0.4609],
          [-0.0286,  0.1950,  0.3993,  ..., -0.3271,  0.1936,  0.4609]],

         [[ 0.0954, -0.1870,  0.3134,  ..., -0.0682,  0.2259,  0.4433],
          [ 0.0954, -0.1870,  0.3134,  ..., -0.0682,  0.2259,  0.4433],
          [ 0.0954, -0.1870,  0.3134,  ..., -0.0682,  0.2259,  0.4433],
          ...,
          [ 0.0954, -0.1870,  0.3134,  ..., -0.0682,  0.2259,  0.4433],
          [ 0.0954, -0.1870,  0.3134,  ..., -0.0682,  0.2259,  0.4433],
          [ 0.0954, -0.1870,  0.3134,  ..., -0.0682,  0.2259,  0.4433]],

         ...,

         [[-0.2481, -0.4364,  0.1122,  ..., -0.2087, -0.2856, -0.0915],
          [-0.2481, -0.4364,  0.1122,  ..., -0.2087, -0.2856, -0.0915],
          [-0.2481, -0.4364,  0.1122,  ..., -0.2087, -0.2856, -0.0915],
          ...,
          [-0.2481, -0.4364,  0.1122,  ..., -0.2087, -0.2856, -0.0915],
          [-0.2481, -0.4364,  0.1122,  ..., -0.2087, -0.2856, -0.0915],
          [-0.2481, -0.4364,  0.1122,  ..., -0.2087, -0.2856, -0.0915]],

         [[-0.3850,  0.2625, -0.4047,  ..., -0.4009,  0.4679, -0.0895],
          [-0.3850,  0.2625, -0.4047,  ..., -0.4009,  0.4679, -0.0895],
          [-0.3850,  0.2625, -0.4047,  ..., -0.4009,  0.4679, -0.0895],
          ...,
          [-0.3850,  0.2625, -0.4047,  ..., -0.4009,  0.4679, -0.0895],
          [-0.3850,  0.2625, -0.4047,  ..., -0.4009,  0.4679, -0.0895],
          [-0.3850,  0.2625, -0.4047,  ..., -0.4009,  0.4679, -0.0895]],

         [[ 0.3178,  0.4213,  0.4564,  ...,  0.3832, -0.0402,  0.1000],
          [ 0.3178,  0.4213,  0.4564,  ...,  0.3832, -0.0402,  0.1000],
          [ 0.3178,  0.4213,  0.4564,  ...,  0.3832, -0.0402,  0.1000],
          ...,
          [ 0.3178,  0.4213,  0.4564,  ...,  0.3832, -0.0402,  0.1000],
          [ 0.3178,  0.4213,  0.4564,  ...,  0.3832, -0.0402,  0.1000],
          [ 0.3178,  0.4213,  0.4564,  ...,  0.3832, -0.0402,  0.1000]]]])

2025-07-09 11:59:34.967652 GPU 5 122281 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 134217728, 1],"float32"), list[32,32,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 32, 134217728, 1],"float32"), list[32,32,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 32320 / 32768 (98.6%)
Greatest absolute difference: 0.986113429069519 at index (0, 14, 14, 0) (up to 0.01 allowed)
Greatest relative difference: 382.0856628417969 at index (0, 17, 11, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[-0.4240, -0.4240, -0.4240,  ..., -0.4240, -0.4240, -0.4240],
          [-0.3188, -0.3188, -0.3188,  ..., -0.3188, -0.3188, -0.3188],
          [ 0.1355,  0.1355,  0.1355,  ...,  0.1355,  0.1355,  0.1355],
          ...,
          [-0.4573, -0.4573, -0.4573,  ..., -0.4573, -0.4573, -0.4573],
          [ 0.3859,  0.3859,  0.3859,  ...,  0.3859,  0.3859,  0.3859],
          [ 0.1284,  0.1284,  0.1284,  ...,  0.1284,  0.1284,  0.1284]],

         [[-0.0935, -0.0935, -0.0935,  ..., -0.0935, -0.0935, -0.0935],
          [-0.3818, -0.3818, -0.3818,  ..., -0.3818, -0.3818, -0.3818],
          [ 0.4629,  0.4629,  0.4629,  ...,  0.4629,  0.4629,  0.4629],
          ...,
          [-0.4031, -0.4031, -0.4031,  ..., -0.4031, -0.4031, -0.4031],
          [ 0.0441,  0.0441,  0.0441,  ...,  0.0441,  0.0441,  0.0441],
          [ 0.2144,  0.2144,  0.2144,  ...,  0.2144,  0.2144,  0.2144]],

         [[ 0.4721,  0.4721,  0.4721,  ...,  0.4721,  0.4721,  0.4721],
          [ 0.2669,  0.2669,  0.2669,  ...,  0.2669,  0.2669,  0.2669],
          [-0.2867, -0.2867, -0.2867,  ..., -0.2867, -0.2867, -0.2867],
          ...,
          [ 0.0859,  0.0859,  0.0859,  ...,  0.0859,  0.0859,  0.0859],
          [-0.3709, -0.3709, -0.3709,  ..., -0.3709, -0.3709, -0.3709],
          [ 0.0715,  0.0715,  0.0715,  ...,  0.0715,  0.0715,  0.0715]],

         ...,

         [[-0.1804, -0.1804, -0.1804,  ..., -0.1804, -0.1804, -0.1804],
          [ 0.2424,  0.2424,  0.2424,  ...,  0.2424,  0.2424,  0.2424],
          [ 0.3264,  0.3264,  0.3264,  ...,  0.3264,  0.3264,  0.3264],
          ...,
          [-0.1157, -0.1157, -0.1157,  ..., -0.1157, -0.1157, -0.1157],
          [ 0.0671,  0.0671,  0.0671,  ...,  0.0671,  0.0671,  0.0671],
          [-0.1684, -0.1684, -0.1684,  ..., -0.1684, -0.1684, -0.1684]],

         [[ 0.0374,  0.0374,  0.0374,  ...,  0.0374,  0.0374,  0.0374],
          [-0.2541, -0.2541, -0.2541,  ..., -0.2541, -0.2541, -0.2541],
          [ 0.1962,  0.1962,  0.1962,  ...,  0.1962,  0.1962,  0.1962],
          ...,
          [-0.2004, -0.2004, -0.2004,  ..., -0.2004, -0.2004, -0.2004],
          [ 0.2254,  0.2254,  0.2254,  ...,  0.2254,  0.2254,  0.2254],
          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444]],

         [[ 0.2358,  0.2358,  0.2358,  ...,  0.2358,  0.2358,  0.2358],
          [-0.4110, -0.4110, -0.4110,  ..., -0.4110, -0.4110, -0.4110],
          [ 0.2527,  0.2527,  0.2527,  ...,  0.2527,  0.2527,  0.2527],
          ...,
          [-0.2768, -0.2768, -0.2768,  ..., -0.2768, -0.2768, -0.2768],
          [-0.1195, -0.1195, -0.1195,  ..., -0.1195, -0.1195, -0.1195],
          [ 0.2714,  0.2714,  0.2714,  ...,  0.2714,  0.2714,  0.2714]]]])
DESIRED: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[-0.1329, -0.1329, -0.1329,  ..., -0.1329, -0.1329, -0.1329],
          [ 0.1706,  0.1706,  0.1706,  ...,  0.1706,  0.1706,  0.1706],
          [ 0.0896,  0.0896,  0.0896,  ...,  0.0896,  0.0896,  0.0896],
          ...,
          [-0.2843, -0.2843, -0.2843,  ..., -0.2843, -0.2843, -0.2843],
          [-0.2219, -0.2219, -0.2219,  ..., -0.2219, -0.2219, -0.2219],
          [-0.1145, -0.1145, -0.1145,  ..., -0.1145, -0.1145, -0.1145]],

         [[ 0.2200,  0.2200,  0.2200,  ...,  0.2200,  0.2200,  0.2200],
          [ 0.0383,  0.0383,  0.0383,  ...,  0.0383,  0.0383,  0.0383],
          [-0.0115, -0.0115, -0.0115,  ..., -0.0115, -0.0115, -0.0115],
          ...,
          [-0.2325, -0.2325, -0.2325,  ..., -0.2325, -0.2325, -0.2325],
          [-0.3393, -0.3393, -0.3393,  ..., -0.3393, -0.3393, -0.3393],
          [-0.1516, -0.1516, -0.1516,  ..., -0.1516, -0.1516, -0.1516]],

         [[ 0.0889,  0.0889,  0.0889,  ...,  0.0889,  0.0889,  0.0889],
          [-0.0534, -0.0534, -0.0534,  ..., -0.0534, -0.0534, -0.0534],
          [-0.2694, -0.2694, -0.2694,  ..., -0.2694, -0.2694, -0.2694],
          ...,
          [-0.4070, -0.4070, -0.4070,  ..., -0.4070, -0.4070, -0.4070],
          [-0.1820, -0.1820, -0.1820,  ..., -0.1820, -0.1820, -0.1820],
          [-0.0970, -0.0970, -0.0970,  ..., -0.0970, -0.0970, -0.0970]],

         ...,

         [[-0.0187, -0.0187, -0.0187,  ..., -0.0187, -0.0187, -0.0187],
          [-0.0422, -0.0422, -0.0422,  ..., -0.0422, -0.0422, -0.0422],
          [-0.3421, -0.3421, -0.3421,  ..., -0.3421, -0.3421, -0.3421],
          ...,
          [-0.3279, -0.3279, -0.3279,  ..., -0.3279, -0.3279, -0.3279],
          [ 0.0823,  0.0823,  0.0823,  ...,  0.0823,  0.0823,  0.0823],
          [-0.2491, -0.2491, -0.2491,  ..., -0.2491, -0.2491, -0.2491]],

         [[ 0.0988,  0.0988,  0.0988,  ...,  0.0988,  0.0988,  0.0988],
          [ 0.2872,  0.2872,  0.2872,  ...,  0.2872,  0.2872,  0.2872],
          [-0.4085, -0.4085, -0.4085,  ..., -0.4085, -0.4085, -0.4085],
          ...,
          [-0.3309, -0.3309, -0.3309,  ..., -0.3309, -0.3309, -0.3309],
          [-0.4643, -0.4643, -0.4643,  ..., -0.4643, -0.4643, -0.4643],
          [ 0.2637,  0.2637,  0.2637,  ...,  0.2637,  0.2637,  0.2637]],

         [[ 0.2704,  0.2704,  0.2704,  ...,  0.2704,  0.2704,  0.2704],
          [ 0.1493,  0.1493,  0.1493,  ...,  0.1493,  0.1493,  0.1493],
          [ 0.3984,  0.3984,  0.3984,  ...,  0.3984,  0.3984,  0.3984],
          ...,
          [ 0.2969,  0.2969,  0.2969,  ...,  0.2969,  0.2969,  0.2969],
          [ 0.1294,  0.1294,  0.1294,  ...,  0.1294,  0.1294,  0.1294],
          [-0.0244, -0.0244, -0.0244,  ..., -0.0244, -0.0244, -0.0244]]]])

2025-07-09 11:59:42.743467 GPU 3 122447 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 2, 35651585],"float32"), list[32,32,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 32, 2, 35651585],"float32"), list[32,32,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 31836 / 32768 (97.2%)
Greatest absolute difference: 0.9652702808380127 at index (0, 24, 31, 20) (up to 0.01 allowed)
Greatest relative difference: 4807.97314453125 at index (0, 17, 13, 22) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[-0.1667,  0.2371, -0.2862,  ...,  0.4638, -0.2678,  0.0329],
          [-0.1497,  0.2432, -0.2841,  ...,  0.4473, -0.2708,  0.0183],
          [-0.1328,  0.2494, -0.2820,  ...,  0.4309, -0.2739,  0.0038],
          ...,
          [ 0.3255,  0.4166, -0.2246,  ..., -0.0141, -0.3563, -0.3878],
          [ 0.3424,  0.4228, -0.2225,  ..., -0.0306, -0.3594, -0.4023],
          [ 0.3594,  0.4290, -0.2203,  ..., -0.0471, -0.3624, -0.4169]],

         [[ 0.0859,  0.2546, -0.1704,  ...,  0.2616,  0.1792,  0.0393],
          [ 0.0847,  0.2421, -0.1784,  ...,  0.2465,  0.1771,  0.0260],
          [ 0.0836,  0.2297, -0.1864,  ...,  0.2315,  0.1749,  0.0128],
          ...,
          [ 0.0536, -0.1073, -0.4022,  ..., -0.1742,  0.1167, -0.3442],
          [ 0.0525, -0.1198, -0.4102,  ..., -0.1892,  0.1145, -0.3575],
          [ 0.0514, -0.1323, -0.4182,  ..., -0.2042,  0.1124, -0.3707]],

         [[-0.0425, -0.3072, -0.1670,  ..., -0.1122, -0.0783, -0.0913],
          [-0.0529, -0.3045, -0.1661,  ..., -0.0980, -0.0816, -0.1026],
          [-0.0634, -0.3017, -0.1653,  ..., -0.0837, -0.0849, -0.1138],
          ...,
          [-0.3448, -0.2277, -0.1427,  ...,  0.3009, -0.1747, -0.4169],
          [-0.3552, -0.2249, -0.1418,  ...,  0.3152, -0.1780, -0.4281],
          [-0.3656, -0.2222, -0.1410,  ...,  0.3294, -0.1813, -0.4394]],

         ...,

         [[ 0.4858, -0.2500, -0.1046,  ..., -0.3304, -0.1120,  0.4934],
          [ 0.4678, -0.2363, -0.0938,  ..., -0.3172, -0.1184,  0.4717],
          [ 0.4497, -0.2227, -0.0829,  ..., -0.3040, -0.1247,  0.4501],
          ...,
          [-0.0367,  0.1464,  0.2111,  ...,  0.0521, -0.2957, -0.1348],
          [-0.0548,  0.1601,  0.2220,  ...,  0.0653, -0.3021, -0.1565],
          [-0.0728,  0.1737,  0.2329,  ...,  0.0785, -0.3084, -0.1781]],

         [[ 0.4614,  0.2220,  0.3979,  ...,  0.1401,  0.1564,  0.2693],
          [ 0.4573,  0.2210,  0.3892,  ...,  0.1219,  0.1557,  0.2476],
          [ 0.4533,  0.2201,  0.3806,  ...,  0.1036,  0.1551,  0.2259],
          ...,
          [ 0.3437,  0.1951,  0.1468,  ..., -0.3887,  0.1383, -0.3598],
          [ 0.3396,  0.1942,  0.1381,  ..., -0.4070,  0.1377, -0.3815],
          [ 0.3356,  0.1932,  0.1294,  ..., -0.4252,  0.1371, -0.4032]],

         [[ 0.2316, -0.0799,  0.1387,  ...,  0.1030, -0.0596,  0.4711],
          [ 0.2099, -0.0722,  0.1346,  ...,  0.0976, -0.0709,  0.4694],
          [ 0.1882, -0.0645,  0.1306,  ...,  0.0922, -0.0822,  0.4677],
          ...,
          [-0.3972,  0.1443,  0.0223,  ..., -0.0543, -0.3882,  0.4205],
          [-0.4189,  0.1520,  0.0183,  ..., -0.0597, -0.3996,  0.4188],
          [-0.4406,  0.1597,  0.0143,  ..., -0.0652, -0.4109,  0.4171]]]])
DESIRED: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[-0.3699,  0.2294,  0.0218,  ..., -0.4087,  0.1554, -0.4220],
          [-0.3699,  0.2294,  0.0218,  ..., -0.4087,  0.1554, -0.4220],
          [-0.3699,  0.2294,  0.0218,  ..., -0.4087,  0.1554, -0.4220],
          ...,
          [ 0.0386,  0.2316, -0.3364,  ...,  0.0878, -0.1561,  0.0716],
          [ 0.0386,  0.2316, -0.3364,  ...,  0.0878, -0.1561,  0.0716],
          [ 0.0386,  0.2316, -0.3364,  ...,  0.0878, -0.1561,  0.0716]],

         [[-0.0529,  0.0411,  0.2367,  ..., -0.0972, -0.1099, -0.3609],
          [-0.0529,  0.0411,  0.2367,  ..., -0.0972, -0.1099, -0.3609],
          [-0.0529,  0.0411,  0.2367,  ..., -0.0972, -0.1099, -0.3609],
          ...,
          [-0.1112, -0.0252, -0.0628,  ...,  0.4470, -0.3958, -0.4734],
          [-0.1112, -0.0252, -0.0628,  ...,  0.4470, -0.3958, -0.4734],
          [-0.1112, -0.0252, -0.0628,  ...,  0.4470, -0.3958, -0.4734]],

         [[ 0.3535, -0.1344,  0.3359,  ..., -0.2679, -0.3134, -0.1873],
          [ 0.3535, -0.1344,  0.3359,  ..., -0.2679, -0.3134, -0.1873],
          [ 0.3535, -0.1344,  0.3359,  ..., -0.2679, -0.3134, -0.1873],
          ...,
          [ 0.1691, -0.1286,  0.4248,  ...,  0.1633,  0.2455,  0.4539],
          [ 0.1691, -0.1286,  0.4248,  ...,  0.1633,  0.2455,  0.4539],
          [ 0.1691, -0.1286,  0.4248,  ...,  0.1633,  0.2455,  0.4539]],

         ...,

         [[-0.1537,  0.0304,  0.4016,  ..., -0.3882, -0.3993,  0.4073],
          [-0.1537,  0.0304,  0.4016,  ..., -0.3882, -0.3993,  0.4073],
          [-0.1537,  0.0304,  0.4016,  ..., -0.3882, -0.3993,  0.4073],
          ...,
          [-0.2869,  0.4408, -0.0337,  ..., -0.2323,  0.0695, -0.4229],
          [-0.2869,  0.4408, -0.0337,  ..., -0.2323,  0.0695, -0.4229],
          [-0.2869,  0.4408, -0.0337,  ..., -0.2323,  0.0695, -0.4229]],

         [[ 0.1490,  0.1051,  0.2451,  ...,  0.2562,  0.0522, -0.1699],
          [ 0.1490,  0.1051,  0.2451,  ...,  0.2562,  0.0522, -0.1699],
          [ 0.1490,  0.1051,  0.2451,  ...,  0.2562,  0.0522, -0.1699],
          ...,
          [-0.1846,  0.3907,  0.2327,  ..., -0.1813,  0.4062, -0.3615],
          [-0.1846,  0.3907,  0.2327,  ..., -0.1813,  0.4062, -0.3615],
          [-0.1846,  0.3907,  0.2327,  ..., -0.1813,  0.4062, -0.3615]],

         [[ 0.2765, -0.0062,  0.1272,  ...,  0.1170, -0.3467, -0.0165],
          [ 0.2765, -0.0062,  0.1272,  ...,  0.1170, -0.3467, -0.0165],
          [ 0.2765, -0.0062,  0.1272,  ...,  0.1170, -0.3467, -0.0165],
          ...,
          [ 0.1243,  0.0780, -0.1209,  ..., -0.3242,  0.4297, -0.2055],
          [ 0.1243,  0.0780, -0.1209,  ..., -0.3242,  0.4297, -0.2055],
          [ 0.1243,  0.0780, -0.1209,  ..., -0.3242,  0.4297, -0.2055]]]])

2025-07-09 11:59:43.218752 GPU 4 122511 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 2, 67108864],"float32"), list[32,32,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 32, 2, 67108864],"float32"), list[32,32,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 31750 / 32768 (96.9%)
Greatest absolute difference: 0.976957380771637 at index (0, 1, 0, 19) (up to 0.01 allowed)
Greatest relative difference: 9372.9814453125 at index (0, 22, 21, 6) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[ 0.4226,  0.2407,  0.4105,  ...,  0.4440,  0.3713, -0.0293],
          [ 0.4006,  0.2195,  0.3927,  ...,  0.4198,  0.3531, -0.0287],
          [ 0.3785,  0.1982,  0.3748,  ...,  0.3956,  0.3349, -0.0282],
          ...,
          [-0.2161, -0.3757, -0.1070,  ..., -0.2577, -0.1557, -0.0130],
          [-0.2381, -0.3970, -0.1248,  ..., -0.2819, -0.1739, -0.0124],
          [-0.2601, -0.4182, -0.1427,  ..., -0.3061, -0.1921, -0.0119]],

         [[ 0.0938,  0.2089, -0.1513,  ..., -0.0771,  0.4291,  0.1940],
          [ 0.1047,  0.2073, -0.1340,  ..., -0.0828,  0.4125,  0.1851],
          [ 0.1157,  0.2057, -0.1167,  ..., -0.0885,  0.3959,  0.1762],
          ...,
          [ 0.4110,  0.1623,  0.3501,  ..., -0.2418, -0.0530, -0.0642],
          [ 0.4220,  0.1607,  0.3674,  ..., -0.2475, -0.0696, -0.0731],
          [ 0.4329,  0.1591,  0.3847,  ..., -0.2532, -0.0862, -0.0820]],

         [[ 0.4032,  0.1724, -0.0605,  ..., -0.4907,  0.4734, -0.3359],
          [ 0.3936,  0.1796, -0.0702,  ..., -0.4883,  0.4547, -0.3385],
          [ 0.3840,  0.1869, -0.0798,  ..., -0.4858,  0.4359, -0.3412],
          ...,
          [ 0.1255,  0.3838, -0.3398,  ..., -0.4205, -0.0698, -0.4128],
          [ 0.1159,  0.3910, -0.3494,  ..., -0.4181, -0.0885, -0.4155],
          [ 0.1063,  0.3983, -0.3590,  ..., -0.4157, -0.1072, -0.4182]],

         ...,

         [[ 0.4457,  0.4626,  0.1080,  ..., -0.2881, -0.2474, -0.2890],
          [ 0.4192,  0.4580,  0.0999,  ..., -0.2759, -0.2446, -0.2840],
          [ 0.3928,  0.4533,  0.0919,  ..., -0.2638, -0.2417, -0.2791],
          ...,
          [-0.3210,  0.3283, -0.1260,  ...,  0.0637, -0.1648, -0.1457],
          [-0.3474,  0.3236, -0.1341,  ...,  0.0758, -0.1620, -0.1408],
          [-0.3739,  0.3190, -0.1422,  ...,  0.0880, -0.1591, -0.1358]],

         [[-0.0480, -0.0399,  0.3155,  ..., -0.1009, -0.1662,  0.0590],
          [-0.0540, -0.0407,  0.2918,  ..., -0.1013, -0.1565,  0.0682],
          [-0.0600, -0.0414,  0.2680,  ..., -0.1017, -0.1469,  0.0773],
          ...,
          [-0.2217, -0.0620, -0.3730,  ..., -0.1119,  0.1139,  0.3235],
          [-0.2277, -0.0628, -0.3968,  ..., -0.1123,  0.1236,  0.3327],
          [-0.2337, -0.0635, -0.4205,  ..., -0.1126,  0.1332,  0.3418]],

         [[ 0.2001, -0.4328,  0.0575,  ..., -0.1614,  0.1588, -0.3202],
          [ 0.1846, -0.4336,  0.0559,  ..., -0.1502,  0.1688, -0.3225],
          [ 0.1691, -0.4344,  0.0543,  ..., -0.1390,  0.1789, -0.3248],
          ...,
          [-0.2492, -0.4560,  0.0113,  ...,  0.1627,  0.4500, -0.3866],
          [-0.2647, -0.4568,  0.0097,  ...,  0.1739,  0.4601, -0.3889],
          [-0.2801, -0.4576,  0.0081,  ...,  0.1850,  0.4701, -0.3912]]]])
DESIRED: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[ 0.2783,  0.0996,  0.0091,  ..., -0.3274, -0.1362, -0.1092],
          [ 0.2783,  0.0996,  0.0091,  ..., -0.3274, -0.1362, -0.1092],
          [ 0.2783,  0.0996,  0.0091,  ..., -0.3274, -0.1362, -0.1092],
          ...,
          [ 0.3424,  0.1593, -0.1164,  ..., -0.2022,  0.3289, -0.4623],
          [ 0.3424,  0.1593, -0.1164,  ..., -0.2022,  0.3289, -0.4623],
          [ 0.3424,  0.1593, -0.1164,  ..., -0.2022,  0.3289, -0.4623]],

         [[ 0.2841, -0.0938, -0.0660,  ...,  0.1802,  0.1245, -0.4818],
          [ 0.2841, -0.0938, -0.0660,  ...,  0.1802,  0.1245, -0.4818],
          [ 0.2841, -0.0938, -0.0660,  ...,  0.1802,  0.1245, -0.4818],
          ...,
          [ 0.2317,  0.2128, -0.1181,  ...,  0.0052,  0.1956,  0.2658],
          [ 0.2317,  0.2128, -0.1181,  ...,  0.0052,  0.1956,  0.2658],
          [ 0.2317,  0.2128, -0.1181,  ...,  0.0052,  0.1956,  0.2658]],

         [[-0.0972, -0.0363, -0.0982,  ...,  0.3045, -0.2137,  0.3650],
          [-0.0972, -0.0363, -0.0982,  ...,  0.3045, -0.2137,  0.3650],
          [-0.0972, -0.0363, -0.0982,  ...,  0.3045, -0.2137,  0.3650],
          ...,
          [-0.0698,  0.3814,  0.4367,  ...,  0.0094,  0.1748,  0.3549],
          [-0.0698,  0.3814,  0.4367,  ...,  0.0094,  0.1748,  0.3549],
          [-0.0698,  0.3814,  0.4367,  ...,  0.0094,  0.1748,  0.3549]],

         ...,

         [[ 0.0219,  0.2358,  0.0183,  ..., -0.2252,  0.4195, -0.2515],
          [ 0.0219,  0.2358,  0.0183,  ..., -0.2252,  0.4195, -0.2515],
          [ 0.0219,  0.2358,  0.0183,  ..., -0.2252,  0.4195, -0.2515],
          ...,
          [ 0.2439,  0.3653,  0.0635,  ...,  0.3929, -0.0967, -0.2108],
          [ 0.2439,  0.3653,  0.0635,  ...,  0.3929, -0.0967, -0.2108],
          [ 0.2439,  0.3653,  0.0635,  ...,  0.3929, -0.0967, -0.2108]],

         [[ 0.1375, -0.2066,  0.1908,  ..., -0.4838,  0.0533, -0.2526],
          [ 0.1375, -0.2066,  0.1908,  ..., -0.4838,  0.0533, -0.2526],
          [ 0.1375, -0.2066,  0.1908,  ..., -0.4838,  0.0533, -0.2526],
          ...,
          [-0.2974,  0.3558,  0.2366,  ..., -0.0282, -0.3998, -0.1281],
          [-0.2974,  0.3558,  0.2366,  ..., -0.0282, -0.3998, -0.1281],
          [-0.2974,  0.3558,  0.2366,  ..., -0.0282, -0.3998, -0.1281]],

         [[ 0.0685,  0.2861,  0.1731,  ...,  0.1577, -0.2306, -0.4611],
          [ 0.0685,  0.2861,  0.1731,  ...,  0.1577, -0.2306, -0.4611],
          [ 0.0685,  0.2861,  0.1731,  ...,  0.1577, -0.2306, -0.4611],
          ...,
          [ 0.1912,  0.1571, -0.0570,  ...,  0.1413,  0.3390,  0.2180],
          [ 0.1912,  0.1571, -0.0570,  ...,  0.1413,  0.3390,  0.2180],
          [ 0.1912,  0.1571, -0.0570,  ...,  0.1413,  0.3390,  0.2180]]]])

2025-07-09 12:00:49.768070 GPU 2 122794 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 2, 67108864],"float32"), size=tuple(2,2,), mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033741 (unix time) try "date -d @1752033741" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dfaa) received by PID 122794 (TID 0x7fee1149a740) from PID 122794 ***]


2025-07-09 12:00:54.642071 GPU 6 122954 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 22369622, 6],"float32"), list[32,32,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 32, 22369622, 6],"float32"), list[32,32,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 31903 / 32768 (97.4%)
Greatest absolute difference: 0.9907520413398743 at index (0, 0, 25, 31) (up to 0.01 allowed)
Greatest relative difference: 1508964.375 at index (0, 13, 0, 10) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[ 0.0039,  0.0566,  0.1093,  ...,  0.1313,  0.0450, -0.0413],
          [-0.1127, -0.0296,  0.0536,  ...,  0.1343,  0.0899,  0.0454],
          [-0.1324, -0.1049, -0.0775,  ...,  0.3023,  0.3257,  0.3491],
          ...,
          [-0.4732, -0.3422, -0.2112,  ..., -0.0985, -0.2108, -0.3230],
          [-0.3426, -0.3665, -0.3905,  ..., -0.0325, -0.0025,  0.0274],
          [-0.2007, -0.0927,  0.0153,  ...,  0.1552,  0.1303,  0.1054]],

         [[-0.3544, -0.2762, -0.1980,  ..., -0.2206, -0.2851, -0.3497],
          [ 0.0650,  0.0628,  0.0606,  ...,  0.2050,  0.1769,  0.1489],
          [-0.2520, -0.1967, -0.1415,  ..., -0.0408, -0.1153, -0.1898],
          ...,
          [ 0.0664,  0.0633,  0.0602,  ...,  0.3518,  0.3746,  0.3973],
          [-0.4280, -0.4158, -0.4037,  ..., -0.1386, -0.1533, -0.1680],
          [ 0.3312,  0.2797,  0.2282,  ..., -0.1853, -0.1161, -0.0468]],

         [[ 0.4658,  0.3229,  0.1800,  ..., -0.1209, -0.1506, -0.1803],
          [ 0.4280,  0.3567,  0.2853,  ..., -0.0891, -0.1723, -0.2555],
          [ 0.2609,  0.1444,  0.0279,  ..., -0.0841, -0.0241,  0.0360],
          ...,
          [ 0.0795,  0.0184, -0.0427,  ..., -0.1016, -0.1961, -0.2905],
          [-0.4596, -0.3649, -0.2702,  ...,  0.2894,  0.3120,  0.3346],
          [-0.0993, -0.1165, -0.1336,  ...,  0.2086,  0.1712,  0.1338]],

         ...,

         [[-0.4756, -0.4286, -0.3817,  ...,  0.2083,  0.2323,  0.2562],
          [ 0.1740,  0.1747,  0.1754,  ...,  0.0268,  0.0190,  0.0113],
          [ 0.0036,  0.0013, -0.0009,  ..., -0.0393, -0.1193, -0.1993],
          ...,
          [ 0.2866,  0.2383,  0.1899,  ..., -0.1693, -0.0919, -0.0146],
          [-0.0192,  0.0427,  0.1046,  ...,  0.1975,  0.2858,  0.3741],
          [-0.2972, -0.3247, -0.3522,  ...,  0.3856,  0.4313,  0.4770]],

         [[ 0.2180,  0.2558,  0.2936,  ..., -0.0128,  0.0165,  0.0458],
          [-0.1867, -0.1991, -0.2114,  ..., -0.1379, -0.1658, -0.1937],
          [-0.2082, -0.2436, -0.2790,  ..., -0.1442, -0.1179, -0.0916],
          ...,
          [ 0.4932,  0.3425,  0.1919,  ...,  0.1050,  0.0306, -0.0438],
          [-0.0727, -0.0172,  0.0383,  ...,  0.3651,  0.4180,  0.4708],
          [ 0.1169,  0.0533, -0.0104,  ...,  0.1173,  0.1336,  0.1498]],

         [[ 0.2722,  0.2747,  0.2772,  ..., -0.3370, -0.3641, -0.3913],
          [ 0.3246,  0.2136,  0.1026,  ..., -0.2660, -0.2551, -0.2441],
          [-0.3429, -0.2815, -0.2201,  ..., -0.0092,  0.0417,  0.0925],
          ...,
          [ 0.2249,  0.1252,  0.0254,  ..., -0.3367, -0.3758, -0.4149],
          [ 0.2642,  0.1510,  0.0378,  ...,  0.4288,  0.4468,  0.4647],
          [ 0.4238,  0.4081,  0.3924,  ..., -0.3179, -0.3854, -0.4530]]]])
DESIRED: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[ 0.2461,  0.2461,  0.2461,  ...,  0.1867,  0.1867,  0.1867],
          [ 0.0571,  0.0571,  0.0571,  ...,  0.2014,  0.2014,  0.2014],
          [-0.1798, -0.1798, -0.1798,  ...,  0.0946,  0.0946,  0.0946],
          ...,
          [-0.0855, -0.0855, -0.0855,  ...,  0.2871,  0.2871,  0.2871],
          [-0.2620, -0.2620, -0.2620,  ...,  0.4494,  0.4494,  0.4494],
          [ 0.0981,  0.0981,  0.0981,  ..., -0.1502, -0.1502, -0.1502]],

         [[-0.1379, -0.1379, -0.1379,  ..., -0.2147, -0.2147, -0.2147],
          [ 0.3962,  0.3962,  0.3962,  ..., -0.1586, -0.1586, -0.1586],
          [ 0.1418,  0.1418,  0.1418,  ..., -0.1385, -0.1385, -0.1385],
          ...,
          [ 0.3662,  0.3662,  0.3662,  ..., -0.1575, -0.1575, -0.1575],
          [ 0.2790,  0.2790,  0.2790,  ..., -0.1687, -0.1687, -0.1687],
          [ 0.0853,  0.0853,  0.0853,  ...,  0.1590,  0.1590,  0.1590]],

         [[-0.2611, -0.2611, -0.2611,  ...,  0.1269,  0.1269,  0.1269],
          [ 0.1837,  0.1837,  0.1837,  ..., -0.3124, -0.3124, -0.3124],
          [-0.4624, -0.4624, -0.4624,  ..., -0.1525, -0.1525, -0.1525],
          ...,
          [ 0.4130,  0.4130,  0.4130,  ..., -0.1078, -0.1078, -0.1078],
          [ 0.1687,  0.1687,  0.1687,  ..., -0.0790, -0.0790, -0.0790],
          [-0.3597, -0.3597, -0.3597,  ...,  0.4405,  0.4405,  0.4405]],

         ...,

         [[-0.3616, -0.3616, -0.3616,  ...,  0.1546,  0.1546,  0.1546],
          [-0.3017, -0.3017, -0.3017,  ...,  0.4053,  0.4053,  0.4053],
          [ 0.4183,  0.4183,  0.4183,  ...,  0.3213,  0.3213,  0.3213],
          ...,
          [-0.1647, -0.1647, -0.1647,  ...,  0.2833,  0.2833,  0.2833],
          [ 0.1190,  0.1190,  0.1190,  ..., -0.1456, -0.1456, -0.1456],
          [-0.4488, -0.4488, -0.4488,  ...,  0.3237,  0.3237,  0.3237]],

         [[-0.1122, -0.1122, -0.1122,  ..., -0.4076, -0.4076, -0.4076],
          [-0.2714, -0.2714, -0.2714,  ..., -0.3011, -0.3011, -0.3011],
          [ 0.0847,  0.0847,  0.0847,  ..., -0.2904, -0.2904, -0.2904],
          ...,
          [ 0.4876,  0.4876,  0.4876,  ...,  0.2752,  0.2752,  0.2752],
          [ 0.3406,  0.3406,  0.3406,  ..., -0.1235, -0.1235, -0.1235],
          [-0.2520, -0.2520, -0.2520,  ..., -0.0423, -0.0423, -0.0423]],

         [[-0.3163, -0.3163, -0.3163,  ..., -0.1187, -0.1187, -0.1187],
          [ 0.3148,  0.3148,  0.3148,  ..., -0.1556, -0.1556, -0.1556],
          [-0.0723, -0.0723, -0.0723,  ...,  0.3224,  0.3224,  0.3224],
          ...,
          [-0.3450, -0.3450, -0.3450,  ..., -0.2778, -0.2778, -0.2778],
          [ 0.0434,  0.0434,  0.0434,  ...,  0.2581,  0.2581,  0.2581],
          [-0.4791, -0.4791, -0.4791,  ...,  0.3631,  0.3631,  0.3631]]]])

2025-07-09 12:01:02.993017 GPU 7 121845 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 3, 44739243],"float32"), list[32,32,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 32, 3, 44739243],"float32"), list[32,32,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 31877 / 32768 (97.3%)
Greatest absolute difference: 0.9632766246795654 at index (0, 1, 0, 14) (up to 0.01 allowed)
Greatest relative difference: 29923.427734375 at index (0, 13, 23, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[ 0.3320,  0.1603,  0.0318,  ...,  0.4269,  0.3282,  0.3681],
          [ 0.3419,  0.1651,  0.0380,  ...,  0.4220,  0.3069,  0.3372],
          [ 0.3519,  0.1699,  0.0443,  ...,  0.4171,  0.2856,  0.3064],
          ...,
          [-0.3359, -0.1005,  0.2482,  ..., -0.3090, -0.2904,  0.3319],
          [-0.3968, -0.1253,  0.2570,  ..., -0.3579, -0.3117,  0.3647],
          [-0.4577, -0.1502,  0.2659,  ..., -0.4068, -0.3331,  0.3974]],

         [[ 0.0894,  0.2324,  0.3446,  ..., -0.1878, -0.3182,  0.0017],
          [ 0.0693,  0.2140,  0.3039,  ..., -0.1978, -0.3243,  0.0032],
          [ 0.0492,  0.1956,  0.2631,  ..., -0.2078, -0.3303,  0.0048],
          ...,
          [ 0.0367,  0.3557,  0.2505,  ..., -0.0529, -0.2897,  0.0869],
          [ 0.0559,  0.3860,  0.2903,  ..., -0.0314, -0.2806,  0.0914],
          [ 0.0750,  0.4163,  0.3301,  ..., -0.0099, -0.2716,  0.0960]],

         [[ 0.0354, -0.0017,  0.0828,  ...,  0.0811, -0.1512,  0.1062],
          [ 0.0289, -0.0029,  0.0845,  ...,  0.0875, -0.1388,  0.0870],
          [ 0.0225, -0.0041,  0.0861,  ...,  0.0939, -0.1264,  0.0677],
          ...,
          [ 0.2506, -0.2918, -0.0159,  ..., -0.0319,  0.1262,  0.2709],
          [ 0.2739, -0.3119, -0.0251,  ..., -0.0476,  0.1325,  0.3051],
          [ 0.2973, -0.3321, -0.0343,  ..., -0.0633,  0.1388,  0.3394]],

         ...,

         [[ 0.4726, -0.2956, -0.1623,  ...,  0.1901,  0.2950,  0.0948],
          [ 0.4461, -0.2635, -0.1368,  ...,  0.1513,  0.2819,  0.1083],
          [ 0.4195, -0.2314, -0.1113,  ...,  0.1126,  0.2688,  0.1218],
          ...,
          [-0.0808,  0.0703,  0.3557,  ..., -0.1976,  0.0263, -0.0174],
          [-0.0913,  0.0606,  0.3648,  ..., -0.1818,  0.0214, -0.0413],
          [-0.1018,  0.0508,  0.3739,  ..., -0.1661,  0.0166, -0.0651]],

         [[-0.1534, -0.3693,  0.0392,  ...,  0.3863,  0.1750, -0.2146],
          [-0.1301, -0.3413,  0.0491,  ...,  0.3631,  0.1556, -0.1990],
          [-0.1068, -0.3133,  0.0589,  ...,  0.3399,  0.1362, -0.1833],
          ...,
          [-0.3753, -0.1887,  0.2451,  ..., -0.1837, -0.4459,  0.4061],
          [-0.4185, -0.2075,  0.2491,  ..., -0.1993, -0.4696,  0.4341],
          [-0.4618, -0.2262,  0.2530,  ..., -0.2150, -0.4933,  0.4621]],

         [[-0.0158,  0.4310, -0.0013,  ..., -0.0219,  0.0826,  0.3134],
          [ 0.0097,  0.3970, -0.0042,  ..., -0.0177,  0.0821,  0.3238],
          [ 0.0353,  0.3630, -0.0071,  ..., -0.0136,  0.0815,  0.3343],
          ...,
          [ 0.0703, -0.0871, -0.0340,  ...,  0.2420,  0.3168,  0.3587],
          [ 0.0473, -0.0864, -0.0330,  ...,  0.2568,  0.3347,  0.3500],
          [ 0.0243, -0.0857, -0.0321,  ...,  0.2716,  0.3527,  0.3414]]]])
DESIRED: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[-0.1651, -0.0411, -0.4734,  ...,  0.0988,  0.3387, -0.4396],
          [-0.1651, -0.0411, -0.4734,  ...,  0.0988,  0.3387, -0.4396],
          [-0.1651, -0.0411, -0.4734,  ...,  0.0988,  0.3387, -0.4396],
          ...,
          [ 0.3022,  0.2171,  0.2883,  ..., -0.2738,  0.3519, -0.2539],
          [ 0.3022,  0.2171,  0.2883,  ..., -0.2738,  0.3519, -0.2539],
          [ 0.3022,  0.2171,  0.2883,  ..., -0.2738,  0.3519, -0.2539]],

         [[-0.0164, -0.3948,  0.1673,  ..., -0.0052,  0.3914,  0.0675],
          [-0.0164, -0.3948,  0.1673,  ..., -0.0052,  0.3914,  0.0675],
          [-0.0164, -0.3948,  0.1673,  ..., -0.0052,  0.3914,  0.0675],
          ...,
          [-0.0894, -0.0816,  0.3121,  ..., -0.0653, -0.0646,  0.3003],
          [-0.0894, -0.0816,  0.3121,  ..., -0.0653, -0.0646,  0.3003],
          [-0.0894, -0.0816,  0.3121,  ..., -0.0653, -0.0646,  0.3003]],

         [[-0.2823,  0.0227,  0.1387,  ...,  0.4430, -0.1617, -0.1454],
          [-0.2823,  0.0227,  0.1387,  ...,  0.4430, -0.1617, -0.1454],
          [-0.2823,  0.0227,  0.1387,  ...,  0.4430, -0.1617, -0.1454],
          ...,
          [ 0.1718,  0.3483,  0.0214,  ..., -0.4051, -0.0642, -0.2194],
          [ 0.1718,  0.3483,  0.0214,  ..., -0.4051, -0.0642, -0.2194],
          [ 0.1718,  0.3483,  0.0214,  ..., -0.4051, -0.0642, -0.2194]],

         ...,

         [[-0.0488,  0.1360, -0.1937,  ..., -0.0438, -0.3145,  0.4726],
          [-0.0488,  0.1360, -0.1937,  ..., -0.0438, -0.3145,  0.4726],
          [-0.0488,  0.1360, -0.1937,  ..., -0.0438, -0.3145,  0.4726],
          ...,
          [ 0.2026,  0.0522, -0.1783,  ...,  0.1649,  0.1908, -0.0635],
          [ 0.2026,  0.0522, -0.1783,  ...,  0.1649,  0.1908, -0.0635],
          [ 0.2026,  0.0522, -0.1783,  ...,  0.1649,  0.1908, -0.0635]],

         [[-0.1959,  0.3265, -0.3033,  ...,  0.4793,  0.3577, -0.4774],
          [-0.1959,  0.3265, -0.3033,  ...,  0.4793,  0.3577, -0.4774],
          [-0.1959,  0.3265, -0.3033,  ...,  0.4793,  0.3577, -0.4774],
          ...,
          [-0.4055, -0.1274,  0.2027,  ..., -0.2953, -0.2491,  0.0729],
          [-0.4055, -0.1274,  0.2027,  ..., -0.2953, -0.2491,  0.0729],
          [-0.4055, -0.1274,  0.2027,  ..., -0.2953, -0.2491,  0.0729]],

         [[-0.2493,  0.0245, -0.3336,  ...,  0.1498, -0.0628, -0.4861],
          [-0.2493,  0.0245, -0.3336,  ...,  0.1498, -0.0628, -0.4861],
          [-0.2493,  0.0245, -0.3336,  ...,  0.1498, -0.0628, -0.4861],
          ...,
          [ 0.3594,  0.0538, -0.3643,  ...,  0.3614, -0.3823, -0.2532],
          [ 0.3594,  0.0538, -0.3643,  ...,  0.3614, -0.3823, -0.2532],
          [ 0.3594,  0.0538, -0.3643,  ...,  0.3614, -0.3823, -0.2532]]]])

2025-07-09 12:01:06.110164 GPU 3 122447 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 35651585, 2],"float32"), list[32,32,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 32, 35651585, 2],"float32"), list[32,32,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 31772 / 32768 (97.0%)
Greatest absolute difference: 0.9804480075836182 at index (0, 11, 22, 0) (up to 0.01 allowed)
Greatest relative difference: 6938.5849609375 at index (0, 7, 23, 12) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[-0.1667, -0.1748, -0.1829,  ..., -0.4009, -0.4090, -0.4171],
          [-0.1689, -0.1761, -0.1834,  ..., -0.3793, -0.3865, -0.3938],
          [ 0.2737,  0.2522,  0.2307,  ..., -0.3498, -0.3713, -0.3928],
          ...,
          [-0.2155, -0.2098, -0.2042,  ..., -0.0512, -0.0455, -0.0398],
          [ 0.3085,  0.3088,  0.3091,  ...,  0.3182,  0.3186,  0.3189],
          [ 0.2589,  0.2371,  0.2153,  ..., -0.3733, -0.3951, -0.4169]],

         [[ 0.0859,  0.0738,  0.0617,  ..., -0.2649, -0.2770, -0.2891],
          [-0.0122, -0.0254, -0.0385,  ..., -0.3940, -0.4072, -0.4204],
          [-0.3033, -0.2839, -0.2644,  ...,  0.2608,  0.2803,  0.2998],
          ...,
          [-0.2426, -0.2443, -0.2460,  ..., -0.2922, -0.2939, -0.2956],
          [-0.0046, -0.0118, -0.0189,  ..., -0.2126, -0.2198, -0.2269],
          [-0.0838, -0.0931, -0.1023,  ..., -0.3522, -0.3614, -0.3707]],

         [[-0.0425, -0.0519, -0.0614,  ..., -0.3159, -0.3254, -0.3348],
          [-0.2532, -0.2401, -0.2271,  ...,  0.1248,  0.1378,  0.1508],
          [-0.1895, -0.1791, -0.1688,  ...,  0.1103,  0.1207,  0.1310],
          ...,
          [-0.3748, -0.3674, -0.3601,  ..., -0.1617, -0.1543, -0.1470],
          [-0.4767, -0.4467, -0.4167,  ...,  0.3923,  0.4223,  0.4522],
          [-0.3214, -0.3252, -0.3290,  ..., -0.4318, -0.4356, -0.4394]],

         ...,

         [[ 0.4858,  0.4722,  0.4587,  ...,  0.0936,  0.0801,  0.0666],
          [-0.1623, -0.1517, -0.1412,  ...,  0.1428,  0.1534,  0.1639],
          [ 0.2396,  0.2353,  0.2310,  ...,  0.1146,  0.1103,  0.1060],
          ...,
          [-0.3466, -0.3321, -0.3177,  ...,  0.0727,  0.0872,  0.1016],
          [-0.2845, -0.2784, -0.2723,  ..., -0.1072, -0.1011, -0.0950],
          [-0.0442, -0.0485, -0.0529,  ..., -0.1695, -0.1738, -0.1781]],

         [[ 0.4614,  0.4494,  0.4374,  ...,  0.1141,  0.1021,  0.0901],
          [ 0.3329,  0.3346,  0.3362,  ...,  0.3811,  0.3827,  0.3844],
          [ 0.2215,  0.2225,  0.2235,  ...,  0.2514,  0.2524,  0.2534],
          ...,
          [ 0.1738,  0.1542,  0.1346,  ..., -0.3948, -0.4144, -0.4340],
          [-0.1632, -0.1729, -0.1827,  ..., -0.4447, -0.4544, -0.4641],
          [-0.2811, -0.2850, -0.2889,  ..., -0.3953, -0.3993, -0.4032]],

         [[ 0.2316,  0.2269,  0.2222,  ...,  0.0952,  0.0905,  0.0857],
          [ 0.1294,  0.1274,  0.1254,  ...,  0.0714,  0.0694,  0.0674],
          [ 0.1656,  0.1513,  0.1370,  ..., -0.2496, -0.2639, -0.2783],
          ...,
          [-0.3028, -0.2875, -0.2723,  ...,  0.1401,  0.1554,  0.1707],
          [ 0.1562,  0.1406,  0.1250,  ..., -0.2955, -0.3111, -0.3266],
          [-0.3458, -0.3212, -0.2966,  ...,  0.3678,  0.3925,  0.4171]]]])
DESIRED: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[ 0.1763,  0.1763,  0.1763,  ..., -0.2962, -0.2962, -0.2962],
          [-0.0259, -0.0259, -0.0259,  ...,  0.0610,  0.0610,  0.0610],
          [ 0.1200,  0.1200,  0.1200,  ..., -0.0480, -0.0480, -0.0480],
          ...,
          [-0.2115, -0.2115, -0.2115,  ...,  0.0543,  0.0543,  0.0543],
          [-0.2968, -0.2968, -0.2968,  ..., -0.1703, -0.1703, -0.1703],
          [ 0.1869,  0.1869,  0.1869,  ...,  0.0530,  0.0530,  0.0530]],

         [[ 0.2907,  0.2907,  0.2907,  ...,  0.0263,  0.0263,  0.0263],
          [ 0.0337,  0.0337,  0.0337,  ...,  0.0843,  0.0843,  0.0843],
          [ 0.0063,  0.0063,  0.0063,  ..., -0.3286, -0.3286, -0.3286],
          ...,
          [-0.3988, -0.3988, -0.3988,  ..., -0.2859, -0.2859, -0.2859],
          [ 0.1827,  0.1827,  0.1827,  ..., -0.1185, -0.1185, -0.1185],
          [-0.4585, -0.4585, -0.4585,  ..., -0.3794, -0.3794, -0.3794]],

         [[ 0.0160,  0.0160,  0.0160,  ..., -0.3168, -0.3168, -0.3168],
          [-0.0122, -0.0122, -0.0122,  ..., -0.0953, -0.0953, -0.0953],
          [ 0.1426,  0.1426,  0.1426,  ...,  0.2169,  0.2169,  0.2169],
          ...,
          [-0.3809, -0.3809, -0.3809,  ..., -0.1915, -0.1915, -0.1915],
          [ 0.0776,  0.0776,  0.0776,  ...,  0.3954,  0.3954,  0.3954],
          [ 0.2825,  0.2825,  0.2825,  ..., -0.3479, -0.3479, -0.3479]],

         ...,

         [[-0.2441, -0.2441, -0.2441,  ..., -0.2720, -0.2720, -0.2720],
          [-0.0247, -0.0247, -0.0247,  ...,  0.0674,  0.0674,  0.0674],
          [-0.0281, -0.0281, -0.0281,  ..., -0.2867, -0.2867, -0.2867],
          ...,
          [-0.1281, -0.1281, -0.1281,  ..., -0.3049, -0.3049, -0.3049],
          [ 0.1216,  0.1216,  0.1216,  ...,  0.2579,  0.2579,  0.2579],
          [-0.3235, -0.3235, -0.3235,  ...,  0.0895,  0.0895,  0.0895]],

         [[ 0.1864,  0.1864,  0.1864,  ..., -0.1664, -0.1664, -0.1664],
          [-0.1909, -0.1909, -0.1909,  ...,  0.1647,  0.1647,  0.1647],
          [ 0.1790,  0.1790,  0.1790,  ...,  0.0519,  0.0519,  0.0519],
          ...,
          [ 0.2656,  0.2656,  0.2656,  ...,  0.2827,  0.2827,  0.2827],
          [ 0.3968,  0.3968,  0.3968,  ..., -0.3171, -0.3171, -0.3171],
          [ 0.1873,  0.1873,  0.1873,  ..., -0.3449, -0.3449, -0.3449]],

         [[ 0.0342,  0.0342,  0.0342,  ..., -0.0573, -0.0573, -0.0573],
          [-0.4446, -0.4446, -0.4446,  ..., -0.1637, -0.1637, -0.1637],
          [-0.2408, -0.2408, -0.2408,  ..., -0.1499, -0.1499, -0.1499],
          ...,
          [-0.0543, -0.0543, -0.0543,  ...,  0.3790,  0.3790,  0.3790],
          [-0.1200, -0.1200, -0.1200,  ...,  0.3912,  0.3912,  0.3912],
          [ 0.0027,  0.0027,  0.0027,  ..., -0.2515, -0.2515, -0.2515]]]])

2025-07-09 12:01:14.894003 GPU 5 122281 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 44739243, 3],"float32"), list[32,32,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 32, 44739243, 3],"float32"), list[32,32,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 31838 / 32768 (97.2%)
Greatest absolute difference: 0.9698477387428284 at index (0, 30, 29, 31) (up to 0.01 allowed)
Greatest relative difference: 39771.55078125 at index (0, 20, 28, 11) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[-4.2402e-01, -4.1112e-01, -3.9822e-01,  ...,  2.6292e-01,  2.9899e-01,  3.3506e-01],
          [ 4.4143e-01,  4.0138e-01,  3.6133e-01,  ...,  2.7408e-01,  3.0767e-01,  3.4126e-01],
          [-2.0853e-02, -9.6686e-03,  1.5156e-03,  ...,  2.4178e-01,  2.4839e-01,  2.5501e-01],
          ...,
          [-6.6902e-02, -7.0287e-02, -7.3673e-02,  ..., -8.6499e-02, -8.4064e-02, -8.1628e-02],
          [ 6.2443e-02,  3.1019e-02, -4.0472e-04,  ...,  5.9146e-02,  9.4981e-02,  1.3082e-01],
          [ 2.7117e-01,  2.6085e-01,  2.5053e-01,  ..., -8.5789e-02, -1.0038e-01, -1.1497e-01]],

         [[-4.5070e-01, -4.3368e-01, -4.1666e-01,  ..., -1.1560e-01, -1.1032e-01, -1.0503e-01],
          [-2.2883e-01, -2.0086e-01, -1.7288e-01,  ..., -1.8264e-01, -2.1134e-01, -2.4004e-01],
          [ 3.8853e-01,  3.6702e-01,  3.4551e-01,  ...,  1.7798e-01,  1.8707e-01,  1.9617e-01],
          ...,
          [ 3.3560e-02,  2.0056e-02,  6.5525e-03,  ..., -2.7153e-01, -2.7863e-01, -2.8572e-01],
          [-2.3372e-01, -1.8831e-01, -1.4290e-01,  ..., -1.1619e-01, -1.5962e-01, -2.0305e-01],
          [ 1.3472e-01,  1.4840e-01,  1.6209e-01,  ..., -2.3476e-01, -2.7784e-01, -3.2093e-01]],

         [[-3.1445e-01, -2.9210e-01, -2.6975e-01,  ...,  2.2192e-01,  2.3598e-01,  2.5005e-01],
          [ 5.3455e-02,  5.1502e-02,  4.9549e-02,  ...,  1.6097e-01,  1.7118e-01,  1.8139e-01],
          [ 9.6634e-02,  6.5717e-02,  3.4800e-02,  ..., -1.0843e-01, -8.8122e-02, -6.7814e-02],
          ...,
          [ 2.1900e-01,  1.8730e-01,  1.5560e-01,  ...,  3.9712e-02,  6.2830e-02,  8.5948e-02],
          [-1.5648e-01, -1.6911e-01, -1.8174e-01,  ...,  2.5072e-01,  2.9538e-01,  3.4004e-01],
          [ 4.2954e-02,  4.7603e-02,  5.2253e-02,  ..., -2.9704e-01, -3.2757e-01, -3.5809e-01]],

         ...,

         [[ 4.5012e-01,  3.9076e-01,  3.3139e-01,  ..., -1.8662e-02,  1.4772e-02,  4.8206e-02],
          [-2.6693e-01, -2.3900e-01, -2.1106e-01,  ..., -1.8652e-01, -2.1263e-01, -2.3874e-01],
          [ 1.9517e-01,  1.9664e-01,  1.9810e-01,  ..., -1.1270e-01, -1.3718e-01, -1.6167e-01],
          ...,
          [-2.5957e-02, -2.2586e-02, -1.9215e-02,  ...,  1.8759e-01,  1.9953e-01,  2.1148e-01],
          [ 2.0493e-01,  2.0909e-01,  2.1325e-01,  ...,  3.7733e-01,  3.8533e-01,  3.9333e-01],
          [-2.8090e-01, -2.6963e-01, -2.5835e-01,  ...,  2.2669e-01,  2.5134e-01,  2.7600e-01]],

         [[ 1.5656e-01,  1.4808e-01,  1.3961e-01,  ..., -6.7332e-02, -7.4186e-02, -8.1041e-02],
          [-1.3601e-01, -1.1580e-01, -9.5594e-02,  ...,  1.9798e-02,  8.1361e-03, -3.5260e-03],
          [-2.6121e-01, -2.5777e-01, -2.5434e-01,  ...,  1.1662e-01,  1.4066e-01,  1.6471e-01],
          ...,
          [ 3.0157e-01,  2.5332e-01,  2.0507e-01,  ...,  3.6335e-01,  4.2332e-01,  4.8330e-01],
          [ 1.0857e-01,  9.7216e-02,  8.5858e-02,  ...,  4.0269e-01,  4.3752e-01,  4.7234e-01],
          [-2.9353e-01, -2.8741e-01, -2.8129e-01,  ...,  2.9826e-01,  3.3507e-01,  3.7188e-01]],

         [[ 1.2818e-01,  1.2536e-01,  1.2254e-01,  ...,  5.5530e-02,  5.3382e-02,  5.1234e-02],
          [-2.5046e-01, -2.5883e-01, -2.6720e-01,  ..., -2.4435e-02,  1.9184e-03,  2.8272e-02],
          [ 3.3697e-02,  4.1119e-02,  4.8541e-02,  ..., -2.5388e-01, -2.8370e-01, -3.1352e-01],
          ...,
          [-4.6524e-01, -4.5130e-01, -4.3736e-01,  ...,  3.5896e-01,  4.0400e-01,  4.4904e-01],
          [-3.3930e-01, -3.4844e-01, -3.5758e-01,  ..., -2.3464e-01, -2.1639e-01, -1.9814e-01],
          [ 3.6742e-02,  6.0003e-02,  8.3263e-02,  ...,  8.1462e-03, -2.0679e-02, -4.9504e-02]]]])
DESIRED: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[ 0.2280,  0.2280,  0.2280,  ...,  0.0686,  0.0686,  0.0686],
          [ 0.0189,  0.0189,  0.0189,  ...,  0.3551,  0.3551,  0.3551],
          [-0.3278, -0.3278, -0.3278,  ...,  0.0375,  0.0375,  0.0375],
          ...,
          [-0.4585, -0.4585, -0.4585,  ..., -0.0437, -0.0437, -0.0437],
          [-0.2434, -0.2434, -0.2434,  ...,  0.0446,  0.0446,  0.0446],
          [-0.1145, -0.1145, -0.1145,  ..., -0.2897, -0.2897, -0.2897]],

         [[ 0.0352,  0.0352,  0.0352,  ...,  0.1676,  0.1676,  0.1676],
          [-0.3808, -0.3808, -0.3808,  ...,  0.0936,  0.0936,  0.0936],
          [-0.0115, -0.0115, -0.0115,  ...,  0.3729,  0.3729,  0.3729],
          ...,
          [ 0.1341,  0.1341,  0.1341,  ..., -0.2106, -0.2106, -0.2106],
          [ 0.4748,  0.4748,  0.4748,  ...,  0.2023,  0.2023,  0.2023],
          [-0.3681, -0.3681, -0.3681,  ..., -0.0070, -0.0070, -0.0070]],

         [[-0.1485, -0.1485, -0.1485,  ...,  0.4436,  0.4436,  0.4436],
          [-0.1767, -0.1767, -0.1767,  ..., -0.1244, -0.1244, -0.1244],
          [ 0.1559,  0.1559,  0.1559,  ...,  0.3959,  0.3959,  0.3959],
          ...,
          [ 0.1594,  0.1594,  0.1594,  ...,  0.1127,  0.1127,  0.1127],
          [-0.3843, -0.3843, -0.3843,  ..., -0.1559, -0.1559, -0.1559],
          [-0.1507, -0.1507, -0.1507,  ...,  0.3724,  0.3724,  0.3724]],

         ...,

         [[-0.3080, -0.3080, -0.3080,  ..., -0.1328, -0.1328, -0.1328],
          [-0.2160, -0.2160, -0.2160,  ...,  0.1097,  0.1097,  0.1097],
          [ 0.1859,  0.1859,  0.1859,  ..., -0.4760, -0.4760, -0.4760],
          ...,
          [-0.0728, -0.0728, -0.0728,  ..., -0.3148, -0.3148, -0.3148],
          [ 0.2979,  0.2979,  0.2979,  ...,  0.4784,  0.4784,  0.4784],
          [ 0.3617,  0.3617,  0.3617,  ..., -0.4038, -0.4038, -0.4038]],

         [[-0.4006, -0.4006, -0.4006,  ...,  0.3250,  0.3250,  0.3250],
          [ 0.0115,  0.0115,  0.0115,  ..., -0.2343, -0.2343, -0.2343],
          [-0.2025, -0.2025, -0.2025,  ...,  0.0784,  0.0784,  0.0784],
          ...,
          [ 0.2425,  0.2425,  0.2425,  ..., -0.4865, -0.4865, -0.4865],
          [-0.4531, -0.4531, -0.4531,  ..., -0.1490, -0.1490, -0.1490],
          [ 0.4534,  0.4534,  0.4534,  ...,  0.3224,  0.3224,  0.3224]],

         [[ 0.3902,  0.3902,  0.3902,  ...,  0.2461,  0.2461,  0.2461],
          [-0.0021, -0.0021, -0.0021,  ..., -0.1419, -0.1419, -0.1419],
          [-0.0592, -0.0592, -0.0592,  ..., -0.0999, -0.0999, -0.0999],
          ...,
          [-0.2068, -0.2068, -0.2068,  ..., -0.1232, -0.1232, -0.1232],
          [-0.2272, -0.2272, -0.2272,  ...,  0.3542,  0.3542,  0.3542],
          [-0.0908, -0.0908, -0.0908,  ..., -0.0178, -0.0178, -0.0178]]]])

2025-07-09 12:01:18.895995 GPU 3 122447 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 6, 22369622],"float32"), list[32,32,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 32, 6, 22369622],"float32"), list[32,32,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 31859 / 32768 (97.2%)
Greatest absolute difference: 0.9737579822540283 at index (0, 1, 0, 14) (up to 0.01 allowed)
Greatest relative difference: 6984.38525390625 at index (0, 5, 20, 5) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[-0.1667, -0.2565,  0.2957,  ..., -0.4221,  0.0184, -0.4432],
          [-0.0922, -0.2622,  0.2339,  ..., -0.2908,  0.0289, -0.3006],
          [-0.0178, -0.2680,  0.1722,  ..., -0.1596,  0.0394, -0.1579],
          ...,
          [ 0.0391,  0.0598, -0.0683,  ...,  0.1065, -0.0892,  0.0664],
          [-0.0100,  0.0145, -0.0532,  ...,  0.0824, -0.1615, -0.0160],
          [-0.0591, -0.0307, -0.0382,  ...,  0.0584, -0.2337, -0.0985]],

         [[ 0.0916,  0.1399,  0.1151,  ...,  0.4349, -0.0067,  0.4487],
          [ 0.0726,  0.1271,  0.1642,  ...,  0.3553, -0.0489,  0.3380],
          [ 0.0537,  0.1143,  0.2133,  ...,  0.2757, -0.0910,  0.2272],
          ...,
          [ 0.0771,  0.1447,  0.0165,  ...,  0.2913,  0.3546,  0.2293],
          [ 0.0106,  0.1995, -0.0747,  ...,  0.3565,  0.3703,  0.1666],
          [-0.0559,  0.2542, -0.1659,  ...,  0.4217,  0.3859,  0.1039]],

         [[ 0.3838, -0.1890,  0.3800,  ..., -0.2634,  0.2913,  0.1235],
          [ 0.2606, -0.1458,  0.3550,  ..., -0.2258,  0.2450,  0.0829],
          [ 0.1374, -0.1025,  0.3300,  ..., -0.1882,  0.1986,  0.0423],
          ...,
          [ 0.4546,  0.1654,  0.1833,  ..., -0.2460,  0.1367,  0.4212],
          [ 0.4603,  0.2467,  0.1573,  ..., -0.2649,  0.2555,  0.4150],
          [ 0.4660,  0.3281,  0.1313,  ..., -0.2838,  0.3743,  0.4088]],

         ...,

         [[-0.3249, -0.0355,  0.4397,  ..., -0.3236, -0.0617,  0.1174],
          [-0.3432, -0.0241,  0.3362,  ..., -0.3436, -0.0163,  0.1704],
          [-0.3615, -0.0127,  0.2327,  ..., -0.3637,  0.0292,  0.2235],
          ...,
          [-0.2455, -0.0180, -0.1996,  ..., -0.3324, -0.2361,  0.4033],
          [-0.1981, -0.0224, -0.1669,  ..., -0.3416, -0.1890,  0.4126],
          [-0.1508, -0.0269, -0.1342,  ..., -0.3509, -0.1420,  0.4219]],

         [[-0.4672,  0.3612,  0.2518,  ...,  0.0400, -0.3962,  0.0991],
          [-0.4067,  0.3539,  0.1626,  ...,  0.0294, -0.2526,  0.0358],
          [-0.3462,  0.3465,  0.0734,  ...,  0.0187, -0.1090, -0.0274],
          ...,
          [-0.1414,  0.1029, -0.0039,  ..., -0.3081, -0.1311,  0.0796],
          [-0.1895,  0.1120, -0.0639,  ..., -0.3106, -0.2730,  0.0262],
          [-0.2376,  0.1211, -0.1239,  ..., -0.3130, -0.4148, -0.0272]],

         [[ 0.0542,  0.0785, -0.0605,  ..., -0.2761, -0.0430,  0.3468],
          [ 0.0167,  0.0537, -0.0175,  ..., -0.1866, -0.0763,  0.2549],
          [-0.0209,  0.0288,  0.0255,  ..., -0.0971, -0.1095,  0.1629],
          ...,
          [-0.0347, -0.0327,  0.0709,  ...,  0.0225, -0.1653,  0.0643],
          [-0.0246,  0.0402,  0.0177,  ...,  0.0607, -0.2428,  0.0621],
          [-0.0145,  0.1132, -0.0356,  ...,  0.0989, -0.3204,  0.0599]]]])
DESIRED: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[ 1.4573e-01,  3.7355e-01,  2.3081e-01,  ..., -2.5951e-01, -4.1739e-01,  3.0813e-01],
          [ 1.4573e-01,  3.7355e-01,  2.3081e-01,  ..., -2.5951e-01, -4.1739e-01,  3.0813e-01],
          [ 1.4573e-01,  3.7355e-01,  2.3081e-01,  ..., -2.5951e-01, -4.1739e-01,  3.0813e-01],
          ...,
          [-6.6588e-02, -2.2574e-02,  6.5656e-02,  ..., -1.6780e-01,  9.6146e-02, -1.9829e-01],
          [-6.6588e-02, -2.2574e-02,  6.5656e-02,  ..., -1.6780e-01,  9.6146e-02, -1.9829e-01],
          [-6.6588e-02, -2.2574e-02,  6.5656e-02,  ..., -1.6780e-01,  9.6146e-02, -1.9829e-01]],

         [[-4.0875e-01,  6.4051e-03,  3.8211e-02,  ..., -3.1517e-01, -2.5518e-01,  1.2895e-01],
          [-4.0875e-01,  6.4051e-03,  3.8211e-02,  ..., -3.1517e-01, -2.5518e-01,  1.2895e-01],
          [-4.0875e-01,  6.4051e-03,  3.8211e-02,  ..., -3.1517e-01, -2.5518e-01,  1.2895e-01],
          ...,
          [ 1.9564e-01, -2.9897e-02,  2.0948e-01,  ...,  4.4044e-03, -3.1811e-02,  1.5270e-01],
          [ 1.9564e-01, -2.9897e-02,  2.0948e-01,  ...,  4.4044e-03, -3.1811e-02,  1.5270e-01],
          [ 1.9564e-01, -2.9897e-02,  2.0948e-01,  ...,  4.4044e-03, -3.1811e-02,  1.5270e-01]],

         [[-1.4235e-01, -3.8144e-04, -2.3974e-01,  ..., -8.2969e-02, -3.5639e-01, -3.5222e-01],
          [-1.4235e-01, -3.8144e-04, -2.3974e-01,  ..., -8.2969e-02, -3.5639e-01, -3.5222e-01],
          [-1.4235e-01, -3.8144e-04, -2.3974e-01,  ..., -8.2969e-02, -3.5639e-01, -3.5222e-01],
          ...,
          [-1.9689e-01,  1.7631e-01, -7.4737e-02,  ...,  1.1320e-01,  4.7207e-01,  8.9926e-03],
          [-1.9689e-01,  1.7631e-01, -7.4737e-02,  ...,  1.1320e-01,  4.7207e-01,  8.9926e-03],
          [-1.9689e-01,  1.7631e-01, -7.4737e-02,  ...,  1.1320e-01,  4.7207e-01,  8.9926e-03]],

         ...,

         [[-3.0069e-01, -2.0060e-01,  2.2558e-01,  ..., -1.1555e-01,  3.2378e-02,  5.3629e-02],
          [-3.0069e-01, -2.0060e-01,  2.2558e-01,  ..., -1.1555e-01,  3.2378e-02,  5.3629e-02],
          [-3.0069e-01, -2.0060e-01,  2.2558e-01,  ..., -1.1555e-01,  3.2378e-02,  5.3629e-02],
          ...,
          [-9.3081e-02,  6.6904e-02,  5.4862e-02,  ...,  4.8372e-01, -1.9512e-01,  1.7668e-01],
          [-9.3081e-02,  6.6904e-02,  5.4862e-02,  ...,  4.8372e-01, -1.9512e-01,  1.7668e-01],
          [-9.3081e-02,  6.6904e-02,  5.4862e-02,  ...,  4.8372e-01, -1.9512e-01,  1.7668e-01]],

         [[-1.0814e-01,  5.6727e-02,  2.9982e-02,  ..., -2.5642e-01,  2.6481e-01, -1.0717e-01],
          [-1.0814e-01,  5.6727e-02,  2.9982e-02,  ..., -2.5642e-01,  2.6481e-01, -1.0717e-01],
          [-1.0814e-01,  5.6727e-02,  2.9982e-02,  ..., -2.5642e-01,  2.6481e-01, -1.0717e-01],
          ...,
          [ 2.9507e-01, -5.7230e-02,  2.8572e-01,  ...,  7.1530e-02, -3.1024e-01, -1.5140e-01],
          [ 2.9507e-01, -5.7230e-02,  2.8572e-01,  ...,  7.1530e-02, -3.1024e-01, -1.5140e-01],
          [ 2.9507e-01, -5.7230e-02,  2.8572e-01,  ...,  7.1530e-02, -3.1024e-01, -1.5140e-01]],

         [[-2.0126e-01,  4.2277e-01, -4.1010e-01,  ...,  3.2639e-01,  4.9619e-01,  2.3022e-01],
          [-2.0126e-01,  4.2277e-01, -4.1010e-01,  ...,  3.2639e-01,  4.9619e-01,  2.3022e-01],
          [-2.0126e-01,  4.2277e-01, -4.1010e-01,  ...,  3.2639e-01,  4.9619e-01,  2.3022e-01],
          ...,
          [-2.8128e-01, -2.2085e-01, -1.2909e-01,  ..., -3.4876e-01, -1.2199e-02, -4.2141e-01],
          [-2.8128e-01, -2.2085e-01, -1.2909e-01,  ..., -3.4876e-01, -1.2199e-02, -4.2141e-01],
          [-2.8128e-01, -2.2085e-01, -1.2909e-01,  ..., -3.4876e-01, -1.2199e-02, -4.2141e-01]]]])

2025-07-09 12:01:25.031042 GPU 4 122511 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 67108864, 2],"float32"), list[32,32,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1, 32, 67108864, 2],"float32"), list[32,32,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 31887 / 32768 (97.3%)
Greatest absolute difference: 0.9676540493965149 at index (0, 13, 23, 31) (up to 0.01 allowed)
Greatest relative difference: 27557.880859375 at index (0, 6, 4, 16) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[ 0.4226,  0.4118,  0.4010,  ...,  0.1098,  0.0990,  0.0882],
          [ 0.4105,  0.4035,  0.3964,  ...,  0.2064,  0.1994,  0.1923],
          [-0.1797, -0.1844, -0.1891,  ..., -0.3168, -0.3216, -0.3263],
          ...,
          [ 0.4575,  0.4506,  0.4436,  ...,  0.2568,  0.2499,  0.2430],
          [-0.3061, -0.2977, -0.2894,  ..., -0.0637, -0.0553, -0.0470],
          [-0.2759, -0.2673, -0.2588,  ..., -0.0289, -0.0204, -0.0119]],

         [[ 0.0938,  0.1000,  0.1062,  ...,  0.2740,  0.2802,  0.2864],
          [-0.1513, -0.1532, -0.1551,  ..., -0.2065, -0.2084, -0.2103],
          [ 0.1436,  0.1425,  0.1413,  ...,  0.1096,  0.1084,  0.1072],
          ...,
          [-0.4493, -0.4197, -0.3901,  ...,  0.4087,  0.4383,  0.4679],
          [-0.2532, -0.2445, -0.2359,  ..., -0.0030,  0.0056,  0.0142],
          [ 0.3051,  0.2926,  0.2802,  ..., -0.0570, -0.0695, -0.0820]],

         [[ 0.4032,  0.3958,  0.3884,  ...,  0.1894,  0.1821,  0.1747],
          [-0.0605, -0.0583, -0.0561,  ...,  0.0035,  0.0057,  0.0079],
          [ 0.1404,  0.1487,  0.1569,  ...,  0.3793,  0.3876,  0.3958],
          ...,
          [-0.4896, -0.4591, -0.4287,  ...,  0.3930,  0.4235,  0.4539],
          [-0.4157, -0.4040, -0.3924,  ..., -0.0777, -0.0660, -0.0544],
          [ 0.0499,  0.0348,  0.0197,  ..., -0.3880, -0.4031, -0.4182]],

         ...,

         [[ 0.4457,  0.4453,  0.4450,  ...,  0.4357,  0.4354,  0.4351],
          [ 0.1080,  0.0892,  0.0704,  ..., -0.4369, -0.4557, -0.4745],
          [ 0.1263,  0.1081,  0.0898,  ..., -0.4033, -0.4215, -0.4398],
          ...,
          [-0.2783, -0.2849, -0.2915,  ..., -0.4705, -0.4772, -0.4838],
          [ 0.0880,  0.0776,  0.0672,  ..., -0.2126, -0.2230, -0.2334],
          [-0.3076, -0.3021, -0.2965,  ..., -0.1469, -0.1414, -0.1358]],

         [[-0.0480, -0.0353, -0.0227,  ...,  0.3190,  0.3317,  0.3443],
          [ 0.3155,  0.3184,  0.3213,  ...,  0.3996,  0.4025,  0.4054],
          [-0.0212, -0.0211, -0.0211,  ..., -0.0195, -0.0194, -0.0194],
          ...,
          [ 0.0986,  0.0958,  0.0930,  ...,  0.0173,  0.0145,  0.0117],
          [-0.1126, -0.1066, -0.1006,  ...,  0.0625,  0.0686,  0.0746],
          [ 0.0463,  0.0558,  0.0654,  ...,  0.3227,  0.3323,  0.3418]],

         [[ 0.2001,  0.1912,  0.1824,  ..., -0.0570, -0.0659, -0.0747],
          [ 0.0575,  0.0632,  0.0690,  ...,  0.2251,  0.2309,  0.2367],
          [ 0.0827,  0.0794,  0.0761,  ..., -0.0119, -0.0152, -0.0185],
          ...,
          [-0.4500, -0.4362, -0.4224,  ..., -0.0489, -0.0351, -0.0213],
          [ 0.1850,  0.1682,  0.1513,  ..., -0.3039, -0.3208, -0.3376],
          [-0.2953, -0.2984, -0.3015,  ..., -0.3850, -0.3881, -0.3912]]]])
DESIRED: (shape=torch.Size([1, 32, 32, 32]), dtype=torch.float32)
tensor([[[[-0.1495, -0.1495, -0.1495,  ...,  0.3135,  0.3135,  0.3135],
          [-0.1741, -0.1741, -0.1741,  ...,  0.0964,  0.0964,  0.0964],
          [ 0.1288,  0.1288,  0.1288,  ...,  0.1240,  0.1240,  0.1240],
          ...,
          [-0.2567, -0.2567, -0.2567,  ..., -0.0308, -0.0308, -0.0308],
          [-0.2950, -0.2950, -0.2950,  ...,  0.1714,  0.1714,  0.1714],
          [-0.3021, -0.3021, -0.3021,  ..., -0.1215, -0.1215, -0.1215]],

         [[-0.2289, -0.2289, -0.2289,  ..., -0.2219, -0.2219, -0.2219],
          [-0.2525, -0.2525, -0.2525,  ...,  0.4210,  0.4210,  0.4210],
          [ 0.0378,  0.0378,  0.0378,  ..., -0.0055, -0.0055, -0.0055],
          ...,
          [-0.4932, -0.4932, -0.4932,  ...,  0.4130,  0.4130,  0.4130],
          [-0.1456, -0.1456, -0.1456,  ...,  0.2129,  0.2129,  0.2129],
          [-0.3061, -0.3061, -0.3061,  ..., -0.4867, -0.4867, -0.4867]],

         [[-0.1374, -0.1374, -0.1374,  ...,  0.3231,  0.3231,  0.3231],
          [-0.1584, -0.1584, -0.1584,  ...,  0.0072,  0.0072,  0.0072],
          [ 0.3346,  0.3346,  0.3346,  ...,  0.2879,  0.2879,  0.2879],
          ...,
          [ 0.2420,  0.2420,  0.2420,  ...,  0.3723,  0.3723,  0.3723],
          [-0.3958, -0.3958, -0.3958,  ..., -0.0523, -0.0523, -0.0523],
          [-0.0138, -0.0138, -0.0138,  ...,  0.0351,  0.0351,  0.0351]],

         ...,

         [[ 0.1766,  0.1766,  0.1766,  ..., -0.0280, -0.0280, -0.0280],
          [ 0.2947,  0.2947,  0.2947,  ..., -0.2131, -0.2131, -0.2131],
          [ 0.1540,  0.1540,  0.1540,  ..., -0.4114, -0.4114, -0.4114],
          ...,
          [ 0.4008,  0.4008,  0.4008,  ..., -0.4128, -0.4128, -0.4128],
          [-0.2097, -0.2097, -0.2097,  ..., -0.1539, -0.1539, -0.1539],
          [ 0.2345,  0.2345,  0.2345,  ...,  0.0665,  0.0665,  0.0665]],

         [[-0.0770, -0.0770, -0.0770,  ...,  0.2230,  0.2230,  0.2230],
          [ 0.0410,  0.0410,  0.0410,  ...,  0.2768,  0.2768,  0.2768],
          [-0.0326, -0.0326, -0.0326,  ..., -0.0327, -0.0327, -0.0327],
          ...,
          [-0.0513, -0.0513, -0.0513,  ..., -0.0053, -0.0053, -0.0053],
          [-0.4876, -0.4876, -0.4876,  ..., -0.4356, -0.4356, -0.4356],
          [-0.0162, -0.0162, -0.0162,  ...,  0.0858,  0.0858,  0.0858]],

         [[ 0.4119,  0.4119,  0.4119,  ..., -0.1696, -0.1696, -0.1696],
          [ 0.1493,  0.1493,  0.1493,  ..., -0.0157, -0.0157, -0.0157],
          [ 0.1483,  0.1483,  0.1483,  ..., -0.0125, -0.0125, -0.0125],
          ...,
          [-0.2040, -0.2040, -0.2040,  ..., -0.1186, -0.1186, -0.1186],
          [ 0.1942,  0.1942,  0.1942,  ...,  0.3321,  0.3321,  0.3321],
          [ 0.1313,  0.1313,  0.1313,  ...,  0.2603,  0.2603,  0.2603]]]])

2025-07-09 12:01:26.179989 GPU 7 121845 test begin: paddle.nn.functional.interpolate(Tensor([1, 32, 67108864, 2],"float32"), size=tuple(2,2,), mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033707 (unix time) try "date -d @1752033707" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dbf5) received by PID 121845 (TID 0x7fd466b84740) from PID 121845 ***]


2025-07-09 12:01:36.809472 GPU 5 122281 test begin: paddle.nn.functional.interpolate(Tensor([1, 38667, 374, 297],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033743 (unix time) try "date -d @1752033743" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dda9) received by PID 122281 (TID 0x7f8799806740) from PID 122281 ***]


2025-07-09 12:01:58.864555 GPU 4 122511 test begin: paddle.nn.functional.interpolate(Tensor([1, 4405095, 25, 39],"float32"), size=list[12,12,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033755 (unix time) try "date -d @1752033755" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1de8f) received by PID 122511 (TID 0x7ff963391740) from PID 122511 ***]


2025-07-09 12:02:26.386744 GPU 2 123412 test begin: paddle.nn.functional.interpolate(Tensor([1, 4405095, 25, 39],"float32"), size=list[16,16,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033867 (unix time) try "date -d @1752033867" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e214) received by PID 123412 (TID 0x7fccdcdb0740) from PID 123412 ***]


2025-07-09 12:02:28.855862 GPU 5 123499 test begin: paddle.nn.functional.interpolate(Tensor([1, 4521019, 25, 38],"float32"), size=list[12,12,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033850 (unix time) try "date -d @1752033850" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e26b) received by PID 123499 (TID 0x7fa26657f740) from PID 123499 ***]


2025-07-09 12:02:30.862210 GPU 7 123657 test begin: paddle.nn.functional.interpolate(Tensor([1, 4521019, 25, 38],"float32"), size=list[16,16,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033864 (unix time) try "date -d @1752033864" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e309) received by PID 123657 (TID 0x7f94ad4fd740) from PID 123657 ***]


2025-07-09 12:02:34.067398 GPU 6 122954 test begin: paddle.nn.functional.interpolate(Tensor([1, 4521019, 38, 25],"float32"), size=list[12,12,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033785 (unix time) try "date -d @1752033785" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e04a) received by PID 122954 (TID 0x7f367d2de740) from PID 122954 ***]


2025-07-09 12:02:41.562132 GPU 4 123900 test begin: paddle.nn.functional.interpolate(Tensor([1, 4521019, 38, 25],"float32"), size=list[16,16,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033876 (unix time) try "date -d @1752033876" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e3fc) received by PID 123900 (TID 0x7ff29d24d740) from PID 123900 ***]


2025-07-09 12:03:02.973640 GPU 3 122447 test begin: paddle.nn.functional.interpolate(Tensor([1, 5052903, 25, 34],"float32"), size=list[12,12,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033819 (unix time) try "date -d @1752033819" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1de4f) received by PID 122447 (TID 0x7fa233522740) from PID 122447 ***]


2025-07-09 12:03:45.154442 GPU 3 124346 test begin: paddle.nn.functional.interpolate(Tensor([1, 5052903, 25, 34],"float32"), size=list[16,16,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033936 (unix time) try "date -d @1752033936" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e5ba) received by PID 124346 (TID 0x7f3d277c1740) from PID 124346 ***]


2025-07-09 12:03:53.242999 GPU 6 124508 test begin: paddle.nn.functional.interpolate(Tensor([1, 54783, 280, 280],"float32"), size=tuple(256,200,), mode="bilinear", data_format="NCHW", )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 12:04:33.745663 GPU 2 124860 test begin: paddle.nn.functional.interpolate(Tensor([1, 6115, 686, 1024],"float32"), tuple(429,640,), mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033983 (unix time) try "date -d @1752033983" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e7bc) received by PID 124860 (TID 0x7f7d3f524740) from PID 124860 ***]


2025-07-09 12:04:53.664868 GPU 5 125179 test begin: paddle.nn.functional.interpolate(Tensor([1, 64, 1048576, 64],"float32"), list[64,128,], mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033993 (unix time) try "date -d @1752033993" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e8fb) received by PID 125179 (TID 0x7fc64295a740) from PID 125179 ***]


2025-07-09 12:05:09.337548 GPU 6 125342 test begin: paddle.nn.functional.interpolate(Tensor([1, 64, 32, 2097152],"float32"), list[64,128,], mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034001 (unix time) try "date -d @1752034001" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e99e) received by PID 125342 (TID 0x7f2dac6d1740) from PID 125342 ***]


2025-07-09 12:05:44.385970 GPU 7 124702 test begin: paddle.nn.functional.interpolate(Tensor([1, 64, 699051, 96],"float32"), size=list[96,96,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752033965 (unix time) try "date -d @1752033965" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e71e) received by PID 124702 (TID 0x7f8c15d39740) from PID 124702 ***]


2025-07-09 12:06:11.272600 GPU 7 125799 test begin: paddle.nn.functional.interpolate(Tensor([1, 64, 729445, 92],"float32"), size=list[92,92,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034073 (unix time) try "date -d @1752034073" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1eb67) received by PID 125799 (TID 0x7fb3d0dc4740) from PID 125799 ***]


2025-07-09 12:06:20.079239 GPU 3 125966 test begin: paddle.nn.functional.interpolate(Tensor([1, 64, 92, 729445],"float32"), size=list[92,92,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034076 (unix time) try "date -d @1752034076" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ec0e) received by PID 125966 (TID 0x7f503b902740) from PID 125966 ***]


2025-07-09 12:06:28.685295 GPU 2 126126 test begin: paddle.nn.functional.interpolate(Tensor([1, 64, 96, 699051],"float32"), size=list[96,96,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034081 (unix time) try "date -d @1752034081" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ecae) received by PID 126126 (TID 0x7f3ab500c740) from PID 126126 ***]


2025-07-09 12:07:24.821935 GPU 5 126739 test begin: paddle.nn.functional.interpolate(Tensor([1, 68830, 200, 312],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034161 (unix time) try "date -d @1752034161" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ef13) received by PID 126739 (TID 0x7f756ec90740) from PID 126739 ***]


2025-07-09 12:08:03.708279 GPU 3 126925 test begin: paddle.nn.functional.interpolate(Tensor([1, 70295, 235, 260],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 12:08:07.333935 GPU 2 127083 test begin: paddle.nn.functional.interpolate(Tensor([1, 70641, 200, 304],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034202 (unix time) try "date -d @1752034202" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f06b) received by PID 127083 (TID 0x7f4a77b92740) from PID 127083 ***]


2025-07-09 12:08:35.389723 GPU 4 125019 test begin: paddle.nn.functional.interpolate(Tensor([1, 70641, 304, 200],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034160 (unix time) try "date -d @1752034160" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e85b) received by PID 125019 (TID 0x7f1afe442740) from PID 125019 ***]


2025-07-09 12:08:43.696333 GPU 7 127262 test begin: paddle.nn.functional.interpolate(Tensor([1, 7282, 768, 768],"float32"), size=list[384,384,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034242 (unix time) try "date -d @1752034242" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f11e) received by PID 127262 (TID 0x7f40c3785740) from PID 127262 ***]


2025-07-09 12:09:36.891942 GPU 3 127874 test begin: paddle.nn.functional.interpolate(Tensor([1, 768, 14, 399458],"float32"), size=tuple(28,28,), align_corners=False, mode="bicubic", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034270 (unix time) try "date -d @1752034270" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f382) received by PID 127874 (TID 0x7f741133e740) from PID 127874 ***]


2025-07-09 12:10:05.884952 GPU 4 128045 test begin: paddle.nn.functional.interpolate(Tensor([1, 768, 16, 349526],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034296 (unix time) try "date -d @1752034296" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f42d) received by PID 128045 (TID 0x7f0f8a22d740) from PID 128045 ***]


2025-07-09 12:10:45.169870 GPU 2 128219 test begin: paddle.nn.functional.interpolate(Tensor([1, 768, 16, 349526],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034339 (unix time) try "date -d @1752034339" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f4db) received by PID 128219 (TID 0x7f39ee2b8740) from PID 128219 ***]


2025-07-09 12:10:46.368689 GPU 6 126301 test begin: paddle.nn.functional.interpolate(Tensor([1, 768, 174763, 32],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 768, 174763, 32],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 5599518 / 4294975488 (0.1%)
Greatest absolute difference: 1.749618649482727 at index (0, 566, 66945, 31) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 704, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 768, 174763, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 768, 174763, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 12:10:47.070309 GPU 5 127713 test begin: paddle.nn.functional.interpolate(Tensor([1, 768, 174763, 32],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 768, 174763, 32],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 5778228 / 4294975488 (0.1%)
Greatest absolute difference: 1.777923822402954 at index (0, 272, 85333, 0) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 682, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 768, 174763, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 768, 174763, 32]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 12:11:15.676770 GPU 3 128401 test begin: paddle.nn.functional.interpolate(Tensor([1, 768, 31, 180401],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034370 (unix time) try "date -d @1752034370" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f591) received by PID 128401 (TID 0x7f386e98a740) from PID 128401 ***]


2025-07-09 12:11:24.240101 GPU 7 128827 test begin: paddle.nn.functional.interpolate(Tensor([1, 768, 32, 174763],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034390 (unix time) try "date -d @1752034390" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f73b) received by PID 128827 (TID 0x7f240d020740) from PID 128827 ***]


2025-07-09 12:12:10.590086 GPU 5 127713 test begin: paddle.nn.functional.interpolate(Tensor([1, 768, 349526, 16],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 768, 349526, 16],"float32"), size=list[124,128,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2858298 / 4294975488 (0.1%)
Greatest absolute difference: 2.7927157878875732 at index (0, 413, 35234, 12) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 1408, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 768, 349526, 16]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 768, 349526, 16]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 12:12:18.611890 GPU 4 129004 test begin: paddle.nn.functional.interpolate(Tensor([1, 768, 349526, 16],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, )
[accuracy error] backward  paddle.nn.functional.interpolate(Tensor([1, 768, 349526, 16],"float32"), size=list[128,128,], mode="bilinear", align_corners=False, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2948509 / 4294975488 (0.1%)
Greatest absolute difference: 2.7997236251831055 at index (0, 459, 307200, 1) (up to 0.01 allowed)
Greatest relative difference: 1.0 at index (0, 0, 1364, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 768, 349526, 16]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
DESIRED: (shape=torch.Size([1, 768, 349526, 16]), dtype=torch.float32)
tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

2025-07-09 12:12:20.010343 GPU 6 126301 test begin: paddle.nn.functional.interpolate(Tensor([1, 768, 399458, 14],"float32"), size=tuple(28,28,), align_corners=False, mode="bicubic", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034365 (unix time) try "date -d @1752034365" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ed5d) received by PID 126301 (TID 0x7fc301627740) from PID 126301 ***]


2025-07-09 12:12:50.025930 GPU 6 129177 test begin: paddle.nn.functional.interpolate(Tensor([1, 78952, 200, 272],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034472 (unix time) try "date -d @1752034472" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f899) received by PID 129177 (TID 0x7f60c965d740) from PID 129177 ***]


2025-07-09 12:12:56.810501 GPU 3 129344 test begin: paddle.nn.functional.interpolate(Tensor([1, 7929, 736, 736],"float32"), size=list[368,368,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034499 (unix time) try "date -d @1752034499" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f940) received by PID 129344 (TID 0x7fbfd23fa740) from PID 129344 ***]


2025-07-09 12:12:59.775275 GPU 2 129431 test begin: paddle.nn.functional.interpolate(Tensor([1, 81809, 250, 210],"float32"), size=tuple(224,224,), mode="bilinear", data_format="NCHW", )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 12:13:28.797279 GPU 5 127713 test begin: paddle.nn.functional.interpolate(Tensor([1, 83887, 256, 200],"float32"), size=tuple(180,160,), mode="bilinear", data_format="NCHW", )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 12:13:54.868923 GPU 7 130109 test begin: paddle.nn.functional.interpolate(Tensor([1, 8666, 704, 704],"float32"), size=list[352,352,], mode="bilinear", align_corners=False, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034548 (unix time) try "date -d @1752034548" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fc3d) received by PID 130109 (TID 0x7f32d1bb8740) from PID 130109 ***]


2025-07-09 12:20:57.437282 GPU 5 131564 test begin: paddle.nn.functional.interpolate(Tensor([1068, 258, 100, 156],"float32"), size=list[36,36,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034885 (unix time) try "date -d @1752034885" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x201ec) received by PID 131564 (TID 0x7fddc03c0740) from PID 131564 ***]


2025-07-09 12:21:03.638882 GPU 2 131989 test begin: paddle.nn.functional.interpolate(Tensor([1068, 258, 100, 156],"float32"), size=list[40,40,], mode="bilinear", align_corners=False, align_mode=0, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752034896 (unix time) try "date -d @1752034896" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20395) received by PID 131989 (TID 0x7f81b378b740) from PID 131989 ***]


2025-07-09 12:21:16.253904 GPU 3 130458 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752035416 (unix time) try "date -d @1752035416" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fd9a) received by PID 130458 (TID 0x7febf3672740) from PID 130458 ***]


2025-07-09 12:21:16.267092 GPU 4 129004 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1246689172 / 1717986928 (72.6%)
Greatest absolute difference: 1.0 at index (656994, 2, 3) (up to 0.01 allowed)
Greatest relative difference: inf at index (40011, 3, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 4, 4]), dtype=torch.float16)
tensor([[[ 0.2661, -0.3418, -0.0131, -0.2537],
         [ 0.1616, -0.1940, -0.3281, -0.0790],
         [ 0.1126,  0.4612,  0.0036, -0.3569],
         [ 0.1467,  0.0238, -0.2466,  0.2441]],

        [[ 0.2201, -0.4941,  0.1271, -0.4912],
         [-0.1460, -0.0283, -0.3750,  0.3528],
         [-0.2026, -0.0299,  0.4868, -0.4390],
         [-0.1143,  0.1010,  0.2610, -0.0839]],

        [[ 0.3948, -0.1440, -0.2715, -0.2717],
         [-0.3022, -0.2974,  0.3560, -0.2170],
         [-0.1470,  0.3057,  0.4321, -0.4143],
         [-0.0387, -0.2666,  0.0713,  0.2390]],

        ...,

        [[ 0.4531,  0.0107,  0.4619,  0.1804],
         [-0.0905,  0.0283,  0.0688, -0.0525],
         [ 0.1322, -0.2351,  0.2849,  0.2047],
         [-0.1326, -0.0078,  0.0363, -0.1603]],

        [[-0.4827,  0.4023, -0.1454, -0.4885],
         [ 0.1620,  0.0981, -0.0414,  0.3257],
         [-0.3958, -0.1353,  0.4697, -0.3047],
         [ 0.3096, -0.0258,  0.1862, -0.1035]],

        [[ 0.1191,  0.0442,  0.1749, -0.0966],
         [-0.1644,  0.1227, -0.2710, -0.1237],
         [ 0.1992, -0.2637,  0.2118, -0.3511],
         [-0.1422,  0.0267,  0.0354,  0.4663]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([107374183, 4, 4]), dtype=torch.float16)
tensor([[[ 0.2661, -0.3418, -0.0131, -0.2537],
         [ 0.4431, -0.3884, -0.3655,  0.1026],
         [ 0.3831,  0.3740, -0.4541,  0.4158],
         [ 0.2483,  0.0032,  0.0321,  0.2961]],

        [[ 0.2201, -0.4941,  0.1271, -0.4912],
         [-0.2610,  0.1619, -0.2744,  0.4912],
         [-0.4719, -0.1067, -0.2292,  0.1979],
         [-0.0442,  0.0308,  0.4287,  0.4424]],

        [[ 0.3948, -0.1440, -0.2715, -0.2717],
         [-0.1556, -0.1129,  0.3198, -0.2957],
         [ 0.4600,  0.2881,  0.0690,  0.4780],
         [ 0.2559,  0.4961,  0.1823,  0.3625]],

        ...,

        [[ 0.4531,  0.0107,  0.4619,  0.1804],
         [ 0.2039,  0.0953, -0.0407,  0.3926],
         [ 0.3901,  0.3513, -0.3127,  0.2118],
         [ 0.0788, -0.0358, -0.1927,  0.0055]],

        [[-0.4827,  0.4023, -0.1454, -0.4885],
         [ 0.0633,  0.0876, -0.1150,  0.3677],
         [-0.0237, -0.2612, -0.4392,  0.1685],
         [-0.3389, -0.4802, -0.1898,  0.0865]],

        [[ 0.1191,  0.0442,  0.1749, -0.0966],
         [-0.0440,  0.1384, -0.0895, -0.3901],
         [ 0.0790,  0.1079,  0.4685, -0.4905],
         [ 0.4292, -0.3706, -0.1675,  0.1996]]], dtype=torch.float16)

2025-07-09 12:21:24.771394 GPU 6 130619 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1606761052 / 1717986928 (93.5%)
Greatest absolute difference: 0.75 at index (5015884, 3, 2) (up to 0.01 allowed)
Greatest relative difference: inf at index (338, 3, 3) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 4, 4]), dtype=torch.float16)
tensor([[[-0.3418,  0.4287,  0.4358, -0.3325],
         [-0.2698, -0.3447,  0.3181,  0.4336],
         [ 0.0909,  0.0799, -0.4971, -0.1582],
         [ 0.4971, -0.0406,  0.2291, -0.4976]],

        [[ 0.4094, -0.4216, -0.0514,  0.1433],
         [ 0.3867,  0.1096,  0.2437, -0.0074],
         [-0.4084, -0.0452, -0.0261,  0.4749],
         [ 0.3535,  0.4790, -0.1405,  0.3691]],

        [[-0.2468,  0.3501,  0.1692,  0.3716],
         [ 0.0771, -0.4624,  0.1128,  0.2162],
         [-0.2288,  0.1818, -0.1877,  0.4148],
         [ 0.2455,  0.4150, -0.4302,  0.2803]],

        ...,

        [[ 0.0227,  0.2529,  0.0758, -0.3872],
         [ 0.2001,  0.1522, -0.3987,  0.0339],
         [ 0.2883, -0.3420, -0.2849,  0.3364],
         [-0.2211,  0.2849, -0.1947, -0.4165]],

        [[ 0.3833,  0.3799, -0.1277, -0.1929],
         [ 0.0615, -0.2629,  0.4639,  0.0179],
         [ 0.1954, -0.2070, -0.0991, -0.1725],
         [ 0.4482, -0.1064,  0.0043, -0.1844]],

        [[ 0.2603,  0.0410,  0.4495, -0.4390],
         [-0.4836, -0.0338,  0.1354, -0.4146],
         [-0.1582, -0.0547, -0.2512, -0.2028],
         [-0.2280, -0.1567, -0.1838, -0.2825]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([107374183, 4, 4]), dtype=torch.float16)
tensor([[[ 0.2712,  0.4189,  0.2220, -0.0526],
         [-0.1747, -0.3711,  0.2057,  0.4128],
         [-0.0445,  0.1576, -0.4153, -0.0762],
         [ 0.3428, -0.2206,  0.2593, -0.2534]],

        [[ 0.2422,  0.1158,  0.3057,  0.1940],
         [ 0.3789,  0.1732,  0.1144, -0.0649],
         [-0.2583, -0.0367,  0.0872,  0.2354],
         [ 0.2605,  0.3669, -0.3914,  0.3628]],

        [[-0.2876, -0.1443,  0.2681,  0.4080],
         [ 0.0467, -0.3589,  0.0900,  0.1631],
         [-0.0709,  0.1103, -0.0447,  0.1987],
         [ 0.1326, -0.1180,  0.2542,  0.3372]],

        ...,

        [[ 0.1814,  0.3027, -0.0510, -0.3992],
         [ 0.2085,  0.0623, -0.2666,  0.1373],
         [ 0.2859, -0.3020, -0.1741,  0.2180],
         [ 0.0981, -0.1798,  0.0833, -0.0912]],

        [[-0.1292,  0.3845, -0.0151, -0.0905],
         [ 0.1436, -0.3188,  0.2253,  0.1335],
         [ 0.2520, -0.1294, -0.1488, -0.1223],
         [-0.0873,  0.1153, -0.1453,  0.0096]],

        [[ 0.3691,  0.0839, -0.1384,  0.0573],
         [-0.4138, -0.1118,  0.1724, -0.2081],
         [-0.1912, -0.0386, -0.1923, -0.2368],
         [-0.3774, -0.2235, -0.1888,  0.1742]]], dtype=torch.float16)

2025-07-09 12:21:42.130786 GPU 2 132460 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.6000000000000001,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752035644 (unix time) try "date -d @1752035644" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2056c) received by PID 132460 (TID 0x7f2af56a2740) from PID 132460 ***]


2025-07-09 12:22:10.545095 GPU 5 132623 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.6000000000000001,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.6000000000000001,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1959987054 / 2576980392 (76.1%)
Greatest absolute difference: 0.66650390625 at index (831324, 5, 2) (up to 0.01 allowed)
Greatest relative difference: inf at index (652, 5, 3) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 6, 4]), dtype=torch.float16)
tensor([[[-0.0878, -0.2256, -0.3398,  0.4453],
         [-0.2922, -0.0836, -0.3076,  0.0238],
         [ 0.1851,  0.1292, -0.4370,  0.3853],
         [-0.0453,  0.3354, -0.4180,  0.1082],
         [ 0.1847, -0.0641, -0.2925, -0.1443],
         [ 0.2284,  0.2410,  0.1583, -0.1987]],

        [[-0.1764, -0.2220,  0.0919, -0.0029],
         [ 0.0121,  0.2296,  0.0347,  0.1720],
         [ 0.1896,  0.1775,  0.3240, -0.4409],
         [ 0.2661, -0.4717, -0.2383,  0.1492],
         [ 0.1483, -0.0442,  0.0627, -0.1669],
         [ 0.3232,  0.1082,  0.2749, -0.2915]],

        [[ 0.4297,  0.0260, -0.2344, -0.3977],
         [-0.1118,  0.2654,  0.2228,  0.0318],
         [-0.2532, -0.1946, -0.0256, -0.4812],
         [ 0.2118, -0.1986,  0.2844, -0.1186],
         [-0.1104, -0.3962, -0.0242, -0.3340],
         [ 0.0994, -0.0853, -0.1774,  0.1890]],

        ...,

        [[-0.3477,  0.3972,  0.4268, -0.1565],
         [ 0.1337,  0.0831,  0.0053,  0.2269],
         [-0.2969,  0.0600, -0.1984, -0.0421],
         [-0.0964,  0.4114, -0.4919, -0.1794],
         [-0.3452, -0.0059, -0.0271,  0.1774],
         [-0.1659, -0.1930,  0.1678,  0.0599]],

        [[ 0.0440, -0.0357,  0.0872, -0.3054],
         [ 0.1105,  0.3035, -0.0586, -0.2111],
         [-0.2233,  0.1086, -0.0570, -0.3027],
         [-0.2277,  0.3340, -0.3728,  0.1272],
         [-0.4692, -0.0380, -0.1917, -0.2688],
         [ 0.2720, -0.2412, -0.1462, -0.3489]],

        [[ 0.0311, -0.3574,  0.3528, -0.3804],
         [-0.2369,  0.0511, -0.0556,  0.2808],
         [-0.1691, -0.1926,  0.2306,  0.0360],
         [ 0.0707,  0.2893,  0.1074,  0.0863],
         [ 0.1520, -0.1158, -0.3325, -0.1177],
         [ 0.2046, -0.4062,  0.0767,  0.2460]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([107374183, 6, 4]), dtype=torch.float16)
tensor([[[-0.0878, -0.2256, -0.3398,  0.4453],
         [-0.3540, -0.1860, -0.3752, -0.0423],
         [ 0.1913,  0.2473, -0.4375,  0.3345],
         [-0.0818,  0.3350, -0.4421, -0.0582],
         [ 0.3391, -0.1763, -0.1008, -0.0941],
         [ 0.0561,  0.3787, -0.1124, -0.1556]],

        [[-0.1764, -0.2220,  0.0919, -0.0029],
         [ 0.0348,  0.3308,  0.0457,  0.2810],
         [ 0.1765,  0.0137,  0.3188, -0.4487],
         [ 0.1201, -0.3608, -0.0269,  0.2664],
         [ 0.3108,  0.0972,  0.0245, -0.4346],
         [ 0.0382, -0.3982,  0.1724, -0.3000]],

        [[ 0.4297,  0.0260, -0.2344, -0.3977],
         [-0.2198,  0.3462,  0.1957,  0.1136],
         [-0.2737, -0.0831,  0.1046, -0.4878],
         [-0.0298, -0.2637, -0.0097, -0.1465],
         [ 0.0829, -0.3984,  0.0527, -0.2288],
         [-0.2871,  0.4180,  0.4531, -0.4014]],

        ...,

        [[-0.3477,  0.3972,  0.4268, -0.1565],
         [ 0.1246,  0.0926, -0.0138,  0.2437],
         [-0.3110,  0.1316, -0.2500, -0.0963],
         [-0.0752,  0.1411, -0.2632, -0.0954],
         [-0.4839,  0.0031, -0.0107,  0.2076],
         [ 0.3733,  0.3760, -0.0340,  0.1091]],

        [[ 0.0440, -0.0357,  0.0872, -0.3054],
         [ 0.2271,  0.3086, -0.1171, -0.1633],
         [-0.0260,  0.2151, -0.1130, -0.1526],
         [-0.3250,  0.2489, -0.3640,  0.0379],
         [-0.2981, -0.1427, -0.1069, -0.3569],
         [ 0.0480, -0.2383, -0.2666, -0.3184]],

        [[ 0.0311, -0.3574,  0.3528, -0.3804],
         [-0.1978,  0.0773, -0.0251,  0.2561],
         [-0.2947, -0.0360,  0.2510, -0.1652],
         [-0.0798,  0.2218, -0.0542, -0.0152],
         [ 0.3501, -0.2793, -0.2073, -0.0362],
         [ 0.1588, -0.2988, -0.4998,  0.3569]]], dtype=torch.float16)

2025-07-09 12:30:43.587645 GPU 6 130619 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.6000000000000001,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.6000000000000001,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2129307558 / 2576980392 (82.6%)
Greatest absolute difference: 0.33349609375 at index (12845300, 0, 2) (up to 0.01 allowed)
Greatest relative difference: inf at index (1292, 5, 3) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 6, 4]), dtype=torch.float16)
tensor([[[-0.3418,  0.4287,  0.4358, -0.3325],
         [ 0.1853,  0.0026,  0.3799,  0.2976],
         [-0.0416, -0.4080,  0.0482,  0.3835],
         [-0.2341,  0.2666, -0.3008,  0.0386],
         [ 0.1952, -0.0084, -0.2993, -0.1111],
         [ 0.4971, -0.0406,  0.2291, -0.4976]],

        [[ 0.4094, -0.4216, -0.0514,  0.1433],
         [ 0.0145,  0.1562,  0.1079,  0.0439],
         [ 0.3682,  0.2622, -0.0665, -0.1455],
         [-0.0482, -0.0247,  0.2457, -0.1001],
         [-0.0401,  0.4268,  0.0013,  0.3792],
         [ 0.3535,  0.4790, -0.1405,  0.3691]],

        [[-0.2468,  0.3501,  0.1692,  0.3716],
         [-0.2637, -0.2410,  0.3066, -0.0617],
         [ 0.0041, -0.2139,  0.0581,  0.0887],
         [ 0.1501,  0.0102,  0.1555, -0.1036],
         [-0.0919,  0.0883,  0.1088, -0.2961],
         [ 0.2455,  0.4150, -0.4302,  0.2803]],

        ...,

        [[ 0.0227,  0.2529,  0.0758, -0.3872],
         [ 0.1790, -0.2324,  0.1384,  0.2722],
         [ 0.2203, -0.0635, -0.0816,  0.2820],
         [ 0.2822, -0.2458, -0.0188,  0.0522],
         [-0.1206, -0.3479, -0.0142, -0.3025],
         [-0.2211,  0.2849, -0.1947, -0.4165]],

        [[ 0.3833,  0.3799, -0.1277, -0.1929],
         [-0.0122, -0.2039, -0.3149,  0.0183],
         [ 0.2585, -0.3972, -0.1088,  0.2957],
         [ 0.3313, -0.0206, -0.2184, -0.0520],
         [ 0.3384, -0.1948, -0.3171, -0.1780],
         [ 0.4482, -0.1064,  0.0043, -0.1844]],

        [[ 0.2603,  0.0410,  0.4495, -0.4390],
         [-0.1002, -0.1207,  0.0010,  0.0192],
         [-0.3159, -0.2211,  0.2242,  0.0809],
         [-0.2374, -0.0161, -0.1096, -0.2847],
         [ 0.2939, -0.0520, -0.0964, -0.2316],
         [-0.2280, -0.1567, -0.1838, -0.2825]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([107374183, 6, 4]), dtype=torch.float16)
tensor([[[-6.9336e-02,  4.2432e-01,  3.4082e-01, -2.0813e-01],
         [ 1.1273e-01, -1.0071e-01,  4.3726e-01,  3.6182e-01],
         [-1.6190e-02, -4.1479e-01,  1.8234e-02,  3.7817e-01],
         [-2.7026e-01,  2.8711e-01, -2.7881e-01,  6.0455e-02],
         [ 1.7114e-01,  5.9601e-02, -4.4141e-01, -9.5886e-02],
         [ 4.2847e-01, -1.2054e-01,  2.4255e-01, -3.8916e-01]],

        [[ 3.3496e-01, -1.8274e-01,  1.0736e-01,  1.6577e-01],
         [-2.8442e-02,  1.2164e-01,  2.8656e-02,  2.2144e-03],
         [ 3.6621e-01,  2.7930e-01, -1.0089e-01, -1.6089e-01],
         [-8.2169e-03, -2.2476e-02,  2.7588e-01, -1.6394e-01],
         [-1.0742e-01,  4.5117e-01,  1.2030e-01,  3.8379e-01],
         [ 3.1201e-01,  4.2920e-01, -2.5195e-01,  3.6646e-01]],

        [[-2.6489e-01,  1.3037e-01,  2.1313e-01,  3.8770e-01],
         [-2.5415e-01, -2.2400e-01,  3.0811e-01, -1.8213e-01],
         [-4.0092e-03, -1.8628e-01,  5.2063e-02,  7.4524e-02],
         [ 1.9226e-01, -8.8730e-03,  1.9360e-01, -1.6125e-01],
         [-1.3867e-01,  1.8433e-01,  1.5388e-02, -4.5923e-01],
         [ 1.9531e-01,  1.7810e-01, -1.2610e-01,  3.0566e-01]],

        ...,

        [[ 9.3262e-02,  2.7515e-01,  1.9455e-02, -3.9258e-01],
         [ 1.6504e-01, -3.7036e-01,  1.9629e-01,  4.4116e-01],
         [ 2.2253e-01, -8.7524e-02, -4.6387e-02,  3.0957e-01],
         [ 2.8174e-01, -2.3511e-01,  1.0826e-02,  2.0584e-02],
         [-2.0190e-01, -3.5107e-01, -6.1707e-02, -3.8257e-01],
         [-7.9163e-02,  7.8369e-02, -7.1167e-02, -2.7197e-01]],

        [[ 1.5552e-01,  3.8184e-01, -7.7637e-02, -1.4734e-01],
         [ 5.9753e-02, -3.5132e-01, -3.9917e-01,  3.6957e-02],
         [ 2.8052e-01, -4.1211e-01, -1.7236e-01,  3.2642e-01],
         [ 3.4644e-01,  1.2219e-04, -2.3157e-01, -3.8635e-02],
         [ 4.8950e-01, -2.9077e-01, -3.4766e-01, -2.4109e-01],
         [ 2.1021e-01, -7.8964e-03, -6.2164e-02, -9.8206e-02]],

        [[ 3.0859e-01,  6.0059e-02,  1.8811e-01, -2.1838e-01],
         [-2.2668e-01, -1.7542e-01,  8.4900e-02, -3.1647e-02],
         [-2.9736e-01, -2.4182e-01,  2.3413e-01,  1.3586e-01],
         [-2.4622e-01, -1.1757e-02, -9.3872e-02, -2.9370e-01],
         [ 4.7437e-01, -3.5915e-03, -7.2876e-02, -3.7109e-01],
         [-2.9443e-01, -1.8640e-01, -1.8604e-01, -7.9529e-02]]], dtype=torch.float16)

2025-07-09 12:31:05.150375 GPU 3 133958 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.6000000000000001,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752036311 (unix time) try "date -d @1752036311" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20b46) received by PID 133958 (TID 0x7f709fc77740) from PID 133958 ***]


2025-07-09 12:31:50.155019 GPU 4 129004 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.8000000000000002,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752036282 (unix time) try "date -d @1752036282" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f7ec) received by PID 129004 (TID 0x7fa2e0929740) from PID 129004 ***]


2025-07-09 12:34:49.415547 GPU 2 134703 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.8000000000000002,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.8000000000000002,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2328303084 / 3435973856 (67.8%)
Greatest absolute difference: 0.25 at index (82476, 7, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (231, 5, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 8, 4]), dtype=torch.float16)
tensor([[[-1.6602e-01, -4.9121e-01, -2.5122e-01, -4.8169e-01],
         [-6.8970e-02,  4.7119e-01, -1.7236e-01, -3.2080e-01],
         [ 2.1606e-01,  1.8530e-01,  2.3169e-01,  2.3254e-02],
         ...,
         [ 3.1952e-02, -1.4929e-01,  1.7029e-01, -3.1909e-01],
         [-1.9238e-01, -6.8237e-02, -7.9285e-02, -1.3672e-01],
         [ 2.7267e-02, -2.7490e-01, -1.8567e-01,  2.3303e-01]],

        [[-2.0459e-01,  4.3677e-01, -4.9878e-01, -4.4995e-01],
         [-1.1530e-01, -1.1981e-01,  6.3293e-02,  6.5186e-02],
         [-1.0254e-01,  2.7686e-01, -1.7365e-02,  2.5269e-01],
         ...,
         [-2.3950e-01, -9.8999e-02,  2.2681e-01,  3.5278e-01],
         [-2.6147e-01, -2.0105e-01,  1.2695e-01,  1.3342e-01],
         [-2.7661e-01, -1.6028e-01, -3.8745e-01,  2.5854e-01]],

        [[ 1.2866e-01, -3.8770e-01, -2.3718e-01,  2.3193e-01],
         [ 2.6758e-01,  6.2561e-03,  5.0201e-02, -7.1533e-02],
         [ 1.2695e-01, -2.5269e-01, -8.3008e-02, -8.4778e-02],
         ...,
         [-3.3691e-01,  4.3164e-01, -2.1716e-01, -2.5391e-01],
         [-1.9690e-01, -6.8848e-02, -3.6255e-01, -3.4302e-01],
         [-6.8726e-02,  4.9316e-02,  2.6807e-01, -1.2177e-01]],

        ...,

        [[-4.5581e-01,  2.7661e-01,  3.4863e-01,  3.8934e-04],
         [ 2.9639e-01,  5.9662e-02, -2.2021e-01, -9.2529e-02],
         [-1.8945e-01, -2.5415e-01,  1.6577e-01, -1.4038e-01],
         ...,
         [-5.5603e-02, -6.7871e-02, -2.7359e-02,  2.4109e-01],
         [-1.6943e-01,  2.6172e-01,  2.3242e-01, -1.0181e-01],
         [ 2.2974e-01, -4.3701e-02,  2.3483e-02, -3.6646e-01]],

        [[-3.0945e-02,  4.9780e-01,  2.4438e-01, -6.2332e-03],
         [-4.8584e-02, -2.8711e-01,  3.4424e-01, -3.2227e-01],
         [ 1.8140e-01, -1.2238e-01,  1.3513e-01, -1.7480e-01],
         ...,
         [ 2.2632e-01, -2.7197e-01, -2.6840e-02, -1.5808e-01],
         [ 3.4131e-01, -1.2695e-02, -1.2274e-01,  8.6060e-03],
         [-2.0837e-01, -5.1361e-02,  1.1505e-01,  1.1612e-02]],

        [[-1.6711e-01,  1.6809e-01,  3.1616e-02, -2.3193e-02],
         [ 1.8591e-01,  1.0400e-01,  2.7856e-01,  1.4014e-01],
         [ 1.0718e-01,  1.0486e-01,  1.8726e-01, -2.4048e-02],
         ...,
         [ 3.5181e-01, -7.9193e-03,  5.5298e-02,  2.2339e-01],
         [ 5.9052e-02, -2.4268e-01, -1.1487e-01, -1.3733e-02],
         [-7.6416e-02,  4.8462e-02, -2.9077e-01, -2.5122e-01]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([107374183, 8, 4]), dtype=torch.float16)
tensor([[[-1.6602e-01, -4.9121e-01, -2.5122e-01, -4.8169e-01],
         [-6.3171e-02,  4.7168e-01, -1.6345e-01, -2.9907e-01],
         [ 2.3926e-01,  1.4307e-01,  2.6294e-01,  6.8359e-03],
         ...,
         [-4.5746e-02, -1.7493e-01,  1.5613e-01, -2.9492e-01],
         [-1.4868e-01,  1.2840e-02, -1.6077e-01, -1.0199e-01],
         [ 6.6467e-02, -4.0674e-01, -1.5771e-01,  3.2935e-01]],

        [[-2.0459e-01,  4.3677e-01, -4.9878e-01, -4.4995e-01],
         [-1.3184e-01, -9.9304e-02,  6.1523e-02,  8.0505e-02],
         [-5.1025e-02,  2.7222e-01, -2.3483e-02,  2.3352e-01],
         ...,
         [-1.8872e-01, -5.7220e-02,  2.6416e-01,  3.0200e-01],
         [-3.6230e-01, -3.2007e-01,  1.6876e-02,  1.3086e-01],
         [-2.0325e-01, -5.4169e-02, -4.7339e-01,  3.0225e-01]],

        [[ 1.2866e-01, -3.8770e-01, -2.3718e-01,  2.3193e-01],
         [ 2.6587e-01, -1.1734e-02,  6.0242e-02, -5.8929e-02],
         [ 1.1176e-01, -2.3572e-01, -1.3220e-01, -1.2451e-01],
         ...,
         [-2.6587e-01,  4.0942e-01, -2.3962e-01, -2.6465e-01],
         [-2.6489e-01, -2.4353e-01, -3.8452e-01, -3.6206e-01],
         [ 2.6962e-02,  2.2461e-01,  4.9536e-01, -3.3234e-02]],

        ...,

        [[-4.5581e-01,  2.7661e-01,  3.4863e-01,  3.8934e-04],
         [ 2.6807e-01,  3.8269e-02, -2.0422e-01, -7.9163e-02],
         [-1.7456e-01, -2.3486e-01,  1.7297e-01, -1.8726e-01],
         ...,
         [-4.5868e-02,  5.0232e-02,  2.2662e-04,  1.9690e-01],
         [-2.3584e-01,  1.9031e-01,  2.9419e-01, -1.6919e-01],
         [ 4.1431e-01, -8.9966e-02, -9.4177e-02, -4.0234e-01]],

        [[-3.0945e-02,  4.9780e-01,  2.4438e-01, -6.2332e-03],
         [-5.2124e-02, -2.9443e-01,  3.3911e-01, -3.1934e-01],
         [ 2.2485e-01, -7.6843e-02,  1.2054e-01, -1.6235e-01],
         ...,
         [ 2.3669e-01, -3.2104e-01, -1.3123e-01, -1.5527e-01],
         [ 3.7158e-01,  1.8677e-01,  2.4078e-02,  7.4951e-02],
         [-4.1528e-01, -2.1936e-01,  8.0139e-02, -3.9001e-02]],

        [[-1.6711e-01,  1.6809e-01,  3.1616e-02, -2.3193e-02],
         [ 1.8115e-01,  1.0742e-01,  2.7783e-01,  1.0980e-01],
         [ 1.1017e-01,  9.4849e-02,  1.7615e-01,  4.3610e-02],
         ...,
         [ 2.6660e-01, -1.4610e-02, -3.4760e-02,  1.1560e-01],
         [ 8.6670e-02, -3.3105e-01, -2.5620e-02,  7.8613e-02],
         [-1.4307e-01,  2.1436e-01, -4.1870e-01, -4.0234e-01]]], dtype=torch.float16)

2025-07-09 12:36:32.915405 GPU 5 132623 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.8000000000000002,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.8000000000000002,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1942161228 / 3435973856 (56.5%)
Greatest absolute difference: 0.125 at index (302675, 0, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (2588, 6, 3) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 8, 4]), dtype=torch.float16)
tensor([[[-0.0878, -0.2256, -0.3398,  0.4453],
         [-0.1163,  0.2090, -0.1146,  0.2126],
         [-0.0899, -0.1561, -0.4536,  0.1957],
         ...,
         [ 0.0699,  0.0782, -0.3589, -0.2028],
         [ 0.3232,  0.0477,  0.1528, -0.1753],
         [ 0.0561,  0.3787, -0.1124, -0.1556]],

        [[-0.1764, -0.2220,  0.0919, -0.0029],
         [-0.0526, -0.0600,  0.0035, -0.1399],
         [ 0.1472,  0.4255,  0.2153, -0.0556],
         ...,
         [ 0.0600, -0.0979,  0.1439,  0.0508],
         [ 0.4104,  0.2671,  0.2184, -0.3398],
         [ 0.0382, -0.3982,  0.1724, -0.3000]],

        [[ 0.4297,  0.0260, -0.2344, -0.3977],
         [ 0.1968,  0.0342,  0.3000, -0.2018],
         [-0.2937,  0.0095, -0.0412, -0.1692],
         ...,
         [-0.2109, -0.3838, -0.1766, -0.2820],
         [ 0.2178, -0.3589, -0.2979,  0.2295],
         [-0.2871,  0.4180,  0.4531, -0.4014]],

        ...,

        [[-0.3477,  0.3972,  0.4268, -0.1565],
         [ 0.1597,  0.0558,  0.0598,  0.1794],
         [-0.1119,  0.0290, -0.0947,  0.1296],
         ...,
         [-0.2373, -0.0983,  0.0112,  0.1250],
         [-0.4529, -0.3059,  0.1689,  0.0969],
         [ 0.3733,  0.3760, -0.0340,  0.1091]],

        [[ 0.0440, -0.0357,  0.0872, -0.3054],
         [-0.2224,  0.2891,  0.1085, -0.3477],
         [-0.0963,  0.1216, -0.0803, -0.3196],
         ...,
         [-0.4697,  0.0189, -0.2485, -0.2072],
         [ 0.1403, -0.2069, -0.0934, -0.3616],
         [ 0.0480, -0.2383, -0.2666, -0.3184]],

        [[ 0.0311, -0.3574,  0.3528, -0.3804],
         [-0.3486, -0.0238, -0.1428,  0.3508],
         [-0.0667, -0.1720,  0.1260,  0.2583],
         ...,
         [-0.0114, -0.0314, -0.3196, -0.1354],
         [ 0.2712, -0.3955,  0.1606,  0.1095],
         [ 0.1588, -0.2988, -0.4998,  0.3569]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([107374183, 8, 4]), dtype=torch.float16)
tensor([[[-0.0749, -0.1438, -0.2937,  0.4338],
         [-0.1576,  0.1404, -0.1599,  0.1683],
         [-0.0565, -0.1389, -0.4517,  0.2273],
         ...,
         [ 0.0442,  0.1102, -0.3738, -0.2158],
         [ 0.3259,  0.0088,  0.1088, -0.1611],
         [ 0.0884,  0.3528, -0.0616, -0.1637]],

        [[-0.1670, -0.2289,  0.0779, -0.0492],
         [-0.0375,  0.0079,  0.0108, -0.0668],
         [ 0.1545,  0.4199,  0.2297, -0.1025],
         ...,
         [ 0.0402, -0.1100,  0.1622,  0.0997],
         [ 0.3931,  0.2375,  0.1848, -0.3564],
         [ 0.0917, -0.3032,  0.1917, -0.2983]],

        [[ 0.4294,  0.0053, -0.1603, -0.3950],
         [ 0.1245,  0.0884,  0.2820, -0.1470],
         [-0.2854, -0.0334, -0.0596, -0.2072],
         ...,
         [-0.2336, -0.3809, -0.2109, -0.2703],
         [ 0.1943, -0.3657, -0.2371,  0.1499],
         [-0.2146,  0.3237,  0.3350, -0.2908]],

        ...,

        [[-0.2817,  0.3521,  0.3860, -0.1190],
         [ 0.1536,  0.0622,  0.0470,  0.1906],
         [-0.1328,  0.0217, -0.0996,  0.1166],
         ...,
         [-0.2131, -0.1190,  0.0198,  0.1132],
         [-0.4583, -0.2522,  0.1378,  0.1161],
         [ 0.2722,  0.2693,  0.0039,  0.0999]],

        [[-0.0205,  0.0035,  0.1055, -0.3235],
         [-0.1444,  0.2925,  0.0693, -0.3157],
         [-0.1431,  0.1033, -0.0686, -0.3408],
         ...,
         [-0.4700,  0.0317, -0.2612, -0.1932],
         [ 0.0642, -0.1958, -0.0958, -0.3608],
         [ 0.0900, -0.2388, -0.2440, -0.3240]],

        [[-0.0268, -0.3228,  0.2827, -0.2825],
         [-0.3225, -0.0062, -0.1224,  0.3345],
         [-0.0598, -0.1990,  0.1359,  0.2620],
         ...,
         [-0.0482, -0.0125, -0.3169, -0.1394],
         [ 0.2849, -0.3752,  0.0967,  0.0842],
         [ 0.1674, -0.3188, -0.3916,  0.3362]]], dtype=torch.float16)

2025-07-09 12:42:07.820882 GPU 6 130619 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float16"), size=None, scale_factor=list[0.8000000000000002,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752036903 (unix time) try "date -d @1752036903" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fe3b) received by PID 130619 (TID 0x7f0cb2b1c740) from PID 130619 ***]


2025-07-09 12:45:25.437145 GPU 4 138105 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752036641 (unix time) try "date -d @1752036641" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21b79) received by PID 138105 (TID 0x7ff206873740) from PID 138105 ***]


2025-07-09 12:46:47.533029 GPU 3 138280 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1960068611 / 2576980392 (76.1%)
Greatest absolute difference: 0.6666420102119446 at index (78766254, 5, 2) (up to 0.01 allowed)
Greatest relative difference: 343249728.0 at index (82772040, 3, 2) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 6, 4]), dtype=torch.float32)
tensor([[[-0.1416, -0.1957,  0.1355,  0.1296],
         [ 0.1294, -0.3037,  0.0931,  0.0489],
         [-0.2145,  0.1396, -0.0132,  0.1132],
         [-0.4909, -0.2048,  0.4664,  0.0553],
         [-0.0695, -0.4127,  0.2678, -0.2166],
         [ 0.3513, -0.1653, -0.2428, -0.0961]],

        [[-0.1986,  0.1513, -0.2787, -0.1390],
         [ 0.1222, -0.2496, -0.0749, -0.1047],
         [-0.2362, -0.0286,  0.2109, -0.2494],
         [-0.3141, -0.0566,  0.1341, -0.1037],
         [-0.3632,  0.4039,  0.2205,  0.1573],
         [ 0.0051,  0.3102, -0.2639, -0.0136]],

        [[ 0.3666,  0.4498,  0.4534, -0.2568],
         [-0.0388,  0.1209, -0.2505, -0.1642],
         [-0.0247,  0.1926,  0.1512,  0.3216],
         [ 0.0455,  0.3060, -0.4556, -0.0945],
         [ 0.1966, -0.0501, -0.2221, -0.2965],
         [ 0.0493, -0.2047, -0.0642, -0.0539]],

        ...,

        [[ 0.3397, -0.0831, -0.2110, -0.2547],
         [-0.2420,  0.0054, -0.1400, -0.0413],
         [ 0.2673,  0.2072, -0.1703, -0.1594],
         [-0.2811, -0.0625,  0.0544,  0.1787],
         [-0.3457,  0.1995, -0.2777, -0.2073],
         [-0.0572,  0.2171,  0.1284, -0.1510]],

        [[-0.4838, -0.1132,  0.3921, -0.4104],
         [-0.0194, -0.1571, -0.2994,  0.2959],
         [ 0.3678, -0.1760,  0.1420,  0.3380],
         [-0.4129,  0.0416,  0.1404,  0.2875],
         [-0.0457,  0.4337, -0.3853,  0.3705],
         [ 0.3174,  0.0850,  0.2429, -0.3295]],

        [[-0.0955, -0.4341,  0.2026, -0.2181],
         [ 0.2222, -0.1021,  0.0887, -0.0046],
         [ 0.1675, -0.3587, -0.2056,  0.0665],
         [ 0.1144,  0.0643,  0.0121,  0.2001],
         [-0.0670,  0.0467, -0.0032, -0.3083],
         [ 0.1623, -0.0923,  0.2027, -0.2788]]])
DESIRED: (shape=torch.Size([107374183, 6, 4]), dtype=torch.float32)
tensor([[[-0.1416, -0.1957,  0.1355,  0.1296],
         [ 0.2041, -0.3146,  0.1317,  0.0210],
         [ 0.0073,  0.0613,  0.0417, -0.0631],
         [-0.2106, -0.2331,  0.4790, -0.0322],
         [-0.0704, -0.4687,  0.0444, -0.1859],
         [ 0.0843,  0.3415,  0.0499, -0.3752]],

        [[-0.1986,  0.1513, -0.2787, -0.1390],
         [ 0.0681, -0.3425, -0.1051, -0.1390],
         [-0.3001,  0.0763,  0.3128, -0.3394],
         [-0.2831,  0.0848,  0.2067, -0.0937],
         [-0.2926,  0.4350,  0.0793,  0.1915],
         [-0.4712,  0.2410, -0.2004,  0.2471]],

        [[ 0.3666,  0.4498,  0.4534, -0.2568],
         [-0.0815,  0.0579, -0.2078, -0.2333],
         [ 0.1814,  0.1353,  0.0055,  0.3574],
         [-0.0212,  0.0805, -0.3155, -0.0481],
         [ 0.3245,  0.0221, -0.2679, -0.3550],
         [-0.2522, -0.4066,  0.2438, -0.2552]],

        ...,

        [[ 0.3397, -0.0831, -0.2110, -0.2547],
         [-0.2845, -0.0344, -0.1252, -0.0101],
         [ 0.2091,  0.0579, -0.2400, -0.2548],
         [-0.3616,  0.1193, -0.0841,  0.1343],
         [-0.2360,  0.0999, -0.2139, -0.3507],
         [-0.0300,  0.4790,  0.3597,  0.2957]],

        [[-0.4838, -0.1132,  0.3921, -0.4104],
         [-0.0636, -0.2693, -0.3795,  0.3130],
         [ 0.4155, -0.0869,  0.0320,  0.3945],
         [-0.2177,  0.1517, -0.0590,  0.3379],
         [-0.0224,  0.4529, -0.2876,  0.2205],
         [ 0.3283, -0.3361,  0.4129, -0.4016]],

        [[-0.0955, -0.4341,  0.2026, -0.2181],
         [ 0.3120, -0.1871,  0.0424, -0.0136],
         [ 0.2137, -0.3780, -0.1852,  0.1334],
         [-0.0889, -0.0177,  0.1342,  0.1247],
         [ 0.1512,  0.0711, -0.1092, -0.4138],
         [-0.2543,  0.1350,  0.3922, -0.4439]]])

2025-07-09 12:49:56.006696 GPU 5 132623 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2129542274 / 2576980392 (82.6%)
Greatest absolute difference: 0.3333236575126648 at index (11898053, 0, 1) (up to 0.01 allowed)
Greatest relative difference: 2554398720.0 at index (69136476, 0, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 6, 4]), dtype=torch.float32)
tensor([[[ 0.0910, -0.2999, -0.4105, -0.3837],
         [ 0.2534,  0.2276,  0.1581, -0.3587],
         [ 0.0667,  0.0899, -0.1246,  0.2838],
         [-0.0819, -0.2580,  0.3546,  0.1482],
         [ 0.2126, -0.2797,  0.1660,  0.1838],
         [-0.2284,  0.0954,  0.0906, -0.0036]],

        [[ 0.4269,  0.4415, -0.3228,  0.0362],
         [ 0.2095,  0.2800, -0.2743, -0.0464],
         [-0.2246,  0.1865, -0.1332, -0.0961],
         [-0.3342, -0.2065,  0.1152,  0.0012],
         [ 0.1169,  0.3019, -0.3959,  0.1047],
         [-0.4341,  0.1381,  0.4006, -0.0167]],

        [[-0.0606, -0.0449,  0.1923,  0.0517],
         [-0.3672, -0.0361,  0.2584, -0.2629],
         [-0.1342,  0.1747,  0.0749,  0.3425],
         [ 0.0490,  0.2000,  0.1870, -0.3109],
         [ 0.1888, -0.1876,  0.0390, -0.3949],
         [ 0.1613,  0.3643, -0.4953,  0.4435]],

        ...,

        [[-0.2696,  0.4801,  0.4279, -0.0050],
         [-0.3017,  0.0699,  0.0216,  0.2524],
         [-0.1317, -0.0910, -0.1709,  0.2016],
         [-0.1489, -0.0076,  0.1354, -0.3490],
         [ 0.2969,  0.2282,  0.0898, -0.2856],
         [-0.1736, -0.4077, -0.3020,  0.3975]],

        [[-0.2231, -0.4413,  0.0119,  0.4314],
         [-0.0851, -0.0488,  0.1268, -0.1761],
         [ 0.1534,  0.2967,  0.0931,  0.3079],
         [-0.1457, -0.0994, -0.4278, -0.4492],
         [-0.0224, -0.2436, -0.0205,  0.1332],
         [-0.0203,  0.2956, -0.4296, -0.2571]],

        [[-0.1786,  0.2749,  0.2006, -0.0052],
         [ 0.1413,  0.1039,  0.1107, -0.0613],
         [ 0.0767, -0.2286, -0.1710, -0.1360],
         [ 0.3141, -0.1032, -0.0528,  0.1036],
         [ 0.2683,  0.3198,  0.2503, -0.2214],
         [-0.4061, -0.1048,  0.4872,  0.2581]]])
DESIRED: (shape=torch.Size([107374183, 6, 4]), dtype=torch.float32)
tensor([[[ 0.2109, -0.2393, -0.1962, -0.2323],
         [ 0.2041,  0.3140,  0.1395, -0.4660],
         [ 0.0511,  0.1000, -0.0902,  0.2721],
         [-0.0506, -0.2648,  0.3779,  0.1148],
         [ 0.2144, -0.4364,  0.1884,  0.2342],
         [-0.0838,  0.1792,  0.0860, -0.0083]],

        [[ 0.1708,  0.2733, -0.2537,  0.1798],
         [ 0.3472,  0.3658, -0.3140, -0.1747],
         [-0.2693,  0.1734, -0.1674, -0.0707],
         [-0.3188, -0.1888,  0.1654,  0.0020],
         [ 0.1934,  0.4364, -0.4229,  0.0938],
         [-0.3524,  0.0133,  0.1711,  0.0383]],

        [[-0.1787,  0.0334, -0.0117,  0.1968],
         [-0.3552, -0.0926,  0.4279, -0.4505],
         [-0.1121,  0.1498,  0.0953,  0.3285],
         [ 0.0228,  0.2299,  0.1631, -0.3188],
         [ 0.2064, -0.1240,  0.0559, -0.4870],
         [ 0.1470,  0.0955, -0.3397,  0.2868]],

        ...,

        [[-0.1051,  0.3372,  0.4384, -0.1622],
         [-0.4332,  0.0745, -0.0879,  0.4346],
         [-0.1847, -0.1176, -0.2028,  0.1878],
         [-0.1706, -0.0502,  0.1199, -0.3484],
         [ 0.4452,  0.3512,  0.2166, -0.4336],
         [-0.2146, -0.3598, -0.3405,  0.3671]],

        [[-0.2194, -0.3533,  0.1645,  0.2463],
         [-0.0534, -0.0168,  0.0411, -0.1892],
         [ 0.1566,  0.3118,  0.0903,  0.3015],
         [-0.1838, -0.1368, -0.4320, -0.4554],
         [-0.0310, -0.3835, -0.0208,  0.0877],
         [-0.0096,  0.3024, -0.2928, -0.0663]],

        [[-0.1641,  0.1945,  0.2425, -0.0550],
         [ 0.2104,  0.1215,  0.0568, -0.0380],
         [ 0.0943, -0.2203, -0.1448, -0.1166],
         [ 0.3295, -0.1245, -0.0941,  0.1681],
         [ 0.4377,  0.3193,  0.2761, -0.2216],
         [-0.4071,  0.0375,  0.3739,  0.0986]]])

2025-07-09 12:49:57.180234 GPU 2 134703 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752036916 (unix time) try "date -d @1752036916" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20e2f) received by PID 134703 (TID 0x7f8c6bd2c740) from PID 134703 ***]


2025-07-09 12:51:26.788596 GPU 4 139382 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752037026 (unix time) try "date -d @1752037026" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22076) received by PID 139382 (TID 0x7f3c168ad740) from PID 139382 ***]


2025-07-09 12:51:48.932945 GPU 7 139550 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2235309411 / 3006477124 (74.3%)
Greatest absolute difference: 0.42856985330581665 at index (92571529, 6, 1) (up to 0.01 allowed)
Greatest relative difference: inf at index (7477464, 1, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 7, 4]), dtype=torch.float32)
tensor([[[ 0.1781, -0.0192, -0.0116,  0.2642],
         [ 0.2472,  0.3758, -0.3872, -0.4828],
         [-0.3350, -0.0298, -0.1749,  0.0009],
         ...,
         [ 0.3671,  0.2657, -0.0875, -0.2489],
         [ 0.2212, -0.3159, -0.1901, -0.3058],
         [ 0.2452,  0.0738, -0.0007,  0.0991]],

        [[-0.2234, -0.3041, -0.1708, -0.2471],
         [-0.0694, -0.3056,  0.0879, -0.0947],
         [ 0.1626,  0.2450,  0.1818,  0.3226],
         ...,
         [ 0.0239,  0.2281,  0.3138,  0.0219],
         [-0.0988,  0.3521,  0.1274, -0.3767],
         [-0.1301, -0.3493, -0.3840, -0.3297]],

        [[ 0.1372,  0.1655, -0.0795,  0.4067],
         [ 0.1503, -0.1044,  0.1794,  0.2731],
         [ 0.3821,  0.2775,  0.2013,  0.0735],
         ...,
         [-0.0327, -0.0425, -0.1921,  0.2333],
         [-0.2433,  0.4275,  0.1181,  0.0872],
         [ 0.2440,  0.0662, -0.1555, -0.2118]],

        ...,

        [[-0.0061,  0.1398,  0.3087, -0.4371],
         [-0.2469,  0.2154, -0.1057,  0.1775],
         [ 0.2976, -0.3538,  0.2457, -0.2597],
         ...,
         [-0.2927, -0.2430, -0.1373,  0.0170],
         [-0.1268, -0.4502,  0.0724,  0.2277],
         [-0.2677, -0.2642, -0.0438, -0.1811]],

        [[ 0.4569,  0.0305,  0.3383, -0.3773],
         [-0.1610, -0.0333,  0.2323,  0.0242],
         [-0.0664, -0.3755,  0.4437,  0.2431],
         ...,
         [-0.2881, -0.2256, -0.0890,  0.1453],
         [ 0.1475, -0.2024, -0.1566, -0.3245],
         [-0.2071, -0.0982,  0.3309, -0.0636]],

        [[-0.1305,  0.1561,  0.1807,  0.4646],
         [ 0.0996, -0.1468, -0.2771,  0.0880],
         [-0.3289, -0.3132,  0.3308,  0.1975],
         ...,
         [-0.1763, -0.3487,  0.3889,  0.2581],
         [-0.2312, -0.1682,  0.2007, -0.0945],
         [ 0.0319,  0.0306, -0.1086, -0.3067]]])
DESIRED: (shape=torch.Size([107374183, 7, 4]), dtype=torch.float32)
tensor([[[ 0.1781, -0.0192, -0.0116,  0.2642],
         [ 0.2436,  0.3696, -0.3935, -0.4828],
         [-0.4271, -0.0892, -0.1311,  0.0815],
         ...,
         [ 0.3143,  0.3382, -0.1787, -0.1767],
         [ 0.3116, -0.3154, -0.3018, -0.3310],
         [ 0.1005,  0.3651,  0.3424,  0.4482]],

        [[-0.2234, -0.3041, -0.1708, -0.2471],
         [-0.1037, -0.3096,  0.0952, -0.1350],
         [ 0.2470,  0.3420,  0.1877,  0.4459],
         ...,
         [-0.1450,  0.4944,  0.2446,  0.0235],
         [ 0.0497,  0.1299, -0.1199, -0.3425],
         [-0.4210, -0.4755, -0.3225, -0.3560]],

        [[ 0.1372,  0.1655, -0.0795,  0.4067],
         [ 0.1425, -0.1417,  0.1298,  0.2818],
         [ 0.4312,  0.3910,  0.2710,  0.0287],
         ...,
         [ 0.1247, -0.0290, -0.3262,  0.1559],
         [-0.0899,  0.2768,  0.1580,  0.0069],
         [ 0.3334,  0.0666, -0.4324, -0.2915]],

        ...,

        [[-0.0061,  0.1398,  0.3087, -0.4371],
         [-0.2432,  0.2024, -0.1026,  0.1554],
         [ 0.3835, -0.4313,  0.3002, -0.3030],
         ...,
         [-0.4938, -0.4207, -0.2580,  0.1699],
         [-0.2536, -0.4066,  0.2103,  0.1868],
         [-0.1452, -0.2032, -0.3792, -0.4140]],

        [[ 0.4569,  0.0305,  0.3383, -0.3773],
         [-0.1553, -0.0618,  0.2427,  0.0419],
         [-0.0583, -0.3945,  0.4651,  0.2560],
         ...,
         [-0.4348, -0.4155, -0.1604,  0.2593],
         [-0.0461, -0.2358,  0.0164, -0.1977],
         [-0.1246,  0.0402,  0.3851, -0.0960]],

        [[-0.1305,  0.1561,  0.1807,  0.4646],
         [ 0.0729, -0.1054, -0.2405,  0.0964],
         [-0.3647, -0.3960,  0.3833,  0.2045],
         ...,
         [-0.2242, -0.4643,  0.3714,  0.3454],
         [-0.0419, -0.1941,  0.2791, -0.2263],
         [-0.1115,  0.2262, -0.4818, -0.2285]]])

2025-07-09 12:52:12.466479 GPU 3 138280 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2072821023 / 3006477124 (68.9%)
Greatest absolute difference: 0.21428075432777405 at index (20313466, 0, 0) (up to 0.01 allowed)
Greatest relative difference: 136141392.0 at index (44480195, 4, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 7, 4]), dtype=torch.float32)
tensor([[[-0.1416, -0.1957,  0.1355,  0.1296],
         [ 0.0361, -0.2900,  0.0448,  0.0838],
         [-0.4917,  0.2374, -0.0819,  0.3335],
         ...,
         [ 0.2098, -0.2756,  0.4980, -0.1636],
         [ 0.1378, -0.4499, -0.1182, -0.0999],
         [ 0.0843,  0.3415,  0.0499, -0.3752]],

        [[-0.1986,  0.1513, -0.2787, -0.1390],
         [ 0.1900, -0.1334, -0.0372, -0.0617],
         [-0.1564, -0.1598,  0.0835, -0.1369],
         ...,
         [-0.2365,  0.2968,  0.3155, -0.0787],
         [-0.0917,  0.4011, -0.0613,  0.0657],
         [-0.4712,  0.2410, -0.2004,  0.2471]],

        [[ 0.3666,  0.4498,  0.4534, -0.2568],
         [ 0.0145,  0.1997, -0.3037, -0.0778],
         [-0.2822,  0.2642,  0.3333,  0.2769],
         ...,
         [-0.1212, -0.2576, -0.1055,  0.0215],
         [ 0.2778, -0.0251, -0.2493, -0.2043],
         [-0.2522, -0.4066,  0.2438, -0.2552]],

        ...,

        [[ 0.3397, -0.0831, -0.2110, -0.2547],
         [-0.1888,  0.0550, -0.1585, -0.0803],
         [ 0.3400,  0.3937, -0.0831, -0.0401],
         ...,
         [-0.4825,  0.3919, -0.2919,  0.0677],
         [-0.1740,  0.0948, -0.1289, -0.3595],
         [-0.0300,  0.4790,  0.3597,  0.2957]],

        [[-0.4838, -0.1132,  0.3921, -0.4104],
         [ 0.0358, -0.0170, -0.1992,  0.2746],
         [ 0.3082, -0.2873,  0.2795,  0.2674],
         ...,
         [ 0.0751,  0.3167, -0.3582,  0.4134],
         [ 0.1030,  0.3939, -0.1205,  0.0278],
         [ 0.3283, -0.3361,  0.4129, -0.4016]],

        [[-0.0955, -0.4341,  0.2026, -0.2181],
         [ 0.1099,  0.0043,  0.1465,  0.0067],
         [ 0.1098, -0.3346, -0.2310, -0.0170],
         ...,
         [-0.3938, -0.1407,  0.3174,  0.0116],
         [ 0.2335, -0.0328, -0.0278, -0.3322],
         [-0.2543,  0.1350,  0.3922, -0.4439]]])
DESIRED: (shape=torch.Size([107374183, 7, 4]), dtype=torch.float32)
tensor([[[-0.1635, -0.2071,  0.0851,  0.1423],
         [ 0.1161, -0.3017,  0.0862,  0.0539],
         [-0.4323,  0.2164, -0.0672,  0.2863],
         ...,
         [ 0.1598, -0.2705,  0.4957, -0.1480],
         [ 0.0387, -0.4588, -0.0408, -0.1408],
         [ 0.1701,  0.1786, -0.0442, -0.2855]],

        [[-0.0718,  0.1650, -0.2027, -0.0948],
         [ 0.1319, -0.2330, -0.0695, -0.0985],
         [-0.1735, -0.1317,  0.1108, -0.1610],
         ...,
         [-0.2420,  0.2716,  0.3026, -0.0805],
         [-0.1874,  0.4172,  0.0057,  0.1256],
         [-0.3181,  0.2633, -0.2208,  0.1633]],

        [[ 0.3254,  0.4469,  0.2569, -0.1629],
         [-0.0312,  0.1322, -0.2581, -0.1518],
         [-0.2270,  0.2489,  0.2942,  0.2865],
         ...,
         [-0.1093, -0.2174, -0.1305,  0.0132],
         [ 0.3000, -0.0026, -0.2581, -0.2761],
         [-0.1553, -0.3417,  0.1448, -0.1905]],

        ...,

        [[ 0.2606, -0.0216, -0.2117, -0.2424],
         [-0.2344,  0.0124, -0.1426, -0.0468],
         [ 0.3245,  0.3537, -0.1018, -0.0656],
         ...,
         [-0.4681,  0.3595, -0.2671,  0.0756],
         [-0.2035,  0.0972, -0.1694, -0.3553],
         [-0.0387,  0.3948,  0.2854,  0.1521]],

        [[-0.3370, -0.0025,  0.3298, -0.2773],
         [-0.0116, -0.1371, -0.2851,  0.2929],
         [ 0.3210, -0.2634,  0.2500,  0.2826],
         ...,
         [ 0.0402,  0.2971, -0.3226,  0.4044],
         [ 0.0433,  0.4220, -0.2001,  0.1196],
         [ 0.3248, -0.2008,  0.3582, -0.3784]],

        [[-0.1237, -0.2718,  0.2278, -0.1627],
         [ 0.2061, -0.0869,  0.0969, -0.0030],
         [ 0.1221, -0.3397, -0.2256,  0.0009],
         ...,
         [-0.3575, -0.1261,  0.2956,  0.0250],
         [ 0.1943,  0.0167, -0.0665, -0.3711],
         [-0.1204,  0.0619,  0.3313, -0.3909]]])

2025-07-09 12:55:20.633237 GPU 5 132623 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752037185 (unix time) try "date -d @1752037185" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2060f) received by PID 132623 (TID 0x7ff52adbb740) from PID 132623 ***]


2025-07-09 12:55:21.958174 GPU 2 140328 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([107374183, 8, 4]) != torch.Size([107374183, 7, 4]).
ACTUAL: (shape=torch.Size([107374183, 8, 4]), dtype=torch.float32)
tensor([[[ 0.0294, -0.3286,  0.0076,  0.2603],
         [ 0.2342, -0.1616, -0.0747, -0.2504],
         [-0.1582, -0.0792, -0.1050, -0.3277],
         ...,
         [-0.2053, -0.0766,  0.0239, -0.1783],
         [-0.0283,  0.3254,  0.0628, -0.3550],
         [-0.3314,  0.2519,  0.1017, -0.1793]],

        [[ 0.3381, -0.1879,  0.0831, -0.0657],
         [ 0.0757, -0.1469, -0.4415, -0.0707],
         [-0.4265,  0.1731, -0.4155,  0.3297],
         ...,
         [-0.0716, -0.0179,  0.4112, -0.1496],
         [ 0.2647,  0.2161,  0.2403,  0.0064],
         [ 0.2305, -0.2525, -0.3776, -0.0618]],

        [[ 0.3045,  0.3106,  0.0694, -0.2953],
         [-0.1619,  0.0801, -0.1676, -0.0365],
         [ 0.1652, -0.0985, -0.0408,  0.2168],
         ...,
         [ 0.3665, -0.3810,  0.2573,  0.2102],
         [ 0.2900,  0.1826, -0.1629,  0.2320],
         [ 0.2802,  0.1067,  0.3035, -0.4013]],

        ...,

        [[ 0.0659, -0.2024, -0.4278, -0.2689],
         [-0.3969,  0.0103, -0.1829, -0.0155],
         [-0.0245, -0.2790, -0.2245, -0.1469],
         ...,
         [ 0.2216, -0.2086,  0.3821,  0.1202],
         [ 0.1792,  0.2377, -0.0021, -0.3774],
         [ 0.1510,  0.1102, -0.0693,  0.3527]],

        [[ 0.1620, -0.3995,  0.3428, -0.3884],
         [ 0.3715, -0.0963, -0.1710, -0.0571],
         [ 0.1709,  0.2594, -0.0097,  0.0806],
         ...,
         [ 0.1030, -0.1922,  0.0289, -0.1706],
         [ 0.0311,  0.1297,  0.2454,  0.0415],
         [-0.1385, -0.0015,  0.0143,  0.1786]],

        [[ 0.4202,  0.3545,  0.2302, -0.0273],
         [ 0.1370,  0.1965,  0.1231,  0.1045],
         [ 0.0818,  0.0875, -0.1749,  0.2611],
         ...,
         [ 0.0117,  0.0248, -0.0789,  0.2985],
         [ 0.2877,  0.2970, -0.2469,  0.2392],
         [-0.2172,  0.1959,  0.1323,  0.0148]]])
DESIRED: (shape=torch.Size([107374183, 7, 4]), dtype=torch.float32)
tensor([[[ 0.0294, -0.3286,  0.0076,  0.2603],
         [ 0.2342, -0.1616, -0.0747, -0.2504],
         [-0.1582, -0.0792, -0.1050, -0.3277],
         ...,
         [-0.4052,  0.3624,  0.3539, -0.2620],
         [-0.2053, -0.0766,  0.0239, -0.1783],
         [-0.0283,  0.3254,  0.0628, -0.3550]],

        [[ 0.3381, -0.1879,  0.0831, -0.0657],
         [ 0.0757, -0.1469, -0.4415, -0.0707],
         [-0.4265,  0.1731, -0.4155,  0.3297],
         ...,
         [-0.3103,  0.0693, -0.0024, -0.0174],
         [-0.0716, -0.0179,  0.4112, -0.1496],
         [ 0.2647,  0.2161,  0.2403,  0.0064]],

        [[ 0.3045,  0.3106,  0.0694, -0.2953],
         [-0.1619,  0.0801, -0.1676, -0.0365],
         [ 0.1652, -0.0985, -0.0408,  0.2168],
         ...,
         [-0.3602, -0.3574, -0.3190,  0.0599],
         [ 0.3665, -0.3810,  0.2573,  0.2102],
         [ 0.2900,  0.1826, -0.1629,  0.2320]],

        ...,

        [[ 0.0659, -0.2024, -0.4278, -0.2689],
         [-0.3969,  0.0103, -0.1829, -0.0155],
         [-0.0245, -0.2790, -0.2245, -0.1469],
         ...,
         [ 0.0118, -0.4437,  0.2110,  0.3019],
         [ 0.2216, -0.2086,  0.3821,  0.1202],
         [ 0.1792,  0.2377, -0.0021, -0.3774]],

        [[ 0.1620, -0.3995,  0.3428, -0.3884],
         [ 0.3715, -0.0963, -0.1710, -0.0571],
         [ 0.1709,  0.2594, -0.0097,  0.0806],
         ...,
         [-0.3411, -0.0902, -0.2458, -0.4587],
         [ 0.1030, -0.1922,  0.0289, -0.1706],
         [ 0.0311,  0.1297,  0.2454,  0.0415]],

        [[ 0.4202,  0.3545,  0.2302, -0.0273],
         [ 0.1370,  0.1965,  0.1231,  0.1045],
         [ 0.0818,  0.0875, -0.1749,  0.2611],
         ...,
         [ 0.2611, -0.0282, -0.2421,  0.1149],
         [ 0.0117,  0.0248, -0.0789,  0.2985],
         [ 0.2877,  0.2970, -0.2469,  0.2392]]])

2025-07-09 12:55:46.826328 GPU 6 140490 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([107374183, 8, 4]) != torch.Size([107374183, 7, 4]).
ACTUAL: (shape=torch.Size([107374183, 8, 4]), dtype=torch.float32)
tensor([[[-0.0889,  0.2070, -0.0652, -0.2741],
         [-0.4309, -0.1933, -0.3117, -0.3486],
         [-0.2999,  0.1606, -0.3639,  0.0126],
         ...,
         [-0.1162, -0.1862,  0.2270,  0.0429],
         [ 0.3181, -0.1610,  0.2680,  0.1854],
         [-0.2060, -0.4800,  0.1811, -0.0946]],

        [[-0.3213, -0.2023, -0.3927, -0.2523],
         [ 0.2201, -0.2372,  0.3229,  0.4150],
         [ 0.0306,  0.1996,  0.0518,  0.4136],
         ...,
         [-0.2490, -0.1543, -0.2723,  0.1313],
         [ 0.0908,  0.0817, -0.1340, -0.1788],
         [ 0.2125, -0.0851, -0.2760, -0.2658]],

        [[-0.4359,  0.0017, -0.3851, -0.2692],
         [ 0.3391, -0.0090, -0.1658,  0.2610],
         [ 0.3122,  0.0916,  0.1489, -0.0819],
         ...,
         [ 0.1791, -0.1688,  0.0206,  0.3453],
         [ 0.4075,  0.3410, -0.1094,  0.0808],
         [ 0.3572,  0.1151, -0.0109,  0.1157]],

        ...,

        [[-0.4365,  0.1429, -0.4540, -0.3576],
         [-0.3246,  0.3154,  0.2722, -0.1660],
         [-0.0550,  0.3904, -0.0327,  0.1913],
         ...,
         [ 0.3080, -0.3741,  0.0583, -0.1092],
         [ 0.2633, -0.0355, -0.1711,  0.0411],
         [-0.1660, -0.2732,  0.1057,  0.0350]],

        [[ 0.1262, -0.4368,  0.1770, -0.1819],
         [ 0.0029,  0.0159, -0.0636,  0.0185],
         [-0.3100,  0.0435,  0.0732, -0.1092],
         ...,
         [-0.2456,  0.3551, -0.2698, -0.1561],
         [ 0.1749, -0.0600, -0.1782, -0.1687],
         [-0.2270,  0.0375,  0.0217,  0.1687]],

        [[ 0.0377, -0.3786, -0.2577, -0.2027],
         [-0.2042,  0.4268, -0.2178,  0.1269],
         [ 0.1717,  0.0704, -0.0709,  0.3560],
         ...,
         [-0.3370,  0.1161, -0.0225,  0.0341],
         [ 0.1650, -0.2839, -0.1913, -0.3757],
         [ 0.2642,  0.2790, -0.3709, -0.4583]]])
DESIRED: (shape=torch.Size([107374183, 7, 4]), dtype=torch.float32)
tensor([[[-0.0889,  0.2070, -0.0652, -0.2741],
         [-0.4250, -0.1531, -0.3327, -0.2844],
         [-0.1867,  0.3939, -0.3531,  0.1814],
         ...,
         [-0.2294, -0.2945,  0.1777, -0.0368],
         [ 0.3181, -0.1610,  0.2680,  0.1854],
         [-0.4122, -0.4863,  0.1878, -0.1556]],

        [[-0.3213, -0.2023, -0.3927, -0.2523],
         [ 0.0042, -0.0451,  0.1894,  0.4064],
         [ 0.4888,  0.0602,  0.1813,  0.4379],
         ...,
         [-0.4186, -0.0947, -0.2761,  0.3308],
         [ 0.0908,  0.0817, -0.1340, -0.1788],
         [ 0.3095, -0.2790, -0.3656, -0.3910]],

        [[-0.4359,  0.0017, -0.3851, -0.2692],
         [ 0.3874,  0.1515, -0.0678,  0.2825],
         [ 0.1403, -0.2892,  0.1695, -0.4893],
         ...,
         [ 0.0882, -0.3545, -0.0515,  0.3360],
         [ 0.4075,  0.3410, -0.1094,  0.0808],
         [ 0.3552,  0.0555,  0.1374,  0.2249]],

        ...,

        [[-0.4365,  0.1429, -0.4540, -0.3576],
         [-0.3671,  0.3697,  0.1830, -0.1447],
         [ 0.3420,  0.3026, -0.0699,  0.4848],
         ...,
         [ 0.3067, -0.4886,  0.1029, -0.2965],
         [ 0.2633, -0.0355, -0.1711,  0.0411],
         [-0.2930, -0.3509,  0.2299,  0.1702]],

        [[ 0.1262, -0.4368,  0.1770, -0.1819],
         [-0.0882, -0.0422,  0.0946, -0.0370],
         [-0.3498,  0.2453, -0.2645, -0.0705],
         ...,
         [-0.3236,  0.4791, -0.1949, -0.1098],
         [ 0.1749, -0.0600, -0.1782, -0.1687],
         [-0.4231,  0.0843, -0.0170,  0.2391]],

        [[ 0.0377, -0.3786, -0.2577, -0.2027],
         [-0.0163,  0.4446, -0.2024,  0.1898],
         [-0.0162, -0.3395,  0.0298,  0.3964],
         ...,
         [-0.4961,  0.2913, -0.0017,  0.1673],
         [ 0.1650, -0.2839, -0.1913, -0.3757],
         [ 0.2890,  0.4247, -0.3954, -0.4825]]])

2025-07-09 12:56:55.157684 GPU 3 138280 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([107374183, 8, 4]) != torch.Size([107374183, 7, 4]).
ACTUAL: (shape=torch.Size([107374183, 8, 4]), dtype=torch.float32)
tensor([[[-0.1416, -0.1957,  0.1355,  0.1296],
         [-0.0840, -0.2725, -0.0172,  0.1287],
         [-0.1455, -0.0062,  0.0345,  0.1816],
         ...,
         [ 0.0302, -0.3637,  0.3500, -0.1977],
         [ 0.2865, -0.4365, -0.2343, -0.0385],
         [ 0.0843,  0.3415,  0.0499, -0.3752]],

        [[-0.1986,  0.1513, -0.2787, -0.1390],
         [ 0.2770,  0.0160,  0.0113, -0.0065],
         [-0.0951, -0.2979, -0.0167, -0.1599],
         ...,
         [-0.3180,  0.3657,  0.2545,  0.0730],
         [ 0.0519,  0.3770, -0.1617, -0.0242],
         [-0.4712,  0.2410, -0.2004,  0.2471]],

        [[ 0.3666,  0.4498,  0.4534, -0.2568],
         [ 0.0831,  0.3011, -0.3722,  0.0333],
         [-0.2236,  0.1352,  0.1287,  0.0138],
         ...,
         [ 0.0831, -0.1242, -0.1805, -0.1829],
         [ 0.2445, -0.0588, -0.2359, -0.0967],
         [-0.2522, -0.4066,  0.2438, -0.2552]],

        ...,

        [[ 0.3397, -0.0831, -0.2110, -0.2547],
         [-0.1205,  0.1189, -0.1822, -0.1304],
         [ 0.0450,  0.1847, -0.0917, -0.0072],
         ...,
         [-0.3945,  0.2682, -0.2827, -0.1091],
         [-0.1298,  0.0911, -0.0682, -0.3659],
         [-0.0300,  0.4790,  0.3597,  0.2957]],

        [[-0.4838, -0.1132,  0.3921, -0.4104],
         [ 0.1067,  0.1632, -0.0705,  0.2473],
         [ 0.1205, -0.3516, -0.0544,  0.2979],
         ...,
         [-0.0025,  0.3919, -0.3756,  0.3858],
         [ 0.1925,  0.3518, -0.0012, -0.1099],
         [ 0.3283, -0.3361,  0.4129, -0.4016]],

        [[-0.0955, -0.4341,  0.2026, -0.2181],
         [-0.0345,  0.1410,  0.2208,  0.0213],
         [ 0.2542, -0.3261, -0.1435, -0.0214],
         ...,
         [-0.1837, -0.0202,  0.1113, -0.1941],
         [ 0.2923, -0.1070,  0.0304, -0.2740],
         [-0.2543,  0.1350,  0.3922, -0.4439]]])
DESIRED: (shape=torch.Size([107374183, 7, 4]), dtype=torch.float32)
tensor([[[-0.1544, -0.2023,  0.1061,  0.1370],
         [-0.0339, -0.2798,  0.0086,  0.1100],
         [-0.1887,  0.0243,  0.0199,  0.2006],
         ...,
         [-0.4033, -0.2136,  0.4704,  0.0280],
         [ 0.0527, -0.3527,  0.3685, -0.1934],
         [ 0.2246, -0.4421, -0.1859, -0.0641]],

        [[-0.1246,  0.1593, -0.2344, -0.1132],
         [ 0.2408, -0.0463, -0.0089, -0.0295],
         [-0.1027, -0.2806, -0.0042, -0.1570],
         ...,
         [-0.3044, -0.0124,  0.1568, -0.1005],
         [-0.3078,  0.3571,  0.2621,  0.0541],
         [-0.0079,  0.3870, -0.1199,  0.0133]],

        [[ 0.3426,  0.4481,  0.3388, -0.2021],
         [ 0.0545,  0.2588, -0.3437, -0.0130],
         [-0.2310,  0.1513,  0.1543,  0.0467],
         ...,
         [ 0.0247,  0.2355, -0.4118, -0.0800],
         [ 0.0576, -0.1409, -0.1711, -0.1574],
         [ 0.2584, -0.0447, -0.2415, -0.1416]],

        ...,

        [[ 0.2935, -0.0472, -0.2114, -0.2475],
         [-0.1490,  0.0923, -0.1723, -0.1095],
         [ 0.0819,  0.2108, -0.0906, -0.0113],
         ...,
         [-0.3062, -0.0057,  0.0111,  0.1648],
         [-0.4055,  0.2837, -0.2839, -0.0870],
         [-0.1482,  0.0926, -0.0935, -0.3632]],

        [[-0.3981, -0.0486,  0.3557, -0.3328],
         [ 0.0772,  0.0881, -0.1241,  0.2587],
         [ 0.1439, -0.3436, -0.0127,  0.2941],
         ...,
         [-0.3519,  0.0760,  0.0781,  0.3032],
         [ 0.0072,  0.3825, -0.3735,  0.3893],
         [ 0.1552,  0.3694, -0.0510, -0.0525]],

        [[-0.1119, -0.3394,  0.2173, -0.1858],
         [ 0.0256,  0.0840,  0.1898,  0.0152],
         [ 0.2361, -0.3271, -0.1545, -0.0208],
         ...,
         [ 0.0509,  0.0386,  0.0502,  0.1766],
         [-0.2100, -0.0353,  0.1370, -0.1684],
         [ 0.2678, -0.0761,  0.0062, -0.2983]]])

2025-07-09 12:57:31.779681 GPU 7 139550 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([107374183, 8, 4]) != torch.Size([107374183, 7, 4]).
ACTUAL: (shape=torch.Size([107374183, 8, 4]), dtype=torch.float32)
tensor([[[ 0.1781, -0.0192, -0.0116,  0.2642],
         [ 0.2545,  0.3882, -0.3746, -0.4828],
         [-0.1506,  0.0890, -0.2625, -0.1603],
         ...,
         [ 0.2589,  0.0578, -0.1645, -0.2277],
         [ 0.3658, -0.3150, -0.3689, -0.3461],
         [ 0.1005,  0.3651,  0.3424,  0.4482]],

        [[-0.2234, -0.3041, -0.1708, -0.2471],
         [-0.0008, -0.2977,  0.0732, -0.0140],
         [-0.0062,  0.0509,  0.1700,  0.0759],
         ...,
         [-0.1506,  0.4715,  0.2368, -0.1539],
         [ 0.1388, -0.0034, -0.2682, -0.3220],
         [-0.4210, -0.4755, -0.3225, -0.3560]],

        [[ 0.1372,  0.1655, -0.0795,  0.4067],
         [ 0.1660, -0.0297,  0.2785,  0.2558],
         [ 0.2840,  0.0506,  0.0618,  0.1632],
         ...,
         [-0.0593,  0.1925, -0.1426,  0.1402],
         [ 0.0021,  0.1863,  0.1819, -0.0413],
         [ 0.3334,  0.0666, -0.4324, -0.2915]],

        ...,

        [[-0.0061,  0.1398,  0.3087, -0.4371],
         [-0.2543,  0.2414, -0.1117,  0.2219],
         [ 0.1259, -0.1988,  0.1366, -0.1730],
         ...,
         [-0.3148, -0.4408, -0.1401,  0.2017],
         [-0.3296, -0.3804,  0.2931,  0.1622],
         [-0.1452, -0.2032, -0.3792, -0.4140]],

        [[ 0.4569,  0.0305,  0.3383, -0.3773],
         [-0.1726,  0.0237,  0.2116, -0.0112],
         [-0.0826, -0.3374,  0.4009,  0.2174],
         ...,
         [-0.1520, -0.3185, -0.1884, -0.0126],
         [-0.1622, -0.2559,  0.1202, -0.1217],
         [-0.1246,  0.0402,  0.3851, -0.0960]],

        [[-0.1305,  0.1561,  0.1807,  0.4646],
         [ 0.1530, -0.2294, -0.3504,  0.0711],
         [-0.2573, -0.1475,  0.2259,  0.1835],
         ...,
         [-0.2597, -0.3329,  0.2848,  0.1795],
         [ 0.0717, -0.2096,  0.3262, -0.3055],
         [-0.1115,  0.2262, -0.4818, -0.2285]]])
DESIRED: (shape=torch.Size([107374183, 7, 4]), dtype=torch.float32)
tensor([[[ 0.1781, -0.0192, -0.0116,  0.2642],
         [ 0.2436,  0.3696, -0.3935, -0.4828],
         [-0.4271, -0.0892, -0.1311,  0.0815],
         ...,
         [ 0.3143,  0.3382, -0.1787, -0.1767],
         [ 0.3116, -0.3154, -0.3018, -0.3310],
         [ 0.1005,  0.3651,  0.3424,  0.4482]],

        [[-0.2234, -0.3041, -0.1708, -0.2471],
         [-0.1037, -0.3096,  0.0952, -0.1350],
         [ 0.2470,  0.3420,  0.1877,  0.4459],
         ...,
         [-0.1450,  0.4944,  0.2446,  0.0235],
         [ 0.0497,  0.1299, -0.1199, -0.3425],
         [-0.4210, -0.4755, -0.3225, -0.3560]],

        [[ 0.1372,  0.1655, -0.0795,  0.4067],
         [ 0.1425, -0.1417,  0.1298,  0.2818],
         [ 0.4312,  0.3910,  0.2710,  0.0287],
         ...,
         [ 0.1247, -0.0290, -0.3262,  0.1559],
         [-0.0899,  0.2768,  0.1580,  0.0069],
         [ 0.3334,  0.0666, -0.4324, -0.2915]],

        ...,

        [[-0.0061,  0.1398,  0.3087, -0.4371],
         [-0.2432,  0.2024, -0.1026,  0.1554],
         [ 0.3835, -0.4313,  0.3002, -0.3030],
         ...,
         [-0.4938, -0.4207, -0.2580,  0.1699],
         [-0.2536, -0.4066,  0.2103,  0.1868],
         [-0.1452, -0.2032, -0.3792, -0.4140]],

        [[ 0.4569,  0.0305,  0.3383, -0.3773],
         [-0.1553, -0.0618,  0.2427,  0.0419],
         [-0.0583, -0.3945,  0.4651,  0.2560],
         ...,
         [-0.4348, -0.4155, -0.1604,  0.2593],
         [-0.0461, -0.2358,  0.0164, -0.1977],
         [-0.1246,  0.0402,  0.3851, -0.0960]],

        [[-0.1305,  0.1561,  0.1807,  0.4646],
         [ 0.0729, -0.1054, -0.2405,  0.0964],
         [-0.3647, -0.3960,  0.3833,  0.2045],
         ...,
         [-0.2242, -0.4643,  0.3714,  0.3454],
         [-0.0419, -0.1941,  0.2791, -0.2263],
         [-0.1115,  0.2262, -0.4818, -0.2285]]])

2025-07-09 12:57:48.990216 GPU 4 140974 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752037451 (unix time) try "date -d @1752037451" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x226ae) received by PID 140974 (TID 0x7f01d26e0740) from PID 140974 ***]


2025-07-09 13:00:34.812247 GPU 5 141454 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1907547966 / 3865470588 (49.3%)
Greatest absolute difference: 0.11111006140708923 at index (50871130, 8, 2) (up to 0.01 allowed)
Greatest relative difference: inf at index (906339, 5, 2) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 9, 4]), dtype=torch.float32)
tensor([[[-0.4483,  0.1483,  0.1010,  0.1465],
         [ 0.2565,  0.0747,  0.2357, -0.1787],
         [-0.0224, -0.2250, -0.1004,  0.0248],
         ...,
         [-0.3764, -0.1845, -0.0698, -0.1987],
         [ 0.0036,  0.0920,  0.0265,  0.1281],
         [-0.4249,  0.3154,  0.1002, -0.3898]],

        [[-0.2323, -0.1729, -0.3783,  0.3011],
         [-0.3499, -0.0042, -0.0802,  0.3245],
         [-0.2135,  0.4347, -0.2084, -0.0155],
         ...,
         [-0.2101,  0.1344,  0.1469, -0.2653],
         [-0.1739,  0.3400, -0.0218,  0.1361],
         [-0.0296, -0.1085,  0.3402,  0.2460]],

        [[-0.0349,  0.1779, -0.0548,  0.3747],
         [-0.1730,  0.3861,  0.3662,  0.4266],
         [ 0.2258,  0.1750, -0.1009,  0.3235],
         ...,
         [ 0.1124, -0.1475,  0.2897,  0.2004],
         [ 0.4408,  0.1091,  0.4795,  0.2206],
         [ 0.4953,  0.1877, -0.3090, -0.0052]],

        ...,

        [[-0.4838, -0.3752, -0.2308, -0.2002],
         [ 0.0510,  0.1163, -0.3642,  0.3955],
         [-0.0150, -0.0171, -0.0021,  0.0680],
         ...,
         [-0.2199, -0.0351, -0.0300, -0.0298],
         [-0.1979, -0.1639,  0.1820,  0.2102],
         [ 0.4010, -0.3490,  0.4032, -0.0817]],

        [[ 0.0013, -0.3647, -0.3244,  0.3115],
         [ 0.4036,  0.2602,  0.3371,  0.3579],
         [ 0.2573,  0.2760,  0.2811,  0.1261],
         ...,
         [ 0.1880,  0.1657, -0.3181, -0.0271],
         [ 0.0623,  0.0617,  0.2696,  0.3222],
         [ 0.2136,  0.0165, -0.1112,  0.0480]],

        [[ 0.0175, -0.0367, -0.3614,  0.4413],
         [-0.0932, -0.4551,  0.0411,  0.0040],
         [-0.0714, -0.2522,  0.1678,  0.0400],
         ...,
         [-0.2685, -0.0879,  0.1220,  0.3314],
         [-0.4297, -0.1942, -0.1243,  0.2014],
         [-0.0230, -0.3359,  0.2271,  0.2228]]])
DESIRED: (shape=torch.Size([107374183, 9, 4]), dtype=torch.float32)
tensor([[[-4.4827e-01,  1.4826e-01,  1.0097e-01,  1.4647e-01],
         [ 2.5286e-01,  6.8301e-02,  2.2843e-01, -1.7570e-01],
         [-2.8479e-02, -2.1126e-01, -8.3829e-02,  2.6358e-02],
         ...,
         [-3.9591e-01, -2.4615e-01, -5.4140e-02, -1.6436e-01],
         [ 6.0885e-02,  1.5737e-01,  3.0715e-02,  1.5180e-01],
         [-4.9482e-01,  3.2463e-01,  1.0824e-01, -4.6131e-01]],

        [[-2.3230e-01, -1.7295e-01, -3.7834e-01,  3.0105e-01],
         [-3.4782e-01,  2.3911e-03, -8.4465e-02,  3.1735e-01],
         [-2.1335e-01,  4.3696e-01, -1.9060e-01, -1.0775e-03],
         ...,
         [-1.8816e-01,  1.7430e-01,  1.6646e-01, -2.5498e-01],
         [-1.8038e-01,  3.4573e-01, -5.2622e-02,  1.8111e-01],
         [-9.7434e-03, -1.6624e-01,  3.9421e-01,  2.4687e-01]],

        [[-3.4865e-02,  1.7787e-01, -5.4778e-02,  3.7475e-01],
         [-1.6401e-01,  3.8531e-01,  3.6025e-01,  4.2478e-01],
         [ 2.0334e-01,  1.5508e-01, -1.1186e-01,  3.2549e-01],
         ...,
         [ 1.7182e-01, -1.8458e-01,  3.3446e-01,  2.5015e-01],
         [ 4.5217e-01,  1.5976e-01,  4.8078e-01,  1.9824e-01],
         [ 4.9885e-01,  1.8304e-01, -4.0792e-01, -2.7038e-02]],

        ...,

        [[-4.8384e-01, -3.7517e-01, -2.3076e-01, -2.0023e-01],
         [ 5.1504e-02,  1.1258e-01, -3.6046e-01,  3.8888e-01],
         [-2.7571e-02, -3.8066e-03,  1.3300e-02,  7.9853e-02],
         ...,
         [-2.3255e-01, -7.0318e-02,  3.9898e-05, -1.6778e-02],
         [-1.8885e-01, -1.6240e-01,  1.9347e-01,  2.3364e-01],
         [ 4.7328e-01, -3.7254e-01,  4.2763e-01, -1.2494e-01]],

        [[ 1.2997e-03, -3.6465e-01, -3.2440e-01,  3.1146e-01],
         [ 4.0320e-01,  2.6342e-01,  3.3839e-01,  3.5626e-01],
         [ 2.4200e-01,  2.5193e-01,  2.6405e-01,  1.1035e-01],
         ...,
         [ 1.9006e-01,  1.9910e-01, -3.2127e-01, -1.7827e-02],
         [ 4.5600e-02,  3.2059e-02,  3.4465e-01,  3.6120e-01],
         [ 2.3724e-01,  1.9276e-02, -1.8019e-01,  2.5627e-03]],

        [[ 1.7483e-02, -3.6712e-02, -3.6145e-01,  4.4127e-01],
         [-9.3363e-02, -4.5223e-01,  4.5288e-02,  2.5048e-03],
         [-6.7109e-02, -2.4999e-01,  1.4988e-01,  5.6267e-02],
         ...,
         [-2.6066e-01, -1.3491e-01,  1.8472e-01,  3.5135e-01],
         [-4.5374e-01, -1.8395e-01, -1.8647e-01,  1.7512e-01],
         [ 3.4700e-02, -3.5656e-01,  2.8878e-01,  2.3296e-01]]])

2025-07-09 13:00:38.440519 GPU 2 140328 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1300366105 / 3865470588 (33.6%)
Greatest absolute difference: 0.055554747581481934 at index (4323060, 8, 2) (up to 0.01 allowed)
Greatest relative difference: 220139840.0 at index (762662, 8, 3) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([107374183, 9, 4]), dtype=torch.float32)
tensor([[[ 0.0009, -0.3308, -0.0162,  0.3373],
         [ 0.2305, -0.2626,  0.0915, -0.2692],
         [ 0.0828,  0.0229, -0.3362, -0.2531],
         ...,
         [-0.0697,  0.1852, -0.1561, -0.1628],
         [-0.0478,  0.3116,  0.1983, -0.4360],
         [-0.3705,  0.2444,  0.0782, -0.1369]],

        [[ 0.3327, -0.2019,  0.1582, -0.0329],
         [ 0.2759, -0.1090, -0.4426, -0.2206],
         [-0.4254, -0.0758, -0.4295,  0.3144],
         ...,
         [-0.0932,  0.1932,  0.4419, -0.2450],
         [ 0.4136,  0.1689,  0.1515,  0.1324],
         [ 0.1937, -0.3093, -0.4469, -0.0985]],

        [[ 0.3750,  0.3144,  0.1436, -0.2875],
         [-0.1800,  0.2159, -0.3555, -0.2453],
         [-0.0038, -0.1951,  0.1651,  0.3781],
         ...,
         [ 0.4087, -0.2627,  0.0924,  0.2461],
         [ 0.2312,  0.3292, -0.2211,  0.2169],
         [ 0.2914,  0.0645,  0.3826, -0.4885]],

        ...,

        [[ 0.1428, -0.2263, -0.4458, -0.3408],
         [-0.4471, -0.0199, -0.2626,  0.1509],
         [-0.1725, -0.0601, -0.0801, -0.3176],
         ...,
         [ 0.0394, -0.0871,  0.3311, -0.1923],
         [ 0.2837,  0.3352, -0.1218, -0.3681],
         [ 0.1246,  0.0711, -0.0533,  0.4550]],

        [[ 0.1150, -0.4286,  0.3916, -0.4149],
         [ 0.4510, -0.1626, -0.0559, -0.1545],
         [ 0.1720,  0.1455, -0.2791,  0.1440],
         ...,
         [ 0.2815,  0.0771,  0.3083, -0.1447],
         [-0.1167,  0.0790,  0.1458,  0.1091],
         [-0.1311, -0.0093,  0.0027,  0.1837]],

        [[ 0.4743,  0.3817,  0.2501, -0.0408],
         [ 0.0733,  0.1754,  0.1015,  0.0793],
         [ 0.2105,  0.1845,  0.0362,  0.2049],
         ...,
         [ 0.1684, -0.0104, -0.2040,  0.1637],
         [ 0.2937,  0.4294, -0.2307,  0.3054],
         [-0.2906,  0.1531,  0.1830, -0.0315]]])
DESIRED: (shape=torch.Size([107374183, 9, 4]), dtype=torch.float32)
tensor([[[ 0.0135, -0.3298, -0.0056,  0.3031],
         [ 0.2312, -0.2458,  0.0638, -0.2660],
         [ 0.0649,  0.0153, -0.3191, -0.2586],
         ...,
         [-0.0798,  0.1658, -0.1428, -0.1639],
         [-0.0446,  0.3139,  0.1757, -0.4225],
         [-0.3532,  0.2477,  0.0886, -0.1558]],

        [[ 0.3351, -0.1957,  0.1248, -0.0475],
         [ 0.2425, -0.1153, -0.4424, -0.1956],
         [-0.4254, -0.0574, -0.4284,  0.3155],
         ...,
         [-0.0916,  0.1775,  0.4397, -0.2380],
         [ 0.3888,  0.1768,  0.1663,  0.1114],
         [ 0.2101, -0.2841, -0.4161, -0.0822]],

        [[ 0.3437,  0.3127,  0.1106, -0.2910],
         [-0.1770,  0.1932, -0.3242, -0.2105],
         [ 0.0087, -0.1879,  0.1498,  0.3661],
         ...,
         [ 0.4056, -0.2715,  0.1047,  0.2434],
         [ 0.2410,  0.3047, -0.2114,  0.2194],
         [ 0.2864,  0.0833,  0.3474, -0.4498]],

        ...,

        [[ 0.1087, -0.2157, -0.4378, -0.3088],
         [-0.4388, -0.0149, -0.2493,  0.1232],
         [-0.1615, -0.0763, -0.0908, -0.3049],
         ...,
         [ 0.0529, -0.0961,  0.3349, -0.1692],
         [ 0.2663,  0.3190, -0.1018, -0.3696],
         [ 0.1364,  0.0885, -0.0604,  0.4095]],

        [[ 0.1359, -0.4157,  0.3699, -0.4031],
         [ 0.4377, -0.1516, -0.0751, -0.1383],
         [ 0.1719,  0.1539, -0.2591,  0.1393],
         ...,
         [ 0.2682,  0.0572,  0.2876, -0.1466],
         [-0.0920,  0.0874,  0.1624,  0.0978],
         [-0.1344, -0.0058,  0.0079,  0.1814]],

        [[ 0.4503,  0.3696,  0.2412, -0.0348],
         [ 0.0839,  0.1789,  0.1051,  0.0835],
         [ 0.2009,  0.1773,  0.0206,  0.2090],
         ...,
         [ 0.1568, -0.0078, -0.1947,  0.1737],
         [ 0.2927,  0.4074, -0.2334,  0.2944],
         [-0.2579,  0.1722,  0.1605, -0.0109]]])

2025-07-09 13:01:03.983423 GPU 3 138280 test begin: paddle.nn.functional.interpolate(Tensor([107374183, 10, 4],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752037563 (unix time) try "date -d @1752037563" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21c28) received by PID 138280 (TID 0x7fa1f69f8740) from PID 138280 ***]


2025-07-09 13:31:11.381165 GPU 5 141454 test begin: paddle.nn.functional.interpolate(Tensor([1073742, 10, 10, 10, 4],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=False, align_mode=1, data_format="NDHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1073742, 10, 10, 10, 4],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=False, align_mode=1, data_format="NDHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 96046632 / 103079232 (93.2%)
Greatest absolute difference: 1.0 at index (12087, 2, 0, 0, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (11, 1, 1, 1, 3) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1073742, 4, 2, 3, 4]), dtype=torch.float16)
tensor([[[[[-8.0017e-02,  4.5093e-01,  9.4666e-02, -2.6562e-01],
           [-1.1029e-01,  1.2323e-01,  1.3496e-02,  1.5356e-01],
           [-2.9932e-01,  2.0752e-01, -5.4901e-02, -1.9207e-03]],

          [[ 4.5483e-01,  5.8479e-03,  4.8511e-01, -1.2646e-01],
           [ 2.1411e-01, -2.8320e-01, -1.4929e-01, -5.8794e-04],
           [ 5.1819e-02, -1.3586e-01,  2.3218e-01, -1.2634e-02]]],


         [[[-2.3071e-02,  2.6184e-02,  2.6929e-01, -8.5571e-02],
           [-4.7546e-02, -1.4844e-01, -1.8848e-01, -9.1919e-02],
           [ 2.0215e-01, -1.6943e-01, -3.9612e-02, -1.6235e-02]],

          [[-3.1323e-01, -3.5950e-02, -2.3413e-01,  6.8848e-02],
           [ 1.7456e-01, -1.7993e-01,  5.7007e-02,  1.0266e-01],
           [ 7.6904e-03, -1.1194e-01, -2.7771e-02,  5.7983e-02]]],


         [[[-2.5903e-01, -2.2375e-01, -4.1821e-01,  2.0093e-01],
           [-2.4963e-02, -1.8982e-01, -2.7124e-01,  1.0175e-01],
           [ 3.5327e-01, -1.8723e-02, -1.7163e-01,  2.0984e-01]],

          [[ 2.0325e-01, -5.3619e-02,  2.7393e-01, -9.7290e-02],
           [ 4.5898e-02, -2.2949e-01,  1.3867e-01, -2.7271e-01],
           [-1.4551e-01,  2.4133e-01, -3.2373e-01,  1.4526e-01]]],


         [[[-2.5635e-01, -3.3740e-01,  4.5380e-02, -3.5522e-01],
           [ 1.0052e-01, -1.4478e-01, -1.7175e-01, -3.3130e-01],
           [-1.3501e-01,  0.0000e+00,  1.9946e-01,  7.9346e-02]],

          [[ 2.8198e-02, -1.2238e-01,  2.0508e-02, -3.1104e-01],
           [ 6.3416e-02,  2.6147e-01,  7.6782e-02, -1.1945e-01],
           [ 1.7615e-01,  1.5808e-01, -3.5645e-02, -2.7612e-01]]]],



        [[[[-3.8330e-01,  2.2205e-01,  3.4790e-01, -1.5674e-01],
           [ 2.2163e-03, -2.7661e-01, -3.7549e-01,  2.3022e-01],
           [ 1.2524e-01,  2.7319e-01,  8.4351e-02, -9.9854e-02]],

          [[-2.7930e-01,  3.6816e-01, -4.0820e-01,  3.6621e-01],
           [ 2.2388e-01, -2.8076e-01,  1.7908e-01,  8.9884e-04],
           [-9.4482e-02,  1.4069e-02, -2.9770e-02,  2.7490e-01]]],


         [[[ 3.9551e-02,  2.8540e-01, -1.4307e-01,  1.0492e-01],
           [ 2.8931e-02,  1.0950e-01, -1.7065e-01, -2.2079e-02],
           [-3.5156e-02,  2.4670e-01, -6.5308e-03,  1.6943e-01]],

          [[ 1.8689e-01,  2.8076e-01, -2.3218e-01,  1.0797e-01],
           [ 1.1578e-01,  9.5337e-02,  9.3628e-02, -1.0242e-01],
           [-9.0790e-03,  1.0791e-01,  9.0393e-02, -9.6436e-02]]],


         [[[ 1.8542e-01,  3.5742e-01,  3.3472e-01,  4.6436e-01],
           [-1.1700e-01,  2.7905e-01,  2.1167e-01,  4.0112e-01],
           [-1.2878e-01, -1.1737e-01,  4.1699e-01, -2.6782e-01]],

          [[ 3.7085e-01, -4.9072e-01, -4.2139e-01,  2.8027e-01],
           [ 2.7637e-01,  1.8958e-01, -1.0272e-01, -2.6855e-02],
           [-2.4255e-01,  1.2634e-01,  7.0496e-02,  1.2384e-01]]],


         [[[-6.6589e-02,  2.2534e-01,  1.7517e-01,  9.3994e-02],
           [-5.5542e-02,  3.6316e-02, -7.2205e-02, -5.9418e-02],
           [ 3.4131e-01, -6.3599e-02, -1.0370e-01, -1.8274e-01]],

          [[ 2.3193e-02, -4.7424e-02,  1.5637e-01, -3.1982e-02],
           [-1.9788e-01, -4.0283e-02, -1.6235e-01,  2.2314e-01],
           [-2.2937e-01,  4.0955e-02,  2.0471e-01,  2.2461e-02]]]],



        [[[[ 9.2529e-02,  2.7527e-02, -2.3163e-02,  3.6602e-03],
           [ 3.3594e-01, -1.9653e-01,  1.6028e-01, -2.6382e-02],
           [ 7.0007e-02,  3.6011e-01, -3.8208e-01,  2.5757e-01]],

          [[-4.3408e-01, -7.3120e-02,  4.6680e-01,  2.9663e-01],
           [-1.8604e-01,  7.8674e-02, -1.1133e-01,  3.2910e-01],
           [ 1.3196e-01,  7.4707e-02, -6.4087e-02, -7.0190e-02]]],


         [[[-1.8005e-02,  7.9651e-02,  1.3379e-01,  1.3086e-01],
           [-4.7913e-02, -1.1792e-01,  1.6138e-01, -6.9397e-02],
           [ 1.5210e-01, -1.1908e-01, -6.0120e-02,  1.2842e-01]],

          [[ 2.7905e-01, -2.1179e-01, -2.3206e-01, -3.2745e-02],
           [-2.0874e-02, -6.1462e-02,  1.4673e-01,  1.5259e-02],
           [-2.1753e-01,  2.2852e-01,  2.3486e-01,  1.8524e-02]]],


         [[[-3.9185e-01,  1.5442e-02, -1.2573e-01,  4.6362e-01],
           [ 1.3379e-01, -8.4961e-02,  4.9902e-01, -2.4011e-01],
           [-2.7075e-01, -4.8364e-01,  3.2593e-02, -3.5645e-01]],

          [[ 1.9312e-01,  4.0088e-01,  1.0034e-01, -8.7219e-02],
           [-1.5656e-02,  2.4463e-01,  1.0266e-01,  3.8605e-02],
           [-2.0142e-01, -5.0140e-02,  3.3569e-01, -1.1884e-01]]],


         [[[-9.5459e-02, -1.7700e-02, -1.5015e-02, -3.4766e-01],
           [-6.1188e-02,  2.1228e-01,  2.6270e-01,  1.5442e-02],
           [-1.4380e-01, -2.3340e-01,  5.1605e-02, -2.6440e-01]],

          [[-1.0175e-01,  4.4043e-01, -1.4490e-01,  2.2729e-01],
           [-1.2494e-01, -1.9775e-01,  5.0934e-02,  2.7271e-01],
           [-7.9346e-03,  1.8152e-01,  2.0129e-01,  1.3037e-01]]]],



        ...,



        [[[[ 3.2153e-01,  1.3635e-01, -4.5508e-01,  4.4824e-01],
           [ 1.3501e-01,  8.2031e-02, -2.1521e-01, -3.3142e-02],
           [ 3.7231e-01,  2.0679e-01,  1.8091e-01, -1.9043e-01]],

          [[ 1.6553e-01, -1.0168e-01,  3.3887e-01, -4.7314e-01],
           [-4.4116e-01, -3.5962e-01,  2.5586e-01, -1.6736e-01],
           [ 1.5247e-01,  3.1250e-01,  1.0785e-01,  1.8799e-01]]],


         [[[-4.0771e-01,  1.3184e-02, -1.8387e-02, -2.1948e-01],
           [ 1.2720e-01, -2.4951e-01,  2.3010e-01,  1.9727e-01],
           [ 4.8035e-02,  2.5439e-01,  3.4180e-03,  1.2805e-01]],

          [[-2.3853e-01,  2.7313e-02, -1.4648e-03, -6.5186e-02],
           [ 2.3035e-01,  1.4197e-01,  3.8513e-02, -8.8440e-02],
           [-3.1592e-01,  6.5613e-02, -8.4778e-02,  5.0964e-02]]],


         [[[ 2.3608e-01, -1.3513e-01, -1.9989e-02, -7.3547e-02],
           [ 2.5146e-01, -2.1240e-01,  8.1726e-02, -1.5784e-01],
           [-7.8003e-02,  1.2439e-01,  7.9956e-02,  3.3057e-01]],

          [[-4.2407e-01, -2.3303e-01, -3.7378e-01, -3.4943e-02],
           [-3.9160e-01,  2.6709e-01,  1.7358e-01, -2.8833e-01],
           [ 1.7773e-01,  2.9541e-01,  1.5343e-02,  2.0844e-02]]],


         [[[ 2.1497e-01,  1.2903e-01,  3.1494e-02, -1.3123e-01],
           [-1.1902e-01,  1.6235e-02, -6.7627e-02,  1.6919e-01],
           [ 3.3356e-02,  1.8127e-02, -1.2622e-01, -1.5369e-01]],

          [[ 4.4434e-01, -1.8066e-02, -1.0699e-01,  1.8994e-01],
           [-1.0632e-01,  1.8164e-01,  4.3427e-02,  1.0339e-01],
           [ 2.5635e-02,  1.9067e-01, -2.0532e-01,  2.4072e-01]]]],



        [[[[-5.4810e-02, -3.3154e-01, -2.8809e-01, -2.8076e-01],
           [-1.0406e-01,  2.0593e-01,  3.4370e-03, -2.8595e-02],
           [-1.3330e-01,  2.4979e-02,  2.3804e-01,  1.0553e-01]],

          [[-3.9307e-01, -4.9609e-01,  4.8438e-01, -2.6221e-01],
           [-2.3022e-03,  8.2825e-02, -3.9160e-01, -1.0284e-01],
           [-9.8145e-02, -3.6816e-01,  2.7563e-01,  1.0754e-01]]],


         [[[ 6.7627e-02,  6.1646e-02,  2.7283e-02,  1.9995e-01],
           [ 1.2769e-01,  9.5367e-03, -1.5572e-02, -2.8003e-01],
           [ 3.3154e-01,  2.7197e-01, -2.4756e-01, -2.2876e-01]],

          [[-1.9238e-01,  1.4612e-01,  1.9727e-01,  2.3926e-01],
           [-1.5015e-02,  2.9236e-02, -3.3386e-02, -1.9897e-01],
           [-1.0474e-01, -1.2219e-01,  1.6199e-01, -9.8145e-02]]],


         [[[-4.8267e-01, -4.2480e-01, -1.6589e-01, -6.5613e-02],
           [ 3.5986e-01, -2.8784e-01, -9.2285e-02,  2.7271e-01],
           [-3.3301e-01,  4.9414e-01,  2.8052e-01, -2.8174e-01]],

          [[ 4.5239e-01, -2.6733e-01,  2.5586e-01, -1.0077e-01],
           [ 6.9504e-03, -3.8971e-02, -2.6831e-01,  2.7222e-01],
           [-2.6074e-01,  8.0505e-02,  7.0068e-02,  2.0618e-01]]],


         [[[ 3.1079e-01, -1.0608e-01,  8.1787e-02, -2.5610e-01],
           [ 1.1182e-01, -2.8992e-02,  1.4514e-01,  1.6357e-02],
           [-1.4832e-01,  1.4319e-01, -8.9233e-02,  5.6610e-02]],

          [[-1.1993e-01, -3.1885e-01,  8.7830e-02, -1.7334e-01],
           [-2.7734e-01, -5.3406e-02, -6.5552e-02, -8.0811e-02],
           [ 1.8738e-01, -6.7444e-02, -2.9907e-01, -2.6538e-01]]]],



        [[[[-3.6084e-01,  9.6130e-02, -4.7632e-01, -2.4243e-01],
           [-2.1936e-01, -1.4526e-01,  4.1455e-01, -1.2535e-02],
           [ 1.6309e-01,  6.9336e-02, -3.1030e-01, -2.6025e-01]],

          [[ 4.6478e-02,  4.1650e-01,  4.8755e-01, -1.6382e-01],
           [-1.3013e-01,  1.8738e-01, -2.6270e-01, -2.6440e-01],
           [ 2.3474e-01, -2.3975e-01,  2.4658e-02, -1.2451e-01]]],


         [[[ 4.1602e-01,  8.5571e-02, -4.1040e-01,  6.8970e-03],
           [ 3.0371e-01, -4.6326e-02, -1.5405e-01,  1.8677e-02],
           [ 9.0088e-02, -3.4088e-02, -7.7698e-02,  8.5632e-02]],

          [[-2.9150e-01, -2.8015e-02,  1.2463e-01, -1.2207e-04],
           [-1.4221e-02, -3.4088e-02,  9.9609e-02,  2.5757e-02],
           [ 2.1240e-01, -2.1179e-02, -6.9763e-02, -3.8483e-02]]],


         [[[ 1.9943e-02,  3.4204e-01,  4.1870e-01,  1.1389e-01],
           [-4.4263e-01, -1.3794e-01,  3.0005e-01,  1.0248e-01],
           [-5.2551e-02,  8.3130e-02, -8.8440e-02,  3.1616e-01]],

          [[ 3.8452e-01, -4.8340e-01,  3.3203e-01,  2.4109e-01],
           [ 1.7236e-01, -1.2891e-01, -3.4521e-01, -1.3130e-02],
           [-1.7627e-01, -7.9895e-02, -3.7354e-01,  1.0101e-01]]],


         [[[-1.8481e-01, -2.4976e-01,  2.9785e-01,  4.7827e-01],
           [-1.8542e-01, -1.2103e-01,  2.3453e-02, -1.6199e-01],
           [-2.1875e-01, -1.8018e-01, -4.6021e-02,  2.2278e-02]],

          [[ 3.0762e-02, -2.3242e-01, -1.7371e-01,  4.6387e-02],
           [-1.3342e-01, -1.1963e-02, -2.5830e-01,  2.7124e-01],
           [ 4.4861e-02,  6.5796e-02,  2.7161e-03, -9.4604e-02]]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([1073742, 4, 2, 3, 4]), dtype=torch.float16)
tensor([[[[[-0.0800,  0.4509,  0.0947, -0.2656],
           [-0.0653, -0.0841,  0.0693, -0.0649],
           [-0.2350, -0.3945, -0.2681,  0.2123]],

          [[ 0.0544,  0.1177,  0.0981,  0.1084],
           [-0.2957, -0.0221,  0.1735,  0.4111],
           [-0.2189, -0.2488, -0.3359,  0.0197]]],


         [[[ 0.0826,  0.2236,  0.4185, -0.4299],
           [-0.1372,  0.2622, -0.3691,  0.1823],
           [-0.0490,  0.0920, -0.4695, -0.3704]],

          [[ 0.4468,  0.1543, -0.1815,  0.1759],
           [ 0.0294,  0.2905,  0.1758, -0.3289],
           [ 0.3086, -0.0291, -0.1875, -0.2612]]],


         [[[ 0.3091, -0.4351,  0.2354,  0.0688],
           [-0.2783,  0.0577,  0.0514, -0.0540],
           [ 0.1324, -0.4253, -0.0107,  0.2944]],

          [[ 0.0409, -0.4636,  0.1204,  0.2590],
           [-0.3044,  0.0128,  0.1993, -0.0088],
           [ 0.4592, -0.1302, -0.3032, -0.1093]]],


         [[[-0.1897, -0.2450,  0.2109,  0.1129],
           [-0.3047,  0.2776,  0.2705,  0.1835],
           [-0.3799,  0.4294,  0.1009, -0.0099]],

          [[-0.0544, -0.0649, -0.4910, -0.3499],
           [ 0.2786,  0.2211,  0.3591, -0.0552],
           [-0.4194, -0.3342, -0.2656, -0.3457]]]],



        [[[[-0.3833,  0.2220,  0.3479, -0.1567],
           [ 0.0789, -0.1512, -0.2167, -0.0573],
           [-0.3105,  0.4878,  0.4231, -0.2058]],

          [[-0.2430, -0.2693, -0.0019, -0.4646],
           [-0.1455,  0.2373, -0.4160, -0.1001],
           [-0.4500,  0.4053,  0.2427, -0.4272]]],


         [[[ 0.0874,  0.1462,  0.0152,  0.3608],
           [-0.0454,  0.0431,  0.3232,  0.4883],
           [ 0.1422, -0.1959,  0.2822,  0.1891]],

          [[ 0.3823, -0.1637,  0.4312, -0.3135],
           [-0.0540,  0.0302, -0.0581, -0.2308],
           [-0.4292,  0.3665,  0.3650,  0.0757]]],


         [[[-0.1492,  0.3672,  0.1669,  0.4797],
           [-0.0346,  0.1666,  0.2744,  0.2239],
           [-0.2896,  0.3887,  0.4094, -0.3064]],

          [[-0.2661,  0.1470, -0.2629, -0.0736],
           [-0.2505,  0.1495,  0.2961, -0.3860],
           [-0.3381, -0.0338, -0.2861,  0.3828]]],


         [[[-0.2620,  0.3916, -0.0213, -0.0751],
           [-0.2050,  0.2615,  0.3408,  0.1610],
           [-0.1035, -0.0372, -0.4709, -0.2517]],

          [[ 0.1348,  0.0845, -0.1620, -0.2908],
           [ 0.0525, -0.3420,  0.3799,  0.1442],
           [-0.1036,  0.2903, -0.4968, -0.0118]]]],



        [[[[ 0.0925,  0.0275, -0.0232,  0.0037],
           [-0.0016, -0.2937, -0.3818, -0.4077],
           [-0.2644, -0.4341, -0.4578, -0.3586]],

          [[ 0.3325, -0.4124, -0.2354,  0.4202],
           [ 0.2388,  0.1465,  0.3159,  0.1515],
           [ 0.1938,  0.1405, -0.4268,  0.4358]]],


         [[[-0.2213,  0.1769,  0.4082,  0.4597],
           [ 0.1447, -0.0099,  0.2219, -0.1602],
           [ 0.4031,  0.1652,  0.1578, -0.0460]],

          [[-0.4155, -0.0208,  0.4080,  0.3774],
           [ 0.0098,  0.0142,  0.1278, -0.1161],
           [ 0.2415,  0.2410, -0.0171,  0.3738]]],


         [[[ 0.2257, -0.4316,  0.4819, -0.2732],
           [ 0.1013, -0.0708,  0.0099,  0.1815],
           [-0.2935, -0.4702, -0.0787, -0.1310]],

          [[-0.3923,  0.4338, -0.1816, -0.2983],
           [-0.3921,  0.1884, -0.2886,  0.1606],
           [-0.3818, -0.2668, -0.3765, -0.0473]]],


         [[[ 0.2605, -0.4268, -0.3220,  0.2434],
           [-0.4558, -0.0448, -0.2139, -0.0385],
           [ 0.3945, -0.4321,  0.4802,  0.3667]],

          [[ 0.0289,  0.4900, -0.4058, -0.4595],
           [-0.0328,  0.0372,  0.2194,  0.1401],
           [-0.1740,  0.2708, -0.4436, -0.2415]]]],



        ...,



        [[[[ 0.3215,  0.1364, -0.4551,  0.4482],
           [-0.0641,  0.2739, -0.1891, -0.1554],
           [-0.3303,  0.2749,  0.1240,  0.4946]],

          [[ 0.1000,  0.2437, -0.4246, -0.3777],
           [ 0.0720, -0.0846,  0.2969, -0.0701],
           [-0.1213, -0.2296, -0.3062,  0.4468]]],


         [[[-0.4893,  0.3479,  0.0059,  0.0102],
           [ 0.1316,  0.1135, -0.2328,  0.2351],
           [ 0.2920, -0.0635, -0.3958,  0.4033]],

          [[ 0.1405,  0.2075,  0.4131,  0.1158],
           [-0.2559,  0.2671,  0.0327,  0.0950],
           [ 0.0202, -0.4390, -0.0119, -0.2727]]],


         [[[-0.4250,  0.2157, -0.1691, -0.1219],
           [ 0.1655,  0.0939, -0.4639,  0.3052],
           [-0.0820, -0.3379,  0.3472,  0.3213]],

          [[-0.0233, -0.0787, -0.4241, -0.3215],
           [ 0.1230, -0.3188, -0.0878, -0.2578],
           [ 0.3264, -0.0158, -0.1870, -0.3865]]],


         [[[-0.1492,  0.2861, -0.0290,  0.1908],
           [ 0.0903, -0.2803, -0.0020,  0.3726],
           [-0.2988, -0.1432, -0.1829, -0.2729]],

          [[-0.4690, -0.3271,  0.4429,  0.4539],
           [-0.1406,  0.1777, -0.0206, -0.0462],
           [-0.1417,  0.3201, -0.2671,  0.4753]]]],



        [[[[-0.0548, -0.3315, -0.2881, -0.2808],
           [-0.3726, -0.1559,  0.0659,  0.0914],
           [ 0.3401,  0.0898, -0.1903,  0.1936]],

          [[-0.1230,  0.0192, -0.2336, -0.4958],
           [ 0.3921, -0.1870,  0.2812, -0.1270],
           [-0.3162, -0.0915,  0.1135, -0.2964]]],


         [[[-0.1076, -0.0142,  0.2048,  0.2898],
           [-0.1398,  0.2349, -0.2065,  0.0887],
           [ 0.4207, -0.1225, -0.2578,  0.1117]],

          [[ 0.3679, -0.2527, -0.4324, -0.1595],
           [ 0.2081, -0.1389,  0.0539, -0.1179],
           [-0.1870, -0.3101, -0.3147,  0.1208]]],


         [[[-0.0919,  0.2583, -0.2776, -0.1494],
           [-0.3091,  0.0493, -0.1627, -0.3574],
           [-0.4622, -0.2308, -0.2371,  0.3203]],

          [[ 0.2209,  0.3933,  0.2252, -0.4089],
           [-0.1151,  0.1277, -0.0646,  0.1277],
           [-0.2983, -0.2837, -0.3267,  0.4343]]],


         [[[-0.4258, -0.3308, -0.3044, -0.0740],
           [-0.0154,  0.0531, -0.0305, -0.2754],
           [ 0.4009, -0.0927, -0.4500,  0.0284]],

          [[-0.3247,  0.1288,  0.4331, -0.1139],
           [ 0.4512, -0.0213,  0.1295, -0.1584],
           [ 0.0049, -0.1084,  0.3638, -0.3945]]]],



        [[[[-0.3608,  0.0961, -0.4763, -0.2424],
           [ 0.0777, -0.2539,  0.3379,  0.3857],
           [-0.2937,  0.3450, -0.1678, -0.1957]],

          [[-0.1078,  0.0164,  0.1969,  0.3469],
           [-0.1602,  0.0345, -0.1895,  0.1035],
           [-0.2522,  0.1570, -0.4485, -0.4475]]],


         [[[ 0.4861,  0.3489, -0.3479, -0.2321],
           [ 0.3979, -0.0332, -0.1702,  0.1439],
           [-0.0118,  0.0379, -0.4734,  0.3364]],

          [[-0.1625,  0.0361, -0.4951,  0.1167],
           [ 0.0408, -0.0731,  0.0710, -0.1794],
           [-0.1995, -0.0392,  0.2903,  0.0701]]],


         [[[-0.2435,  0.4521, -0.0207,  0.2917],
           [-0.3296,  0.1072,  0.2544,  0.1270],
           [-0.4207, -0.0620, -0.0162,  0.3210]],

          [[-0.3972,  0.1964,  0.4622, -0.3391],
           [ 0.0067, -0.1754,  0.3364,  0.0017],
           [-0.2893,  0.0229, -0.2003,  0.0845]]],


         [[[-0.1450, -0.4214, -0.4626, -0.4062],
           [-0.2485,  0.1692,  0.0138, -0.1497],
           [-0.1018,  0.2773,  0.2686, -0.2202]],

          [[ 0.3577,  0.2047,  0.4197, -0.4685],
           [-0.0412,  0.0187,  0.2380,  0.1328],
           [-0.0953, -0.4617,  0.0408, -0.1769]]]]], dtype=torch.float16)

2025-07-09 13:31:30.579653 GPU 3 148070 test begin: paddle.nn.functional.interpolate(Tensor([1073742, 10, 10, 10, 4],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=True, align_mode=0, data_format="NDHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([1073742, 10, 10, 10, 4],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=True, align_mode=0, data_format="NDHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 100357136 / 103079232 (97.4%)
Greatest absolute difference: 0.9833984375 at index (362260, 1, 1, 0, 2) (up to 0.01 allowed)
Greatest relative difference: inf at index (306, 3, 0, 2, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1073742, 4, 2, 3, 4]), dtype=torch.float16)
tensor([[[[[-0.3198,  0.1001,  0.2783, -0.3545],
           [-0.1813,  0.0468, -0.0706, -0.1042],
           [ 0.3552,  0.3552,  0.2354, -0.2617]],

          [[-0.4377,  0.4346,  0.4622, -0.2627],
           [-0.1584, -0.1343,  0.0171, -0.3804],
           [ 0.0605, -0.1541, -0.3320, -0.0265]]],


         [[[ 0.3596, -0.4260, -0.1952,  0.3281],
           [-0.0789, -0.2620,  0.2087, -0.4624],
           [ 0.2006, -0.4478,  0.3394,  0.1235]],

          [[-0.1461, -0.0784,  0.4563,  0.1846],
           [-0.2322,  0.0526,  0.0819, -0.0929],
           [ 0.3203,  0.3738, -0.0111,  0.2258]]],


         [[[ 0.1527,  0.1317,  0.0998, -0.0613],
           [-0.0714, -0.4272,  0.0271, -0.0313],
           [ 0.0393, -0.2416, -0.1230, -0.3208]],

          [[-0.0488,  0.3447,  0.2399,  0.1246],
           [-0.1070, -0.0101, -0.0397, -0.0202],
           [-0.2158, -0.3325,  0.0258,  0.2874]]],


         [[[ 0.1208,  0.4326,  0.1549, -0.3525],
           [ 0.1591, -0.0765,  0.0316,  0.0226],
           [ 0.0174, -0.1251,  0.4280,  0.0798]],

          [[ 0.0631,  0.3032, -0.0917,  0.4976],
           [ 0.2358,  0.1237,  0.2949, -0.3679],
           [ 0.0353, -0.0983,  0.4690,  0.3586]]]],



        [[[[ 0.3733, -0.2939, -0.3157,  0.0515],
           [ 0.2717,  0.2009, -0.0710, -0.1310],
           [ 0.1616, -0.0977, -0.0599,  0.4380]],

          [[-0.3125,  0.4854,  0.1903, -0.4075],
           [-0.2134, -0.2900, -0.1550, -0.3325],
           [ 0.3062,  0.4915, -0.2949, -0.1937]]],


         [[[-0.3843,  0.3574,  0.0031, -0.4500],
           [ 0.1302, -0.0618,  0.0541, -0.2896],
           [ 0.1827,  0.4951, -0.2717,  0.0029]],

          [[ 0.3315, -0.4751, -0.1986, -0.2534],
           [-0.0151,  0.2146,  0.2637, -0.0125],
           [-0.3379, -0.2236,  0.1060, -0.4587]]],


         [[[-0.3643,  0.4426,  0.0207, -0.3464],
           [ 0.2273,  0.0264,  0.0792, -0.1703],
           [-0.1309,  0.0543,  0.3545,  0.0240]],

          [[ 0.2104, -0.2401,  0.2361,  0.0278],
           [-0.3391,  0.1102, -0.1235, -0.2417],
           [-0.1008,  0.0218,  0.3740, -0.3938]]],


         [[[ 0.1670,  0.3679,  0.3000, -0.2891],
           [-0.0500,  0.0559,  0.1470, -0.4934],
           [ 0.2664,  0.3879, -0.3057,  0.2463]],

          [[-0.2291, -0.2246, -0.3777, -0.4812],
           [ 0.3162, -0.4727, -0.1812, -0.0955],
           [ 0.0005, -0.2197,  0.2510,  0.4734]]]],



        [[[[-0.0692, -0.2708, -0.2334, -0.3271],
           [-0.3086,  0.2465,  0.0237,  0.2269],
           [ 0.4307,  0.1437,  0.2629, -0.2764]],

          [[ 0.0814,  0.2944, -0.2100,  0.4619],
           [-0.0585, -0.1790,  0.1109, -0.1014],
           [ 0.2871, -0.1588, -0.2209, -0.3337]]],


         [[[-0.2030,  0.0812, -0.2844,  0.1771],
           [-0.2913, -0.0389,  0.1241, -0.2030],
           [-0.3640, -0.3684,  0.0886, -0.1389]],

          [[-0.4727, -0.3889,  0.1350,  0.3718],
           [ 0.4341,  0.0975, -0.2681,  0.2142],
           [ 0.4844,  0.1050,  0.1351,  0.0155]]],


         [[[ 0.1145,  0.4258,  0.4087,  0.0265],
           [ 0.1196,  0.1799,  0.1105,  0.0605],
           [ 0.3496,  0.2998,  0.3271,  0.4568]],

          [[ 0.4448,  0.4314,  0.2742,  0.1239],
           [ 0.1903,  0.0235, -0.0820,  0.0015],
           [-0.0566, -0.3318, -0.3416, -0.4712]]],


         [[[-0.2893, -0.1787, -0.4841,  0.0537],
           [ 0.1003, -0.1714, -0.1301,  0.1119],
           [ 0.3147,  0.2817, -0.1931,  0.1635]],

          [[-0.0214, -0.4900, -0.2280, -0.3389],
           [ 0.1799, -0.1283,  0.1724, -0.1298],
           [-0.0870, -0.2318,  0.2852, -0.1225]]]],



        ...,



        [[[[ 0.4951, -0.3792,  0.1068,  0.3406],
           [ 0.0782, -0.0453,  0.2294, -0.0245],
           [-0.1077, -0.2201, -0.2957,  0.0473]],

          [[-0.4475, -0.4199, -0.0117,  0.2847],
           [-0.2090,  0.0486,  0.1096,  0.2566],
           [-0.0634,  0.2959,  0.3552,  0.0305]]],


         [[[ 0.2203, -0.2637,  0.0595, -0.4187],
           [ 0.1472, -0.0630, -0.0189,  0.3735],
           [-0.1824, -0.4006,  0.1107, -0.2283]],

          [[-0.0746,  0.1399,  0.0349, -0.3289],
           [-0.0023,  0.2881, -0.1937,  0.0049],
           [ 0.1066,  0.2195, -0.0131,  0.0574]]],


         [[[-0.1826,  0.2262, -0.4563, -0.4236],
           [-0.2671,  0.0229, -0.1283,  0.2620],
           [-0.0851, -0.4465,  0.2512, -0.1761]],

          [[-0.2014, -0.1076, -0.1765, -0.1995],
           [-0.0703, -0.2651, -0.1954, -0.2847],
           [-0.0576,  0.1808,  0.1783,  0.1586]]],


         [[[ 0.4253, -0.2050,  0.1036,  0.2469],
           [-0.4160, -0.4282,  0.4585,  0.3779],
           [ 0.4783, -0.3188, -0.0290, -0.1899]],

          [[ 0.2148,  0.0597, -0.4651,  0.3435],
           [-0.1587, -0.0393,  0.0539,  0.3171],
           [ 0.0385, -0.4160,  0.4617, -0.2323]]]],



        [[[[-0.1980, -0.3105, -0.1603,  0.2864],
           [ 0.0149,  0.0698,  0.0391,  0.2976],
           [-0.1835, -0.3782,  0.4578,  0.3015]],

          [[-0.4656, -0.3994,  0.1229, -0.2295],
           [-0.1185, -0.0383, -0.3789,  0.2083],
           [ 0.1138, -0.0044,  0.4321,  0.1768]]],


         [[[-0.3538, -0.3154,  0.1627,  0.0839],
           [ 0.1981,  0.0800, -0.1516, -0.1187],
           [ 0.4600,  0.2654,  0.0042, -0.2705]],

          [[ 0.4805, -0.4795, -0.1086,  0.4702],
           [-0.0876, -0.0249, -0.2173, -0.0370],
           [-0.2690, -0.4819,  0.2067, -0.1990]]],


         [[[ 0.4541,  0.1119,  0.0370,  0.4197],
           [-0.1498, -0.1172,  0.1923,  0.0959],
           [-0.0580, -0.1022,  0.4346, -0.3818]],

          [[ 0.2854, -0.0491, -0.1648,  0.3484],
           [-0.1870,  0.2744,  0.1202,  0.3201],
           [ 0.3467,  0.2422,  0.4495,  0.4453]]],


         [[[ 0.4717, -0.1896, -0.1295,  0.2615],
           [ 0.2454, -0.1377, -0.3145,  0.1166],
           [-0.0872,  0.4392, -0.2484,  0.4175]],

          [[ 0.2439, -0.1631, -0.0021,  0.3054],
           [ 0.3169, -0.2871,  0.0243,  0.3442],
           [ 0.0775,  0.0189, -0.1952, -0.0015]]]],



        [[[[-0.2355, -0.0715, -0.3960,  0.0913],
           [ 0.0438,  0.0436,  0.1755,  0.2549],
           [-0.4094,  0.3865, -0.2100, -0.2493]],

          [[ 0.0676, -0.4658, -0.3999,  0.2346],
           [ 0.3687, -0.3774, -0.1420, -0.3503],
           [ 0.3264, -0.0075, -0.0593, -0.2328]]],


         [[[-0.2874, -0.0317, -0.0193,  0.1064],
           [ 0.0859,  0.0660,  0.3479, -0.0404],
           [-0.0209, -0.2197, -0.3899, -0.1932]],

          [[ 0.2120,  0.1411, -0.0588, -0.1968],
           [-0.1840, -0.2888, -0.2443,  0.0660],
           [ 0.4822, -0.2072, -0.2822, -0.1699]]],


         [[[-0.3831, -0.1387, -0.2561,  0.4749],
           [-0.1698, -0.0553, -0.0209, -0.2666],
           [-0.3579, -0.4336,  0.0474, -0.3147]],

          [[-0.0928,  0.1428,  0.3975, -0.4243],
           [-0.0109,  0.3733, -0.1193,  0.3320],
           [ 0.4390, -0.1689, -0.4463, -0.4487]]],


         [[[-0.2169,  0.2866,  0.0013, -0.2395],
           [-0.2202, -0.0594, -0.0278, -0.2012],
           [ 0.4944,  0.1520, -0.4580,  0.0485]],

          [[ 0.3936,  0.4917,  0.2048, -0.2085],
           [-0.1249,  0.1931,  0.2107,  0.2268],
           [-0.4268, -0.2935,  0.0945,  0.0688]]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([1073742, 4, 2, 3, 4]), dtype=torch.float16)
tensor([[[[[-2.4731e-01, -1.4893e-01, -1.1676e-01,  3.7201e-02],
           [-1.1664e-01,  6.6406e-02, -1.3696e-01,  4.2694e-02],
           [ 2.8979e-01,  3.0225e-01,  1.8542e-01, -4.1943e-01]],

          [[ 1.4488e-02,  1.5601e-01,  1.3806e-01, -1.8970e-01],
           [ 1.0651e-01, -1.4905e-01, -1.8787e-01,  4.2938e-02],
           [-1.3135e-01,  2.9282e-02,  1.2512e-01,  1.1316e-01]]],


         [[[-5.4016e-02,  2.3145e-01, -1.0150e-01,  2.5537e-01],
           [ 1.4038e-01,  7.5928e-02, -9.9670e-02, -1.9165e-01],
           [-1.5552e-01, -1.2152e-01,  1.9116e-01, -2.7313e-02]],

          [[-2.2668e-01,  4.7974e-02, -1.6467e-01,  1.0291e-01],
           [-9.7534e-02,  1.7102e-01,  1.4258e-01, -3.3789e-01],
           [ 2.9224e-01,  1.3281e-01, -1.5625e-01,  2.3987e-02]]],


         [[[-5.8899e-02,  1.6983e-02,  1.1279e-01,  2.0801e-01],
           [-3.4326e-01, -1.5845e-01,  5.1086e-02,  1.8030e-01],
           [-5.9448e-02, -1.3901e-02, -2.6562e-01,  7.7026e-02]],

          [[ 2.0996e-02, -3.8477e-01,  2.7954e-01, -2.0416e-02],
           [ 1.9641e-01,  1.5442e-01, -1.0352e-01,  2.3462e-01],
           [ 6.9946e-02,  5.6732e-02,  1.9211e-02,  3.2471e-01]]],


         [[[ 9.8038e-03,  2.4976e-01,  2.4402e-01,  2.0215e-01],
           [ 1.3013e-01, -1.2042e-01, -1.4502e-01,  2.6306e-02],
           [ 1.5662e-01, -2.6855e-01,  2.0032e-01, -1.2245e-02]],

          [[ 1.2103e-01, -2.0764e-01,  1.7126e-01,  1.8420e-01],
           [ 7.3242e-02, -1.8408e-01,  7.2876e-02,  1.9568e-01],
           [ 3.4729e-02, -1.9617e-01,  1.6028e-01, -1.2903e-01]]]],



        [[[[-2.5391e-01,  4.8706e-02, -2.9712e-01,  2.1558e-01],
           [-1.2305e-01, -6.1188e-03,  1.7120e-02, -2.8223e-01],
           [ 1.0956e-01, -2.4500e-01,  1.6748e-01,  6.2286e-02]],

          [[ 2.8809e-01, -8.6121e-02,  2.4219e-01,  2.4158e-01],
           [-6.8787e-02, -2.7612e-01,  7.6660e-02,  2.7856e-01],
           [-3.6035e-01, -2.5781e-01,  2.3669e-01,  2.3132e-01]]],


         [[[-4.1992e-02,  1.6956e-01,  2.5903e-01, -2.6416e-01],
           [ 1.5637e-01,  1.2183e-01, -1.4697e-01, -1.9482e-01],
           [-2.5488e-01,  1.9836e-01,  1.1761e-01, -2.0007e-01]],

          [[-3.4937e-01, -6.3477e-02, -2.5928e-01,  2.1155e-01],
           [-2.8491e-01,  7.9193e-03,  1.3428e-01, -4.2969e-02],
           [ 1.5710e-01, -2.9907e-01,  2.2873e-02,  7.0343e-03]]],


         [[[ 1.0803e-01, -2.2632e-01, -4.3823e-01, -4.7607e-02],
           [ 8.8989e-02,  1.7444e-01, -6.2073e-02,  3.1738e-01],
           [-6.3354e-02,  1.8774e-01,  8.5083e-02,  3.9551e-02]],

          [[-7.6332e-03,  2.6953e-01, -1.6675e-01,  1.3391e-01],
           [ 6.5918e-02,  2.1118e-01, -1.3831e-01, -1.3412e-02],
           [-2.3022e-01, -4.2694e-02,  1.2830e-01,  2.5610e-01]]],


         [[[-9.1614e-02,  3.5004e-02, -1.6797e-01, -1.1206e-01],
           [ 2.4805e-01, -9.7046e-02, -1.2219e-01, -9.3445e-02],
           [-2.1106e-01,  3.4302e-01, -2.0825e-01,  1.7285e-01]],

          [[ 2.1729e-01,  1.8030e-01, -3.0811e-01,  1.4282e-01],
           [-2.8687e-01,  2.2473e-01, -8.8867e-02,  1.5244e-02],
           [-5.6915e-02, -8.3984e-02, -1.8860e-01,  6.5918e-02]]]],



        [[[[ 7.8857e-02,  1.9482e-01,  2.0654e-01, -4.2236e-02],
           [ 1.3565e-02, -2.6886e-02,  4.3091e-02,  2.8955e-01],
           [-3.6377e-02,  1.4502e-01,  6.2561e-02, -3.3618e-01]],

          [[ 2.2339e-02, -2.6221e-01, -4.9377e-02, -5.2917e-02],
           [ 2.6672e-02,  3.0518e-02,  2.3596e-01, -9.1629e-03],
           [-3.2129e-01, -1.3199e-03,  3.0444e-01, -2.4384e-02]]],


         [[[ 1.2634e-01, -1.1609e-01, -2.8101e-01, -2.1045e-01],
           [-1.6449e-02, -2.2827e-01, -1.9421e-01,  1.7188e-01],
           [ 4.3732e-02,  1.0211e-01, -2.1631e-01, -4.6234e-02]],

          [[-4.5929e-02, -8.3160e-04, -3.4839e-01,  2.0605e-01],
           [-2.1942e-02,  1.6919e-01,  1.7273e-02, -7.5867e-02],
           [-1.1334e-01, -2.9077e-01,  1.6980e-01,  2.9248e-01]]],


         [[[-7.0740e-02,  1.7896e-01, -3.4473e-01,  2.3169e-01],
           [ 4.3945e-02, -2.3767e-01,  1.2671e-01,  1.6211e-01],
           [ 2.9932e-01, -2.0508e-01,  2.9077e-01,  4.4525e-02]],

          [[ 5.2948e-02,  1.7932e-01,  2.9565e-01,  8.9722e-02],
           [ 3.2959e-01,  9.2651e-02, -8.7219e-02, -1.2646e-01],
           [ 3.0225e-01,  2.5195e-01,  1.3757e-01, -1.3049e-01]]],


         [[[ 7.2449e-02, -5.3375e-02, -1.0429e-02,  1.2891e-01],
           [ 1.0742e-01,  4.3411e-03, -2.5317e-01,  1.2891e-01],
           [ 2.2803e-01,  1.4893e-01, -1.8945e-01, -5.9479e-02]],

          [[ 1.3892e-01,  1.6736e-01, -2.5049e-01, -2.0667e-01],
           [-1.9165e-01, -9.6985e-02, -1.0187e-01, -2.0630e-02],
           [ 7.9651e-02,  1.9394e-02, -1.4105e-03,  1.1652e-01]]]],



        ...,



        [[[[-2.4734e-02, -1.4441e-01,  2.0447e-01,  2.5342e-01],
           [-1.3208e-01,  8.2825e-02, -2.5024e-01,  6.8115e-02],
           [-1.5295e-01,  2.3575e-02,  3.1030e-01, -2.1252e-01]],

          [[ 2.5439e-01,  2.3242e-01,  3.0411e-02, -3.7866e-01],
           [-1.2964e-01,  2.1130e-01, -1.2866e-01, -4.3884e-02],
           [-2.5024e-01,  1.7615e-01,  2.8516e-01,  8.8745e-02]]],


         [[[ 1.8091e-01, -2.0166e-01,  2.9395e-01,  2.9370e-01],
           [ 9.4055e-02, -4.1962e-05, -4.4342e-02, -1.0626e-01],
           [ 2.3914e-01, -1.4771e-01,  1.8994e-01, -3.6792e-01]],

          [[ 1.9092e-01,  9.6069e-02,  2.1729e-01, -2.6782e-01],
           [-6.2561e-02,  3.5522e-02, -1.3220e-01,  2.0959e-01],
           [ 1.0083e-01, -1.4746e-01,  2.2937e-01,  3.5596e-01]]],


         [[[-1.3870e-02,  2.6855e-01, -2.5818e-02, -2.3022e-01],
           [-1.3037e-01,  1.6919e-01,  2.1057e-03,  1.1316e-01],
           [-2.1545e-01, -3.4961e-01,  3.4332e-03, -6.3599e-02]],

          [[ 4.2755e-02,  2.8534e-02, -2.1777e-01,  4.4983e-02],
           [-1.1493e-01,  1.8579e-01, -1.8091e-01, -1.9080e-01],
           [-2.3901e-01,  3.6926e-02, -2.6392e-01, -6.6589e-02]]],


         [[[-6.7810e-02, -1.1603e-01, -3.6716e-03, -1.4661e-01],
           [-1.1237e-01, -5.2094e-02,  1.2671e-01,  9.6069e-02],
           [-4.7638e-02,  1.5588e-01, -1.3293e-01,  1.5149e-01]],

          [[ 2.6416e-01,  2.7783e-01,  1.2805e-01,  1.7664e-01],
           [-2.0203e-02, -2.2522e-01,  2.5366e-01, -5.0323e-02],
           [-2.0508e-01,  1.5259e-01, -4.1504e-02,  1.5625e-01]]]],



        [[[[-2.0984e-01, -1.0889e-01, -3.7866e-01, -4.1357e-01],
           [-1.3831e-01,  7.3669e-02,  1.7041e-01, -1.3452e-01],
           [-2.2266e-01,  4.8401e-02, -3.8422e-02, -2.2412e-01]],

          [[ 6.0852e-02, -7.3120e-02,  1.5320e-02, -1.1591e-01],
           [ 8.7341e-02,  9.2529e-02, -1.3452e-01,  3.3142e-02],
           [ 3.3984e-01,  1.4014e-01,  1.2683e-01,  8.5632e-02]]],


         [[[ 3.3936e-01, -1.9873e-01,  1.4294e-01,  7.5623e-02],
           [-1.6199e-01,  3.9380e-01, -5.1178e-02, -1.8079e-01],
           [-2.1741e-01,  2.4048e-01,  2.1985e-01,  1.5332e-01]],

          [[ 2.8833e-01,  1.0938e-01,  5.1636e-02, -2.6562e-01],
           [-6.4209e-02, -9.7168e-02, -2.3413e-01,  3.0029e-01],
           [ 3.6792e-01,  1.0034e-01,  5.5023e-02,  8.9355e-02]]],


         [[[ 2.4268e-01, -1.8692e-02, -3.3618e-01, -3.3032e-01],
           [-2.3254e-02,  1.3342e-01,  1.2067e-01,  2.7435e-02],
           [ 1.0754e-01,  4.7546e-02,  3.7085e-01, -9.5093e-02]],

          [[ 2.0325e-02,  4.9225e-02, -3.0005e-01, -3.0273e-01],
           [-1.0046e-01, -7.4097e-02,  7.6721e-02, -4.1534e-02],
           [-1.6602e-01,  1.8652e-01, -1.4478e-01,  4.7394e-02]]],


         [[[-1.4111e-01,  1.4502e-01,  1.0748e-01, -1.7834e-01],
           [-1.1426e-01,  2.6953e-01,  1.6150e-01,  1.5686e-02],
           [-2.4878e-01,  1.1133e-01,  2.8589e-01,  2.3999e-01]],

          [[-1.8921e-01, -1.2115e-01, -2.4719e-02,  2.3071e-02],
           [ 8.7280e-02,  9.2407e-02,  1.3153e-02,  1.0254e-01],
           [ 2.8638e-01, -6.9031e-02,  2.2791e-01,  2.6657e-02]]]],



        [[[[-2.2058e-01,  1.5320e-01,  2.4585e-01,  4.2915e-03],
           [ 2.1460e-01,  1.1792e-01,  1.0510e-01, -3.4961e-01],
           [ 2.1777e-01,  6.9214e-02,  2.0459e-01,  2.6270e-01]],

          [[ 1.8372e-01, -3.9551e-01,  1.6479e-01,  3.1226e-01],
           [-2.1591e-02, -1.5820e-01,  2.0615e-02, -2.0703e-01],
           [ 2.8101e-01,  5.7831e-02, -3.1763e-01,  4.0283e-02]]],


         [[[ 7.4097e-02,  3.6255e-01, -1.2708e-01,  2.0801e-01],
           [-9.1553e-03,  1.9604e-01, -1.3074e-01, -2.6685e-01],
           [-2.2571e-01,  1.9226e-01,  1.3513e-01,  2.8152e-02]],

          [[ 1.9055e-01,  3.7479e-03,  8.3374e-02,  1.4282e-01],
           [-1.2457e-01, -2.7563e-01, -1.6223e-01, -2.5928e-01],
           [ 3.3325e-01, -1.6650e-01, -2.7051e-01, -1.3904e-01]]],


         [[[-2.3035e-01, -3.8257e-01, -1.0461e-01, -3.8135e-01],
           [-7.5623e-02,  1.4648e-01, -1.7700e-01,  1.2886e-02],
           [ 3.6133e-02,  9.7900e-02,  1.9568e-01,  2.6294e-01]],

          [[ 1.2720e-01, -5.0140e-02, -3.2544e-01,  3.0005e-01],
           [ 1.0480e-01,  1.2720e-01,  1.6739e-02,  3.3081e-02],
           [ 2.0459e-01, -3.3722e-02, -1.5479e-01, -1.3623e-01]]],


         [[[-8.8684e-02,  2.6147e-01, -2.0679e-01, -3.6987e-02],
           [-1.2573e-01,  1.1646e-01, -2.7051e-01, -1.5710e-01],
           [-1.3782e-01,  5.4077e-02,  7.3792e-02,  1.5088e-01]],

          [[-1.6602e-01,  1.7810e-01, -3.7781e-02, -3.2397e-01],
           [-5.2460e-02,  8.7769e-02,  2.2812e-02, -1.4197e-01],
           [-1.8750e-01,  1.5186e-01,  2.4329e-01, -7.0557e-02]]]]], dtype=torch.float16)

2025-07-09 13:32:14.896562 GPU 7 143189 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,0.6,], mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,0.6,], mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 744698024 / 821412504 (90.7%)
Greatest absolute difference: 0.8835244178771973 at index (9186068, 1, 5, 5) (up to 0.01 allowed)
Greatest relative difference: 298352096.0 at index (10300379, 0, 0, 5) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 6, 6]), dtype=torch.float32)
tensor([[[[ 4.0299e-01, -3.0428e-01,  1.7201e-01, -8.2920e-02, -3.2999e-01,  3.3218e-01],
          [ 1.9187e-01, -7.4915e-02,  2.1645e-01,  3.5854e-01,  2.5413e-01,  1.7046e-01],
          [-1.1895e-02,  1.0236e-01, -1.3623e-01,  2.7975e-01,  3.4763e-02,  9.9415e-02],
          [-4.1172e-01,  8.9798e-03,  2.5476e-01, -9.6956e-02,  1.8782e-02, -1.3078e-01],
          [ 2.7120e-01, -5.7708e-02, -2.0912e-01,  3.8381e-01, -1.2255e-01,  1.5819e-01],
          [-1.3700e-01, -1.1418e-01, -1.1101e-01, -9.8099e-02, -9.2154e-02, -3.5233e-02]],

         [[-4.3098e-02, -3.0171e-01, -4.5294e-01, -1.6882e-01, -1.8904e-01,  2.3169e-02],
          [ 1.8910e-01, -1.9327e-01, -6.1610e-02, -1.8455e-01, -1.9757e-02,  1.4089e-01],
          [-1.0909e-01, -3.2089e-02, -1.1739e-01,  3.5063e-01,  1.7400e-01,  9.5437e-02],
          [-3.6241e-02, -3.2587e-01,  1.1562e-01, -1.6069e-01, -1.5203e-01, -2.7752e-01],
          [-1.0006e-01, -2.2980e-01,  1.8989e-01, -3.1918e-02, -1.0490e-01,  8.6389e-02],
          [ 2.1041e-01,  6.1705e-02,  1.4674e-01, -9.4330e-02, -1.2304e-01,  8.7054e-02]]],


        [[[-4.1097e-01,  1.4641e-01,  3.8313e-01, -4.1519e-01, -3.0301e-01,  4.2909e-01],
          [-1.1006e-01,  7.7240e-02,  1.9620e-01,  1.6533e-01, -2.2018e-01, -3.9655e-01],
          [ 4.2258e-01, -5.9586e-02, -5.9921e-02,  1.4915e-02, -1.7667e-02, -1.9960e-02],
          [-3.1729e-02, -8.3353e-02, -2.8000e-01,  1.0901e-01,  1.0296e-01, -6.0640e-02],
          [ 1.6293e-01,  5.9118e-02,  3.2077e-01,  6.0431e-02, -2.8598e-01,  3.0556e-03],
          [ 2.7892e-01,  1.0849e-01,  7.7351e-02, -8.2323e-02, -9.4026e-02,  7.0349e-02]],

         [[-1.6670e-01,  2.7394e-01, -1.0636e-01,  9.2188e-02, -2.2807e-01,  7.8301e-02],
          [-1.4667e-01,  3.1863e-01, -9.4959e-02, -3.1659e-01,  6.6730e-02,  2.1464e-02],
          [-1.0865e-01, -7.9274e-02, -4.1376e-02, -2.4209e-01,  7.8263e-02,  1.5620e-01],
          [-4.3253e-01, -1.5137e-01,  2.1461e-01,  1.8533e-01, -1.0778e-01,  2.3318e-01],
          [-4.6008e-01, -4.6936e-02, -3.2694e-01,  3.1756e-03,  2.0978e-01, -1.3687e-01],
          [-4.5360e-01, -1.4634e-01,  5.5866e-02,  2.2472e-02, -5.5260e-02, -1.3076e-01]]],


        [[[ 2.6778e-01, -2.1101e-01,  4.1997e-02, -1.3465e-01,  1.7858e-01,  1.7685e-01],
          [ 1.6027e-01, -6.3821e-02, -6.1248e-02, -1.0067e-02, -1.7250e-01,  2.3100e-01],
          [ 7.8751e-02,  1.5445e-01, -3.1507e-01,  2.2112e-03, -6.1200e-02,  1.0219e-03],
          [-7.4040e-03, -2.3354e-01, -7.6638e-02, -2.6637e-01, -3.2378e-02, -4.6386e-01],
          [-2.0130e-01, -1.7733e-01,  2.5221e-01,  2.1639e-02, -2.0334e-01, -7.0599e-03],
          [-4.3289e-01, -2.3326e-01,  2.8876e-01,  8.1023e-02, -4.9926e-02, -7.5718e-02]],

         [[-1.9970e-01,  3.9465e-02,  1.4673e-01, -8.0409e-02,  2.4536e-01, -1.5342e-01],
          [ 2.3470e-02,  7.9355e-02, -3.9986e-02, -7.7591e-02, -9.3160e-02,  5.8659e-02],
          [ 4.8835e-02,  1.1767e-01,  1.0231e-01, -2.3318e-01,  1.1620e-01,  8.0664e-02],
          [-1.5987e-01, -2.4417e-01,  2.6256e-01, -1.6817e-01,  5.9204e-02, -1.6429e-02],
          [ 2.7535e-01,  8.0756e-03, -5.2665e-02,  9.1033e-02, -3.8357e-01,  1.6899e-01],
          [ 2.3792e-02, -3.6328e-01, -6.7249e-02,  2.9705e-02,  1.4179e-01,  1.4407e-01]]],


        ...,


        [[[ 4.5028e-02,  2.8358e-01, -1.3808e-01, -2.2149e-01, -3.6106e-01, -1.6951e-01],
          [ 1.3920e-01, -2.7271e-01, -2.1604e-01, -1.0024e-01,  6.3779e-02,  8.1003e-02],
          [-1.1993e-01,  2.1425e-01,  2.2702e-02,  4.6728e-01, -2.7146e-01, -2.1885e-01],
          [ 4.4744e-01,  1.3457e-01,  4.1326e-01, -1.9037e-01,  3.3416e-01, -4.8066e-01],
          [ 4.2111e-02, -5.2874e-02, -8.0781e-03, -1.6729e-01,  3.5512e-01, -2.2420e-01],
          [ 1.5778e-01, -1.9150e-02,  1.4264e-01,  1.7925e-01, -1.2921e-01, -5.6514e-02]],

         [[ 1.9146e-01, -1.0887e-01, -3.9361e-02, -4.3846e-01, -3.5941e-02, -1.0882e-01],
          [ 2.0062e-01,  1.6939e-01,  1.0662e-01,  4.4000e-02, -2.9970e-01, -1.5058e-01],
          [ 2.1358e-02,  8.5007e-02,  8.8343e-02, -1.4138e-01, -2.8243e-02, -1.8150e-01],
          [-1.1255e-01,  1.2667e-01, -1.2120e-02,  1.0677e-01,  3.4327e-02,  2.1746e-02],
          [ 3.5690e-01, -1.9489e-01, -2.0268e-02,  4.5368e-02, -2.9814e-01, -1.0321e-01],
          [ 7.3248e-02, -1.4198e-01, -3.7149e-03, -1.3655e-01, -2.6355e-01, -1.5837e-01]]],


        [[[ 3.5965e-02,  2.3444e-01, -2.4180e-01,  1.6344e-02,  2.1431e-01, -1.1169e-01],
          [ 1.2133e-01, -2.5150e-02,  1.7485e-01,  4.4779e-01,  8.7996e-02,  1.4491e-01],
          [-1.0335e-01, -1.4316e-01,  3.6902e-01,  4.3713e-01, -1.7456e-01,  9.2030e-02],
          [ 4.1875e-01,  3.9515e-02, -4.3253e-04, -1.7981e-01,  8.2776e-02, -3.6238e-01],
          [ 1.9764e-01, -1.4838e-01,  1.1359e-01, -1.7974e-01,  3.2098e-02, -1.2828e-01],
          [ 3.0362e-03,  1.2405e-02, -9.7372e-02,  1.2324e-01,  7.5180e-02, -2.4396e-01]],

         [[-6.8038e-02, -1.8525e-01, -7.0564e-02, -3.8802e-01, -1.7709e-01,  2.2383e-01],
          [-1.2320e-01, -2.6154e-02, -2.7850e-01,  9.4861e-02,  1.4070e-01, -1.4215e-01],
          [-1.2318e-01,  2.4974e-01,  1.4844e-01, -3.2449e-01,  1.0177e-02,  4.2242e-02],
          [-1.2656e-01, -1.8580e-01,  1.2077e-01, -2.2856e-01, -3.4137e-03, -1.5136e-01],
          [-2.8054e-01, -1.3913e-01, -4.7972e-02,  3.2622e-04, -5.0440e-02, -8.3575e-02],
          [ 5.5850e-02, -1.5241e-01,  1.3137e-02,  6.9155e-02,  6.2650e-02, -7.5606e-02]]],


        [[[ 2.4559e-01, -4.2220e-02, -4.4092e-01,  4.0769e-01,  2.1765e-01,  1.1378e-01],
          [-3.7617e-01,  1.9193e-01,  1.5856e-01,  5.0337e-02, -3.2390e-01, -9.3530e-03],
          [ 6.9962e-04,  8.9678e-02,  2.0311e-01,  3.0699e-01,  8.1512e-02, -8.6025e-02],
          [ 4.4685e-01,  4.2046e-01, -2.1091e-01,  4.0529e-01, -1.2275e-01,  1.5106e-01],
          [-2.9413e-01, -6.8108e-02,  2.3079e-01,  2.5684e-01, -1.9837e-02,  5.7943e-02],
          [-2.8319e-02, -1.9188e-01, -6.2631e-02,  1.0691e-01,  7.4003e-02, -8.6927e-03]],

         [[-3.4033e-01,  1.5724e-01, -3.3073e-01,  1.2188e-01,  1.1617e-01, -1.8896e-01],
          [-8.7875e-02, -1.6358e-01, -1.7288e-01,  2.2791e-01, -1.4774e-01, -8.8865e-02],
          [-2.5545e-01,  2.6036e-01,  2.7698e-02, -1.5792e-01, -1.0118e-01,  3.2240e-01],
          [-1.1195e-01,  1.3629e-01, -2.3017e-01,  1.8671e-01,  3.3155e-01,  1.5436e-01],
          [-9.1729e-02, -9.3201e-02, -2.8617e-02, -1.7783e-01, -4.8152e-02,  1.0500e-01],
          [-2.6309e-01,  1.0558e-01, -1.2028e-01,  1.3979e-01,  9.3970e-03,  4.6380e-02]]]])
DESIRED: (shape=torch.Size([11408507, 2, 6, 6]), dtype=torch.float32)
tensor([[[[ 0.4030, -0.2818, -0.0335, -0.1393, -0.2183,  0.1147],
          [ 0.3118,  0.0439,  0.1376,  0.3178,  0.2987,  0.1504],
          [-0.0685,  0.2512, -0.0292,  0.0823,  0.0861,  0.3439],
          [-0.2409, -0.1419,  0.1838, -0.0378, -0.0610,  0.3131],
          [ 0.3232,  0.0786, -0.1838,  0.0229,  0.0912, -0.1524],
          [-0.4497, -0.3542,  0.0431,  0.2626,  0.0330, -0.3097]],

         [[-0.0431, -0.3545, -0.4180,  0.0491, -0.3737,  0.0341],
          [ 0.1377, -0.1458,  0.0041, -0.2212,  0.0882,  0.1742],
          [-0.2162, -0.1708, -0.1553,  0.2241,  0.1390, -0.0799],
          [ 0.1016, -0.2362, -0.1267,  0.0675, -0.3585,  0.1190],
          [-0.1690, -0.3016,  0.1849, -0.1457, -0.0343,  0.0824],
          [-0.1134,  0.1988,  0.1798,  0.1953,  0.2068, -0.2945]]],


        [[[-0.4110,  0.2590,  0.2954, -0.4389, -0.0823,  0.3723],
          [-0.0665,  0.0799,  0.0611,  0.0379, -0.2999, -0.3030],
          [ 0.3708, -0.0207,  0.0233, -0.1095,  0.1521, -0.3794],
          [-0.1112,  0.0606, -0.0796, -0.0174,  0.0541,  0.1349],
          [ 0.3874,  0.0169,  0.2777, -0.0215, -0.3611, -0.2230],
          [-0.1608, -0.0973, -0.1139,  0.3537,  0.1137,  0.0963]],

         [[-0.1667,  0.3318,  0.0480,  0.0597, -0.2775,  0.2293],
          [-0.1456,  0.3543,  0.1185, -0.2607,  0.0847,  0.2468],
          [ 0.0639, -0.0809, -0.0105, -0.1062,  0.0783, -0.1953],
          [-0.4226, -0.1888,  0.0028,  0.0377, -0.0087,  0.2412],
          [-0.4810, -0.1318, -0.3228,  0.0943,  0.0427,  0.1628],
          [-0.4406, -0.1246,  0.1627,  0.0827,  0.4652, -0.2651]]],


        [[[ 0.2678, -0.2082,  0.1530,  0.0339,  0.1946, -0.4186],
          [ 0.2425, -0.0653,  0.0059, -0.1163, -0.1871,  0.2636],
          [ 0.1273,  0.1058, -0.2480, -0.0035,  0.1194, -0.0549],
          [ 0.0984, -0.0917,  0.0154,  0.0258, -0.2721, -0.0806],
          [-0.4270, -0.2556,  0.2169,  0.0626, -0.2995,  0.1850],
          [-0.4728, -0.2728,  0.4463,  0.1336, -0.2841, -0.4837]],

         [[-0.1997,  0.0619, -0.1035,  0.0944,  0.1486, -0.4281],
          [-0.0361,  0.1213,  0.0936, -0.0392, -0.1455,  0.1881],
          [ 0.0980,  0.0098,  0.0620, -0.0598, -0.0813, -0.1651],
          [-0.0123, -0.2485,  0.0930, -0.0862, -0.0443,  0.3210],
          [ 0.2407, -0.1026,  0.1281, -0.1177, -0.2359,  0.3224],
          [ 0.1331, -0.3338, -0.2675,  0.1120,  0.3184, -0.0717]]],


        ...,


        [[[ 0.0450,  0.3266,  0.0808, -0.3242, -0.3119,  0.1913],
          [ 0.1410, -0.3482, -0.2643, -0.0048, -0.0261, -0.3465],
          [-0.2455,  0.1213, -0.0643,  0.3165, -0.3661, -0.0176],
          [ 0.2755,  0.1408,  0.3879,  0.0976,  0.1935, -0.3028],
          [ 0.0806, -0.0235, -0.1363, -0.0453,  0.2248, -0.3975],
          [ 0.1025, -0.1812, -0.0698, -0.3788,  0.1853, -0.4772]],

         [[ 0.1915, -0.1334, -0.0438, -0.1052, -0.2564,  0.2273],
          [ 0.1577,  0.2410,  0.1818, -0.0540, -0.3719,  0.1923],
          [ 0.1981, -0.0624, -0.1193, -0.1183,  0.1945, -0.4132],
          [ 0.1082, -0.0704, -0.0363, -0.0777,  0.0119, -0.0214],
          [ 0.2784, -0.0647, -0.0542, -0.0231, -0.3670, -0.0319],
          [-0.0382, -0.2791, -0.2353, -0.2429, -0.1516,  0.0210]]],


        [[[ 0.0360,  0.3175, -0.3083,  0.1281,  0.0639,  0.4151],
          [ 0.0731, -0.0265,  0.3247,  0.3268,  0.1827,  0.3865],
          [-0.2089, -0.2249,  0.2824,  0.2723, -0.1312,  0.1688],
          [ 0.3957,  0.0362,  0.0853, -0.2634,  0.0624, -0.3173],
          [ 0.0877, -0.1941,  0.0717, -0.1594,  0.2381, -0.2609],
          [ 0.0597,  0.0617,  0.0826,  0.1680, -0.1140,  0.2322]],

         [[-0.0680, -0.1869,  0.1447, -0.3565, -0.0398,  0.1819],
          [-0.1975, -0.1035, -0.2538,  0.1888,  0.0562,  0.4079],
          [-0.2410,  0.0437,  0.2288, -0.1337, -0.0066,  0.2420],
          [-0.0714, -0.1681,  0.0508, -0.2699, -0.0736, -0.0345],
          [-0.3680, -0.2201, -0.0176,  0.2681, -0.0989, -0.1004],
          [ 0.4361, -0.1378, -0.0331, -0.0510,  0.0487,  0.1662]]],


        [[[ 0.2456, -0.1167, -0.4069,  0.2991,  0.2321,  0.0875],
          [-0.3521,  0.2546,  0.1302, -0.0341, -0.2756, -0.1680],
          [-0.0710,  0.0618,  0.1128,  0.1958,  0.1034,  0.2598],
          [ 0.1540,  0.1405, -0.0815,  0.2013, -0.0762,  0.4019],
          [-0.2187, -0.0758,  0.3399,  0.3469, -0.1527, -0.1987],
          [-0.2864, -0.2965,  0.1607, -0.1199, -0.1746, -0.3144]],

         [[-0.3403,  0.2183, -0.3276,  0.0852,  0.0696,  0.0109],
          [-0.1869, -0.1979, -0.0767,  0.0746, -0.0899, -0.3233],
          [-0.2924,  0.2862, -0.1695, -0.1031, -0.0049,  0.2755],
          [-0.1349,  0.0487,  0.0822,  0.0063,  0.1019,  0.1397],
          [-0.0808, -0.1310, -0.0900,  0.0331,  0.0302,  0.3639],
          [-0.4047,  0.3043,  0.0240,  0.0441, -0.1991,  0.0395]]]])

2025-07-09 13:32:56.478704 GPU 7 143189 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,0.6,], mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,0.6,], mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 736113529 / 821412504 (89.6%)
Greatest absolute difference: 0.5520280599594116 at index (9186068, 1, 5, 5) (up to 0.01 allowed)
Greatest relative difference: 180860048.0 at index (1838104, 1, 2, 5) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 6, 6]), dtype=torch.float32)
tensor([[[[ 0.4030, -0.2818, -0.0335, -0.1393, -0.2183,  0.1147],
          [ 0.3118,  0.0439,  0.1376,  0.3178,  0.2987,  0.1504],
          [-0.0685,  0.2512, -0.0292,  0.0823,  0.0861,  0.3439],
          [-0.2409, -0.1419,  0.1838, -0.0378, -0.0610,  0.3131],
          [ 0.3232,  0.0786, -0.1838,  0.0229,  0.0912, -0.1524],
          [-0.4497, -0.3542,  0.0431,  0.2626,  0.0330, -0.3097]],

         [[-0.0431, -0.3545, -0.4180,  0.0491, -0.3737,  0.0341],
          [ 0.1377, -0.1458,  0.0041, -0.2212,  0.0882,  0.1742],
          [-0.2162, -0.1708, -0.1553,  0.2241,  0.1390, -0.0799],
          [ 0.1016, -0.2362, -0.1267,  0.0675, -0.3585,  0.1190],
          [-0.1690, -0.3016,  0.1849, -0.1457, -0.0343,  0.0824],
          [-0.1134,  0.1988,  0.1798,  0.1953,  0.2068, -0.2945]]],


        [[[-0.4110,  0.2590,  0.2954, -0.4389, -0.0823,  0.3723],
          [-0.0665,  0.0799,  0.0611,  0.0379, -0.2999, -0.3030],
          [ 0.3708, -0.0207,  0.0233, -0.1095,  0.1521, -0.3794],
          [-0.1112,  0.0606, -0.0796, -0.0174,  0.0541,  0.1349],
          [ 0.3874,  0.0169,  0.2777, -0.0215, -0.3611, -0.2230],
          [-0.1608, -0.0973, -0.1139,  0.3537,  0.1137,  0.0963]],

         [[-0.1667,  0.3318,  0.0480,  0.0597, -0.2775,  0.2293],
          [-0.1456,  0.3543,  0.1185, -0.2607,  0.0847,  0.2468],
          [ 0.0639, -0.0809, -0.0105, -0.1062,  0.0783, -0.1953],
          [-0.4226, -0.1888,  0.0028,  0.0377, -0.0087,  0.2412],
          [-0.4810, -0.1318, -0.3228,  0.0943,  0.0427,  0.1628],
          [-0.4406, -0.1246,  0.1627,  0.0827,  0.4652, -0.2651]]],


        [[[ 0.2678, -0.2082,  0.1530,  0.0339,  0.1946, -0.4186],
          [ 0.2425, -0.0653,  0.0059, -0.1163, -0.1871,  0.2636],
          [ 0.1273,  0.1058, -0.2480, -0.0035,  0.1194, -0.0549],
          [ 0.0984, -0.0917,  0.0154,  0.0258, -0.2721, -0.0806],
          [-0.4270, -0.2556,  0.2169,  0.0626, -0.2995,  0.1850],
          [-0.4728, -0.2728,  0.4463,  0.1336, -0.2841, -0.4837]],

         [[-0.1997,  0.0619, -0.1035,  0.0944,  0.1486, -0.4281],
          [-0.0361,  0.1213,  0.0936, -0.0392, -0.1455,  0.1881],
          [ 0.0980,  0.0098,  0.0620, -0.0598, -0.0813, -0.1651],
          [-0.0123, -0.2485,  0.0930, -0.0862, -0.0443,  0.3210],
          [ 0.2407, -0.1026,  0.1281, -0.1177, -0.2359,  0.3224],
          [ 0.1331, -0.3338, -0.2675,  0.1120,  0.3184, -0.0717]]],


        ...,


        [[[ 0.0450,  0.3266,  0.0808, -0.3242, -0.3119,  0.1913],
          [ 0.1410, -0.3482, -0.2643, -0.0048, -0.0261, -0.3465],
          [-0.2455,  0.1213, -0.0643,  0.3165, -0.3661, -0.0176],
          [ 0.2755,  0.1408,  0.3879,  0.0976,  0.1935, -0.3028],
          [ 0.0806, -0.0235, -0.1363, -0.0453,  0.2248, -0.3975],
          [ 0.1025, -0.1812, -0.0698, -0.3788,  0.1853, -0.4772]],

         [[ 0.1915, -0.1334, -0.0438, -0.1052, -0.2564,  0.2273],
          [ 0.1577,  0.2410,  0.1818, -0.0540, -0.3719,  0.1923],
          [ 0.1981, -0.0624, -0.1193, -0.1183,  0.1945, -0.4132],
          [ 0.1082, -0.0704, -0.0363, -0.0777,  0.0119, -0.0214],
          [ 0.2784, -0.0647, -0.0542, -0.0231, -0.3670, -0.0319],
          [-0.0382, -0.2791, -0.2353, -0.2429, -0.1516,  0.0210]]],


        [[[ 0.0360,  0.3175, -0.3083,  0.1281,  0.0639,  0.4151],
          [ 0.0731, -0.0265,  0.3247,  0.3268,  0.1827,  0.3865],
          [-0.2089, -0.2249,  0.2824,  0.2723, -0.1312,  0.1688],
          [ 0.3957,  0.0362,  0.0853, -0.2634,  0.0624, -0.3173],
          [ 0.0877, -0.1941,  0.0717, -0.1594,  0.2381, -0.2609],
          [ 0.0597,  0.0617,  0.0826,  0.1680, -0.1140,  0.2322]],

         [[-0.0680, -0.1869,  0.1447, -0.3565, -0.0398,  0.1819],
          [-0.1975, -0.1035, -0.2538,  0.1888,  0.0562,  0.4079],
          [-0.2410,  0.0437,  0.2288, -0.1337, -0.0066,  0.2420],
          [-0.0714, -0.1681,  0.0508, -0.2699, -0.0736, -0.0345],
          [-0.3680, -0.2201, -0.0176,  0.2681, -0.0989, -0.1004],
          [ 0.4361, -0.1378, -0.0331, -0.0510,  0.0487,  0.1662]]],


        [[[ 0.2456, -0.1167, -0.4069,  0.2991,  0.2321,  0.0875],
          [-0.3521,  0.2546,  0.1302, -0.0341, -0.2756, -0.1680],
          [-0.0710,  0.0618,  0.1128,  0.1958,  0.1034,  0.2598],
          [ 0.1540,  0.1405, -0.0815,  0.2013, -0.0762,  0.4019],
          [-0.2187, -0.0758,  0.3399,  0.3469, -0.1527, -0.1987],
          [-0.2864, -0.2965,  0.1607, -0.1199, -0.1746, -0.3144]],

         [[-0.3403,  0.2183, -0.3276,  0.0852,  0.0696,  0.0109],
          [-0.1869, -0.1979, -0.0767,  0.0746, -0.0899, -0.3233],
          [-0.2924,  0.2862, -0.1695, -0.1031, -0.0049,  0.2755],
          [-0.1349,  0.0487,  0.0822,  0.0063,  0.1019,  0.1397],
          [-0.0808, -0.1310, -0.0900,  0.0331,  0.0302,  0.3639],
          [-0.4047,  0.3043,  0.0240,  0.0441, -0.1991,  0.0395]]]])
DESIRED: (shape=torch.Size([11408507, 2, 6, 6]), dtype=torch.float32)
tensor([[[[-0.0375, -0.3026, -0.1185,  0.0497, -0.2979,  0.1306],
          [ 0.2238,  0.2683,  0.1829,  0.3028,  0.4124,  0.2541],
          [-0.0072,  0.3107,  0.0037,  0.1013,  0.0886,  0.2500],
          [-0.0580, -0.2504,  0.2072, -0.0425, -0.0102,  0.1807],
          [ 0.1062,  0.2254, -0.1537,  0.1588,  0.1039, -0.0989],
          [-0.3208, -0.1782, -0.0567,  0.0589,  0.0575, -0.1464]],

         [[ 0.0514, -0.4159, -0.3320, -0.0298, -0.2876,  0.0215],
          [-0.0634, -0.0435,  0.0772, -0.2422,  0.0882,  0.1828],
          [-0.1275, -0.2774, -0.1651,  0.2138,  0.1747, -0.0561],
          [ 0.0569, -0.3248, -0.1724,  0.0416, -0.3498, -0.0801],
          [-0.2448, -0.4658,  0.2133, -0.1382, -0.0342,  0.2236],
          [ 0.0235,  0.2070,  0.1187,  0.0810,  0.0255, -0.1761]]],


        [[[-0.2990,  0.3478,  0.3413, -0.2610, -0.2202,  0.1262],
          [-0.1083,  0.1310, -0.0681,  0.0512, -0.2763, -0.3278],
          [ 0.2248, -0.0165,  0.0210, -0.0699,  0.1413, -0.1866],
          [-0.0838,  0.0561, -0.1053, -0.0119,  0.0984,  0.1004],
          [ 0.2371, -0.0305,  0.3518,  0.0430, -0.4746, -0.2414],
          [ 0.0538, -0.0135, -0.1403,  0.1825, -0.0825,  0.1938]],

         [[-0.1270,  0.3692,  0.1138,  0.0027, -0.1161,  0.1266],
          [ 0.0671,  0.3724,  0.1691, -0.3111,  0.0601,  0.1684],
          [ 0.0637, -0.0841,  0.0063, -0.1292,  0.0135, -0.0592],
          [-0.2314, -0.2899,  0.0125,  0.0607, -0.0892,  0.2161],
          [-0.1739, -0.2390, -0.4094,  0.1182,  0.3203,  0.0089],
          [-0.2808, -0.2228,  0.1270,  0.1032,  0.1565, -0.0914]]],


        [[[ 0.0173, -0.2289,  0.0264,  0.0904,  0.1946, -0.1132],
          [ 0.2722, -0.0559,  0.0805, -0.2064, -0.4557,  0.3503],
          [ 0.2055,  0.0456, -0.2379,  0.0469,  0.1159,  0.0649],
          [-0.0304, -0.0791,  0.0296, -0.0163, -0.2839, -0.1613],
          [-0.4196, -0.2421,  0.2331,  0.0603, -0.3679, -0.0017],
          [-0.3936, -0.2356,  0.3222,  0.1046, -0.1842, -0.2552]],

         [[-0.0767,  0.0216, -0.1055, -0.0488,  0.1959, -0.0674],
          [ 0.0407,  0.1522,  0.1643,  0.0181, -0.2495,  0.0220],
          [ 0.1336, -0.0613,  0.0302, -0.1183, -0.1302, -0.1146],
          [ 0.0433, -0.3803,  0.1371, -0.0820,  0.0507,  0.1318],
          [ 0.3470, -0.1423,  0.1524, -0.1735, -0.4371,  0.3722],
          [-0.0028, -0.4022, -0.1489,  0.1092,  0.2630, -0.0451]]],


        ...,


        [[[ 0.1116,  0.2190, -0.0250, -0.0815, -0.1260,  0.0355],
          [-0.0619, -0.4212, -0.2594, -0.1387, -0.1895, -0.1874],
          [-0.0948,  0.0619, -0.0901,  0.3529, -0.3684, -0.1463],
          [ 0.1478,  0.2298,  0.3847,  0.0572,  0.3128, -0.3402],
          [-0.0822,  0.0394, -0.2022, -0.1137,  0.4316, -0.3772],
          [ 0.1172, -0.1569, -0.0357, -0.1893,  0.0559, -0.1623]],

         [[ 0.2275, -0.2492, -0.0366, -0.0332, -0.3314, -0.0570],
          [ 0.0981,  0.4248,  0.2359, -0.1051, -0.3713,  0.1169],
          [ 0.1936, -0.1412, -0.1735, -0.0700,  0.2391, -0.2258],
          [ 0.0698, -0.0708, -0.0305, -0.0424,  0.0794, -0.0139],
          [ 0.0667,  0.0234, -0.1002,  0.0064, -0.4779,  0.0008],
          [-0.0961, -0.1739, -0.1466, -0.2186, -0.1989, -0.1004]]],


        [[[ 0.0894,  0.1437, -0.1483,  0.0985,  0.0007,  0.1948],
          [ 0.0569,  0.0110,  0.3850,  0.4185,  0.3571,  0.2714],
          [-0.1411, -0.3090,  0.2611,  0.3125, -0.1945,  0.1783],
          [ 0.2800,  0.0369,  0.0414, -0.2687,  0.1416, -0.2716],
          [-0.0144, -0.2230,  0.2114, -0.1841,  0.4153, -0.2885],
          [ 0.0846, -0.0117, -0.0789,  0.1177, -0.0026, -0.0023]],

         [[-0.0120, -0.0979,  0.0451, -0.2675, -0.1582,  0.1849],
          [-0.1026, -0.2619, -0.2472,  0.2387,  0.2864,  0.1221],
          [-0.0402, -0.1022,  0.2588, -0.1348,  0.0161,  0.1536],
          [-0.0375, -0.2266,  0.0449, -0.2644, -0.0648, -0.0570],
          [-0.3172, -0.2391,  0.0393,  0.2917, -0.0921, -0.1105],
          [ 0.1972, -0.2467, -0.0500,  0.0120,  0.0350,  0.0062]]],


        [[[ 0.0926, -0.2949, -0.2720,  0.1510,  0.0191,  0.0010],
          [-0.0684,  0.3866,  0.1609,  0.0406, -0.3483, -0.0469],
          [ 0.0366,  0.0040,  0.0868,  0.1860,  0.1966,  0.1063],
          [ 0.1914,  0.1944, -0.0851,  0.2286, -0.1235,  0.2690],
          [-0.0438, -0.1736,  0.3926,  0.4066, -0.3773, -0.0584],
          [-0.0468, -0.3723,  0.1970, -0.0398, -0.0670, -0.1408]],

         [[-0.1276,  0.3647, -0.2847,  0.1278, -0.0326, -0.0975],
          [-0.3837, -0.2887, -0.0162,  0.0668, -0.0332, -0.2450],
          [-0.0840,  0.2850, -0.2190, -0.1217, -0.0763,  0.2918],
          [-0.0814,  0.0784,  0.1147,  0.0211,  0.1828,  0.0777],
          [ 0.0422, -0.2527, -0.0572, -0.0064, -0.0032,  0.3283],
          [-0.1709,  0.2011, -0.0266,  0.1122, -0.2037,  0.1240]]]])

2025-07-09 13:33:14.712353 GPU 5 141454 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,1.7999999999999998,], mode="bicubic", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,1.7999999999999998,], mode="bicubic", align_corners=False, align_mode=1, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2162558348 / 2464237512 (87.8%)
Greatest absolute difference: 0.48233744502067566 at index (10062487, 1, 5, 15) (up to 0.01 allowed)
Greatest relative difference: 445630176.0 at index (1992478, 1, 2, 16) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 6, 18]), dtype=torch.float32)
tensor([[[[-0.4239, -0.2272,  0.0467,  ...,  0.0978, -0.1491, -0.3330],
          [-0.3573, -0.0530,  0.3713,  ..., -0.4512, -0.4373, -0.4286],
          [-0.1291, -0.1403, -0.1614,  ..., -0.1486,  0.0478,  0.2151],
          [-0.1025, -0.0267,  0.0658,  ...,  0.2100,  0.1333,  0.0540],
          [ 0.2704,  0.0818, -0.1850,  ...,  0.4176,  0.3244,  0.2320],
          [-0.0674,  0.0991,  0.2970,  ...,  0.2187,  0.1381,  0.0712]],

         [[ 0.1692,  0.2971,  0.4581,  ...,  0.0438, -0.1230, -0.2450],
          [-0.2754, -0.3076, -0.3183,  ...,  0.4969,  0.2503,  0.0331],
          [ 0.1861,  0.1562,  0.1152,  ...,  0.4404,  0.2146,  0.0204],
          [-0.3506, -0.2075, -0.0226,  ..., -0.2504, -0.3020, -0.3387],
          [-0.3084, -0.0040,  0.3834,  ..., -0.0352, -0.1697, -0.2527],
          [ 0.2828,  0.2291,  0.1561,  ..., -0.0844, -0.3165, -0.4742]]],


        [[[ 0.1685,  0.2129,  0.2496,  ..., -0.0368, -0.2988, -0.4715],
          [-0.5091, -0.2386,  0.1269,  ...,  0.3789,  0.1801,  0.0282],
          [ 0.3168,  0.1765, -0.0040,  ...,  0.0604, -0.3426, -0.6556],
          [-0.4494, -0.4229, -0.3552,  ..., -0.2126,  0.2945,  0.6690],
          [-0.5355, -0.3819, -0.1296,  ...,  0.2285, -0.0008, -0.1927],
          [ 0.0790,  0.0915,  0.1130,  ...,  0.0135, -0.2614, -0.4401]],

         [[ 0.0694, -0.0981, -0.3062,  ...,  0.1216,  0.3128,  0.4494],
          [ 0.1588,  0.0988, -0.0010,  ..., -0.4397, -0.0141,  0.3207],
          [ 0.1987,  0.1887,  0.1800,  ...,  0.3738, -0.0210, -0.2995],
          [ 0.1284, -0.0360, -0.2731,  ...,  0.1803,  0.2423,  0.2873],
          [-0.5367, -0.2605,  0.1423,  ..., -0.4243, -0.1975, -0.0045],
          [ 0.2151,  0.0237, -0.2412,  ...,  0.0134, -0.0603, -0.1252]]],


        [[[-0.3518, -0.1504,  0.1139,  ...,  0.0945,  0.0595,  0.0126],
          [ 0.0030, -0.1836, -0.4461,  ..., -0.2580, -0.0045,  0.1735],
          [-0.3417, -0.3338, -0.3080,  ...,  0.2645, -0.0643, -0.2956],
          [-0.0304, -0.0855, -0.1543,  ..., -0.2954, -0.0989,  0.0595],
          [-0.0789, -0.0814, -0.1064,  ...,  0.3506, -0.1234, -0.4533],
          [ 0.1301,  0.1441,  0.1652,  ..., -0.1451,  0.1716,  0.4020]],

         [[-0.1648, -0.2280, -0.3225,  ..., -0.2325, -0.2320, -0.2050],
          [ 0.1135,  0.3025,  0.5110,  ...,  0.4523,  0.4675,  0.4353],
          [-0.2642, -0.2948, -0.3365,  ...,  0.1240, -0.0656, -0.2177],
          [-0.1959, -0.0792,  0.0751,  ..., -0.3349, -0.1330,  0.0108],
          [-0.5845, -0.1626,  0.4191,  ...,  0.2824,  0.2752,  0.2660],
          [-0.3630, -0.1471,  0.1457,  ..., -0.0884,  0.0153,  0.0850]]],


        ...,


        [[[-0.4373, -0.3141, -0.1058,  ..., -0.0293,  0.0942,  0.1948],
          [ 0.5173,  0.2135, -0.2225,  ..., -0.4622, -0.1248,  0.1328],
          [ 0.2123,  0.0252, -0.2329,  ...,  0.2810, -0.0226, -0.2521],
          [-0.2084, -0.2799, -0.3664,  ...,  0.4991,  0.0541, -0.2893],
          [ 0.5118,  0.2472, -0.1247,  ..., -0.3562,  0.1769,  0.5839],
          [ 0.4126,  0.2157, -0.0760,  ..., -0.1758, -0.2828, -0.3538]],

         [[ 0.0752,  0.1299,  0.1701,  ..., -0.0975, -0.2081, -0.2863],
          [-0.1155, -0.0674, -0.0108,  ..., -0.4923, -0.4482, -0.3724],
          [ 0.1015, -0.0697, -0.2922,  ...,  0.1024, -0.0083, -0.0896],
          [ 0.2643,  0.0181, -0.3016,  ...,  0.1134,  0.2488,  0.3355],
          [-0.2080, -0.0488,  0.1418,  ...,  0.2963, -0.0483, -0.3130],
          [-0.3472, -0.2331, -0.0782,  ...,  0.0672, -0.1058, -0.2305]]],


        [[[-0.0395, -0.1242, -0.2059,  ..., -0.3304,  0.0523,  0.3433],
          [ 0.0315, -0.0592, -0.1929,  ...,  0.1833,  0.1426,  0.1222],
          [ 0.0162, -0.0467, -0.1092,  ..., -0.0427, -0.1152, -0.1827],
          [-0.3653, -0.2909, -0.2043,  ...,  0.2270, -0.0885, -0.3019],
          [-0.5281, -0.2482,  0.1374,  ...,  0.2570, -0.0804, -0.3301],
          [ 0.3261,  0.1455, -0.1198,  ..., -0.2184,  0.1910,  0.5029]],

         [[ 0.1580,  0.1438,  0.1203,  ..., -0.1063, -0.1552, -0.2053],
          [-0.2542,  0.0240,  0.3990,  ..., -0.0380, -0.2904, -0.4670],
          [-0.2306, -0.1881, -0.1083,  ..., -0.2498,  0.1005,  0.3333],
          [-0.4429, -0.2086,  0.1120,  ...,  0.1270,  0.0891,  0.0755],
          [-0.1271, -0.2385, -0.3839,  ..., -0.2647,  0.0037,  0.2062],
          [ 0.0575,  0.0208, -0.0197,  ...,  0.1818, -0.0470, -0.2287]]],


        [[[-0.5121, -0.1084,  0.4286,  ...,  0.1790,  0.1690,  0.1476],
          [ 0.5451,  0.2591, -0.1153,  ...,  0.1469, -0.1164, -0.3063],
          [-0.0635, -0.1522, -0.2495,  ..., -0.3324, -0.1675, -0.0428],
          [ 0.3226,  0.1515, -0.0782,  ...,  0.0351,  0.3034,  0.4829],
          [-0.1388, -0.0100,  0.1839,  ..., -0.4282,  0.1428,  0.5806],
          [ 0.0855,  0.0621,  0.0246,  ..., -0.0895,  0.0873,  0.2180]],

         [[-0.0691, -0.0538, -0.0328,  ...,  0.3566, -0.0956, -0.4294],
          [-0.4837, -0.3468, -0.1255,  ...,  0.4582,  0.4165,  0.3659],
          [ 0.0349,  0.1168,  0.2272,  ..., -0.0468,  0.0264,  0.0820],
          [-0.2041, -0.1273, -0.0157,  ...,  0.0266,  0.0128,  0.0152],
          [-0.4636, -0.3449, -0.1949,  ..., -0.3854, -0.1412,  0.0701],
          [ 0.0308,  0.0307,  0.0341,  ...,  0.1924,  0.2572,  0.2980]]]])
DESIRED: (shape=torch.Size([11408507, 2, 6, 18]), dtype=torch.float32)
tensor([[[[-4.4827e-01, -1.2560e-01,  1.6926e-01,  ...,  2.8496e-02, -1.1986e-01, -3.3514e-01],
          [-2.9771e-01,  9.2145e-02,  4.1578e-01,  ..., -3.9121e-01, -3.8178e-01, -3.9045e-01],
          [-1.1213e-01, -1.3881e-01, -1.7455e-01,  ..., -8.2677e-02, -5.8425e-02,  1.1676e-01],
          [-2.1590e-02,  7.5656e-02,  1.0286e-01,  ...,  1.2387e-01,  1.6821e-01,  8.8085e-02],
          [ 1.5414e-01, -4.6830e-02, -2.2677e-01,  ...,  4.3718e-01,  4.1291e-01,  2.6660e-01],
          [-1.3626e-02,  2.2776e-01,  2.8671e-01,  ...,  1.1724e-01,  8.8852e-02, -3.7206e-02]],

         [[ 1.0181e-02,  2.8150e-01,  4.3053e-01,  ...,  3.4958e-01,  1.0267e-01, -2.9594e-01],
          [-1.4844e-01, -1.8885e-01, -1.2738e-01,  ...,  3.0833e-01,  2.8152e-01,  8.3087e-02],
          [ 1.5385e-01,  8.3609e-02,  4.7486e-02,  ...,  3.7507e-01,  2.9698e-01,  3.4554e-02],
          [-3.2860e-01, -1.7401e-01, -7.0167e-02,  ..., -2.1517e-01, -2.6634e-01, -3.2371e-01],
          [-2.3675e-01,  2.1361e-01,  4.8109e-01,  ...,  1.2354e-01, -9.2525e-02, -2.7256e-01],
          [ 4.6795e-01,  2.3180e-01,  7.7568e-03,  ..., -3.0194e-01, -3.7831e-01, -3.9924e-01]]],


        [[[ 4.4702e-01,  3.1537e-01,  1.0025e-01,  ...,  3.2442e-01,  3.2547e-04, -3.4755e-01],
          [-4.8071e-01, -7.2316e-02,  2.2958e-01,  ...,  2.4535e-01,  1.3514e-01,  5.4396e-03],
          [ 2.3781e-01,  8.2642e-02,  2.9098e-03,  ...,  9.6171e-02, -1.5709e-01, -5.8958e-01],
          [-4.0393e-01, -3.9818e-01, -3.0282e-01,  ..., -2.3070e-01,  1.2414e-01,  5.7718e-01],
          [-4.6560e-01, -3.1270e-01, -5.9511e-02,  ...,  1.0834e-01,  1.3438e-03, -2.1432e-01],
          [ 1.4135e-01,  2.4024e-01,  3.6137e-01,  ...,  3.1558e-01, -5.6643e-02, -4.5107e-01]],

         [[ 1.8372e-01, -1.5338e-01, -3.4918e-01,  ...,  2.4573e-01,  3.0660e-01,  3.8072e-01],
          [ 9.1086e-02,  2.7107e-02, -7.1575e-02,  ..., -5.0602e-01, -2.0850e-01,  2.9328e-01],
          [ 1.6218e-01,  1.5951e-01,  1.7520e-01,  ...,  4.6204e-01,  1.5089e-01, -1.9145e-01],
          [ 1.2808e-01, -1.0399e-01, -3.2652e-01,  ...,  1.2292e-01,  1.9968e-01,  3.0929e-01],
          [-4.8777e-01, -1.1633e-01,  2.3537e-01,  ..., -4.1648e-01, -3.2689e-01, -7.8772e-02],
          [ 2.6364e-01, -1.0509e-01, -3.5710e-01,  ...,  1.7735e-01,  6.2044e-02, -1.6397e-01]]],


        [[[-3.0388e-01, -1.2768e-01, -5.6869e-02,  ..., -8.1022e-02, -9.0905e-02, -2.0352e-01],
          [-5.9308e-02, -2.0316e-01, -3.4585e-01,  ..., -2.5669e-01, -7.6268e-04,  2.4225e-01],
          [-3.3776e-01, -3.3951e-01, -2.8066e-01,  ...,  3.3236e-01,  4.4664e-02, -2.5785e-01],
          [-4.4648e-02, -6.8689e-02, -8.0841e-02,  ..., -3.1493e-01, -2.1367e-01, -2.5268e-03],
          [-6.5394e-02, -1.1028e-01, -1.9272e-01,  ...,  5.2674e-01,  1.2742e-01, -3.1985e-01],
          [ 1.6872e-01,  2.3487e-01,  2.6375e-01,  ..., -4.1048e-01, -5.7708e-02,  4.6087e-01]],

         [[-2.0219e-01, -2.6489e-01, -3.4814e-01,  ...,  1.1479e-02, -2.2913e-01, -4.0513e-01],
          [ 1.5542e-01,  3.9821e-01,  4.1742e-01,  ...,  2.8185e-01,  4.4475e-01,  4.4148e-01],
          [-2.8880e-01, -3.4230e-01, -3.7149e-01,  ...,  3.8881e-02, -1.7873e-02, -1.5575e-01],
          [-1.0370e-01,  2.4955e-02,  1.0716e-01,  ..., -3.9046e-01, -2.4584e-01, -6.8843e-02],
          [-4.9953e-01,  7.6383e-02,  5.1997e-01,  ...,  3.3148e-01,  2.9267e-01,  1.9389e-01],
          [-4.0767e-01, -1.5910e-01,  9.7700e-02,  ..., -2.5540e-01,  3.6121e-03,  3.2956e-01]]],


        ...,


        [[[-4.8375e-01, -1.7633e-01,  1.7234e-01,  ..., -2.9356e-01, -5.3858e-02,  3.4968e-01],
          [ 3.8584e-01, -3.0617e-02, -3.8738e-01,  ..., -3.4387e-01, -1.9788e-01,  3.0291e-02],
          [ 1.5669e-01, -4.8121e-02, -2.2616e-01,  ...,  2.6577e-01,  7.9951e-02, -1.8269e-01],
          [-2.0592e-01, -2.9541e-01, -3.3429e-01,  ...,  5.3496e-01,  2.7088e-01, -1.7743e-01],
          [ 4.7170e-01,  1.5452e-01, -1.4444e-01,  ..., -3.9579e-01, -7.2509e-02,  4.4119e-01],
          [ 3.6944e-01,  3.2179e-02, -2.9631e-01,  ..., -2.5423e-01, -3.6598e-01, -4.3667e-01]],

         [[ 3.2761e-01,  2.8695e-01,  1.0246e-01,  ..., -2.6960e-02, -1.3162e-01, -2.5268e-01],
          [-1.6703e-01, -5.5000e-02, -4.0521e-03,  ..., -4.5010e-01, -5.2682e-01, -3.6779e-01],
          [ 3.3719e-02, -1.5463e-01, -2.6205e-01,  ...,  1.7285e-01,  5.7462e-02, -1.0615e-01],
          [ 2.0571e-01, -1.3650e-01, -3.4392e-01,  ...,  6.0236e-02,  1.8956e-01,  2.9769e-01],
          [-2.2676e-01, -1.3666e-02,  6.9952e-02,  ...,  2.4655e-01,  3.2374e-02, -2.9882e-01],
          [-3.1498e-01, -3.9105e-02,  1.5120e-01,  ...,  2.5376e-01,  8.8682e-02, -1.5388e-01]]],


        [[[-3.7633e-02, -2.0393e-01, -2.3579e-01,  ..., -2.9004e-01, -3.7281e-02,  2.9401e-01],
          [-3.1553e-02, -1.2522e-01, -1.9924e-01,  ...,  1.4030e-01,  1.0863e-01,  1.6292e-01],
          [ 4.2996e-02, -6.1319e-02, -7.9166e-02,  ..., -9.3193e-02, -8.2148e-02, -1.6101e-01],
          [-3.2430e-01, -2.6465e-01, -2.6564e-01,  ...,  2.9080e-01,  2.4528e-02, -2.0775e-01],
          [-3.9114e-01, -8.0501e-02,  1.6008e-01,  ...,  1.8398e-01,  1.0489e-02, -2.0221e-01],
          [ 2.9691e-01,  1.2476e-01, -6.8293e-02,  ..., -1.0367e-01,  1.2024e-01,  4.3255e-01]],

         [[ 3.1268e-02, -5.2811e-03, -1.2313e-03,  ...,  3.7638e-02, -1.3919e-01, -4.5495e-01],
          [-6.7536e-02,  2.6190e-01,  4.8931e-01,  ..., -5.2701e-02, -2.0831e-01, -3.5716e-01],
          [-2.4445e-01, -1.8237e-01, -7.3164e-02,  ..., -3.4659e-01, -1.3764e-03,  2.9138e-01],
          [-3.6620e-01, -6.8369e-02,  1.6390e-01,  ...,  1.7563e-01,  1.1415e-01,  1.0602e-01],
          [-2.2883e-01, -3.3400e-01, -4.0631e-01,  ..., -2.8226e-01, -1.5214e-01,  6.1296e-02],
          [ 3.2225e-01,  1.2591e-01,  4.7813e-02,  ...,  2.3618e-01,  1.4936e-01, -9.9259e-02]]],


        [[[-3.9743e-01,  1.0544e-01,  4.5138e-01,  ...,  4.8217e-01,  3.7662e-01,  9.7484e-02],
          [ 3.8217e-01,  1.3815e-01, -2.1384e-02,  ...,  1.0934e-01, -3.9732e-02, -1.9931e-01],
          [-7.9039e-02, -2.1687e-01, -2.3631e-01,  ..., -3.6759e-01, -2.4456e-01, -7.1194e-02],
          [ 2.4045e-01,  6.3423e-02, -6.9318e-02,  ..., -3.3939e-02,  2.1160e-01,  4.1963e-01],
          [-1.8832e-01,  2.8778e-02,  2.5771e-01,  ..., -4.1427e-01, -1.1272e-01,  3.6882e-01],
          [ 3.6826e-01,  1.0995e-01, -1.0881e-01,  ..., -2.1168e-01,  3.9259e-02,  4.2285e-01]],

         [[-2.6424e-01, -1.3087e-01, -1.1536e-02,  ...,  4.3173e-01,  9.5054e-02, -3.8485e-01],
          [-3.5237e-01, -2.4003e-01, -4.8634e-02,  ...,  4.7637e-01,  4.3773e-01,  2.9554e-01],
          [ 5.7789e-02,  1.5401e-01,  2.2739e-01,  ..., -8.9478e-02, -1.6539e-02,  7.8096e-02],
          [-1.4237e-01, -6.1692e-02,  3.2285e-02,  ...,  1.0202e-01,  1.6307e-02, -3.1661e-02],
          [-4.5302e-01, -2.4657e-01, -1.1939e-01,  ..., -2.7632e-01, -1.5218e-01,  1.4864e-01],
          [ 1.0524e-01, -2.9086e-02, -1.4919e-01,  ...,  2.3470e-02,  6.7623e-02,  1.7952e-01]]]])

2025-07-09 13:33:36.606837 GPU 7 143189 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,1.7999999999999998,], mode="bicubic", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,1.7999999999999998,], mode="bicubic", align_corners=True, align_mode=0, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2162460491 / 2464237512 (87.8%)
Greatest absolute difference: 0.47421497106552124 at index (4405121, 1, 5, 2) (up to 0.01 allowed)
Greatest relative difference: 468923776.0 at index (6046818, 1, 3, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 6, 18]), dtype=torch.float32)
tensor([[[[ 0.4030, -0.0508, -0.4427,  ...,  0.4149,  0.3701,  0.1147],
          [ 0.3554, -0.0392, -0.3047,  ...,  0.3358,  0.2339,  0.1321],
          [-0.0743, -0.0032,  0.1230,  ...,  0.0702,  0.2052,  0.3418],
          [-0.3015,  0.1674,  0.4188,  ..., -0.1527,  0.0885,  0.3593],
          [ 0.3804, -0.1719, -0.5052,  ...,  0.2064,  0.0241, -0.1855],
          [-0.4497, -0.4766, -0.4682,  ..., -0.3483, -0.3911, -0.3097]],

         [[-0.0431, -0.0010, -0.0557,  ..., -0.0078,  0.0738,  0.0341],
          [ 0.1437, -0.1251, -0.3310,  ...,  0.0500,  0.1274,  0.2212],
          [-0.2604, -0.0029,  0.1483,  ...,  0.0741, -0.0379, -0.1106],
          [ 0.1784,  0.1363,  0.0184,  ..., -0.5292, -0.2205,  0.1244],
          [-0.2324, -0.2244, -0.2509,  ...,  0.3652,  0.2871,  0.0836],
          [-0.1134,  0.0447,  0.1992,  ...,  0.0027, -0.1773, -0.2945]]],


        [[[-0.4110, -0.4975, -0.3776,  ...,  0.4300,  0.4845,  0.3723],
          [-0.1036, -0.1573, -0.1396,  ..., -0.5122, -0.4347, -0.3026],
          [ 0.4387,  0.1600, -0.0879,  ...,  0.3017, -0.0583, -0.4650],
          [-0.1864, -0.1594, -0.0829,  ..., -0.0320,  0.0790,  0.2299],
          [ 0.4566,  0.2655,  0.0561,  ..., -0.0786, -0.1117, -0.2526],
          [-0.1608, -0.1281, -0.0978,  ...,  0.3949,  0.2936,  0.0963]],

         [[-0.1667, -0.1288,  0.0128,  ..., -0.0233,  0.1432,  0.2293],
          [-0.1277,  0.1517,  0.4220,  ..., -0.0862,  0.0689,  0.2812],
          [ 0.1402,  0.0248, -0.0887,  ...,  0.2853,  0.0271, -0.2956],
          [-0.4989, -0.1297,  0.1201,  ...,  0.1149,  0.2295,  0.3005],
          [-0.4896,  0.1230,  0.5122,  ..., -0.4858, -0.2293,  0.2004],
          [-0.4406, -0.2393, -0.0637,  ...,  0.4471,  0.0915, -0.2651]]],


        [[[ 0.2678,  0.0028, -0.2438,  ...,  0.4928,  0.0922, -0.4186],
          [ 0.2783,  0.1745,  0.0370,  ...,  0.2519,  0.3752,  0.3461],
          [ 0.1268,  0.3214,  0.4243,  ...,  0.3111,  0.1560, -0.0404],
          [ 0.1223, -0.1260, -0.3033,  ..., -0.2089, -0.1500, -0.1293],
          [-0.4919, -0.4681, -0.4094,  ..., -0.4097, -0.1378,  0.1743],
          [-0.4728, -0.3325, -0.2165,  ..., -0.3156, -0.4031, -0.4837]],

         [[-0.1997, -0.1487, -0.0609,  ...,  0.0092, -0.2288, -0.4281],
          [-0.0451,  0.1252,  0.2511,  ..., -0.1821,  0.0265,  0.2363],
          [ 0.1441,  0.1725,  0.1581,  ...,  0.0548, -0.0655, -0.2422],
          [-0.0598,  0.1301,  0.1694,  ..., -0.3160, -0.0333,  0.3638],
          [ 0.2545,  0.3191,  0.2677,  ...,  0.3748,  0.4630,  0.3482],
          [ 0.1331,  0.0839, -0.0549,  ..., -0.2459, -0.2521, -0.0717]]],


        ...,


        [[[ 0.0450,  0.0257,  0.0865,  ..., -0.3684, -0.1019,  0.1913],
          [ 0.1536, -0.1290, -0.4121,  ...,  0.3944,  0.0502, -0.4319],
          [-0.3540,  0.0684,  0.3986,  ..., -0.4036, -0.1697,  0.0581],
          [ 0.3732,  0.0317, -0.1867,  ..., -0.3149, -0.4012, -0.3067],
          [ 0.0830, -0.1670, -0.3171,  ..., -0.1935, -0.3827, -0.4378],
          [ 0.1025,  0.1564,  0.1129,  ...,  0.4765,  0.0527, -0.4772]],

         [[ 0.1915,  0.1151, -0.0018,  ..., -0.2970, -0.0440,  0.2273],
          [ 0.1834,  0.1546,  0.1608,  ..., -0.2997, -0.0094,  0.2551],
          [ 0.2472,  0.1909,  0.0741,  ...,  0.2119, -0.1642, -0.5404],
          [ 0.0482,  0.0698,  0.0573,  ..., -0.1454, -0.0844,  0.0307],
          [ 0.2750, -0.1081, -0.3726,  ..., -0.1396, -0.0213,  0.0175],
          [-0.0382, -0.2706, -0.4444,  ..., -0.0532, -0.0009,  0.0210]]],


        [[[ 0.0360, -0.1412, -0.1588,  ..., -0.3807, -0.0604,  0.4151],
          [ 0.0696,  0.1808,  0.2139,  ...,  0.0510,  0.2106,  0.4343],
          [-0.2966, -0.0816,  0.0275,  ...,  0.1507,  0.2471,  0.2295],
          [ 0.4989,  0.2642,  0.0495,  ..., -0.0954, -0.2716, -0.3967],
          [ 0.0655, -0.1191, -0.2810,  ..., -0.2764, -0.3798, -0.2958],
          [ 0.0597,  0.2094,  0.2830,  ..., -0.2366, -0.0245,  0.2322]],

         [[-0.0680, -0.1251, -0.1817,  ...,  0.2312,  0.2506,  0.1819],
          [-0.2352,  0.0723,  0.2432,  ..., -0.5095, -0.1265,  0.4596],
          [-0.2537,  0.1994,  0.5023,  ..., -0.0572,  0.1093,  0.2923],
          [-0.0101,  0.0590,  0.0552,  ...,  0.0338, -0.0132, -0.0964],
          [-0.4308, -0.2320, -0.0851,  ..., -0.3287, -0.2714, -0.1368],
          [ 0.4361,  0.2394,  0.0036,  ...,  0.2217,  0.2222,  0.1662]]],


        [[[ 0.2456,  0.3459,  0.3080,  ...,  0.1349,  0.0955,  0.0875],
          [-0.4046,  0.0429,  0.4347,  ...,  0.1502,  0.0354, -0.2318],
          [-0.1125,  0.1148,  0.2525,  ..., -0.2182, -0.0447,  0.2615],
          [ 0.2313,  0.1446,  0.0888,  ..., -0.0615,  0.2100,  0.4753],
          [-0.2203,  0.1680,  0.4009,  ...,  0.1554,  0.0188, -0.2398],
          [-0.2864,  0.1342,  0.3387,  ...,  0.2714,  0.0554, -0.3144]],

         [[-0.3403, -0.2831, -0.1176,  ..., -0.2794, -0.1932,  0.0109],
          [-0.1989, -0.4136, -0.5338,  ..., -0.0467, -0.2001, -0.3737],
          [-0.3104,  0.1066,  0.4728,  ...,  0.3087,  0.3923,  0.3606],
          [-0.1171, -0.0828, -0.0284,  ..., -0.0454, -0.0088,  0.0858],
          [-0.0612,  0.1728,  0.2671,  ...,  0.1134,  0.2468,  0.3698],
          [-0.4047, -0.1776,  0.1087,  ...,  0.4193,  0.3326,  0.0395]]]])
DESIRED: (shape=torch.Size([11408507, 2, 6, 18]), dtype=torch.float32)
tensor([[[[ 0.1460, -0.0548, -0.3384,  ...,  0.2150,  0.1457,  0.0718],
          [ 0.5726,  0.2065, -0.2761,  ...,  0.3839,  0.2495,  0.1557],
          [-0.1066, -0.0162,  0.1271,  ...,  0.1386,  0.2675,  0.3548],
          [-0.4011, -0.0552,  0.3868,  ..., -0.1655,  0.1857,  0.4412],
          [ 0.4877,  0.0833, -0.4454,  ...,  0.2053, -0.1007, -0.3217],
          [-0.3189, -0.3457, -0.3721,  ..., -0.2615, -0.1527, -0.0616]],

         [[ 0.1532,  0.0868, -0.0327,  ...,  0.1947,  0.0343, -0.1016],
          [ 0.0983, -0.0714, -0.2952,  ...,  0.0198,  0.1829,  0.3015],
          [-0.3354, -0.1248,  0.1451,  ..., -0.0220, -0.0771, -0.1040],
          [ 0.1590,  0.1146,  0.0303,  ..., -0.4395, -0.0905,  0.1568],
          [-0.3223, -0.2293, -0.1199,  ...,  0.2651,  0.2386,  0.2063],
          [ 0.0881,  0.0271, -0.0408,  ...,  0.1153, -0.1989, -0.4257]]],


        [[[-0.4331, -0.3344, -0.1618,  ...,  0.1983,  0.1570,  0.1090],
          [ 0.0312, -0.1276, -0.3229,  ..., -0.4307, -0.3336, -0.2579],
          [ 0.4613,  0.2803,  0.0220,  ...,  0.2119, -0.2211, -0.5303],
          [-0.1713, -0.1458, -0.0991,  ..., -0.0676,  0.1452,  0.3050],
          [ 0.3966,  0.2451,  0.0273,  ...,  0.0364, -0.2206, -0.4256],
          [ 0.0546,  0.0579,  0.0583,  ...,  0.1925,  0.2323,  0.2486]],

         [[-0.1699, -0.1651, -0.1268,  ..., -0.0881,  0.1290,  0.2815],
          [-0.2077,  0.0619,  0.4386,  ...,  0.0418,  0.1704,  0.2622],
          [ 0.2231,  0.0965, -0.0814,  ...,  0.2110, -0.1173, -0.3570],
          [-0.5739, -0.2581,  0.1569,  ...,  0.1667,  0.2891,  0.3624],
          [-0.5807, -0.1529,  0.4101,  ..., -0.4215, -0.0207,  0.2965],
          [-0.4938, -0.2790,  0.0095,  ...,  0.1325, -0.1053, -0.2736]]],


        [[[ 0.0919,  0.0076, -0.1193,  ...,  0.3549, -0.1438, -0.5021],
          [ 0.3943,  0.2853,  0.1200,  ...,  0.3062,  0.3922,  0.4185],
          [ 0.1190,  0.2512,  0.4164,  ...,  0.3310,  0.1175, -0.0400],
          [ 0.1313, -0.0471, -0.2867,  ..., -0.2617, -0.2224, -0.1978],
          [-0.4337, -0.4289, -0.4111,  ..., -0.3565,  0.0073,  0.2642],
          [-0.4713, -0.3980, -0.2906,  ..., -0.1903, -0.2623, -0.3123]],

         [[ 0.0096, -0.0874, -0.2104,  ...,  0.0934, -0.0705, -0.1806],
          [-0.1756,  0.0438,  0.3417,  ..., -0.1700,  0.0308,  0.1693],
          [ 0.1561,  0.1698,  0.1756,  ..., -0.0209, -0.1525, -0.2524],
          [-0.1148,  0.0375,  0.2123,  ..., -0.2519,  0.1442,  0.4413],
          [ 0.2969,  0.3763,  0.4505,  ...,  0.3524,  0.4150,  0.4235],
          [ 0.0896, -0.0100, -0.1669,  ..., -0.0462, -0.0815, -0.0912]]],


        ...,


        [[[ 0.0549,  0.1208,  0.2157,  ..., -0.1981,  0.0506,  0.2303],
          [ 0.2059, -0.0534, -0.4199,  ...,  0.3088, -0.1724, -0.5355],
          [-0.4650, -0.1368,  0.3108,  ..., -0.3188, -0.1002,  0.0487],
          [ 0.4691,  0.1908, -0.1739,  ..., -0.4240, -0.3986, -0.3443],
          [ 0.0957, -0.0966, -0.3438,  ..., -0.2783, -0.4176, -0.4840],
          [ 0.1381,  0.1443,  0.1342,  ...,  0.2222, -0.1487, -0.4198]],

         [[ 0.2919,  0.2673,  0.2011,  ..., -0.2553, -0.0646,  0.0668],
          [ 0.0918,  0.0802,  0.0856,  ..., -0.2108,  0.1332,  0.3694],
          [ 0.3260,  0.2433,  0.1067,  ...,  0.1635, -0.2996, -0.6221],
          [-0.0072,  0.0478,  0.1154,  ..., -0.1454,  0.0030,  0.1187],
          [ 0.3910,  0.0553, -0.3926,  ..., -0.0651,  0.0245,  0.0692],
          [ 0.0359, -0.1024, -0.2907,  ..., -0.0789, -0.1067, -0.1312]]],


        [[[ 0.1771,  0.0928, -0.0166,  ..., -0.2641,  0.1911,  0.5240],
          [-0.0161,  0.0626,  0.1640,  ...,  0.1016,  0.2617,  0.3866],
          [-0.3663, -0.1871,  0.0399,  ...,  0.2164,  0.2728,  0.2901],
          [ 0.5469,  0.3572,  0.0898,  ..., -0.2131, -0.3655, -0.4558],
          [ 0.1553, -0.0101, -0.2426,  ..., -0.2669, -0.3256, -0.3362],
          [ 0.0063,  0.0956,  0.2085,  ..., -0.2980,  0.0015,  0.2259]],

         [[ 0.0819, -0.0008, -0.1156,  ...,  0.1053,  0.2049,  0.2623],
          [-0.3713, -0.0823,  0.2908,  ..., -0.4288,  0.0968,  0.5012],
          [-0.3714, -0.0134,  0.4571,  ..., -0.0444,  0.2044,  0.3824],
          [-0.0298, -0.0039,  0.0180,  ...,  0.0035, -0.0936, -0.1657],
          [-0.4595, -0.3155, -0.1196,  ..., -0.3133, -0.1176,  0.0310],
          [ 0.2887,  0.2433,  0.1532,  ...,  0.1330,  0.0043, -0.0909]]],


        [[[-0.0250,  0.1202,  0.2855,  ...,  0.0545, -0.0017, -0.0428],
          [-0.3910, -0.0799,  0.3614,  ...,  0.1488, -0.0249, -0.1704],
          [-0.1829,  0.0098,  0.2611,  ..., -0.1729,  0.0720,  0.2662],
          [ 0.3013,  0.2344,  0.1466,  ...,  0.0245,  0.3276,  0.5334],
          [-0.3756, -0.0224,  0.4377,  ...,  0.1665, -0.0346, -0.2015],
          [-0.1716, -0.0216,  0.1562,  ...,  0.1770, -0.1428, -0.3788]],

         [[-0.0512, -0.1357, -0.2155,  ..., -0.1239, -0.0932, -0.0679],
          [-0.3208, -0.3916, -0.4795,  ..., -0.0880, -0.2517, -0.3651],
          [-0.3994, -0.0808,  0.3673,  ...,  0.3370,  0.3768,  0.3836],
          [-0.1186, -0.0895, -0.0411,  ...,  0.0015,  0.0229,  0.0485],
          [-0.0817,  0.0639,  0.2383,  ...,  0.1778,  0.3416,  0.4493],
          [-0.3961, -0.1913,  0.1053,  ...,  0.2352,  0.1306,  0.0366]]]])

2025-07-09 13:33:39.193508 GPU 6 142738 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,1.7999999999999998,], mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:33:47.166866 GPU 4 142578 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,1.7999999999999998,], mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:33:47.212212 GPU 3 148070 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,2.9999999999999996,], mode="bilinear", align_corners=False, align_mode=0, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:33:51.294242 GPU 2 140328 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,2.9999999999999996,], mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:34:00.799199 GPU 6 148556 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,2.9999999999999996,], mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:34:10.698388 GPU 4 148716 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[0.6,2.9999999999999996,], mode="bilinear", align_corners=True, align_mode=1, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:34:22.379976 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[1.7999999999999998,0.6,], mode="bicubic", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[1.7999999999999998,0.6,], mode="bicubic", align_corners=False, align_mode=1, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2162582021 / 2464237512 (87.8%)
Greatest absolute difference: 0.4759449362754822 at index (7311826, 0, 15, 5) (up to 0.01 allowed)
Greatest relative difference: 1789038592.0 at index (1843702, 1, 1, 2) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 18, 6]), dtype=torch.float32)
tensor([[[[-0.0268,  0.1846,  0.0417, -0.2278, -0.3750,  0.1663],
          [-0.2174,  0.0852,  0.0267, -0.3266, -0.3476,  0.0618],
          [-0.4392, -0.0495, -0.0159, -0.4223, -0.3122, -0.0606],
          ...,
          [-0.0847, -0.0345,  0.0025,  0.0373, -0.3709, -0.1774],
          [-0.2393, -0.0988, -0.2137,  0.2265, -0.1403, -0.2899],
          [-0.3380, -0.1605, -0.3813,  0.3476,  0.0480, -0.3836]],

         [[-0.2946,  0.2445,  0.1353,  0.5096,  0.1179, -0.3989],
          [-0.1440,  0.1320,  0.1337,  0.2844,  0.1744, -0.3815],
          [ 0.0706, -0.0165,  0.1331, -0.0330,  0.2625, -0.3168],
          ...,
          [ 0.0632,  0.2886,  0.0502, -0.0718, -0.0387,  0.3328],
          [ 0.0262, -0.1819,  0.0138, -0.1157,  0.1830,  0.1578],
          [-0.0223, -0.5456,  0.0015, -0.1420,  0.3223,  0.0267]]],


        [[[-0.1616,  0.5280,  0.1955, -0.0505,  0.5308,  0.4708],
          [ 0.0830,  0.2121,  0.0659, -0.0105,  0.3740,  0.2270],
          [ 0.4169, -0.2452, -0.1043,  0.0565,  0.1517, -0.0915],
          ...,
          [ 0.3454, -0.4158,  0.0317, -0.1127, -0.1248, -0.1006],
          [ 0.2290, -0.2278, -0.0581,  0.0412,  0.2112, -0.1196],
          [ 0.1244, -0.0947, -0.1111,  0.1461,  0.4686, -0.1421]],

         [[-0.0871,  0.2426,  0.1079, -0.2229, -0.0384, -0.2015],
          [-0.0823,  0.2897, -0.0933,  0.1014, -0.1290, -0.1784],
          [-0.0717,  0.3489, -0.3491,  0.5165, -0.2703, -0.1385],
          ...,
          [ 0.4013,  0.3663, -0.2581, -0.0447,  0.4337,  0.3868],
          [ 0.3018,  0.0414, -0.3721,  0.2405,  0.0790,  0.0471],
          [ 0.2214, -0.2042, -0.4605,  0.4526, -0.1740, -0.2031]]],


        [[[ 0.3504, -0.0844,  0.0740,  0.1084,  0.4524, -0.3654],
          [ 0.1897,  0.1610, -0.1113,  0.0172,  0.4305, -0.1786],
          [-0.0385,  0.4538, -0.3525, -0.0812,  0.3673,  0.0937],
          ...,
          [ 0.2002, -0.0796,  0.1225,  0.1159,  0.2619,  0.1611],
          [ 0.1487, -0.2259,  0.0567,  0.1149,  0.0940,  0.1772],
          [ 0.1192, -0.3448, -0.0028,  0.0983, -0.0361,  0.1629]],

         [[-0.0275,  0.2071,  0.0737, -0.1462,  0.3443, -0.2625],
          [ 0.1197,  0.2617,  0.2777, -0.1063,  0.4300, -0.2401],
          [ 0.3339,  0.3438,  0.5170, -0.0501,  0.4906, -0.2028],
          ...,
          [ 0.1900,  0.1343,  0.4168, -0.3329,  0.1580,  0.0789],
          [ 0.2062, -0.1529,  0.4652, -0.0207, -0.0391,  0.2811],
          [ 0.2102, -0.3402,  0.4782,  0.2294, -0.1987,  0.4307]]],


        ...,


        [[[ 0.1105, -0.4598,  0.1685,  0.1799,  0.0071, -0.0410],
          [-0.0096, -0.1603,  0.0721,  0.1369,  0.1402, -0.1537],
          [-0.1511,  0.2272, -0.0499,  0.0779,  0.2967, -0.2883],
          ...,
          [-0.3767, -0.4122,  0.1259,  0.1243,  0.4478,  0.0096],
          [-0.2111, -0.0542,  0.0730, -0.0531,  0.1506, -0.2260],
          [-0.0699,  0.2226,  0.0202, -0.1743, -0.0990, -0.3788]],

         [[ 0.4331, -0.1636, -0.3923,  0.2503,  0.0929, -0.4314],
          [ 0.2678,  0.0440, -0.1671,  0.0055,  0.1320, -0.1712],
          [ 0.0330,  0.3155,  0.1490, -0.3085,  0.1984,  0.1819],
          ...,
          [ 0.2688, -0.2785, -0.3385,  0.2547, -0.1455,  0.3177],
          [ 0.2044, -0.0994, -0.0239, -0.1949, -0.0213,  0.1172],
          [ 0.1300,  0.0586,  0.2013, -0.5121,  0.0814, -0.0534]]],


        [[[-0.3156, -0.1608, -0.1218,  0.4865,  0.5346, -0.0775],
          [-0.1145, -0.2423, -0.1467,  0.2997,  0.1941, -0.1876],
          [ 0.1540, -0.3276, -0.1893,  0.0315, -0.2503, -0.2974],
          ...,
          [-0.1166,  0.2306, -0.2775,  0.0406, -0.2333, -0.2773],
          [-0.2400,  0.2080, -0.2081,  0.1252,  0.0386, -0.1458],
          [-0.3435,  0.1681, -0.1441,  0.1711,  0.2347, -0.0504]],

         [[-0.0413, -0.0847, -0.4609,  0.0409, -0.0028,  0.2462],
          [ 0.1275, -0.0786, -0.2489, -0.1685, -0.1038, -0.0646],
          [ 0.3254, -0.0633,  0.0529, -0.4306, -0.2276, -0.4645],
          ...,
          [ 0.2836,  0.2292,  0.4460, -0.3558,  0.0643,  0.1525],
          [ 0.1159, -0.1036,  0.3620, -0.3848,  0.2553,  0.1035],
          [ 0.0073, -0.3659,  0.2758, -0.3828,  0.3904,  0.0726]]],


        [[[-0.1071,  0.5088,  0.1426, -0.1990,  0.1181, -0.2479],
          [-0.0735,  0.1052,  0.0903, -0.3172,  0.1781, -0.1524],
          [-0.0166, -0.4116,  0.0108, -0.4427,  0.2423, -0.0101],
          ...,
          [-0.0422,  0.2729, -0.2819, -0.0851,  0.1804, -0.1401],
          [ 0.1642,  0.3275, -0.2103, -0.2232,  0.0097, -0.2569],
          [ 0.3066,  0.3474, -0.1275, -0.3189, -0.1307, -0.3367]],

         [[ 0.5317, -0.3455, -0.0618, -0.3636, -0.4305,  0.5124],
          [ 0.1764, -0.0319,  0.0201, -0.1334, -0.4918,  0.3000],
          [-0.2914,  0.3927,  0.1212,  0.1932, -0.5165, -0.0163],
          ...,
          [ 0.4251, -0.1963, -0.1719,  0.3234, -0.2144, -0.0256],
          [ 0.2524, -0.2648,  0.0446,  0.3565,  0.0326,  0.1800],
          [ 0.1226, -0.3132,  0.1871,  0.3657,  0.2210,  0.3305]]]])
DESIRED: (shape=torch.Size([11408507, 2, 18, 6]), dtype=torch.float32)
tensor([[[[-2.8105e-01,  1.8315e-01,  9.2373e-02, -1.7836e-01, -3.6574e-01,  1.7762e-01],
          [-4.1355e-01,  1.0536e-02,  3.0980e-02, -3.4524e-01, -3.3985e-01,  6.7627e-02],
          [-3.5367e-01, -1.0509e-01, -1.0067e-01, -3.4449e-01, -3.2604e-01,  5.4856e-02],
          ...,
          [-1.3645e-01, -4.0282e-02,  4.1624e-02,  2.8625e-02, -4.2450e-01, -1.4039e-01],
          [-2.7152e-01, -6.3304e-02, -9.6972e-02,  1.7700e-01, -2.9986e-01, -1.9705e-01],
          [-3.1404e-01, -1.7542e-01, -3.2767e-01,  2.5006e-01, -2.1940e-02, -3.4900e-01]],

         [[-1.9154e-01,  1.6634e-01,  8.5814e-02,  4.5525e-01,  8.8267e-04, -3.0649e-01],
          [-1.1790e-01,  1.1560e-01,  6.7788e-02,  1.3981e-01,  9.7705e-02, -2.7644e-01],
          [ 2.2881e-02,  6.0399e-02,  7.8866e-02, -1.2778e-01,  2.3096e-01, -1.3812e-01],
          ...,
          [-5.8646e-02,  3.4583e-01,  4.9727e-02, -7.4649e-02, -8.1949e-02,  4.4427e-01],
          [-6.0518e-02,  7.8691e-02,  8.6412e-03, -9.2712e-02,  1.2210e-01,  2.7378e-01],
          [-1.7648e-01, -4.0330e-01,  4.6644e-02, -7.7199e-02,  2.3218e-01,  7.6491e-02]]],


        [[[ 5.3312e-02,  3.5898e-01,  1.2860e-01, -5.8315e-02,  5.5159e-01,  3.8090e-01],
          [ 2.8619e-01,  3.1726e-02,  1.5828e-02,  9.4220e-04,  3.1731e-01,  2.6345e-02],
          [ 4.2031e-01, -2.8580e-01, -5.9299e-02,  7.9449e-02,  1.1781e-01, -1.9721e-01],
          ...,
          [ 1.5950e-01, -3.0106e-01,  4.9937e-02, -1.8965e-01,  5.4511e-03, -4.2611e-01],
          [ 2.0118e-01, -1.9248e-01, -4.2224e-02, -7.1690e-02,  1.6756e-01, -2.8808e-01],
          [ 1.0760e-01, -7.9483e-02, -9.4065e-02,  3.7888e-02,  4.2342e-01, -1.5097e-01]],

         [[ 9.1012e-02,  1.7435e-01,  3.4740e-02, -9.1065e-02, -4.9235e-02, -3.8513e-01],
          [ 2.4475e-02,  2.8182e-01, -2.3876e-01,  3.1444e-01, -2.2142e-01, -1.9622e-01],
          [-3.9239e-02,  3.5582e-01, -3.8606e-01,  5.3196e-01, -3.7933e-01, -6.9979e-02],
          ...,
          [ 3.5651e-01,  4.2914e-01, -2.0841e-01, -1.9494e-02,  4.9527e-01,  4.3595e-01],
          [ 4.2782e-01,  1.8072e-01, -2.8912e-01,  1.5473e-01,  1.5937e-01,  2.7594e-01],
          [ 4.1452e-01, -1.5370e-01, -4.2946e-01,  3.7407e-01, -2.0789e-01,  4.7867e-02]]],


        [[[ 2.2020e-01, -2.4386e-04,  9.2972e-02,  1.0158e-01,  3.7762e-01, -3.8352e-01],
          [ 4.1591e-03,  3.3811e-01, -1.9465e-01, -4.2828e-02,  3.9352e-01, -8.1157e-02],
          [-2.2218e-01,  4.6582e-01, -3.7393e-01, -5.9648e-02,  3.0328e-01,  2.0208e-01],
          ...,
          [ 2.8050e-01, -6.0859e-02,  8.4331e-02,  9.5286e-02,  2.1296e-01,  1.8028e-01],
          [ 1.7720e-01, -1.0303e-01,  4.6576e-02,  1.2861e-01,  1.4017e-01,  2.7236e-01],
          [ 8.3678e-02, -2.2629e-01, -4.2289e-02,  6.8698e-02,  8.7470e-03,  2.1257e-01]],

         [[ 1.0573e-01,  1.4067e-01,  1.4247e-01, -8.0657e-02,  2.3768e-01, -2.0076e-01],
          [ 3.2413e-01,  2.0994e-01,  4.0977e-01, -2.7926e-02,  4.0053e-01, -2.7514e-01],
          [ 5.0985e-01,  3.2181e-01,  4.8150e-01,  4.0161e-03,  3.7548e-01, -3.4339e-01],
          ...,
          [ 2.7967e-01,  2.0713e-01,  3.1112e-01, -2.6137e-01,  1.3647e-02,  3.2206e-01],
          [ 2.9905e-01, -6.9629e-02,  4.3219e-01, -1.4537e-01, -1.7465e-02,  4.3155e-01],
          [ 2.3533e-01, -2.9113e-01,  4.7175e-01,  1.2658e-01, -1.0339e-01,  4.7188e-01]]],


        ...,


        [[[-1.1085e-01, -3.0406e-01,  1.6746e-01,  1.8729e-01,  1.0997e-01, -3.5459e-01],
          [-2.4396e-01,  3.8752e-02,  4.1212e-02,  1.0358e-01,  2.3891e-01, -3.9188e-01],
          [-2.6022e-01,  2.4017e-01, -2.1237e-02,  2.7580e-02,  2.6599e-01, -3.2534e-01],
          ...,
          [-2.7870e-01, -5.0547e-01,  1.2076e-01,  1.8202e-01,  3.2458e-01,  2.2484e-01],
          [-2.8678e-01, -2.3587e-01,  9.2428e-02,  3.2020e-02,  2.1666e-01, -1.0573e-01],
          [-1.2252e-01,  1.9357e-01, -3.2341e-02, -1.1543e-01, -7.9912e-02, -4.1151e-01]],

         [[ 4.3004e-01, -5.9688e-02, -3.1748e-01,  1.8494e-01,  2.7095e-02, -3.3747e-01],
          [ 3.4061e-01,  9.2747e-02, -1.9023e-02, -1.5001e-01,  1.2775e-01,  5.4444e-02],
          [ 2.4106e-01,  1.7785e-01,  2.3033e-01, -3.3566e-01,  2.4872e-01,  3.2372e-01],
          ...,
          [ 8.0299e-02, -1.2622e-01, -3.8039e-01,  3.4446e-01, -9.0800e-02,  2.5512e-01],
          [ 2.6345e-01, -1.4775e-01, -1.2827e-01, -3.2238e-02, -5.2844e-02,  2.5765e-01],
          [ 3.6673e-01, -6.7963e-02,  1.7115e-01, -4.3666e-01,  4.0621e-02,  9.5975e-02]]],


        [[[-2.3855e-01, -2.5381e-01, -6.8827e-02,  4.4025e-01,  3.4911e-01, -2.9238e-03],
          [-2.0301e-02, -2.6380e-01, -1.4899e-01,  1.7871e-01, -6.7566e-02, -1.5230e-01],
          [ 8.6966e-02, -1.6654e-01, -2.4880e-01, -6.1558e-02, -3.0967e-01, -1.3938e-01],
          ...,
          [-1.9798e-01,  2.2111e-01, -2.9454e-01, -2.8625e-02, -3.0487e-01, -2.6182e-01],
          [-2.5652e-01,  2.3928e-01, -2.4464e-01,  6.4612e-02, -7.3811e-02, -1.7665e-01],
          [-3.9012e-01,  1.0583e-01, -9.9535e-02,  1.0519e-01,  2.1325e-01, -1.1891e-01]],

         [[-5.8254e-02, -5.5663e-02, -3.7446e-01, -4.0566e-02,  7.7392e-02,  7.5587e-02],
          [ 1.8519e-01, -2.9841e-02, -1.1368e-01, -3.0442e-01, -1.2322e-01, -3.0069e-01],
          [ 2.3929e-01,  1.3803e-02,  1.1724e-01, -4.3269e-01, -2.5694e-01, -4.3972e-01],
          ...,
          [ 4.5613e-01,  1.8963e-01,  4.0562e-01, -3.3032e-01, -1.6241e-02,  4.5482e-01],
          [ 3.6647e-01, -1.4745e-02,  3.8030e-01, -3.9693e-01,  1.6142e-01,  2.6404e-01],
          [ 2.7824e-01, -3.4667e-01,  2.2121e-01, -3.4955e-01,  3.8591e-01, -1.0536e-02]]],


        [[[-2.6202e-01,  4.1330e-01,  1.6126e-01, -1.6049e-01,  1.2941e-01, -4.0322e-01],
          [-9.3356e-02, -1.2873e-01,  9.1790e-02, -3.5272e-01,  2.2392e-01, -1.9808e-01],
          [ 1.2071e-01, -4.5186e-01,  6.3555e-03, -3.9982e-01,  2.2093e-01,  8.5534e-02],
          ...,
          [ 1.0645e-01,  1.4121e-01, -2.3880e-01, -8.9665e-02,  1.6842e-01, -9.1441e-02],
          [ 2.6992e-01,  2.7548e-01, -2.7939e-01, -1.9499e-01,  1.0366e-01, -2.6913e-01],
          [ 3.4410e-01,  3.4557e-01, -1.4806e-01, -2.7374e-01, -8.1740e-02, -4.5449e-01]],

         [[ 4.6476e-01, -2.0391e-01, -2.4598e-02, -2.6998e-01, -3.0629e-01,  3.6522e-01],
          [ 6.8943e-02,  9.2214e-02,  5.8836e-02, -7.3924e-03, -4.7422e-01,  1.8543e-01],
          [-1.7037e-01,  3.1687e-01,  9.1660e-02,  2.3599e-01, -4.2852e-01, -4.5897e-02],
          ...,
          [ 4.0562e-01, -7.9690e-02, -2.3599e-01,  3.1363e-01, -2.0866e-01, -2.0420e-01],
          [ 3.7434e-01, -1.8105e-01, -4.6547e-02,  3.8010e-01, -4.1699e-02, -4.0765e-02],
          [ 3.2756e-01, -3.0678e-01,  1.0496e-01,  3.7945e-01,  2.0250e-01,  2.0612e-01]]]])

2025-07-09 13:35:01.244247 GPU 5 141454 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[1.7999999999999998,0.6,], mode="bicubic", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[1.7999999999999998,0.6,], mode="bicubic", align_corners=True, align_mode=0, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2162460687 / 2464237512 (87.8%)
Greatest absolute difference: 0.4796521067619324 at index (5740596, 0, 15, 0) (up to 0.01 allowed)
Greatest relative difference: 585171264.0 at index (8276743, 0, 10, 5) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 18, 6]), dtype=torch.float32)
tensor([[[[-0.4483,  0.1192,  0.2519,  0.2098, -0.2013, -0.3351],
          [-0.3282, -0.0927,  0.3402, -0.0280,  0.0181, -0.2602],
          [-0.2275, -0.1724,  0.3257, -0.1669,  0.0812, -0.2367],
          ...,
          [-0.0242, -0.0515, -0.0186,  0.4601,  0.4413,  0.3444],
          [-0.0507, -0.1333, -0.2596,  0.4386,  0.1899,  0.1693],
          [-0.0136, -0.2725, -0.4804,  0.3464, -0.2050, -0.0372]],

         [[ 0.0102,  0.0721, -0.2864,  0.1295, -0.0980, -0.2959],
          [ 0.3284,  0.3180,  0.1180,  0.2425,  0.0074, -0.1625],
          [ 0.4517,  0.4778,  0.4080,  0.2780,  0.0296, -0.0113],
          ...,
          [-0.2000,  0.4990, -0.3353, -0.4150,  0.4461, -0.4607],
          [ 0.1340,  0.3474, -0.2095, -0.2061,  0.1781, -0.4602],
          [ 0.4680,  0.0088,  0.1003,  0.0134, -0.1519, -0.3992]]],


        [[[ 0.4470, -0.2323,  0.0841, -0.1028,  0.3453, -0.3476],
          [-0.0151, -0.0189, -0.2468,  0.0907,  0.1970, -0.4841],
          [-0.4597,  0.1419, -0.5230,  0.2774,  0.0797, -0.4782],
          ...,
          [-0.1403, -0.3235,  0.0207,  0.0143,  0.2548, -0.2385],
          [ 0.0440, -0.0270, -0.0060, -0.0524,  0.3839, -0.3530],
          [ 0.1413,  0.4737, -0.0111, -0.0712,  0.3609, -0.4511]],

         [[ 0.1837,  0.0351,  0.4817, -0.1968,  0.3041,  0.3807],
          [-0.0701,  0.0192,  0.3628,  0.0076,  0.0355,  0.4431],
          [-0.2228, -0.0419,  0.1723,  0.2151, -0.1402,  0.4524],
          ...,
          [-0.1316, -0.3829, -0.0019,  0.0945, -0.3023, -0.0052],
          [ 0.1073, -0.2935, -0.0623, -0.0633, -0.2787, -0.0778],
          [ 0.2636, -0.0098, -0.1745, -0.2175, -0.1370, -0.1640]]],


        [[[-0.3039, -0.3738,  0.1204, -0.3930, -0.4199, -0.2035],
          [-0.3110,  0.0236, -0.0042, -0.3388, -0.2471,  0.1727],
          [-0.2616,  0.2331, -0.1634, -0.2488, -0.1362,  0.4557],
          ...,
          [ 0.0256,  0.1216,  0.2927,  0.4053, -0.2095, -0.0035],
          [ 0.1101,  0.2131,  0.1999,  0.5259, -0.1845,  0.2725],
          [ 0.1687,  0.1539,  0.0863,  0.4685,  0.0347,  0.4609]],

         [[-0.2022, -0.4795, -0.1006,  0.3489,  0.2511, -0.4051],
          [-0.1673, -0.4416,  0.2611,  0.0143,  0.3283, -0.0656],
          [-0.0756, -0.3889,  0.5392, -0.3131,  0.2458,  0.2976],
          ...,
          [-0.1729, -0.2292,  0.0515,  0.0016, -0.2329, -0.3839],
          [-0.2469, -0.1001,  0.1553,  0.0887, -0.2224, -0.1177],
          [-0.4077,  0.1554,  0.3651,  0.2248, -0.0878,  0.3296]]],


        ...,


        [[[-0.4838,  0.3622,  0.0021, -0.1447,  0.2116,  0.3497],
          [-0.3464,  0.4053, -0.1777,  0.0930,  0.2663,  0.0516],
          [-0.0801,  0.2749, -0.2920,  0.2341,  0.2267, -0.1735],
          ...,
          [ 0.3679, -0.2230,  0.3691,  0.1980, -0.0267,  0.0172],
          [ 0.3577, -0.3090,  0.4728,  0.1488, -0.1179, -0.2679],
          [ 0.3694, -0.3716,  0.3775,  0.1289, -0.1551, -0.4367]],

         [[ 0.3276, -0.3698, -0.0539,  0.1353, -0.1005, -0.2527],
          [-0.0913, -0.2897,  0.0137,  0.1637, -0.1508, -0.2776],
          [-0.4176, -0.1968,  0.0300,  0.1872, -0.0903, -0.3171],
          ...,
          [-0.3050, -0.2684, -0.3985,  0.3434,  0.0123, -0.3016],
          [-0.3260, -0.1649, -0.2790,  0.2532,  0.0524, -0.2343],
          [-0.3150, -0.0630, -0.0122, -0.0231,  0.0712, -0.1539]]],


        [[[-0.0376,  0.1665,  0.3236, -0.4042, -0.0752,  0.2940],
          [-0.0630,  0.4042,  0.2227, -0.3896,  0.1080,  0.2693],
          [-0.0688,  0.4508,  0.0710, -0.2825,  0.3035,  0.2224],
          ...,
          [ 0.1044, -0.4632,  0.3091,  0.3069,  0.2181,  0.2983],
          [ 0.2725, -0.3723,  0.2307,  0.3507,  0.2021,  0.4384],
          [ 0.2969, -0.1572,  0.0508,  0.2906,  0.1599,  0.4326]],

         [[ 0.0313,  0.1436, -0.0171,  0.1352, -0.4061, -0.4550],
          [ 0.2415,  0.0621, -0.0853,  0.0970, -0.4568, -0.0158],
          [ 0.3171,  0.0210, -0.0772,  0.0290, -0.3922,  0.2391],
          ...,
          [-0.4974, -0.5094,  0.2037,  0.5161, -0.0340, -0.2935],
          [-0.1388, -0.0648,  0.0457,  0.1717, -0.1449, -0.2562],
          [ 0.3223,  0.4631, -0.1771, -0.3200, -0.2581, -0.0993]]],


        [[[-0.3974,  0.0084, -0.3580,  0.2036, -0.0784,  0.0975],
          [-0.4428,  0.0229, -0.3324,  0.2040, -0.1609,  0.1864],
          [-0.2899,  0.0774, -0.2002,  0.2220, -0.1750,  0.1642],
          ...,
          [-0.4992, -0.0038, -0.3238,  0.3208, -0.3391, -0.1934],
          [-0.1233, -0.0832, -0.0142, -0.0315, -0.1281,  0.0221],
          [ 0.3683, -0.0565,  0.4140, -0.4348,  0.2245,  0.4228]],

         [[-0.2642, -0.0338, -0.3573,  0.0484,  0.0631, -0.3848],
          [ 0.0755, -0.0196, -0.3351, -0.2208,  0.2509, -0.3339],
          [ 0.2317,  0.0485, -0.2162, -0.4484,  0.3591, -0.1515],
          ...,
          [-0.2002,  0.3707, -0.2439,  0.1716, -0.1056,  0.4387],
          [-0.0184,  0.2354, -0.1715,  0.0775,  0.0266,  0.3632],
          [ 0.1052, -0.1248, -0.1129, -0.0537,  0.2847,  0.1795]]]])
DESIRED: (shape=torch.Size([11408507, 2, 18, 6]), dtype=torch.float32)
tensor([[[[-0.2693,  0.1306,  0.2527,  0.2210, -0.2246, -0.2214],
          [-0.2272, -0.0178,  0.3156,  0.0687, -0.1065, -0.1491],
          [-0.1607, -0.1969,  0.3736, -0.1244,  0.0282, -0.0703],
          ...,
          [ 0.0501, -0.0780, -0.1419,  0.4768,  0.4181,  0.3273],
          [ 0.0991, -0.2860, -0.3706,  0.4159,  0.0019,  0.1381],
          [ 0.1351, -0.4383, -0.5241,  0.3605, -0.3190,  0.0041]],

         [[ 0.1415, -0.0671, -0.3871,  0.1337, -0.2359, -0.0502],
          [ 0.2971,  0.1479, -0.0595,  0.1926, -0.0427, -0.1230],
          [ 0.4675,  0.4381,  0.3809,  0.2608,  0.1896, -0.1973],
          ...,
          [ 0.0404,  0.4669, -0.3789, -0.3773,  0.3052, -0.1855],
          [ 0.2291,  0.2034, -0.0918, -0.1495,  0.0597, -0.3165],
          [ 0.3606, -0.0171,  0.1442,  0.0107, -0.1172, -0.4084]]],


        [[[ 0.4197, -0.2643,  0.1037, -0.0987,  0.2767, -0.1045],
          [ 0.2129, -0.1567, -0.1312,  0.0481,  0.2418, -0.2988],
          [-0.0890, -0.0126, -0.4544,  0.2566,  0.1889, -0.5280],
          ...,
          [-0.1321, -0.2958,  0.0330, -0.0626,  0.4572, -0.3144],
          [ 0.0915,  0.1725, -0.0049, -0.0350,  0.3619, -0.2614],
          [ 0.2384,  0.5363, -0.0258, -0.0030,  0.2544, -0.2097]],

         [[ 0.0074,  0.1841,  0.4968, -0.2479,  0.3212,  0.3490],
          [-0.0981,  0.1409,  0.3965, -0.0806,  0.1513,  0.3128],
          [-0.2263,  0.0608,  0.2319,  0.1647, -0.0653,  0.2450],
          ...,
          [-0.0390, -0.4775,  0.0471,  0.0442, -0.2835, -0.1203],
          [ 0.0237, -0.1565, -0.0845, -0.1121, -0.2684, -0.0603],
          [ 0.0580,  0.1075, -0.1899, -0.2205, -0.2316, -0.0214]]],


        [[[-0.2150, -0.5183,  0.2211, -0.4302, -0.4934, -0.1940],
          [-0.1504, -0.2046,  0.0628, -0.3657, -0.3921,  0.0595],
          [-0.0665,  0.1985, -0.1696, -0.2671, -0.2514,  0.3922],
          ...,
          [ 0.0178,  0.2415,  0.2755,  0.4790, -0.2894,  0.1741],
          [ 0.1441,  0.1864,  0.1559,  0.4945, -0.0064,  0.1716],
          [ 0.2290,  0.1164,  0.0674,  0.4690,  0.2284,  0.1564]],

         [[-0.2508, -0.4376, -0.2189,  0.4225,  0.2814, -0.3305],
          [-0.2280, -0.4003,  0.0808,  0.1763,  0.3571, -0.2320],
          [-0.1659, -0.3510,  0.4865, -0.1826,  0.4063, -0.0592],
          ...,
          [ 0.0457, -0.3806,  0.1039,  0.0619, -0.3752, -0.1731],
          [-0.1471, -0.0450,  0.2763,  0.1786, -0.1692,  0.0153],
          [-0.2930,  0.2183,  0.4126,  0.2681,  0.0023,  0.1689]]],


        ...,


        [[[-0.3059,  0.3674,  0.0113, -0.1918,  0.3137,  0.1218],
          [-0.3141,  0.4301, -0.1198,  0.0175,  0.2608,  0.0942],
          [-0.2933,  0.4677, -0.2940,  0.2873,  0.1721,  0.0444],
          ...,
          [ 0.2960, -0.2518,  0.4520,  0.1325, -0.0331, -0.0552],
          [ 0.2157, -0.2857,  0.4172,  0.0939, -0.0718, -0.2828],
          [ 0.1566, -0.3025,  0.3530,  0.0765, -0.0839, -0.4334]],

         [[ 0.3542, -0.4914, -0.0135,  0.0929, -0.0731, -0.1699],
          [ 0.1299, -0.4111,  0.0175,  0.1216, -0.0952, -0.2081],
          [-0.1799, -0.2911,  0.0502,  0.1610, -0.0891, -0.2733],
          ...,
          [-0.3555, -0.2307, -0.3883,  0.4057,  0.0674, -0.2624],
          [-0.2331, -0.1767, -0.1237,  0.1895,  0.0135, -0.1057],
          [-0.1327, -0.1459,  0.0912,  0.0012, -0.0352,  0.0148]]],


        [[[-0.1382,  0.2533,  0.3222, -0.4448, -0.0659,  0.1112],
          [-0.1242,  0.3888,  0.2706, -0.4222,  0.1140,  0.0523],
          [-0.1018,  0.5287,  0.1784, -0.3619,  0.3683, -0.0204],
          ...,
          [ 0.0296, -0.4413,  0.3507,  0.3701,  0.3727,  0.0701],
          [ 0.1455, -0.2799,  0.1915,  0.3586,  0.2733,  0.1910],
          [ 0.2150, -0.1464,  0.0573,  0.3314,  0.1868,  0.2697]],

         [[-0.0264,  0.1753,  0.0013,  0.1553, -0.4779, -0.3009],
          [ 0.1438,  0.0680, -0.0395,  0.1093, -0.4481, -0.1552],
          [ 0.3626, -0.0718, -0.0795,  0.0386, -0.3813,  0.0301],
          ...,
          [-0.3195, -0.4181,  0.1758,  0.4524, -0.0387, -0.2016],
          [ 0.0208,  0.1602, -0.0351, -0.0369, -0.2562, -0.0470],
          [ 0.2652,  0.5794, -0.1928, -0.4023, -0.4078,  0.0717]]],


        [[[-0.1057, -0.1134, -0.4014,  0.1798, -0.1738,  0.2910],
          [-0.1084, -0.0954, -0.3796,  0.1762, -0.1365,  0.1690],
          [-0.0898, -0.0446, -0.3165,  0.1779, -0.0749, -0.0103],
          ...,
          [-0.2069, -0.1189, -0.3100,  0.1939, -0.3208, -0.0970],
          [ 0.0621, -0.0433,  0.1303, -0.1997,  0.0344,  0.0873],
          [ 0.2614,  0.0323,  0.4620, -0.4793,  0.3145,  0.2285]],

         [[-0.2190, -0.0581, -0.3946,  0.1623, -0.1137, -0.1127],
          [-0.0538, -0.0435, -0.3516, -0.0720,  0.0525, -0.0956],
          [ 0.1481, -0.0030, -0.2639, -0.4001,  0.2696, -0.0421],
          ...,
          [ 0.0085,  0.3883, -0.2669,  0.1875, -0.2098,  0.4698],
          [ 0.0307,  0.0908, -0.1665,  0.0708,  0.0808,  0.2572],
          [ 0.0303, -0.1587, -0.0989, -0.0179,  0.3170,  0.0791]]]])

2025-07-09 13:35:21.081289 GPU 3 149332 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[1.7999999999999998,0.6,], mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:35:24.544455 GPU 7 143189 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[1.7999999999999998,0.6,], mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:35:40.699777 GPU 4 149495 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[2.9999999999999996,0.6,], mode="bilinear", align_corners=False, align_mode=0, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:35:48.509823 GPU 7 149654 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[2.9999999999999996,0.6,], mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:36:13.218552 GPU 6 150009 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[2.9999999999999996,0.6,], mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:36:47.000338 GPU 5 141454 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=None, scale_factor=list[2.9999999999999996,0.6,], mode="bilinear", align_corners=True, align_mode=1, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:36:53.463878 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[13,13,], scale_factor=None, mode="bicubic", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[13,13,], scale_factor=None, mode="bicubic", align_corners=False, align_mode=1, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2656388920 / 3856075366 (68.9%)
Greatest absolute difference: 0.17753411829471588 at index (1646105, 1, 11, 11) (up to 0.01 allowed)
Greatest relative difference: inf at index (8901431, 1, 6, 3) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 13, 13]), dtype=torch.float32)
tensor([[[[ 0.2005, -0.2084, -0.0423,  ..., -0.2421, -0.2440,  0.3070],
          [ 0.3411,  0.3669,  0.1391,  ...,  0.0278, -0.0801,  0.2366],
          [ 0.2325,  0.2517, -0.0881,  ...,  0.0510,  0.1180,  0.1490],
          ...,
          [ 0.1257, -0.0086, -0.0991,  ...,  0.1752,  0.3777,  0.3789],
          [ 0.1057, -0.1526, -0.1450,  ..., -0.0150,  0.1054,  0.4558],
          [-0.0759, -0.2673, -0.2405,  ..., -0.2839, -0.2538,  0.2655]],

         [[ 0.2782, -0.0849,  0.0011,  ..., -0.1448, -0.2232,  0.5243],
          [-0.2604, -0.3840, -0.1965,  ...,  0.0688, -0.1677,  0.2917],
          [-0.1548, -0.4680, -0.4728,  ...,  0.0399,  0.0310,  0.3403],
          ...,
          [ 0.0653,  0.0268,  0.0631,  ...,  0.4089,  0.2158, -0.3975],
          [ 0.5364,  0.4865,  0.3367,  ...,  0.2917,  0.0552, -0.2787],
          [ 0.3707,  0.2677, -0.1058,  ..., -0.0705, -0.0442, -0.0699]]],


        [[[ 0.2688, -0.3713, -0.0930,  ..., -0.3792, -0.5088, -0.2780],
          [ 0.2036, -0.3036, -0.3310,  ...,  0.3691,  0.0579, -0.1663],
          [ 0.3854,  0.1517, -0.2446,  ...,  0.3068,  0.0589, -0.2510],
          ...,
          [ 0.3652,  0.6123,  0.1421,  ...,  0.4520,  0.0130, -0.2775],
          [ 0.3017,  0.2576, -0.0458,  ...,  0.0636, -0.1657, -0.0758],
          [ 0.3494, -0.1809, -0.2100,  ..., -0.3547, -0.3582, -0.1433]],

         [[ 0.0878, -0.0405,  0.1412,  ...,  0.1072, -0.1022, -0.3944],
          [-0.0272,  0.0283,  0.1304,  ..., -0.0409, -0.1534, -0.4580],
          [ 0.1258,  0.0544, -0.1354,  ...,  0.1592,  0.1009, -0.1251],
          ...,
          [-0.3448, -0.1423, -0.0806,  ...,  0.1281,  0.1215, -0.2117],
          [-0.0807,  0.0671, -0.0395,  ...,  0.1748,  0.1705, -0.3044],
          [ 0.4652,  0.0517, -0.0071,  ..., -0.0827, -0.3023,  0.2203]]],


        [[[-0.2443, -0.1252,  0.1820,  ..., -0.1372, -0.3349,  0.0365],
          [-0.3450,  0.0212,  0.2436,  ..., -0.3317, -0.2629, -0.1114],
          [-0.2181,  0.3392,  0.2694,  ...,  0.0168,  0.1279,  0.0494],
          ...,
          [ 0.0331, -0.0806, -0.2670,  ...,  0.1275,  0.1096,  0.1185],
          [ 0.0923, -0.0333, -0.3057,  ...,  0.3210,  0.1757, -0.4486],
          [-0.5759,  0.1579,  0.2512,  ...,  0.2784,  0.1719, -0.4579]],

         [[-0.3401,  0.1839,  0.5205,  ...,  0.1075, -0.0644, -0.1214],
          [-0.5611, -0.1677,  0.3236,  ..., -0.1734, -0.1394, -0.0480],
          [-0.1684,  0.0701,  0.1057,  ..., -0.3183, -0.0844, -0.0517],
          ...,
          [ 0.1621,  0.4825,  0.2890,  ...,  0.1992,  0.5301,  0.1442],
          [-0.1451,  0.0332,  0.1956,  ...,  0.1023,  0.3039,  0.4945],
          [-0.1999, -0.3362, -0.0172,  ...,  0.0112, -0.2382,  0.1717]]],


        ...,


        [[[ 0.4633,  0.4409,  0.3303,  ...,  0.0458,  0.0583,  0.4250],
          [ 0.3383,  0.4639,  0.2358,  ...,  0.0617, -0.0159, -0.0606],
          [-0.1809,  0.1173,  0.2089,  ...,  0.2622,  0.1243, -0.4551],
          ...,
          [ 0.0029,  0.1240, -0.0219,  ...,  0.0658, -0.0067, -0.1605],
          [-0.0183, -0.1157, -0.0749,  ...,  0.0024,  0.1651,  0.0466],
          [-0.1955, -0.4789, -0.3602,  ..., -0.2965, -0.3382, -0.2267]],

         [[ 0.0044,  0.3899,  0.1008,  ...,  0.1729,  0.4637,  0.2246],
          [-0.3441,  0.1668,  0.0935,  ..., -0.2428, -0.1494, -0.1992],
          [-0.3785, -0.0261,  0.2396,  ..., -0.1603, -0.4214, -0.4830],
          ...,
          [-0.3094, -0.2421, -0.2072,  ...,  0.2285, -0.1938, -0.5525],
          [-0.3401, -0.3600, -0.1174,  ...,  0.2147, -0.2964, -0.4356],
          [-0.4949, -0.1711,  0.2524,  ...,  0.2851, -0.1380, -0.3408]]],


        [[[-0.1041, -0.0755, -0.1475,  ...,  0.1240, -0.2099, -0.5254],
          [-0.0170,  0.1133, -0.0668,  ..., -0.1782, -0.3254, -0.1578],
          [ 0.2353,  0.3047, -0.0269,  ..., -0.2160, -0.3028, -0.2282],
          ...,
          [ 0.0365,  0.0452, -0.1412,  ..., -0.1924, -0.0548,  0.0496],
          [-0.3746,  0.0661,  0.0196,  ..., -0.2188, -0.1469, -0.4497],
          [-0.4376, -0.1909,  0.2579,  ..., -0.0526, -0.1623, -0.4469]],

         [[ 0.4931,  0.2289,  0.1806,  ...,  0.2248, -0.1659, -0.3799],
          [ 0.0854,  0.1202, -0.1176,  ...,  0.4808,  0.0755, -0.5296],
          [-0.3312, -0.1515, -0.0924,  ...,  0.2880, -0.0618, -0.3404],
          ...,
          [-0.2356, -0.4267, -0.4414,  ..., -0.0294, -0.0441,  0.1600],
          [-0.3379, -0.3215, -0.2518,  ...,  0.0179,  0.2293,  0.2536],
          [-0.2123,  0.0429,  0.2852,  ..., -0.0082,  0.0947, -0.3038]]],


        [[[ 0.2350,  0.3705,  0.5350,  ..., -0.0446,  0.0091, -0.1348],
          [-0.0670,  0.0151,  0.0162,  ...,  0.0761, -0.1111,  0.0959],
          [-0.1329, -0.3112, -0.4801,  ...,  0.1421,  0.0977,  0.0970],
          ...,
          [-0.2062,  0.0677, -0.0120,  ...,  0.1778,  0.0769,  0.0904],
          [-0.5909,  0.2089,  0.3547,  ..., -0.0190, -0.0228,  0.1074],
          [-0.4350,  0.0709,  0.1053,  ..., -0.3720, -0.4134, -0.1037]],

         [[-0.1472, -0.4014,  0.0142,  ...,  0.5863,  0.2626, -0.3453],
          [-0.2060,  0.0372,  0.1863,  ..., -0.1859, -0.1122, -0.1772],
          [-0.1988, -0.0357, -0.0982,  ..., -0.3419, -0.1870, -0.2183],
          ...,
          [ 0.0358,  0.0621, -0.1778,  ..., -0.1458, -0.1688, -0.3007],
          [ 0.2914,  0.0866, -0.0729,  ...,  0.1220,  0.1430, -0.0516],
          [-0.3250, -0.3125, -0.2075,  ...,  0.3860,  0.1736,  0.3029]]]])
DESIRED: (shape=torch.Size([11408507, 2, 13, 13]), dtype=torch.float32)
tensor([[[[ 0.1817, -0.2024,  0.0261,  ..., -0.1638, -0.2903,  0.2559],
          [ 0.3550,  0.4306,  0.1065,  ...,  0.1082, -0.0851,  0.2025],
          [ 0.1961,  0.1618, -0.1856,  ...,  0.0136,  0.1264,  0.1428],
          ...,
          [ 0.0993, -0.0140, -0.1069,  ...,  0.1410,  0.3794,  0.3491],
          [ 0.1066, -0.1658, -0.1050,  ...,  0.0365,  0.1143,  0.4467],
          [-0.0765, -0.2787, -0.2074,  ..., -0.2118, -0.2814,  0.2375]],

         [[ 0.2085, -0.1480,  0.0294,  ..., -0.0424, -0.2972,  0.4479],
          [-0.3264, -0.4260, -0.1904,  ...,  0.1737, -0.1963,  0.2337],
          [-0.1125, -0.4954, -0.4675,  ...,  0.0408,  0.0147,  0.3470],
          ...,
          [-0.0237, -0.0676, -0.0130,  ...,  0.3494,  0.3087, -0.3404],
          [ 0.5338,  0.4864,  0.3716,  ...,  0.3402,  0.1253, -0.2721],
          [ 0.3628,  0.2549, -0.1153,  ..., -0.0444, -0.0276, -0.0847]]],


        [[[ 0.2183, -0.4246, -0.0099,  ..., -0.2774, -0.4814, -0.2840],
          [ 0.1601, -0.3395, -0.3188,  ...,  0.4784,  0.1664, -0.1288],
          [ 0.3983,  0.1746, -0.2515,  ...,  0.2287,  0.0421, -0.2485],
          ...,
          [ 0.3925,  0.6351,  0.0625,  ...,  0.5039,  0.0705, -0.2817],
          [ 0.2935,  0.3103, -0.0720,  ...,  0.1720, -0.1357, -0.0752],
          [ 0.3053, -0.1997, -0.1516,  ..., -0.2935, -0.3602, -0.1558]],

         [[ 0.0739, -0.0423,  0.1728,  ...,  0.0987, -0.0610, -0.3666],
          [-0.0295,  0.0481,  0.1268,  ..., -0.0699, -0.1101, -0.4262],
          [ 0.1496,  0.0323, -0.1989,  ...,  0.1912,  0.1694, -0.0538],
          ...,
          [-0.3154, -0.1563, -0.0861,  ...,  0.0791,  0.0911, -0.1292],
          [-0.1367,  0.0868, -0.0848,  ...,  0.1420,  0.2931, -0.3148],
          [ 0.3856,  0.0050,  0.0182,  ...,  0.0120, -0.3098,  0.1464]]],


        [[[-0.2338, -0.0819,  0.2152,  ..., -0.0881, -0.3579,  0.0010],
          [-0.3163,  0.1017,  0.2395,  ..., -0.3496, -0.2586, -0.1355],
          [-0.1450,  0.4380,  0.1954,  ...,  0.0669,  0.1883,  0.0966],
          ...,
          [-0.0464, -0.0752, -0.2251,  ...,  0.1016,  0.0870,  0.1948],
          [ 0.1462, -0.0924, -0.3964,  ...,  0.2804,  0.2507, -0.3778],
          [-0.4666,  0.2230,  0.1470,  ...,  0.2336,  0.2474, -0.3963]],

         [[-0.3050,  0.2295,  0.5020,  ...,  0.0983, -0.0585, -0.1089],
          [-0.5357, -0.1412,  0.3295,  ..., -0.2127, -0.1601, -0.0488],
          [-0.0707,  0.1659,  0.0629,  ..., -0.3503, -0.0752, -0.0630],
          ...,
          [ 0.2231,  0.5429,  0.2172,  ...,  0.1152,  0.5554,  0.0928],
          [-0.1082,  0.1205,  0.2207,  ...,  0.0923,  0.3475,  0.5042],
          [-0.1942, -0.3018,  0.0558,  ...,  0.0920, -0.2306,  0.1637]]],


        ...,


        [[[ 0.4425,  0.4322,  0.3076,  ...,  0.0829,  0.0151,  0.3519],
          [ 0.3156,  0.4647,  0.1773,  ...,  0.0705, -0.0046, -0.1206],
          [-0.2197,  0.1037,  0.2044,  ...,  0.2495,  0.2320, -0.4107],
          ...,
          [-0.0053,  0.1339, -0.0835,  ...,  0.0495, -0.0634, -0.2084],
          [-0.0007, -0.0720, -0.0286,  ..., -0.0028,  0.2314,  0.0829],
          [-0.1994, -0.4718, -0.2952,  ..., -0.2576, -0.3094, -0.2140]],

         [[ 0.0033,  0.4050,  0.0236,  ...,  0.0800,  0.4258,  0.1987],
          [-0.3436,  0.1934,  0.0370,  ..., -0.3031, -0.2277, -0.2568],
          [-0.3193,  0.0164,  0.2780,  ..., -0.0743, -0.3788, -0.4693],
          ...,
          [-0.3143, -0.1911, -0.1890,  ...,  0.2607, -0.1025, -0.5234],
          [-0.3203, -0.3783, -0.1274,  ...,  0.2701, -0.2798, -0.4305],
          [-0.4497, -0.1355,  0.2484,  ...,  0.3233, -0.1124, -0.3270]]],


        [[[-0.0916, -0.0558, -0.1551,  ...,  0.1166, -0.1749, -0.4678],
          [ 0.0096,  0.1519, -0.0982,  ..., -0.1735, -0.3603, -0.1318],
          [ 0.2627,  0.3081, -0.0864,  ..., -0.1752, -0.2871, -0.2779],
          ...,
          [ 0.0819,  0.0035, -0.1576,  ..., -0.1732, -0.0665,  0.1082],
          [-0.3176,  0.1460, -0.0780,  ..., -0.2858, -0.1085, -0.4103],
          [-0.3980, -0.1263,  0.2690,  ..., -0.0831, -0.1221, -0.4141]],

         [[ 0.4320,  0.1848,  0.1733,  ...,  0.2741, -0.1062, -0.3621],
          [ 0.0256,  0.0998, -0.1990,  ...,  0.4997,  0.1883, -0.4810],
          [-0.3344, -0.1558, -0.0572,  ...,  0.2668, -0.0738, -0.2789],
          ...,
          [-0.2243, -0.4297, -0.3991,  ..., -0.0101, -0.1200,  0.0738],
          [-0.3488, -0.3658, -0.3159,  ..., -0.0044,  0.2199,  0.3068],
          [-0.1981,  0.0468,  0.2483,  ..., -0.0600,  0.1409, -0.2315]]],


        [[[ 0.2210,  0.3544,  0.4969,  ..., -0.0538,  0.0160, -0.1060],
          [-0.0997, -0.0297, -0.0729,  ...,  0.1461, -0.1407,  0.1076],
          [-0.1305, -0.3521, -0.5103,  ...,  0.1361,  0.1504,  0.0779],
          ...,
          [-0.1232,  0.0476, -0.1275,  ...,  0.1914,  0.0512,  0.0672],
          [-0.5270,  0.3216,  0.3134,  ...,  0.0413,  0.0150,  0.1227],
          [-0.3980,  0.1427,  0.0735,  ..., -0.3070, -0.4111, -0.1094]],

         [[-0.1629, -0.3792,  0.1044,  ...,  0.5084,  0.3062, -0.2821],
          [-0.1853,  0.1265,  0.1836,  ..., -0.3027, -0.1611, -0.1602],
          [-0.1866, -0.0825, -0.1796,  ..., -0.3168, -0.1544, -0.2357],
          ...,
          [-0.0634,  0.0197, -0.2538,  ..., -0.1625, -0.2003, -0.2917],
          [ 0.3357,  0.1011, -0.0646,  ...,  0.0538,  0.1548, -0.0846],
          [-0.2769, -0.2775, -0.1847,  ...,  0.4029,  0.1589,  0.2633]]]])

2025-07-09 13:37:14.722408 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[13,13,], scale_factor=None, mode="bicubic", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[13,13,], scale_factor=None, mode="bicubic", align_corners=True, align_mode=0, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2656314227 / 3856075366 (68.9%)
Greatest absolute difference: 0.17931878566741943 at index (3667525, 0, 1, 1) (up to 0.01 allowed)
Greatest relative difference: 2257523712.0 at index (8290647, 0, 10, 5) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 13, 13]), dtype=torch.float32)
tensor([[[[-0.2249, -0.3218, -0.3958,  ...,  0.3258,  0.1462, -0.2132],
          [ 0.0471, -0.0291,  0.0763,  ...,  0.0881, -0.1494, -0.3460],
          [ 0.1605,  0.2823,  0.4473,  ...,  0.0486, -0.3269, -0.5365],
          ...,
          [ 0.2663,  0.2135, -0.1458,  ...,  0.0039,  0.2680, -0.0254],
          [ 0.2945,  0.3258,  0.0061,  ..., -0.0330,  0.0884, -0.3882],
          [ 0.0030,  0.0176,  0.1111,  ..., -0.2078, -0.0375, -0.1529]],

         [[-0.1402, -0.1348,  0.2855,  ..., -0.0817, -0.3069,  0.4416],
          [ 0.1810,  0.0424,  0.2482,  ..., -0.1568, -0.3144,  0.4400],
          [ 0.2003,  0.1721,  0.2639,  ..., -0.1520,  0.0054,  0.5393],
          ...,
          [ 0.1288,  0.2828,  0.2119,  ..., -0.0719, -0.2769,  0.1083],
          [-0.0027,  0.1173, -0.0458,  ..., -0.3128, -0.3573,  0.0725],
          [ 0.4051,  0.4228,  0.3410,  ..., -0.0367,  0.0384, -0.0654]]],


        [[[-0.3008,  0.2336,  0.5377,  ..., -0.5100, -0.2694,  0.3699],
          [-0.0170,  0.3298,  0.5889,  ..., -0.1756, -0.4261,  0.0023],
          [-0.2011,  0.4090,  0.3513,  ..., -0.2331, -0.2151, -0.0588],
          ...,
          [ 0.2433,  0.3022,  0.2832,  ...,  0.1974,  0.2107, -0.4454],
          [ 0.2988,  0.0969,  0.0061,  ...,  0.1987,  0.2335, -0.1405],
          [ 0.4629,  0.3124,  0.1468,  ..., -0.2521, -0.4295,  0.0992]],

         [[-0.4225, -0.1425,  0.1837,  ..., -0.1103, -0.0386, -0.3825],
          [-0.3046, -0.2593, -0.2684,  ..., -0.0284,  0.1094, -0.0982],
          [-0.4329, -0.1792, -0.4050,  ..., -0.1292, -0.1584, -0.0879],
          ...,
          [ 0.2734, -0.1241, -0.0730,  ..., -0.0973, -0.0454, -0.1049],
          [ 0.4517, -0.0484, -0.1426,  ..., -0.0428,  0.3082,  0.3178],
          [ 0.4048,  0.1564,  0.0825,  ..., -0.0712,  0.1557,  0.4675]]],


        [[[ 0.3249, -0.1347, -0.0385,  ...,  0.2772,  0.1584, -0.3550],
          [-0.2346, -0.5183, -0.1606,  ...,  0.1183, -0.1583,  0.0587],
          [-0.1644, -0.0941,  0.0718,  ..., -0.0041,  0.0563,  0.0134],
          ...,
          [-0.2590, -0.0841, -0.1331,  ..., -0.1476, -0.0656, -0.0247],
          [-0.4340, -0.4641, -0.1683,  ..., -0.2257, -0.2057, -0.2328],
          [-0.4428, -0.0121,  0.2178,  ...,  0.0631, -0.1206, -0.1669]],

         [[ 0.0575,  0.0396,  0.1425,  ...,  0.1413, -0.0142, -0.2228],
          [-0.2080, -0.2607,  0.0502,  ..., -0.1196, -0.1949, -0.4164],
          [ 0.1845, -0.2929, -0.2649,  ..., -0.2189,  0.0855, -0.0047],
          ...,
          [ 0.4642, -0.2342, -0.2903,  ...,  0.1455, -0.1474,  0.1529],
          [ 0.1942, -0.2039, -0.3610,  ..., -0.0147, -0.4274,  0.0856],
          [-0.0254, -0.4315, -0.3424,  ..., -0.1387, -0.2743,  0.0816]]],


        ...,


        [[[ 0.2641, -0.3490, -0.4111,  ...,  0.0074, -0.3042,  0.4330],
          [ 0.4148, -0.1162, -0.2130,  ..., -0.1707, -0.1761, -0.0570],
          [ 0.0159, -0.2517, -0.2176,  ..., -0.0932, -0.0889, -0.0594],
          ...,
          [-0.3507,  0.0051,  0.1617,  ..., -0.2262, -0.2659,  0.0221],
          [-0.2617, -0.2552, -0.1650,  ..., -0.4209, -0.2820,  0.3835],
          [ 0.1095, -0.0883,  0.0919,  ..., -0.3097, -0.3509, -0.0183]],

         [[-0.1005, -0.2696, -0.4690,  ..., -0.0936, -0.4358,  0.1753],
          [-0.4918,  0.0480, -0.1722,  ..., -0.0431,  0.2530,  0.0995],
          [-0.0306,  0.1029, -0.1249,  ..., -0.0801,  0.0204,  0.2694],
          ...,
          [ 0.0434,  0.2394,  0.1701,  ..., -0.0423,  0.0134,  0.2364],
          [-0.1916,  0.0337,  0.0465,  ..., -0.0092,  0.0806,  0.0871],
          [-0.2758,  0.2339,  0.0696,  ...,  0.1202,  0.1462,  0.2991]]],


        [[[-0.0911, -0.2821, -0.1696,  ...,  0.1479,  0.1341,  0.1684],
          [-0.1100,  0.1802, -0.0167,  ..., -0.1156, -0.0487, -0.1767],
          [ 0.0068,  0.4990,  0.1579,  ..., -0.0115, -0.1141, -0.1513],
          ...,
          [-0.3458,  0.2945,  0.0632,  ...,  0.3620,  0.0326, -0.0681],
          [-0.3795,  0.1670,  0.2388,  ...,  0.1462,  0.0832,  0.2005],
          [-0.4082, -0.4425,  0.0297,  ..., -0.4778, -0.3032,  0.4202]],

         [[-0.1606,  0.0503,  0.3920,  ...,  0.2440, -0.0834, -0.2650],
          [ 0.0388, -0.1498, -0.1075,  ...,  0.1056, -0.0854, -0.1578],
          [ 0.1984,  0.2003,  0.0532,  ..., -0.0505,  0.0572, -0.0397],
          ...,
          [ 0.4526,  0.0243, -0.3367,  ..., -0.0675, -0.4265, -0.1110],
          [ 0.3610,  0.1642,  0.0220,  ...,  0.0583, -0.4471, -0.3842],
          [-0.0803,  0.1289,  0.3076,  ...,  0.2894, -0.0686,  0.0088]]],


        [[[ 0.0562,  0.0782,  0.0368,  ..., -0.0521,  0.2426, -0.0550],
          [ 0.2433, -0.0868,  0.1281,  ...,  0.0221,  0.3623, -0.2134],
          [ 0.4843, -0.0144,  0.1823,  ...,  0.1162,  0.2169, -0.2814],
          ...,
          [ 0.0539, -0.1648, -0.0608,  ..., -0.3479,  0.0523, -0.0782],
          [-0.5354, -0.1179,  0.3200,  ..., -0.2409,  0.0176,  0.3029],
          [-0.3921,  0.0383,  0.3698,  ...,  0.0054, -0.4052, -0.0863]],

         [[ 0.2519,  0.0089,  0.1829,  ..., -0.0678,  0.0594, -0.1342],
          [ 0.0517,  0.1447,  0.4093,  ..., -0.0173,  0.3146, -0.2979],
          [ 0.1235,  0.3442,  0.1987,  ...,  0.0885,  0.4142,  0.0045],
          ...,
          [ 0.1086,  0.0541, -0.1432,  ...,  0.1982,  0.1317, -0.0929],
          [ 0.4811, -0.1499, -0.2697,  ..., -0.0549, -0.0049,  0.2678],
          [ 0.3276,  0.1240, -0.2120,  ..., -0.0336, -0.4663,  0.0917]]]])
DESIRED: (shape=torch.Size([11408507, 2, 13, 13]), dtype=torch.float32)
tensor([[[[-0.2392, -0.3354, -0.4337,  ...,  0.3464,  0.1245, -0.2329],
          [ 0.0179, -0.0638, -0.0085,  ...,  0.1033, -0.1425, -0.3480],
          [ 0.1541,  0.2450,  0.4131,  ...,  0.0039, -0.3572, -0.5383],
          ...,
          [ 0.2960,  0.2633, -0.0840,  ...,  0.0664,  0.2176, -0.1139],
          [ 0.2624,  0.2956,  0.0628,  ..., -0.0077,  0.0199, -0.4050],
          [-0.0226, -0.0112,  0.1060,  ..., -0.1925, -0.0561, -0.1440]],

         [[-0.1734, -0.1643,  0.2296,  ..., -0.1604, -0.2264,  0.5000],
          [ 0.1488,  0.0253,  0.2130,  ..., -0.2244, -0.2363,  0.4992],
          [ 0.2238,  0.1721,  0.2415,  ..., -0.1787,  0.0319,  0.5703],
          ...,
          [ 0.0705,  0.2213,  0.1704,  ..., -0.1766, -0.2692,  0.1435],
          [ 0.0362,  0.1417,  0.0243,  ..., -0.3145, -0.2705,  0.0853],
          [ 0.4378,  0.4483,  0.3795,  ...,  0.0049,  0.0538, -0.0877]]],


        [[[-0.3702,  0.1520,  0.5322,  ..., -0.5446, -0.1743,  0.4574],
          [-0.0790,  0.2676,  0.5772,  ..., -0.2740, -0.3651,  0.0822],
          [-0.2008,  0.3381,  0.4243,  ..., -0.2187, -0.2328, -0.0612],
          ...,
          [ 0.2354,  0.2566,  0.2390,  ...,  0.2815,  0.1800, -0.4746],
          [ 0.3405,  0.1469,  0.0135,  ...,  0.1628,  0.1238, -0.1254],
          [ 0.4932,  0.3521,  0.1656,  ..., -0.3553, -0.4179,  0.1684]],

         [[-0.4601, -0.1788,  0.1938,  ..., -0.0793, -0.0905, -0.4356],
          [-0.3241, -0.2586, -0.2096,  ..., -0.0021,  0.0742, -0.1505],
          [-0.4193, -0.2180, -0.3703,  ..., -0.1141, -0.1046, -0.0643],
          ...,
          [ 0.3334, -0.0772, -0.1325,  ..., -0.0688,  0.0051, -0.0644],
          [ 0.4919,  0.0368, -0.1366,  ..., -0.0012,  0.3151,  0.3562],
          [ 0.4233,  0.2029,  0.0951,  ..., -0.0671,  0.1950,  0.5162]]],


        [[[ 0.4083, -0.0521, -0.0772,  ...,  0.3176,  0.1097, -0.4366],
          [-0.1482, -0.4547, -0.2192,  ...,  0.0954, -0.1168,  0.0180],
          [-0.2176, -0.1876,  0.0056,  ..., -0.0010,  0.0100,  0.0518],
          ...,
          [-0.2910, -0.1798, -0.1571,  ..., -0.1674, -0.0852, -0.0540],
          [-0.4460, -0.4322, -0.1600,  ..., -0.1914, -0.2034, -0.2333],
          [-0.4875, -0.0433,  0.2521,  ...,  0.0602, -0.1254, -0.1701]],

         [[ 0.0743,  0.0629,  0.1417,  ...,  0.1542, -0.0317, -0.2308],
          [-0.1862, -0.2277,  0.0279,  ..., -0.0845, -0.2098, -0.4232],
          [ 0.1442, -0.2619, -0.2675,  ..., -0.1802,  0.0295, -0.0796],
          ...,
          [ 0.4953, -0.1328, -0.3326,  ...,  0.0634, -0.1611,  0.1693],
          [ 0.1956, -0.1812, -0.3707,  ..., -0.1220, -0.3672,  0.1190],
          [-0.0146, -0.4049, -0.3846,  ..., -0.1917, -0.2268,  0.1072]]],


        ...,


        [[[ 0.3078, -0.2920, -0.4604,  ..., -0.0811, -0.2301,  0.5333],
          [ 0.4535, -0.0729, -0.2627,  ..., -0.1713, -0.1733,  0.0163],
          [ 0.1046, -0.1803, -0.2254,  ..., -0.1103, -0.0898, -0.0922],
          ...,
          [-0.3913, -0.0835,  0.1048,  ..., -0.2834, -0.2197,  0.1276],
          [-0.2154, -0.2433, -0.1577,  ..., -0.4407, -0.2124,  0.3985],
          [ 0.1562, -0.0567,  0.0674,  ..., -0.3327, -0.3211, -0.0216]],

         [[-0.0543, -0.2664, -0.4804,  ..., -0.1945, -0.4213,  0.2283],
          [-0.4918, -0.0420, -0.1481,  ..., -0.0135,  0.1739,  0.1013],
          [-0.1380,  0.0953, -0.0612,  ..., -0.0589,  0.1226,  0.2551],
          ...,
          [ 0.0052,  0.1781,  0.1709,  ..., -0.0529,  0.0412,  0.2185],
          [-0.2307,  0.0200,  0.0656,  ...,  0.0162,  0.0967,  0.1110],
          [-0.3277,  0.1880,  0.1312,  ...,  0.1214,  0.1740,  0.3284]]],


        [[[-0.0745, -0.2995, -0.2189,  ...,  0.1622,  0.1553,  0.1994],
          [-0.1290,  0.0935, -0.0013,  ..., -0.0691, -0.0375, -0.1460],
          [-0.0443,  0.4374,  0.2235,  ..., -0.0486, -0.1284, -0.1833],
          ...,
          [-0.3977,  0.2457,  0.1785,  ...,  0.3403,  0.0466, -0.0600],
          [-0.4262,  0.0297,  0.2339,  ...,  0.0481,  0.0606,  0.2500],
          [-0.4122, -0.4993, -0.0578,  ..., -0.5565, -0.2378,  0.5068]],

         [[-0.2034,  0.0245,  0.3967,  ...,  0.2242, -0.1172, -0.2958],
          [ 0.0201, -0.1204, -0.0716,  ...,  0.1030, -0.1054, -0.1847],
          [ 0.1985,  0.1492,  0.0106,  ..., -0.0234,  0.0309, -0.0521],
          ...,
          [ 0.5094,  0.1057, -0.2877,  ..., -0.1390, -0.4313, -0.1567],
          [ 0.3189,  0.1834,  0.0680,  ...,  0.0208, -0.4105, -0.3468],
          [-0.1400,  0.0930,  0.3238,  ...,  0.2571, -0.0381,  0.0392]]],


        [[[ 0.0356,  0.0874,  0.0395,  ..., -0.0010,  0.2105, -0.0601],
          [ 0.2374, -0.0385,  0.0700,  ...,  0.0955,  0.2964, -0.2335],
          [ 0.5002,  0.0173,  0.1174,  ...,  0.1626,  0.1842, -0.3235],
          ...,
          [-0.0272, -0.1583, -0.0452,  ..., -0.2871,  0.0786,  0.0007],
          [-0.5749, -0.1611,  0.3090,  ..., -0.2058,  0.0070,  0.2922],
          [-0.4282, -0.0103,  0.3669,  ..., -0.0580, -0.4141, -0.0933]],

         [[ 0.2869,  0.0229,  0.1270,  ..., -0.0474,  0.0212, -0.1363],
          [ 0.0665,  0.1116,  0.3564,  ...,  0.0546,  0.2219, -0.3287],
          [ 0.0826,  0.3005,  0.2672,  ...,  0.1558,  0.3718, -0.0814],
          ...,
          [ 0.1840,  0.0322, -0.1503,  ...,  0.1717,  0.1246, -0.0459],
          [ 0.5239, -0.0476, -0.2899,  ..., -0.0833, -0.0250,  0.2826],
          [ 0.3390,  0.1741, -0.1741,  ..., -0.1384, -0.4488,  0.1234]]]])

2025-07-09 13:37:15.867660 GPU 4 150702 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[13,13,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:37:29.790360 GPU 7 151220 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[13,13,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


2025-07-09 13:37:32.561292 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[13,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[13,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 467598653 / 593242364 (78.8%)
Greatest absolute difference: 0.9999438524246216 at index (4765091, 1, 0, 1) (up to 0.01 allowed)
Greatest relative difference: 35959852.0 at index (9006894, 1, 9, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 13, 2]), dtype=torch.float32)
tensor([[[[-0.2811, -0.4813],
          [-0.3616, -0.4456],
          [ 0.0319, -0.1418],
          ...,
          [-0.0414, -0.2158],
          [-0.2320,  0.0959],
          [-0.3140,  0.4891]],

         [[-0.1915,  0.4496],
          [-0.0438,  0.1732],
          [ 0.1570, -0.0569],
          ...,
          [-0.1789,  0.0299],
          [-0.1039, -0.1354],
          [-0.1765, -0.3558]]],


        [[[ 0.0533,  0.0650],
          [ 0.3376,  0.0604],
          [ 0.2275,  0.2712],
          ...,
          [ 0.0147,  0.0133],
          [ 0.1477,  0.2018],
          [ 0.1076,  0.4029]],

         [[ 0.0910, -0.3552],
          [-0.0053,  0.2603],
          [-0.0352,  0.1408],
          ...,
          [ 0.2687, -0.1372],
          [ 0.3934,  0.0166],
          [ 0.4145,  0.4352]]],


        [[[ 0.2202,  0.0234],
          [-0.1036, -0.0334],
          [-0.2461,  0.1220],
          ...,
          [ 0.2888, -0.0163],
          [ 0.1842,  0.1235],
          [ 0.0837,  0.2216]],

         [[ 0.1057, -0.3405],
          [ 0.4071, -0.2096],
          [ 0.4456, -0.0905],
          ...,
          [ 0.1924, -0.2655],
          [ 0.2668, -0.0785],
          [ 0.2353,  0.4019]]],


        ...,


        [[[-0.1108,  0.0963],
          [-0.2370,  0.1672],
          [-0.0261,  0.2593],
          ...,
          [-0.0927,  0.1326],
          [-0.2200, -0.0474],
          [-0.1225, -0.2100]],

         [[ 0.4300,  0.1845],
          [ 0.2926, -0.1256],
          [ 0.2195, -0.0369],
          ...,
          [-0.0008,  0.2323],
          [ 0.2272, -0.0354],
          [ 0.3667, -0.3890]]],


        [[[-0.2385,  0.3606],
          [ 0.0177,  0.1683],
          [-0.1270, -0.0234],
          ...,
          [-0.2759, -0.0478],
          [-0.2828,  0.1794],
          [-0.3901,  0.2929]],

         [[-0.0583,  0.0837],
          [ 0.1887, -0.2591],
          [-0.1309, -0.0420],
          ...,
          [ 0.4516, -0.0240],
          [ 0.3690, -0.2506],
          [ 0.2782, -0.3928]]],


        [[[-0.2620, -0.4232],
          [ 0.0135, -0.4107],
          [ 0.2174,  0.0296],
          ...,
          [ 0.0129, -0.0110],
          [ 0.2303, -0.1241],
          [ 0.3441, -0.3600]],

         [[ 0.4648, -0.4322],
          [-0.0258,  0.1598],
          [ 0.1309,  0.2876],
          ...,
          [ 0.3870,  0.0406],
          [ 0.3691,  0.2066],
          [ 0.3276,  0.2853]]]])
DESIRED: (shape=torch.Size([11408507, 2, 13, 2]), dtype=torch.float32)
tensor([[[[-0.2811,  0.1776],
          [-0.3595,  0.0750],
          [ 0.0020,  0.2281],
          ...,
          [ 0.0338, -0.3106],
          [-0.1998, -0.1853],
          [-0.3140, -0.3490]],

         [[-0.1915, -0.3065],
          [-0.0475, -0.2007],
          [ 0.1458,  0.0626],
          ...,
          [-0.2646,  0.3928],
          [-0.0754,  0.3431],
          [-0.1765,  0.0765]]],


        [[[ 0.0533,  0.3809],
          [ 0.3305, -0.0529],
          [ 0.2415,  0.0362],
          ...,
          [-0.0900, -0.4299],
          [ 0.1635, -0.3483],
          [ 0.1076, -0.1510]],

         [[ 0.0910, -0.3851],
          [-0.0029, -0.1491],
          [-0.0351, -0.1776],
          ...,
          [ 0.2021,  0.3154],
          [ 0.3850,  0.3362],
          [ 0.4145,  0.0479]]],


        [[[ 0.2202, -0.3835],
          [-0.0955,  0.0390],
          [-0.2429,  0.1697],
          ...,
          [ 0.3004, -0.0963],
          [ 0.2236,  0.2112],
          [ 0.0837,  0.2126]],

         [[ 0.1057, -0.2008],
          [ 0.3996, -0.3037],
          [ 0.4493, -0.3318],
          ...,
          [ 0.1291,  0.1925],
          [ 0.2792,  0.3738],
          [ 0.2353,  0.4719]]],


        ...,


        [[[-0.1108, -0.3546],
          [-0.2338, -0.3484],
          [-0.0439, -0.1358],
          ...,
          [ 0.0391,  0.2775],
          [-0.2583,  0.0421],
          [-0.1225, -0.4115]],

         [[ 0.4300, -0.3375],
          [ 0.2960,  0.1550],
          [ 0.2218,  0.1076],
          ...,
          [-0.0686, -0.0431],
          [ 0.1724,  0.2336],
          [ 0.3667,  0.0960]]],


        [[[-0.2385, -0.0029],
          [ 0.0113, -0.1244],
          [-0.1112,  0.1536],
          ...,
          [-0.3291, -0.3111],
          [-0.2406, -0.2172],
          [-0.3901, -0.1189]],

         [[-0.0583,  0.0756],
          [ 0.1825, -0.3282],
          [-0.1028, -0.0115],
          ...,
          [ 0.4547,  0.3009],
          [ 0.4046,  0.3344],
          [ 0.2782, -0.0105]]],


        [[[-0.2620, -0.4032],
          [ 0.0066, -0.0641],
          [ 0.2087,  0.2304],
          ...,
          [-0.0620, -0.1038],
          [ 0.1855, -0.1931],
          [ 0.3441, -0.4545]],

         [[ 0.4648,  0.3652],
          [-0.0136,  0.0772],
          [ 0.1092, -0.1446],
          ...,
          [ 0.3760, -0.0569],
          [ 0.3854, -0.1006],
          [ 0.3276,  0.2061]]]])

2025-07-09 13:37:55.433567 GPU 6 151383 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[13,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[13,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 577032466 / 593242364 (97.3%)
Greatest absolute difference: 0.9999892711639404 at index (5954004, 1, 12, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (1183921, 0, 6, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 13, 2]), dtype=torch.float32)
tensor([[[[ 0.0063, -0.2606],
          [-0.2460, -0.0931],
          [-0.3031, -0.1936],
          ...,
          [ 0.1506,  0.0575],
          [ 0.1150,  0.3921],
          [ 0.2946,  0.2296]],

         [[-0.1572, -0.4420],
          [ 0.2152, -0.3153],
          [-0.0188,  0.0771],
          ...,
          [-0.1999, -0.1793],
          [-0.1548,  0.0510],
          [ 0.1240,  0.4346]]],


        [[[-0.3044,  0.4089],
          [-0.4223, -0.1671],
          [-0.1086, -0.2513],
          ...,
          [ 0.2326,  0.2223],
          [ 0.0713,  0.1045],
          [-0.3155,  0.1928]],

         [[-0.1173, -0.4827],
          [ 0.0198,  0.2295],
          [ 0.1702,  0.4483],
          ...,
          [-0.2300,  0.0526],
          [-0.0994,  0.1492],
          [-0.1170, -0.2779]]],


        [[[-0.2070,  0.3923],
          [-0.1870, -0.1287],
          [-0.1724,  0.0495],
          ...,
          [ 0.1206,  0.0198],
          [ 0.0367, -0.1081],
          [ 0.4806,  0.3572]],

         [[-0.4234, -0.3281],
          [-0.0238,  0.1886],
          [-0.0744,  0.3026],
          ...,
          [-0.1221,  0.1459],
          [-0.3697,  0.3473],
          [-0.4576,  0.1345]]],


        ...,


        [[[-0.0303, -0.3179],
          [ 0.0990, -0.3813],
          [-0.0481, -0.1634],
          ...,
          [ 0.0549, -0.4698],
          [ 0.1053, -0.4385],
          [-0.1852, -0.2857]],

         [[ 0.3295,  0.0309],
          [-0.2193,  0.0051],
          [-0.1499, -0.0724],
          ...,
          [ 0.0329,  0.1239],
          [ 0.1562,  0.1225],
          [-0.0812,  0.3137]]],


        [[[ 0.1257,  0.3565],
          [-0.0833,  0.4433],
          [ 0.1338,  0.1162],
          ...,
          [-0.1147, -0.0293],
          [-0.0620, -0.1703],
          [-0.3111,  0.3526]],

         [[ 0.3149,  0.1997],
          [ 0.3199,  0.3483],
          [ 0.0803,  0.0524],
          ...,
          [ 0.2004,  0.0760],
          [ 0.1237,  0.3418],
          [-0.4431,  0.1456]]],


        [[[-0.4009, -0.1110],
          [ 0.1085,  0.0251],
          [ 0.1117, -0.0710],
          ...,
          [-0.4139, -0.1573],
          [-0.2514,  0.0966],
          [ 0.3406,  0.3243]],

         [[ 0.2741, -0.2211],
          [-0.0825,  0.1561],
          [-0.1248,  0.1129],
          ...,
          [ 0.1262,  0.2919],
          [ 0.1720,  0.3228],
          [-0.2868, -0.0026]]]])
DESIRED: (shape=torch.Size([11408507, 2, 13, 2]), dtype=torch.float32)
tensor([[[[ 0.3993, -0.1091],
          [ 0.2640, -0.0801],
          [ 0.2362,  0.1150],
          ...,
          [ 0.4428,  0.3448],
          [ 0.4303,  0.0522],
          [ 0.3425, -0.4622]],

         [[ 0.2177,  0.3237],
          [ 0.3118, -0.0987],
          [ 0.2960, -0.0411],
          ...,
          [ 0.1080,  0.1069],
          [ 0.4042, -0.0924],
          [ 0.3690, -0.2031]]],


        [[[-0.1100,  0.2036],
          [-0.1016,  0.0695],
          [-0.2630, -0.1899],
          ...,
          [ 0.0678, -0.3046],
          [-0.0420, -0.4196],
          [-0.0419, -0.3607]],

         [[ 0.3475,  0.0637],
          [ 0.1292, -0.1679],
          [ 0.0694, -0.2570],
          ...,
          [ 0.2520, -0.2665],
          [ 0.3781, -0.0376],
          [ 0.3761,  0.2163]]],


        [[[ 0.4357,  0.3608],
          [ 0.1182, -0.0815],
          [ 0.1006, -0.3319],
          ...,
          [-0.1572,  0.0801],
          [ 0.0435,  0.3497],
          [ 0.2496,  0.1623]],

         [[-0.0139, -0.0450],
          [-0.0600, -0.0906],
          [ 0.0494, -0.0641],
          ...,
          [-0.0120,  0.0639],
          [-0.1719,  0.1056],
          [-0.3937, -0.2057]]],


        ...,


        [[[-0.0390, -0.1476],
          [-0.1719, -0.0941],
          [-0.1262, -0.2246],
          ...,
          [-0.1254, -0.0481],
          [-0.2475, -0.0083],
          [ 0.1959,  0.2153]],

         [[ 0.1976, -0.4743],
          [ 0.3757, -0.3116],
          [ 0.2092, -0.1593],
          ...,
          [-0.0692,  0.1569],
          [ 0.1207, -0.0406],
          [ 0.0248, -0.4255]]],


        [[[ 0.0296, -0.4271],
          [ 0.2357, -0.3671],
          [ 0.2726, -0.3442],
          ...,
          [-0.0628, -0.3620],
          [ 0.1762, -0.1839],
          [ 0.3980,  0.0436]],

         [[ 0.0740,  0.4596],
          [-0.1924,  0.3332],
          [-0.0447,  0.2427],
          ...,
          [ 0.0482,  0.2071],
          [-0.0145, -0.0789],
          [-0.0024, -0.3577]]],


        [[[-0.2855, -0.1044],
          [ 0.0441,  0.0732],
          [-0.0737, -0.0983],
          ...,
          [ 0.3658,  0.2860],
          [ 0.1736,  0.2220],
          [-0.1806, -0.2515]],

         [[ 0.3458, -0.3889],
          [ 0.4287,  0.1453],
          [ 0.0971,  0.0884],
          ...,
          [-0.1233, -0.1208],
          [ 0.0818, -0.1004],
          [ 0.3042, -0.0600]]]])

2025-07-09 13:38:12.635812 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,13,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,13,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 467605144 / 593242364 (78.8%)
Greatest absolute difference: 0.9997961521148682 at index (10438835, 0, 1, 12) (up to 0.01 allowed)
Greatest relative difference: inf at index (5107789, 0, 1, 10) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 2, 13]), dtype=torch.float32)
tensor([[[[-0.2811,  0.2224,  0.2595,  ..., -0.1061,  0.0878,  0.1776],
          [-0.3237, -0.2972, -0.2161,  ...,  0.1123,  0.1467, -0.1161]],

         [[-0.1915, -0.2824, -0.0247,  ..., -0.2758, -0.3878, -0.3065],
          [-0.2774,  0.2361, -0.0494,  ..., -0.1030,  0.0389,  0.3825]]],


        [[[ 0.0533, -0.2302,  0.0968,  ...,  0.4953,  0.4430,  0.3809],
          [-0.2872, -0.2524, -0.1201,  ..., -0.1903, -0.0396,  0.0672]],

         [[ 0.0910, -0.2564, -0.0305,  ...,  0.1038, -0.0820, -0.3851],
          [-0.1868, -0.3794, -0.1607,  ...,  0.0990, -0.0737, -0.4585]]],


        [[[ 0.2202,  0.3806,  0.1785,  ...,  0.0765, -0.2230, -0.3835],
          [ 0.2512, -0.1976, -0.3222,  ..., -0.0124, -0.0769, -0.2904]],

         [[ 0.1057, -0.0819,  0.0553,  ..., -0.0761, -0.2363, -0.2008],
          [ 0.0285,  0.0352, -0.0857,  ...,  0.0837,  0.1032,  0.1564]]],


        ...,


        [[[-0.1108,  0.2677, -0.0367,  ...,  0.3496,  0.1004, -0.3546],
          [ 0.3195,  0.3624,  0.1184,  ...,  0.1038, -0.0830, -0.4858]],

         [[ 0.4300,  0.2904,  0.0511,  ..., -0.2198, -0.3517, -0.3375],
          [-0.4859, -0.0253,  0.3187,  ..., -0.2854, -0.3758, -0.3014]]],


        [[[-0.2385, -0.2974, -0.2394,  ...,  0.0156, -0.0993, -0.0029],
          [-0.1833,  0.3125,  0.1301,  ...,  0.2536,  0.1620,  0.1132]],

         [[-0.0583,  0.0419, -0.0110,  ...,  0.2289,  0.2185,  0.0756],
          [-0.3462, -0.1345,  0.1002,  ..., -0.3998, -0.1857,  0.0767]]],


        [[[-0.2620,  0.1593,  0.3594,  ...,  0.1515, -0.0990, -0.4032],
          [ 0.0801,  0.1026,  0.1645,  ..., -0.0068,  0.0257,  0.1596]],

         [[ 0.4648,  0.3555,  0.0014,  ...,  0.2107,  0.4361,  0.3652],
          [ 0.4353,  0.4507,  0.3672,  ..., -0.3758, -0.3005, -0.1237]]]])
DESIRED: (shape=torch.Size([11408507, 2, 2, 13]), dtype=torch.float32)
tensor([[[[-0.2811,  0.2098,  0.2677,  ..., -0.1791,  0.0525,  0.1776],
          [-0.3140, -0.2902, -0.2162,  ..., -0.1643, -0.3411, -0.3490]],

         [[-0.1915, -0.2801, -0.0451,  ..., -0.1622, -0.4198, -0.3065],
          [-0.1765,  0.1142, -0.1279,  ...,  0.1683,  0.0586,  0.0765]]],


        [[[ 0.0533, -0.2231,  0.0674,  ...,  0.4947,  0.4675,  0.3809],
          [ 0.1076,  0.1537,  0.0213,  ...,  0.1887, -0.0639, -0.1510]],

         [[ 0.0910, -0.2478, -0.0541,  ...,  0.0575,  0.0370, -0.3851],
          [ 0.4145, -0.0066, -0.1483,  ..., -0.3066, -0.3625,  0.0479]]],


        [[[ 0.2202,  0.3766,  0.1963,  ...,  0.1777, -0.1600, -0.3835],
          [ 0.0837,  0.1283, -0.0887,  ...,  0.0138,  0.0798,  0.2126]],

         [[ 0.1057, -0.0772,  0.0415,  ...,  0.0431, -0.2503, -0.2008],
          [ 0.2353,  0.1190, -0.1053,  ...,  0.0035,  0.2477,  0.4719]]],


        ...,


        [[[-0.1108,  0.2582, -0.0068,  ...,  0.2616,  0.2791, -0.3546],
          [-0.1225, -0.0389,  0.0758,  ..., -0.1048, -0.2226, -0.4115]],

         [[ 0.4300,  0.2939,  0.0652,  ..., -0.1297, -0.3573, -0.3375],
          [ 0.3667, -0.1306, -0.1335,  ..., -0.0814, -0.1437,  0.0960]]],


        [[[-0.2385, -0.2959, -0.2448,  ...,  0.1391, -0.1372, -0.0029],
          [-0.3901, -0.1913,  0.0230,  ...,  0.1205,  0.0095, -0.1189]],

         [[-0.0583,  0.0394, -0.0050,  ...,  0.1588,  0.2746,  0.0756],
          [ 0.2782, -0.2783, -0.3871,  ...,  0.3250,  0.2163, -0.0105]]],


        [[[-0.2620,  0.1488,  0.3541,  ...,  0.1451,  0.0205, -0.4032],
          [ 0.3441,  0.1962,  0.2422,  ..., -0.0661, -0.1359, -0.4545]],

         [[ 0.4648,  0.3582,  0.0244,  ...,  0.0319,  0.4639,  0.3652],
          [ 0.3276, -0.0995, -0.2717,  ...,  0.3119,  0.3847,  0.2061]]]])

2025-07-09 13:38:45.760410 GPU 4 151556 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,13,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,13,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 577037939 / 593242364 (97.3%)
Greatest absolute difference: 0.9997982978820801 at index (9609079, 0, 1, 12) (up to 0.01 allowed)
Greatest relative difference: 260074448.0 at index (5944950, 1, 0, 12) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 2, 13]), dtype=torch.float32)
tensor([[[[ 3.6301e-01, -1.1458e-01, -1.9745e-02,  ...,  1.0829e-01,  1.9087e-01,  8.2155e-02],
          [ 2.0405e-01,  1.6735e-01,  2.0127e-01,  ..., -4.2186e-01, -4.0823e-01, -3.0706e-01]],

         [[-1.0121e-04,  1.8265e-01,  5.7444e-02,  ...,  5.8286e-02, -1.7295e-01, -1.8164e-01],
          [ 1.8190e-01, -4.4264e-02, -3.7268e-02,  ...,  1.4745e-01,  3.6418e-01,  3.8350e-01]]],


        [[[ 7.5908e-02,  3.6327e-01,  1.2468e-01,  ..., -1.2465e-01, -2.4322e-01,  3.0011e-01],
          [-4.7314e-01, -5.5434e-03,  1.2376e-01,  ...,  3.2510e-01,  2.7710e-01,  4.9636e-01]],

         [[ 4.0737e-01,  4.0649e-01,  1.2646e-01,  ...,  2.5088e-01,  3.0237e-01,  2.8801e-01],
          [-4.6981e-01, -4.9199e-01, -2.2171e-01,  ..., -1.0368e-01, -7.1617e-02,  1.7151e-01]]],


        [[[ 1.7386e-01,  4.1640e-01,  2.2388e-01,  ...,  4.1342e-01,  2.2443e-01, -4.3659e-01],
          [-1.2980e-01,  2.2043e-01,  2.0186e-01,  ...,  1.5073e-01,  2.5476e-01,  2.8483e-01]],

         [[ 3.3616e-01, -8.2949e-03,  1.2432e-02,  ...,  4.2568e-02,  3.8710e-01,  1.9395e-01],
          [ 3.8723e-01, -1.9555e-01, -1.9666e-01,  ...,  6.8186e-02,  3.4957e-01,  3.6881e-01]]],


        ...,


        [[[-1.6107e-01,  2.8617e-01,  1.6765e-01,  ...,  3.6312e-01,  2.7745e-01,  2.7825e-01],
          [-3.0276e-01,  6.6457e-02,  2.4281e-01,  ..., -4.1359e-01, -3.3699e-01, -2.2238e-01]],

         [[ 1.3597e-02, -3.0617e-01, -2.4203e-01,  ...,  2.8376e-01, -2.6641e-02, -3.8128e-01],
          [ 1.0068e-01,  3.3616e-01, -1.9627e-02,  ..., -1.4127e-01, -3.6433e-01, -2.8251e-01]]],


        [[[ 6.1310e-03,  1.0170e-03, -1.7103e-01,  ...,  1.6659e-01,  7.0820e-02,  2.3165e-01],
          [ 3.4335e-01,  1.8547e-01,  1.4969e-01,  ...,  3.9625e-02, -6.2007e-02, -5.6440e-02]],

         [[-2.8320e-01,  4.2124e-02, -1.6360e-01,  ...,  6.6644e-02, -1.9699e-01,  1.3418e-01],
          [ 5.4596e-02,  1.6226e-01,  3.4466e-01,  ..., -7.8007e-02, -3.6028e-01, -3.7558e-01]]],


        [[[ 8.3967e-03, -3.7066e-02, -1.2312e-01,  ...,  3.3931e-01,  1.0892e-01, -3.9113e-01],
          [-4.0897e-02, -9.1753e-02, -5.7117e-02,  ...,  1.4227e-01, -2.2780e-01, -3.2009e-01]],

         [[-3.3415e-01, -7.4196e-02, -2.0769e-01,  ...,  2.1518e-01,  9.8999e-02,  3.9670e-01],
          [ 2.5797e-01,  1.0407e-01,  2.2913e-01,  ..., -4.0211e-01, -3.2862e-01, -1.1926e-01]]]])
DESIRED: (shape=torch.Size([11408507, 2, 2, 13]), dtype=torch.float32)
tensor([[[[ 0.0515, -0.2294, -0.4122,  ...,  0.3532,  0.1547, -0.1742],
          [ 0.2632,  0.0691, -0.1251,  ...,  0.2526,  0.0450, -0.3849]],

         [[ 0.1617, -0.0693, -0.1916,  ...,  0.1600,  0.3029, -0.0315],
          [ 0.4679,  0.3213,  0.0295,  ..., -0.2851, -0.1314,  0.3893]]],


        [[[-0.4639, -0.0362, -0.0378,  ..., -0.1166, -0.1547,  0.4399],
          [-0.2038, -0.0006, -0.0411,  ..., -0.1528, -0.1056,  0.3876]],

         [[ 0.0492,  0.2114,  0.1250,  ..., -0.1372, -0.0545, -0.2958],
          [ 0.2089, -0.1078, -0.2980,  ...,  0.0403,  0.1382,  0.2134]]],


        [[[ 0.4663,  0.0241,  0.0676,  ..., -0.1371, -0.1606, -0.0584],
          [-0.4774, -0.0206,  0.2579,  ...,  0.3140,  0.0549, -0.4692]],

         [[-0.2005,  0.1696,  0.2004,  ..., -0.2566, -0.1955, -0.3416],
          [ 0.0561,  0.1418,  0.1663,  ..., -0.1654, -0.1877, -0.1488]]],


        ...,


        [[[-0.1165, -0.2793, -0.3398,  ...,  0.3073,  0.2773,  0.3367],
          [ 0.2117,  0.1402,  0.0296,  ..., -0.1539, -0.0617,  0.2151]],

         [[ 0.4534,  0.2661, -0.0896,  ...,  0.1976,  0.3712,  0.4694],
          [ 0.2215,  0.1425, -0.1356,  ..., -0.1905,  0.0418,  0.3376]]],


        [[[ 0.3017,  0.3509,  0.3842,  ...,  0.2944,  0.4522,  0.4570],
          [-0.4493, -0.3813, -0.2529,  ...,  0.2646,  0.2701, -0.1265]],

         [[-0.2801,  0.2072,  0.2884,  ..., -0.2783, -0.4350, -0.4241],
          [-0.1929, -0.3682, -0.0568,  ..., -0.0491, -0.1928, -0.2603]]],


        [[[-0.1944, -0.3580, -0.2566,  ..., -0.1262, -0.2554, -0.4153],
          [-0.1599,  0.1907,  0.2772,  ...,  0.3739,  0.3097,  0.0083]],

         [[-0.4915, -0.4026, -0.1300,  ...,  0.0339, -0.0335, -0.1297],
          [-0.4149, -0.0075,  0.1791,  ...,  0.1208,  0.3746,  0.2967]]]])

2025-07-09 13:38:50.374234 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 66756685 / 91268056 (73.1%)
Greatest absolute difference: 0.9999438524246216 at index (4765091, 1, 0, 1) (up to 0.01 allowed)
Greatest relative difference: 24287076.0 at index (8044150, 0, 1, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 2, 2]), dtype=torch.float32)
tensor([[[[-0.2811, -0.4813],
          [-0.3237, -0.1138]],

         [[-0.1915,  0.4496],
          [-0.2774,  0.3698]]],


        [[[ 0.0533,  0.0650],
          [-0.2872, -0.2616]],

         [[ 0.0910, -0.3552],
          [-0.1868, -0.3592]]],


        [[[ 0.2202,  0.0234],
          [ 0.2512,  0.4721]],

         [[ 0.1057, -0.3405],
          [ 0.0285,  0.0357]]],


        ...,


        [[[-0.1108,  0.0963],
          [ 0.3195,  0.3626]],

         [[ 0.4300,  0.1845],
          [-0.4859,  0.0625]]],


        [[[-0.2385,  0.3606],
          [-0.1833, -0.0471]],

         [[-0.0583,  0.0837],
          [-0.3462, -0.0893]]],


        [[[-0.2620, -0.4232],
          [ 0.0801,  0.4969]],

         [[ 0.4648, -0.4322],
          [ 0.4353,  0.2309]]]])
DESIRED: (shape=torch.Size([11408507, 2, 2, 2]), dtype=torch.float32)
tensor([[[[-0.2811,  0.1776],
          [-0.3140, -0.3490]],

         [[-0.1915, -0.3065],
          [-0.1765,  0.0765]]],


        [[[ 0.0533,  0.3809],
          [ 0.1076, -0.1510]],

         [[ 0.0910, -0.3851],
          [ 0.4145,  0.0479]]],


        [[[ 0.2202, -0.3835],
          [ 0.0837,  0.2126]],

         [[ 0.1057, -0.2008],
          [ 0.2353,  0.4719]]],


        ...,


        [[[-0.1108, -0.3546],
          [-0.1225, -0.4115]],

         [[ 0.4300, -0.3375],
          [ 0.3667,  0.0960]]],


        [[[-0.2385, -0.0029],
          [-0.3901, -0.1189]],

         [[-0.0583,  0.0756],
          [ 0.2782, -0.0105]]],


        [[[-0.2620, -0.4032],
          [ 0.3441, -0.4545]],

         [[ 0.4648,  0.3652],
          [ 0.3276,  0.2061]]]])

2025-07-09 13:39:16.552072 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 89006107 / 91268056 (97.5%)
Greatest absolute difference: 0.9999130964279175 at index (3593342, 1, 0, 0) (up to 0.01 allowed)
Greatest relative difference: 78179840.0 at index (2964520, 1, 1, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 2, 2]), dtype=torch.float32)
tensor([[[[-0.2811,  0.1776],
          [-0.3140, -0.3490]],

         [[-0.1915, -0.3065],
          [-0.1765,  0.0765]]],


        [[[ 0.0533,  0.3809],
          [ 0.1076, -0.1510]],

         [[ 0.0910, -0.3851],
          [ 0.4145,  0.0479]]],


        [[[ 0.2202, -0.3835],
          [ 0.0837,  0.2126]],

         [[ 0.1057, -0.2008],
          [ 0.2353,  0.4719]]],


        ...,


        [[[-0.1108, -0.3546],
          [-0.1225, -0.4115]],

         [[ 0.4300, -0.3375],
          [ 0.3667,  0.0960]]],


        [[[-0.2385, -0.0029],
          [-0.3901, -0.1189]],

         [[-0.0583,  0.0756],
          [ 0.2782, -0.0105]]],


        [[[-0.2620, -0.4032],
          [ 0.3441, -0.4545]],

         [[ 0.4648,  0.3652],
          [ 0.3276,  0.2061]]]])
DESIRED: (shape=torch.Size([11408507, 2, 2, 2]), dtype=torch.float32)
tensor([[[[ 0.0418, -0.3644],
          [-0.3905,  0.1833]],

         [[ 0.1452,  0.3912],
          [-0.3913, -0.4277]]],


        [[[-0.4162,  0.1397],
          [-0.4106,  0.3077]],

         [[ 0.2338, -0.4776],
          [ 0.0202,  0.3902]]],


        [[[-0.3867, -0.1350],
          [-0.4198,  0.0120]],

         [[ 0.4204, -0.4564],
          [ 0.4849, -0.2875]]],


        ...,


        [[[-0.3328, -0.1979],
          [ 0.1051, -0.4217]],

         [[-0.0255,  0.3718],
          [ 0.4106,  0.1799]]],


        [[[ 0.1396,  0.3223],
          [-0.2953, -0.1526]],

         [[ 0.0396,  0.0708],
          [-0.3613,  0.0640]]],


        [[[ 0.4269, -0.0818],
          [-0.1419, -0.2538]],

         [[ 0.1037,  0.4525],
          [-0.1966,  0.0858]]]])

2025-07-09 13:39:16.581430 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,24,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,24,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 957474121 / 1095216672 (87.4%)
Greatest absolute difference: 0.9998395442962646 at index (221592, 0, 1, 0) (up to 0.01 allowed)
Greatest relative difference: 311600416.0 at index (6259959, 1, 1, 12) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 2, 24]), dtype=torch.float32)
tensor([[[[ 0.0714,  0.0509,  0.0303,  ...,  0.0121,  0.0087,  0.0087],
          [ 0.0382,  0.0849,  0.1317,  ...,  0.0071, -0.1451, -0.1451]],

         [[ 0.3460,  0.0639, -0.2182,  ...,  0.3752,  0.3754,  0.3754],
          [-0.3952, -0.3796, -0.3640,  ..., -0.3570, -0.3750, -0.3750]]],


        [[[-0.3030, -0.0841,  0.1348,  ...,  0.3294,  0.3715,  0.3715],
          [-0.0423,  0.1804,  0.4031,  ..., -0.4025, -0.3942, -0.3942]],

         [[-0.4265, -0.2466, -0.0668,  ..., -0.3380, -0.4561, -0.4561],
          [ 0.2327,  0.2786,  0.3244,  ..., -0.3389, -0.3573, -0.3573]]],


        [[[-0.2908, -0.1864, -0.0821,  ..., -0.1972, -0.4000, -0.4000],
          [-0.2103, -0.0646,  0.0811,  ...,  0.1299,  0.2866,  0.2866]],

         [[-0.3664, -0.1602,  0.0460,  ..., -0.0390, -0.0277, -0.0277],
          [ 0.2219,  0.1777,  0.1335,  ..., -0.1180, -0.0629, -0.0629]]],


        ...,


        [[[-0.0960, -0.0128,  0.0703,  ...,  0.1418,  0.2542,  0.2542],
          [ 0.3430,  0.2633,  0.1835,  ...,  0.4222,  0.4510,  0.4510]],

         [[ 0.2506,  0.2505,  0.2505,  ...,  0.1330,  0.0211,  0.0211],
          [ 0.0021, -0.0927, -0.1874,  ..., -0.1678, -0.0617, -0.0617]]],


        [[[ 0.1508, -0.0117, -0.1741,  ...,  0.0271, -0.0435, -0.0435],
          [ 0.3935,  0.1308, -0.1318,  ...,  0.0760,  0.0684,  0.0684]],

         [[-0.0886, -0.0295,  0.0297,  ...,  0.2569,  0.3366,  0.3366],
          [-0.4583, -0.3515, -0.2448,  ...,  0.1577,  0.1311,  0.1311]]],


        [[[ 0.3337,  0.2897,  0.2457,  ..., -0.3429, -0.4862, -0.4862],
          [ 0.4499,  0.1876, -0.0748,  ..., -0.4330, -0.4364, -0.4364]],

         [[ 0.4132,  0.0796, -0.2540,  ..., -0.1521, -0.1948, -0.1948],
          [ 0.3185,  0.1941,  0.0696,  ..., -0.0621, -0.0047, -0.0047]]]])
DESIRED: (shape=torch.Size([11408507, 2, 2, 24]), dtype=torch.float32)
tensor([[[[ 0.0714,  0.0521,  0.0328,  ...,  0.0195,  0.0141,  0.0087],
          [ 0.0815,  0.1537,  0.2259,  ..., -0.1091, -0.1158, -0.1224]],

         [[ 0.3460,  0.0811, -0.1838,  ...,  0.3749,  0.3751,  0.3754],
          [-0.0530, -0.1348, -0.2166,  ...,  0.0282, -0.2000, -0.4281]]],


        [[[-0.3030, -0.0974,  0.1081,  ...,  0.2395,  0.3055,  0.3715],
          [-0.0410, -0.0404, -0.0397,  ..., -0.2752, -0.0548,  0.1656]],

         [[-0.4265, -0.2576, -0.0887,  ..., -0.0864, -0.2713, -0.4561],
          [-0.4576, -0.1857,  0.0862,  ...,  0.2599,  0.3516,  0.4432]]],


        [[[-0.2908, -0.1928, -0.0948,  ...,  0.2350, -0.0825, -0.4000],
          [ 0.2533,  0.1086, -0.0361,  ..., -0.1464, -0.0967, -0.0470]],

         [[-0.3664, -0.1727,  0.0209,  ..., -0.0632, -0.0454, -0.0277],
          [ 0.1277, -0.1097, -0.3472,  ..., -0.3710, -0.3275, -0.2839]]],


        ...,


        [[[-0.0960, -0.0179,  0.0602,  ..., -0.0975,  0.0783,  0.2542],
          [ 0.2237,  0.3159,  0.4082,  ..., -0.2432, -0.1766, -0.1100]],

         [[ 0.2506,  0.2505,  0.2505,  ...,  0.3712,  0.1962,  0.0211],
          [ 0.4578,  0.1995, -0.0589,  ..., -0.2189, -0.2827, -0.3465]]],


        [[[ 0.1508, -0.0018, -0.1543,  ...,  0.1777,  0.0671, -0.0435],
          [-0.4481, -0.2726, -0.0970,  ...,  0.2433,  0.3270,  0.4107]],

         [[-0.0886, -0.0331,  0.0225,  ...,  0.0872,  0.2119,  0.3366],
          [-0.1146, -0.0226,  0.0694,  ...,  0.2220,  0.2050,  0.1880]]],


        [[[ 0.3337,  0.2924,  0.2510,  ..., -0.0375, -0.2619, -0.4862],
          [-0.0927, -0.1407, -0.1886,  ...,  0.1018, -0.0939, -0.2895]],

         [[ 0.4132,  0.0999, -0.2134,  ..., -0.0611, -0.1280, -0.1948],
          [ 0.2623,  0.2041,  0.1459,  ..., -0.1419,  0.0478,  0.2376]]]])

2025-07-09 13:39:43.091680 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,24,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[2,24,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1065716186 / 1095216672 (97.3%)
Greatest absolute difference: 0.9998832941055298 at index (7669383, 1, 0, 23) (up to 0.01 allowed)
Greatest relative difference: inf at index (6737820, 1, 1, 7) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 2, 24]), dtype=torch.float32)
tensor([[[[-0.2811, -0.0249,  0.2312,  ...,  0.0470,  0.1123,  0.1776],
          [-0.3140, -0.3016, -0.2892,  ..., -0.3408, -0.3449, -0.3490]],

         [[-0.1915, -0.2378, -0.2840,  ..., -0.4247, -0.3656, -0.3065],
          [-0.1765, -0.0248,  0.1269,  ...,  0.0578,  0.0671,  0.0765]]],


        [[[ 0.0533, -0.0909, -0.2352,  ...,  0.4712,  0.4261,  0.3809],
          [ 0.1076,  0.1317,  0.1557,  ..., -0.0601, -0.1055, -0.1510]],

         [[ 0.0910, -0.0857, -0.2625,  ...,  0.0554, -0.1649, -0.3851],
          [ 0.4145,  0.1948, -0.0249,  ..., -0.3803, -0.1662,  0.0479]]],


        [[[ 0.2202,  0.3018,  0.3834,  ..., -0.1502, -0.2669, -0.3835],
          [ 0.0837,  0.1069,  0.1302,  ...,  0.0741,  0.1433,  0.2126]],

         [[ 0.1057,  0.0103, -0.0851,  ..., -0.2524, -0.2266, -0.2008],
          [ 0.2353,  0.1747,  0.1140,  ...,  0.2379,  0.3549,  0.4719]]],


        ...,


        [[[-0.1108,  0.0817,  0.2743,  ...,  0.3067, -0.0240, -0.3546],
          [-0.1225, -0.0789, -0.0353,  ..., -0.2144, -0.3129, -0.4115]],

         [[ 0.4300,  0.3590,  0.2880,  ..., -0.3582, -0.3478, -0.3375],
          [ 0.3667,  0.1073, -0.1522,  ..., -0.1541, -0.0291,  0.0960]]],


        [[[-0.2385, -0.2685, -0.2984,  ..., -0.1430, -0.0730, -0.0029],
          [-0.3901, -0.2864, -0.1826,  ...,  0.0150, -0.0519, -0.1189]],

         [[-0.0583, -0.0073,  0.0436,  ...,  0.2832,  0.1794,  0.0756],
          [ 0.2782, -0.0121, -0.3024,  ...,  0.2262,  0.1078, -0.0105]]],


        [[[-0.2620, -0.0477,  0.1666,  ...,  0.0389, -0.1822, -0.4032],
          [ 0.3441,  0.2670,  0.1898,  ..., -0.1221, -0.2883, -0.4545]],

         [[ 0.4648,  0.4092,  0.3536,  ...,  0.4682,  0.4167,  0.3652],
          [ 0.3276,  0.1048, -0.1180,  ...,  0.3925,  0.2993,  0.2061]]]])
DESIRED: (shape=torch.Size([11408507, 2, 2, 24]), dtype=torch.float32)
tensor([[[[ 0.3898,  0.3545,  0.2367,  ...,  0.2358,  0.3740,  0.4154],
          [ 0.2294,  0.1915,  0.0654,  ..., -0.4331, -0.4772, -0.4905]],

         [[ 0.2911,  0.2230, -0.0041,  ...,  0.3246,  0.2985,  0.2906],
          [-0.4876, -0.4661, -0.3946,  ...,  0.0275,  0.2783,  0.3536]]],


        [[[ 0.0601,  0.1094,  0.2737,  ...,  0.3249,  0.2826,  0.2699],
          [-0.3621, -0.2898, -0.0487,  ..., -0.1738, -0.3830, -0.4457]],

         [[-0.0361, -0.0237,  0.0175,  ...,  0.0794, -0.2006, -0.2846],
          [ 0.0290,  0.0794,  0.2474,  ...,  0.1710,  0.1921,  0.1984]]],


        [[[-0.2850, -0.2096,  0.0417,  ...,  0.2328,  0.1764,  0.1595],
          [ 0.3304,  0.3243,  0.3039,  ..., -0.3708, -0.3959, -0.4034]],

         [[ 0.4010,  0.4110,  0.4445,  ..., -0.0245, -0.2561, -0.3256],
          [-0.0355, -0.0055,  0.0947,  ...,  0.2761,  0.0975,  0.0440]]],


        ...,


        [[[ 0.1871,  0.2121,  0.2956,  ...,  0.0872,  0.0776,  0.0747],
          [ 0.3817,  0.3002,  0.0287,  ...,  0.2340,  0.3321,  0.3616]],

         [[ 0.1922,  0.1279, -0.0865,  ...,  0.0226, -0.0747, -0.1039],
          [-0.2448, -0.2738, -0.3706,  ..., -0.2506, -0.3392, -0.3658]]],


        [[[-0.3169, -0.2319,  0.0513,  ...,  0.3829,  0.4515,  0.4721],
          [-0.4674, -0.4658, -0.4602,  ..., -0.1405, -0.3187, -0.3722]],

         [[-0.4684, -0.4051, -0.1940,  ..., -0.0143,  0.3350,  0.4399],
          [ 0.4627,  0.4625,  0.4619,  ...,  0.2647,  0.1783,  0.1524]]],


        [[[ 0.3213,  0.2586,  0.0496,  ..., -0.0782,  0.2988,  0.4119],
          [-0.2567, -0.2097, -0.0528,  ..., -0.0762, -0.0957, -0.1016]],

         [[ 0.3914,  0.3369,  0.1551,  ..., -0.2072, -0.2558, -0.2705],
          [ 0.3473,  0.3074,  0.1742,  ...,  0.0809,  0.0872,  0.0891]]]])

2025-07-09 13:39:50.319831 GPU 6 151383 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[24,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[24,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 957465148 / 1095216672 (87.4%)
Greatest absolute difference: 0.9999837875366211 at index (10317984, 0, 0, 1) (up to 0.01 allowed)
Greatest relative difference: 109330968.0 at index (8198307, 1, 14, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 24, 2]), dtype=torch.float32)
tensor([[[[ 0.0063,  0.1055],
          [-0.1339, -0.0006],
          [-0.2740, -0.1067],
          ...,
          [ 0.2347,  0.2624],
          [ 0.2946,  0.2992],
          [ 0.2946,  0.2992]],

         [[-0.1572,  0.2030],
          [ 0.0497,  0.0731],
          [ 0.2565, -0.0567],
          ...,
          [ 0.0311,  0.0264],
          [ 0.1240,  0.1568],
          [ 0.1240,  0.1568]]],


        [[[-0.3044, -0.0590],
          [-0.3699, -0.1708],
          [-0.4354, -0.2827],
          ...,
          [-0.1866,  0.4190],
          [-0.3155,  0.4292],
          [-0.3155,  0.4292]],

         [[-0.1173,  0.4481],
          [-0.0411,  0.2311],
          [ 0.0350,  0.0140],
          ...,
          [-0.1111, -0.0503],
          [-0.1170, -0.1371],
          [-0.1170, -0.1371]]],


        [[[-0.2070, -0.2945],
          [-0.1959, -0.2119],
          [-0.1848, -0.1293],
          ...,
          [ 0.3326, -0.0780],
          [ 0.4806, -0.0115],
          [ 0.4806, -0.0115]],

         [[-0.4234,  0.1767],
          [-0.2014,  0.2463],
          [ 0.0206,  0.3159],
          ...,
          [-0.4283,  0.0660],
          [-0.4576,  0.0474],
          [-0.4576,  0.0474]]],


        ...,


        [[[-0.0303, -0.0644],
          [ 0.0415, -0.0446],
          [ 0.1133, -0.0247],
          ...,
          [-0.0884, -0.2018],
          [-0.1852, -0.1094],
          [-0.1852, -0.1094]],

         [[ 0.3295, -0.1415],
          [ 0.0246,  0.1138],
          [-0.2803,  0.3691],
          ...,
          [-0.0021, -0.1278],
          [-0.0812, -0.2185],
          [-0.0812, -0.2185]]],


        [[[ 0.1257, -0.2893],
          [ 0.0096, -0.2770],
          [-0.1066, -0.2648],
          ...,
          [-0.2281,  0.3396],
          [-0.3111,  0.3824],
          [-0.3111,  0.3824]],

         [[ 0.3149, -0.3752],
          [ 0.3177, -0.2346],
          [ 0.3204, -0.0940],
          ...,
          [-0.2541,  0.1013],
          [-0.4431,  0.1371],
          [-0.4431,  0.1371]]],


        [[[-0.4009,  0.1980],
          [-0.1179,  0.2126],
          [ 0.1651,  0.2271],
          ...,
          [ 0.1432,  0.0883],
          [ 0.3406,  0.1952],
          [ 0.3406,  0.1952]],

         [[ 0.2741, -0.0055],
          [ 0.0760,  0.1808],
          [-0.1221,  0.3671],
          ...,
          [-0.1339,  0.2598],
          [-0.2868,  0.4587],
          [-0.2868,  0.4587]]]])
DESIRED: (shape=torch.Size([11408507, 2, 24, 2]), dtype=torch.float32)
tensor([[[[ 0.0063, -0.2606],
          [-0.1253, -0.1732],
          [-0.2570, -0.0858],
          ...,
          [ 0.1072,  0.3991],
          [ 0.2009,  0.3144],
          [ 0.2946,  0.2296]],

         [[-0.1572, -0.4420],
          [ 0.0371, -0.3759],
          [ 0.2314, -0.3098],
          ...,
          [-0.1669,  0.0343],
          [-0.0215,  0.2345],
          [ 0.1240,  0.4346]]],


        [[[-0.3044,  0.4089],
          [-0.3659,  0.1084],
          [-0.4275, -0.1921],
          ...,
          [ 0.0881,  0.1007],
          [-0.1137,  0.1467],
          [-0.3155,  0.1928]],

         [[-0.1173, -0.4827],
          [-0.0458, -0.1111],
          [ 0.0257,  0.2604],
          ...,
          [-0.0986,  0.1677],
          [-0.1078, -0.0551],
          [-0.1170, -0.2779]]],


        [[[-0.2070,  0.3923],
          [-0.1966,  0.1205],
          [-0.1861, -0.1513],
          ...,
          [ 0.0174, -0.1283],
          [ 0.2490,  0.1145],
          [ 0.4806,  0.3572]],

         [[-0.4234, -0.3281],
          [-0.2149, -0.0585],
          [-0.0064,  0.2111],
          ...,
          [-0.3659,  0.3566],
          [-0.4118,  0.2456],
          [-0.4576,  0.1345]]],


        ...,


        [[[-0.0303, -0.3179],
          [ 0.0371, -0.3510],
          [ 0.1046, -0.3840],
          ...,
          [ 0.1179, -0.4451],
          [-0.0336, -0.3654],
          [-0.1852, -0.2857]],

         [[ 0.3295,  0.0309],
          [ 0.0432,  0.0174],
          [-0.2432,  0.0039],
          ...,
          [ 0.1665,  0.1142],
          [ 0.0426,  0.2139],
          [-0.0812,  0.3137]]],


        [[[ 0.1257,  0.3565],
          [ 0.0166,  0.4018],
          [-0.0924,  0.4471],
          ...,
          [-0.0512, -0.1930],
          [-0.1812,  0.0798],
          [-0.3111,  0.3526]],

         [[ 0.3149,  0.1997],
          [ 0.3175,  0.2772],
          [ 0.3201,  0.3547],
          ...,
          [ 0.1484,  0.3503],
          [-0.1474,  0.2480],
          [-0.4431,  0.1456]]],


        [[[-0.4009, -0.1110],
          [-0.1351, -0.0400],
          [ 0.1307,  0.0310],
          ...,
          [-0.2772,  0.0867],
          [ 0.0317,  0.2055],
          [ 0.3406,  0.3243]],

         [[ 0.2741, -0.2211],
          [ 0.0880, -0.0243],
          [-0.0980,  0.1725],
          ...,
          [ 0.1919,  0.3369],
          [-0.0475,  0.1672],
          [-0.2868, -0.0026]]]])

2025-07-09 13:40:00.660155 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[24,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([11408507, 2, 10, 10],"float32"), size=list[24,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NCHW", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1065709481 / 1095216672 (97.3%)
Greatest absolute difference: 0.9998992085456848 at index (2458108, 0, 0, 1) (up to 0.01 allowed)
Greatest relative difference: 661870656.0 at index (3394115, 1, 7, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([11408507, 2, 24, 2]), dtype=torch.float32)
tensor([[[[ 0.1817,  0.2559],
          [ 0.2496,  0.2223],
          [ 0.3175,  0.1886],
          ...,
          [ 0.0963,  0.4163],
          [ 0.0099,  0.3269],
          [-0.0765,  0.2375]],

         [[ 0.2085,  0.4479],
          [-0.0310,  0.3517],
          [-0.2705,  0.2555],
          ...,
          [ 0.4385, -0.2721],
          [ 0.4006, -0.1784],
          [ 0.3628, -0.0847]]],


        [[[ 0.2183, -0.2840],
          [ 0.2051, -0.2205],
          [ 0.1918, -0.1569],
          ...,
          [ 0.3048, -0.1079],
          [ 0.3050, -0.1319],
          [ 0.3053, -0.1558]],

         [[ 0.0739, -0.3666],
          [ 0.0360, -0.3626],
          [-0.0019, -0.3587],
          ...,
          [-0.1317, -0.2555],
          [ 0.1269, -0.0545],
          [ 0.3856,  0.1464]]],


        [[[-0.2338,  0.0010],
          [-0.2595, -0.0495],
          [-0.2852, -0.0999],
          ...,
          [ 0.0806, -0.2961],
          [-0.1930, -0.3462],
          [-0.4666, -0.3963]],

         [[-0.3050, -0.1089],
          [-0.3773, -0.0824],
          [-0.4496, -0.0558],
          ...,
          [-0.0616,  0.4195],
          [-0.1279,  0.2916],
          [-0.1942,  0.1637]]],


        ...,


        [[[ 0.4425,  0.3519],
          [ 0.3454,  0.1058],
          [ 0.2482, -0.1404],
          ...,
          [-0.0063,  0.0311],
          [-0.1028, -0.0915],
          [-0.1994, -0.2140]],

         [[ 0.0033,  0.1987],
          [-0.1567, -0.0330],
          [-0.3166, -0.2648],
          ...,
          [-0.3168, -0.4338],
          [-0.3833, -0.3804],
          [-0.4497, -0.3270]]],


        [[[-0.0916, -0.4678],
          [-0.0254, -0.3169],
          [ 0.0408, -0.1660],
          ...,
          [-0.2589, -0.3318],
          [-0.3284, -0.3730],
          [-0.3980, -0.4141]],

         [[ 0.4320, -0.3621],
          [ 0.2116, -0.3991],
          [-0.0088, -0.4360],
          ...,
          [-0.3191,  0.2445],
          [-0.2586,  0.0065],
          [-0.1981, -0.2315]]],


        [[[ 0.2210, -0.1060],
          [ 0.0661, -0.0058],
          [-0.0887,  0.0943],
          ...,
          [-0.4539,  0.0995],
          [-0.4260, -0.0050],
          [-0.3980, -0.1094]],

         [[-0.1629, -0.2821],
          [-0.1704, -0.2305],
          [-0.1780, -0.1789],
          ...,
          [ 0.2489, -0.1009],
          [-0.0140,  0.0812],
          [-0.2769,  0.2633]]]])
DESIRED: (shape=torch.Size([11408507, 2, 24, 2]), dtype=torch.float32)
tensor([[[[ 3.5318e-01,  1.1557e-01],
          [ 2.6955e-01,  1.3268e-01],
          [-9.2199e-03,  1.8973e-01],
          ...,
          [-1.6469e-02,  6.8212e-02],
          [-8.8785e-02,  1.2214e-02],
          [-1.1048e-01, -4.5850e-03]],

         [[ 3.0103e-01,  4.5064e-01],
          [ 2.5648e-01,  4.5358e-01],
          [ 1.0797e-01,  4.6337e-01],
          ...,
          [ 1.8334e-03,  1.3028e-01],
          [-2.4240e-01, -5.2045e-02],
          [-3.1567e-01, -1.0674e-01]]],


        [[[ 4.9073e-01, -1.0359e-01],
          [ 3.9919e-01, -2.8215e-02],
          [ 9.4067e-02,  2.2303e-01],
          ...,
          [-2.1663e-01,  2.1168e-01],
          [ 4.9040e-04, -6.7539e-03],
          [ 6.5627e-02, -7.2284e-02]],

         [[ 3.7729e-01,  6.0401e-02],
          [ 3.3134e-01,  4.0381e-02],
          [ 1.7817e-01, -2.6351e-02],
          ...,
          [-6.4739e-02,  5.6477e-03],
          [ 1.2456e-01,  3.2228e-01],
          [ 1.8135e-01,  4.1727e-01]]],


        [[[ 3.7311e-01,  2.8503e-01],
          [ 3.3541e-01,  2.0667e-01],
          [ 2.0976e-01, -5.4494e-02],
          ...,
          [-2.8058e-01,  1.9118e-02],
          [-1.7114e-01, -6.2159e-02],
          [-1.3830e-01, -8.6541e-02]],

         [[ 3.6940e-01,  2.3020e-01],
          [ 3.6402e-01,  1.7256e-01],
          [ 3.4606e-01, -1.9607e-02],
          ...,
          [ 2.1259e-01,  8.7243e-02],
          [ 3.2526e-01,  3.1679e-01],
          [ 3.5906e-01,  3.8566e-01]]],


        ...,


        [[[ 1.9010e-01,  2.4095e-01],
          [ 1.4828e-01,  2.1565e-01],
          [ 8.8843e-03,  1.3131e-01],
          ...,
          [-8.6093e-04, -1.5874e-01],
          [ 2.4772e-03, -1.7926e-01],
          [ 3.4787e-03, -1.8542e-01]],

         [[-4.2081e-01, -3.0004e-01],
          [-3.8729e-01, -2.8647e-01],
          [-2.7557e-01, -2.4121e-01],
          ...,
          [ 1.4111e-01,  4.4560e-01],
          [ 3.4371e-01,  4.5596e-01],
          [ 4.0449e-01,  4.5907e-01]]],


        [[[-2.4907e-01,  2.1044e-01],
          [-2.5868e-01,  1.7962e-01],
          [-2.9069e-01,  7.6887e-02],
          ...,
          [-6.1575e-02, -2.7336e-01],
          [ 3.1357e-01, -1.4123e-01],
          [ 4.2612e-01, -1.0159e-01]],

         [[ 2.1485e-01,  4.4737e-01],
          [ 1.3405e-01,  4.3648e-01],
          [-1.3532e-01,  4.0020e-01],
          ...,
          [-2.9517e-02, -1.5965e-01],
          [ 1.9849e-01, -2.8448e-01],
          [ 2.6689e-01, -3.2193e-01]]],


        [[[ 4.3453e-01, -8.0898e-02],
          [ 3.5070e-01, -9.7692e-03],
          [ 7.1288e-02,  2.2733e-01],
          ...,
          [-1.6801e-01,  6.2266e-02],
          [-2.0899e-01, -1.7065e-03],
          [-2.2129e-01, -2.0898e-02]],

         [[ 4.6905e-01,  3.2176e-01],
          [ 4.0380e-01,  2.2343e-01],
          [ 1.8631e-01, -1.0436e-01],
          ...,
          [-1.2583e-01,  7.7468e-02],
          [-1.3650e-01,  4.0214e-01],
          [-1.3970e-01,  4.9954e-01]]]])

2025-07-09 13:40:25.712737 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([16, 160, 148549, 6],"float32"), size=list[16,12,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([16, 160, 148549, 6],"float32"), size=list[16,12,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 476304 / 491520 (96.9%)
Greatest absolute difference: 0.9804041385650635 at index (15, 83, 5, 0) (up to 0.01 allowed)
Greatest relative difference: 367326.625 at index (5, 18, 12, 5) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 160, 16, 12]), dtype=torch.float32)
tensor([[[[-2.2491e-01, -2.8350e-01, -3.4209e-01,  ..., -4.3563e-01, -2.6521e-01, -9.4801e-02],
          [ 3.1114e-02,  1.1126e-01,  1.9140e-01,  ..., -2.1100e-01, -3.9559e-02,  1.3189e-01],
          [ 1.5849e-01,  2.1955e-01,  2.8061e-01,  ..., -4.0017e-01, -3.6444e-01, -3.2872e-01],
          ...,
          [-9.0576e-02, -1.5898e-01, -2.2738e-01,  ..., -3.6926e-01, -1.1359e-01,  1.4208e-01],
          [ 2.7040e-01,  1.2849e-01, -1.3411e-02,  ...,  1.0519e-02,  1.8440e-03, -6.8316e-03],
          [-4.5254e-02,  1.8102e-01,  4.0729e-01,  ...,  3.5290e-02,  1.2139e-01,  2.0749e-01]],

         [[ 3.6541e-01,  3.3500e-01,  3.0460e-01,  ..., -2.7967e-02,  1.0107e-01,  2.3012e-01],
          [ 3.3776e-02, -4.0649e-02, -1.1507e-01,  ...,  2.1800e-01,  1.1766e-02, -1.9446e-01],
          [ 9.6945e-02, -1.6550e-02, -1.3005e-01,  ..., -1.9761e-01, -9.4211e-02,  9.1872e-03],
          ...,
          [ 1.8533e-01, -5.3364e-03, -1.9601e-01,  ..., -1.9693e-03, -5.3394e-02, -1.0482e-01],
          [-1.6719e-01, -1.9324e-01, -2.1929e-01,  ...,  3.0749e-01,  3.2982e-01,  3.5214e-01],
          [ 1.5715e-01, -2.9701e-02, -2.1655e-01,  ...,  3.7042e-01,  2.6658e-01,  1.6273e-01]],

         [[-7.6395e-02, -1.6689e-01, -2.5739e-01,  ..., -1.4401e-01,  7.7971e-02,  2.9995e-01],
          [-3.2030e-01, -1.8165e-01, -4.3003e-02,  ...,  3.7694e-01,  3.3883e-01,  3.0072e-01],
          [-6.1797e-02,  5.1943e-02,  1.6568e-01,  ...,  2.8965e-01,  1.3313e-01, -2.3403e-02],
          ...,
          [-1.5253e-01, -1.2328e-01, -9.4041e-02,  ..., -3.8092e-01, -1.6577e-01,  4.9388e-02],
          [ 3.3499e-02, -2.7128e-02, -8.7755e-02,  ..., -2.1734e-01,  5.3428e-02,  3.2419e-01],
          [-6.0256e-02,  8.0345e-02,  2.2095e-01,  ..., -4.1168e-01,  8.6647e-03,  4.2901e-01]],

         ...,

         [[-3.3366e-01, -1.3507e-01,  6.3517e-02,  ..., -3.5999e-01, -1.5756e-01,  4.4879e-02],
          [-1.2948e-01, -1.2565e-01, -1.2181e-01,  ..., -3.0349e-01, -3.0156e-01, -2.9964e-01],
          [-4.1050e-01, -3.0047e-01, -1.9044e-01,  ..., -3.4455e-01, -1.4565e-01,  5.3241e-02],
          ...,
          [ 7.4578e-02, -6.6032e-02, -2.0664e-01,  ..., -2.8959e-01, -3.7695e-01, -4.6431e-01],
          [-1.7008e-01, -1.0082e-01, -3.1560e-02,  ...,  1.3158e-01,  1.1818e-01,  1.0479e-01],
          [-2.9554e-01, -2.0226e-01, -1.0897e-01,  ...,  2.6485e-01,  1.3294e-01,  1.0353e-03]],

         [[ 3.2962e-01,  3.0020e-02, -2.6958e-01,  ...,  2.3588e-01,  3.5765e-01,  4.7942e-01],
          [ 2.2076e-01,  1.5281e-01,  8.4863e-02,  ...,  1.1369e-01,  4.1333e-02, -3.1027e-02],
          [-3.4159e-02,  1.7131e-02,  6.8421e-02,  ..., -2.2956e-01, -2.5770e-01, -2.8585e-01],
          ...,
          [ 1.9568e-01,  1.3904e-01,  8.2411e-02,  ..., -4.5908e-02, -9.1802e-03,  2.7547e-02],
          [ 3.0343e-01,  2.4539e-01,  1.8736e-01,  ...,  7.8076e-02, -7.2423e-02, -2.2292e-01],
          [ 2.2551e-01,  3.1858e-01,  4.1165e-01,  ...,  1.5081e-01,  1.7982e-01,  2.0883e-01]],

         [[-4.6282e-01, -2.2959e-01,  3.6463e-03,  ...,  9.7766e-04, -1.1250e-01, -2.2598e-01],
          [-2.8808e-01, -2.9823e-01, -3.0838e-01,  ..., -2.2857e-01, -1.1942e-01, -1.0261e-02],
          [-1.3563e-01, -1.2296e-01, -1.1029e-01,  ...,  3.3376e-01,  3.2750e-01,  3.2123e-01],
          ...,
          [ 2.6171e-01,  1.1373e-01, -3.4249e-02,  ...,  2.9150e-01, -8.0953e-03, -3.0770e-01],
          [ 1.1684e-01, -3.8548e-02, -1.9394e-01,  ...,  1.5768e-01,  1.9070e-01,  2.2371e-01],
          [-3.9501e-01, -4.6148e-02,  3.0271e-01,  ...,  4.2034e-01,  2.7212e-01,  1.2389e-01]]],


        [[[ 1.3210e-01,  2.0361e-01,  2.7511e-01,  ...,  1.3505e-02,  4.1213e-02,  6.8920e-02],
          [ 1.9678e-01,  8.6080e-02, -2.4621e-02,  ..., -3.5541e-01, -2.2021e-01, -8.5016e-02],
          [-1.7932e-01, -2.1305e-01, -2.4679e-01,  ..., -3.2019e-02, -1.7364e-01, -3.1526e-01],
          ...,
          [-1.1098e-01,  1.5599e-04,  1.1130e-01,  ...,  1.1212e-01, -9.5764e-02, -3.0364e-01],
          [-2.6452e-01, -1.8034e-01, -9.6168e-02,  ..., -1.7441e-01, -1.5631e-01, -1.3820e-01],
          [ 1.3602e-01,  2.4898e-01,  3.6193e-01,  ...,  5.1697e-02, -4.2697e-02, -1.3709e-01]],

         [[ 7.3900e-02, -7.7857e-02, -2.2961e-01,  ..., -2.8582e-01, -8.7693e-02,  1.1044e-01],
          [ 2.9307e-01,  7.8142e-02, -1.3679e-01,  ..., -4.1298e-01, -3.6121e-01, -3.0944e-01],
          [-2.2041e-01,  1.3617e-02,  2.4765e-01,  ...,  3.5985e-01,  1.1746e-01, -1.2493e-01],
          ...,
          [-1.0265e-01, -4.7027e-02,  8.5983e-03,  ..., -1.2737e-01,  3.5218e-02,  1.9780e-01],
          [ 7.6691e-03, -8.3026e-02, -1.7372e-01,  ...,  2.8157e-01,  2.0919e-02, -2.3973e-01],
          [ 2.1070e-01, -4.9673e-02, -3.1005e-01,  ...,  3.2309e-01,  1.5477e-01, -1.3558e-02]],

         [[ 4.0703e-01,  3.9644e-01,  3.8584e-01,  ...,  3.1995e-01, -6.2225e-02, -4.4440e-01],
          [ 3.1683e-01,  2.8426e-01,  2.5169e-01,  ...,  2.1200e-01, -1.6656e-02, -2.4531e-01],
          [-1.3114e-01, -1.6814e-03,  1.2778e-01,  ...,  2.3315e-01,  1.4258e-01,  5.2007e-02],
          ...,
          [-5.0710e-02,  8.0662e-02,  2.1203e-01,  ...,  9.1946e-02,  5.9162e-02,  2.6379e-02],
          [ 9.4003e-02, -8.2146e-03, -1.1043e-01,  ..., -2.3647e-01, -1.6301e-02,  2.0387e-01],
          [-7.7113e-02, -8.2326e-02, -8.7540e-02,  ...,  1.5617e-01,  1.3649e-01,  1.1681e-01]],

         ...,

         [[-3.0351e-01, -2.8511e-01, -2.6671e-01,  ...,  1.0471e-01,  2.7203e-01,  4.3935e-01],
          [-8.3348e-02,  7.6768e-02,  2.3689e-01,  ...,  6.2423e-02, -1.1925e-01, -3.0091e-01],
          [-1.2592e-01, -2.4766e-01, -3.6939e-01,  ..., -3.1992e-01, -2.0638e-01, -9.2842e-02],
          ...,
          [-1.6240e-01, -1.7900e-01, -1.9561e-01,  ..., -1.4087e-01, -1.2432e-01, -1.0776e-01],
          [ 7.3749e-02,  2.3932e-01,  4.0489e-01,  ..., -1.1814e-01,  6.4248e-02,  2.4664e-01],
          [-3.4393e-01, -2.0683e-01, -6.9733e-02,  ...,  3.3707e-01,  1.0418e-01, -1.2871e-01]],

         [[ 1.3156e-01,  3.1466e-02, -6.8631e-02,  ...,  4.1807e-01,  2.4072e-01,  6.3369e-02],
          [ 7.7501e-02,  3.5662e-02, -6.1779e-03,  ..., -1.8026e-01, -4.1539e-02,  9.7186e-02],
          [ 8.0294e-02,  4.7506e-02,  1.4718e-02,  ..., -2.6782e-01, -2.7331e-01, -2.7879e-01],
          ...,
          [-2.5670e-01, -1.2956e-01, -2.4276e-03,  ...,  3.0829e-01,  1.4336e-01, -2.1564e-02],
          [ 3.5537e-01,  1.8773e-01,  2.0094e-02,  ..., -2.2571e-01,  7.6466e-02,  3.7864e-01],
          [ 2.6189e-01,  1.9338e-01,  1.2487e-01,  ...,  4.5837e-01,  3.5806e-01,  2.5775e-01]],

         [[ 8.5345e-02,  1.3415e-01,  1.8295e-01,  ..., -3.9591e-01, -1.0544e-01,  1.8503e-01],
          [-2.9157e-01, -6.6046e-02,  1.5948e-01,  ...,  1.1986e-01, -3.3553e-02, -1.8696e-01],
          [ 2.9835e-01,  1.0561e-01, -8.7132e-02,  ..., -1.8041e-01,  1.1954e-01,  4.1950e-01],
          ...,
          [ 1.0668e-02, -5.8246e-02, -1.2716e-01,  ..., -1.2043e-01, -2.9271e-01, -4.6500e-01],
          [-3.4705e-02,  3.0164e-02,  9.5032e-02,  ...,  3.4681e-01,  1.0861e-01, -1.2958e-01],
          [ 4.8260e-01,  7.9787e-02, -3.2303e-01,  ...,  2.9030e-01,  2.8106e-03, -2.8468e-01]]],


        [[[-4.1529e-01, -3.1146e-01, -2.0763e-01,  ...,  3.5643e-01,  1.8089e-01,  5.3507e-03],
          [-8.8607e-02, -1.0242e-01, -1.1624e-01,  ...,  7.7983e-02, -1.5235e-01, -3.8269e-01],
          [-2.3225e-01, -1.2531e-01, -1.8378e-02,  ..., -3.1941e-01, -2.4689e-01, -1.7436e-01],
          ...,
          [ 1.7162e-01,  6.9456e-02, -3.2710e-02,  ...,  5.4778e-02,  9.2754e-04, -5.2923e-02],
          [-3.1670e-01, -2.5438e-01, -1.9205e-01,  ..., -2.3069e-01, -1.3812e-01, -4.5551e-02],
          [ 2.1490e-01,  2.2832e-02, -1.6923e-01,  ...,  2.4587e-01,  2.0537e-01,  1.6486e-01]],

         [[-1.7717e-01, -1.9490e-01, -2.1263e-01,  ...,  2.1315e-01,  8.2249e-03, -1.9670e-01],
          [-2.3284e-01, -2.9481e-01, -3.5679e-01,  ...,  1.1221e-02,  7.9444e-02,  1.4767e-01],
          [-1.4461e-02, -4.7321e-02, -8.0181e-02,  ..., -1.6499e-01, -2.5256e-01, -3.4014e-01],
          ...,
          [-2.1977e-01, -1.0610e-01,  7.5755e-03,  ..., -2.6943e-01, -1.7095e-01, -7.2462e-02],
          [-5.9988e-02,  6.7565e-02,  1.9512e-01,  ...,  9.1397e-02,  2.1945e-01,  3.4750e-01],
          [ 3.8434e-01,  1.1787e-03, -3.8198e-01,  ...,  2.5480e-01, -1.0736e-01, -4.6952e-01]],

         [[-3.4676e-01, -2.6599e-01, -1.8521e-01,  ...,  1.2613e-01,  7.1185e-02,  1.6238e-02],
          [ 1.2841e-01, -1.7430e-02, -1.6327e-01,  ...,  4.1177e-01,  3.5474e-01,  2.9772e-01],
          [-1.4758e-01, -1.2721e-01, -1.0685e-01,  ..., -2.7433e-01, -8.6807e-02,  1.0071e-01],
          ...,
          [ 8.5049e-02, -5.7676e-03, -9.6584e-02,  ...,  2.9335e-02,  4.1854e-03, -2.0964e-02],
          [-8.9555e-02,  9.6857e-02,  2.8327e-01,  ...,  1.8894e-01,  1.1319e-01,  3.7431e-02],
          [-2.4447e-01, -5.1689e-02,  1.4109e-01,  ...,  3.6884e-01,  3.5560e-01,  3.4237e-01]],

         ...,

         [[ 9.7058e-02,  6.3157e-02,  2.9255e-02,  ..., -4.6086e-01, -4.6494e-01, -4.6903e-01],
          [ 3.4804e-01,  2.0309e-01,  5.8141e-02,  ...,  3.2553e-01,  8.7695e-02, -1.5014e-01],
          [-1.1132e-01, -9.0447e-02, -6.9571e-02,  ...,  3.6126e-01, -3.0254e-02, -4.2176e-01],
          ...,
          [ 2.2460e-01,  2.0844e-01,  1.9228e-01,  ..., -1.1100e-02,  2.4882e-02,  6.0865e-02],
          [-4.5493e-01, -1.4226e-01,  1.7042e-01,  ..., -1.8341e-01, -2.8357e-01, -3.8373e-01],
          [ 2.8255e-01, -3.1207e-02, -3.4497e-01,  ...,  2.3062e-01,  1.2272e-01,  1.4820e-02]],

         [[ 1.7245e-01, -6.2418e-03, -1.8493e-01,  ..., -9.2705e-02, -5.4666e-02, -1.6628e-02],
          [ 1.0409e-01,  2.3453e-01,  3.6498e-01,  ..., -2.5218e-01, -1.5002e-01, -4.7861e-02],
          [-1.8183e-02,  3.0407e-02,  7.8998e-02,  ...,  1.7839e-01, -7.7196e-02, -3.3279e-01],
          ...,
          [ 1.8332e-01,  1.0402e-01,  2.4717e-02,  ..., -2.2543e-01, -8.8042e-02,  4.9342e-02],
          [-1.0672e-01, -1.3637e-01, -1.6602e-01,  ...,  2.2199e-01,  2.8891e-01,  3.5583e-01],
          [ 5.7590e-03,  1.2369e-01,  2.4161e-01,  ...,  4.5152e-01,  3.2637e-01,  2.0121e-01]],

         [[-3.1971e-01, -3.3000e-02,  2.5370e-01,  ..., -1.7296e-01, -1.8201e-01, -1.9107e-01],
          [-5.3425e-03,  8.7080e-05,  5.5167e-03,  ..., -2.0887e-02,  5.1221e-02,  1.2333e-01],
          [ 3.8426e-02,  1.4849e-01,  2.5856e-01,  ...,  1.9174e-01, -2.4766e-02, -2.4127e-01],
          ...,
          [-3.7319e-01, -2.6363e-01, -1.5406e-01,  ...,  3.0222e-01,  1.3443e-01, -3.3360e-02],
          [-3.1631e-02, -1.4419e-01, -2.5675e-01,  ..., -1.7791e-01, -9.4789e-02, -1.1671e-02],
          [-5.7468e-02, -2.0282e-01, -3.4817e-01,  ...,  9.1762e-02,  2.2502e-01,  3.5827e-01]]],


        ...,


        [[[-3.3796e-01, -7.1257e-02,  1.9545e-01,  ..., -3.6688e-01, -1.3235e-01,  1.0218e-01],
          [-3.6147e-01, -2.6256e-01, -1.6364e-01,  ...,  1.0518e-01,  1.3371e-01,  1.6224e-01],
          [-1.8491e-01, -7.1077e-02,  4.2760e-02,  ..., -3.7141e-01, -1.6533e-01,  4.0750e-02],
          ...,
          [-3.3033e-01, -2.7683e-01, -2.2333e-01,  ..., -2.8820e-01, -3.1362e-01, -3.3904e-01],
          [ 3.3167e-01,  1.3893e-01, -5.3805e-02,  ..., -3.2636e-01, -3.1473e-01, -3.0310e-01],
          [ 1.2751e-01, -8.7843e-02, -3.0319e-01,  ...,  3.9047e-01, -2.6518e-02, -4.4350e-01]],

         [[ 2.1978e-01,  1.5610e-01,  9.2425e-02,  ..., -1.3027e-01, -2.9037e-01, -4.5046e-01],
          [-1.2719e-01,  4.0594e-03,  1.3531e-01,  ...,  1.5173e-01,  9.6025e-02,  4.0324e-02],
          [-2.2946e-01, -3.2599e-02,  1.6426e-01,  ..., -2.1950e-01, -1.1611e-01, -1.2729e-02],
          ...,
          [ 3.5925e-01,  2.2677e-01,  9.4304e-02,  ..., -1.0191e-02,  9.7434e-02,  2.0506e-01],
          [ 1.0937e-02,  2.7761e-04, -1.0382e-02,  ..., -1.4829e-01, -1.2421e-01, -1.0013e-01],
          [ 1.0348e-01, -6.0327e-02, -2.2413e-01,  ..., -9.6729e-02, -1.6490e-01, -2.3307e-01]],

         [[ 1.5062e-01,  2.4915e-01,  3.4769e-01,  ..., -3.3330e-01, -3.1468e-01, -2.9605e-01],
          [-3.6678e-01, -2.3627e-01, -1.0576e-01,  ..., -2.2359e-01,  5.3977e-02,  3.3155e-01],
          [-1.0233e-01, -2.6878e-02,  4.8569e-02,  ...,  9.4251e-02,  1.1701e-01,  1.3977e-01],
          ...,
          [ 1.1513e-01,  1.2900e-01,  1.4287e-01,  ...,  6.6907e-02, -5.2022e-02, -1.7095e-01],
          [-1.3248e-01, -1.5257e-01, -1.7266e-01,  ...,  3.0335e-01,  1.9597e-01,  8.8593e-02],
          [ 3.3613e-01,  2.8494e-01,  2.3374e-01,  ..., -3.4590e-01, -1.5976e-01,  2.6389e-02]],

         ...,

         [[ 4.8989e-01,  2.7642e-01,  6.2953e-02,  ..., -1.9561e-01,  1.2851e-01,  4.5263e-01],
          [ 3.9514e-01,  2.5657e-01,  1.1800e-01,  ..., -2.0139e-01,  3.4391e-02,  2.7017e-01],
          [ 1.5905e-01,  2.4546e-02, -1.0996e-01,  ..., -3.0342e-02,  5.0081e-02,  1.3050e-01],
          ...,
          [ 3.6893e-01,  1.6706e-01, -3.4815e-02,  ...,  5.0839e-02,  2.8053e-02,  5.2679e-03],
          [ 7.6209e-02, -7.6841e-02, -2.2989e-01,  ...,  2.5327e-01,  2.2397e-01,  1.9466e-01],
          [-6.1791e-02,  8.2870e-02,  2.2753e-01,  ...,  2.3857e-01,  2.9011e-01,  3.4164e-01]],

         [[ 8.4404e-03, -1.6934e-01, -3.4713e-01,  ..., -3.9905e-02,  1.1223e-02,  6.2352e-02],
          [ 6.5572e-02,  1.0095e-01,  1.3634e-01,  ..., -3.3331e-02, -1.6415e-01, -2.9497e-01],
          [-4.7399e-02, -1.7068e-01, -2.9395e-01,  ...,  2.1735e-01, -3.6161e-02, -2.8967e-01],
          ...,
          [ 7.6869e-02, -5.0135e-02, -1.7714e-01,  ...,  3.5770e-02, -3.5212e-02, -1.0619e-01],
          [ 2.3900e-01,  2.5667e-01,  2.7434e-01,  ..., -3.7481e-01, -3.4443e-02,  3.0592e-01],
          [-1.3986e-01, -8.4566e-02, -2.9274e-02,  ..., -3.7359e-01, -1.5251e-01,  6.8579e-02]],

         [[-3.9861e-01, -8.6584e-02,  2.2545e-01,  ..., -2.2484e-01, -1.8772e-01, -1.5060e-01],
          [-3.3159e-01, -2.2719e-01, -1.2280e-01,  ..., -2.3328e-01, -2.4695e-01, -2.6061e-01],
          [ 6.9401e-02, -5.3212e-03, -8.0044e-02,  ...,  1.7803e-02,  2.0875e-02,  2.3947e-02],
          ...,
          [-4.2065e-01, -1.2776e-01,  1.6513e-01,  ...,  1.7706e-01,  5.8491e-02, -6.0076e-02],
          [ 5.0214e-03, -1.7050e-01, -3.4602e-01,  ...,  1.8055e-01,  1.8435e-01,  1.8814e-01],
          [-1.2260e-01,  5.7119e-03,  1.3403e-01,  ..., -3.2966e-02,  1.2794e-01,  2.8884e-01]]],


        [[[ 2.4656e-01,  2.2217e-01,  1.9778e-01,  ..., -1.9787e-01, -2.6383e-01, -3.2979e-01],
          [ 1.0074e-01,  8.1775e-02,  6.2809e-02,  ...,  6.9379e-02, -7.8113e-02, -2.2560e-01],
          [ 1.4949e-01, -5.2692e-02, -2.5487e-01,  ..., -2.2603e-01, -3.2565e-01, -4.2527e-01],
          ...,
          [ 3.6757e-01,  2.5377e-01,  1.3998e-01,  ..., -1.7152e-01, -2.2741e-01, -2.8329e-01],
          [ 3.1937e-01,  2.7537e-01,  2.3137e-01,  ..., -2.4902e-02, -7.8931e-02, -1.3296e-01],
          [-2.9884e-01, -1.4570e-01,  7.4421e-03,  ..., -4.8472e-02, -7.4119e-02, -9.9766e-02]],

         [[ 4.6853e-01,  3.9978e-02, -3.8857e-01,  ...,  9.0719e-02,  2.5009e-01,  4.0945e-01],
          [ 2.2220e-01,  3.3752e-01,  4.5283e-01,  ..., -3.2614e-01, -1.9617e-01, -6.6191e-02],
          [-4.3417e-02, -3.0733e-02, -1.8049e-02,  ..., -9.7901e-02, -7.9528e-02, -6.1156e-02],
          ...,
          [-3.4161e-01, -2.7614e-01, -2.1068e-01,  ...,  2.2161e-02, -3.2785e-02, -8.7730e-02],
          [ 3.2177e-01,  3.0465e-01,  2.8753e-01,  ...,  1.1420e-01,  1.8693e-01,  2.5965e-01],
          [-4.8174e-01, -2.8134e-01, -8.0936e-02,  ..., -1.6882e-01, -2.4316e-01, -3.1750e-01]],

         [[-1.1881e-01, -5.5211e-02,  8.3894e-03,  ...,  3.1326e-01, -3.3300e-02, -3.7986e-01],
          [-2.7425e-01, -2.2544e-01, -1.7662e-01,  ...,  1.1620e-01, -5.7306e-02, -2.3081e-01],
          [-1.1100e-01,  8.9317e-02,  2.8964e-01,  ..., -2.6818e-01, -2.1800e-01, -1.6782e-01],
          ...,
          [ 1.9914e-02, -2.9857e-02, -7.9628e-02,  ..., -2.2311e-01,  3.9147e-02,  3.0140e-01],
          [ 4.1796e-01,  2.1639e-01,  1.4820e-02,  ...,  3.1707e-01, -1.2797e-02, -3.4267e-01],
          [ 1.1254e-01, -1.4876e-01, -4.1006e-01,  ..., -2.3242e-01, -3.4380e-01, -4.5518e-01]],

         ...,

         [[-1.0029e-01, -2.1284e-01, -3.2540e-01,  ...,  1.1601e-01,  1.8982e-01,  2.6364e-01],
          [ 4.0110e-01,  1.0599e-01, -1.8912e-01,  ...,  3.6503e-01,  4.0338e-01,  4.4172e-01],
          [-2.4716e-02,  1.2111e-01,  2.6693e-01,  ...,  2.0660e-01, -1.6537e-02, -2.3968e-01],
          ...,
          [-4.3687e-01, -2.5902e-01, -8.1166e-02,  ..., -1.9432e-01, -3.3102e-02,  1.2811e-01],
          [-1.7323e-01, -1.0945e-01, -4.5672e-02,  ...,  9.6128e-02, -1.2043e-03, -9.8537e-02],
          [-2.8020e-01, -1.0385e-01,  7.2502e-02,  ..., -4.0628e-01, -1.5407e-01,  9.8136e-02]],

         [[ 6.5178e-02,  2.1305e-01,  3.6091e-01,  ...,  5.2302e-02, -1.5503e-01, -3.6236e-01],
          [-3.4974e-01, -3.1770e-01, -2.8565e-01,  ..., -8.2444e-02,  5.5333e-02,  1.9311e-01],
          [ 1.9012e-02,  8.6570e-02,  1.5413e-01,  ...,  1.7867e-01,  6.7174e-04, -1.7733e-01],
          ...,
          [ 3.4510e-01,  2.2763e-01,  1.1016e-01,  ..., -1.0375e-01, -9.8511e-02, -9.3272e-02],
          [ 1.3359e-01,  1.2428e-02, -1.0873e-01,  ..., -2.0734e-01,  2.5594e-02,  2.5853e-01],
          [-1.4589e-01, -1.2332e-01, -1.0075e-01,  ...,  1.4163e-01,  1.7071e-01,  1.9978e-01]],

         [[ 3.0301e-03,  6.4529e-02,  1.2603e-01,  ..., -2.1508e-01, -1.7227e-05,  2.1504e-01],
          [-2.3853e-01, -2.0317e-01, -1.6781e-01,  ..., -6.3426e-02,  3.5626e-02,  1.3468e-01],
          [-1.4985e-01, -1.8308e-02,  1.1323e-01,  ..., -1.5211e-02, -4.7399e-02, -7.9586e-02],
          ...,
          [-3.2840e-01, -1.3717e-01,  5.4068e-02,  ...,  2.3448e-02, -9.6633e-02, -2.1671e-01],
          [-5.2914e-02, -2.0750e-01, -3.6209e-01,  ...,  2.6522e-01,  1.8457e-01,  1.0391e-01],
          [-2.7903e-01, -3.2882e-01, -3.7861e-01,  ...,  5.6579e-02, -2.2143e-01, -4.9945e-01]]],


        [[[ 4.7510e-01,  1.4355e-01, -1.8799e-01,  ..., -1.4970e-01, -2.7705e-01, -4.0440e-01],
          [-3.1452e-01, -1.6356e-01, -1.2595e-02,  ...,  1.8819e-01,  7.4260e-03, -1.7333e-01],
          [ 1.1974e-01,  1.4110e-01,  1.6245e-01,  ...,  8.2041e-02,  9.4492e-02,  1.0694e-01],
          ...,
          [ 7.5993e-02,  7.4668e-03, -6.1059e-02,  ...,  1.3507e-01,  5.9497e-02, -1.6072e-02],
          [-4.5206e-01, -1.2055e-01,  2.1096e-01,  ..., -3.3711e-02,  6.4978e-02,  1.6367e-01],
          [-2.0647e-02,  1.5784e-01,  3.3632e-01,  ...,  3.5845e-01, -5.5989e-02, -4.7043e-01]],

         [[-1.9007e-01, -2.5659e-01, -3.2312e-01,  ...,  4.6664e-01,  3.7771e-01,  2.8877e-01],
          [-1.9878e-01,  4.0318e-02,  2.7941e-01,  ...,  5.6407e-02,  9.3246e-02,  1.3009e-01],
          [-7.4838e-02, -1.8123e-01, -2.8763e-01,  ...,  2.0323e-01, -3.3836e-02, -2.7090e-01],
          ...,
          [-2.7091e-01, -2.9243e-01, -3.1395e-01,  ...,  1.8597e-01, -5.3716e-02, -2.9340e-01],
          [-2.5890e-01, -6.1447e-03,  2.4661e-01,  ..., -3.4669e-01, -2.2655e-01, -1.0641e-01],
          [ 8.0751e-02,  1.0421e-01,  1.2767e-01,  ..., -8.1244e-02, -2.7709e-01, -4.7294e-01]],

         [[-1.5307e-01, -4.4736e-03,  1.4412e-01,  ...,  4.0069e-02,  1.9278e-01,  3.4549e-01],
          [ 1.1657e-01, -2.5975e-02, -1.6852e-01,  ...,  4.6076e-02,  2.4185e-02,  2.2932e-03],
          [ 9.3205e-03, -2.0998e-02, -5.1317e-02,  ...,  5.9277e-02, -3.4743e-02, -1.2876e-01],
          ...,
          [-1.6403e-01,  1.2134e-02,  1.8830e-01,  ...,  2.4914e-01,  1.5621e-01,  6.3279e-02],
          [ 3.9369e-01,  1.3466e-01, -1.2438e-01,  ..., -3.4106e-01, -1.9285e-02,  3.0249e-01],
          [ 1.7439e-01,  3.0211e-02, -1.1396e-01,  ..., -2.5737e-01, -3.1006e-01, -3.6275e-01]],

         ...,

         [[-2.9111e-02,  1.5620e-01,  3.4151e-01,  ..., -1.9721e-02, -7.5754e-02, -1.3179e-01],
          [-1.2584e-01, -2.1229e-01, -2.9873e-01,  ...,  7.0687e-02, -2.1731e-02, -1.1415e-01],
          [-3.6924e-01, -1.0983e-01,  1.4957e-01,  ...,  1.1487e-01,  4.0505e-02, -3.3857e-02],
          ...,
          [-2.2964e-02, -7.0038e-02, -1.1711e-01,  ..., -2.4998e-01,  7.7752e-02,  4.0548e-01],
          [ 3.5472e-01,  1.4887e-01, -5.6985e-02,  ...,  2.4295e-01, -5.1089e-02, -3.4513e-01],
          [ 2.4437e-01,  2.5301e-01,  2.6164e-01,  ...,  8.5015e-02,  2.8348e-01,  4.8195e-01]],

         [[ 5.4283e-02,  8.2525e-02,  1.1077e-01,  ..., -9.5148e-02,  4.3113e-03,  1.0377e-01],
          [ 1.8320e-01, -1.2747e-02, -2.0869e-01,  ..., -2.0129e-01, -4.3748e-02,  1.1380e-01],
          [ 1.4033e-01,  5.8618e-03, -1.2861e-01,  ...,  3.6856e-01,  3.1915e-01,  2.6975e-01],
          ...,
          [-1.5191e-04, -6.3498e-02, -1.2684e-01,  ..., -1.0528e-01, -1.0496e-01, -1.0465e-01],
          [ 3.2758e-01,  2.0163e-01,  7.5674e-02,  ...,  2.9030e-01,  2.6261e-01,  2.3491e-01],
          [-3.5408e-02, -2.4465e-01, -4.5389e-01,  ...,  5.9448e-02, -1.9820e-01, -4.5584e-01]],

         [[-4.1680e-01, -1.1047e-02,  3.9470e-01,  ..., -4.6728e-02, -8.5885e-02, -1.2504e-01],
          [-3.2908e-01, -1.4153e-01,  4.6019e-02,  ..., -5.2961e-03, -7.2824e-02, -1.4035e-01],
          [ 1.0535e-01, -8.7265e-02, -2.7988e-01,  ..., -4.5445e-02, -5.0164e-02, -5.4883e-02],
          ...,
          [-8.9435e-02,  8.5166e-02,  2.5977e-01,  ..., -1.8107e-01, -2.0926e-02,  1.3922e-01],
          [ 9.9415e-02,  1.2257e-01,  1.4573e-01,  ..., -1.0130e-01,  9.5325e-02,  2.9195e-01],
          [-3.3897e-01, -1.5721e-01,  2.4545e-02,  ...,  9.3369e-02, -8.1428e-02, -2.5622e-01]]]])
DESIRED: (shape=torch.Size([16, 160, 16, 12]), dtype=torch.float32)
tensor([[[[ 1.5640e-01,  7.6929e-02, -8.2002e-02,  ..., -9.3433e-02, -1.7429e-01, -2.1472e-01],
          [-2.7646e-01, -1.2360e-01,  1.8213e-01,  ..., -1.3797e-01, -1.0432e-01, -8.7497e-02],
          [-2.2093e-01, -1.5753e-01, -3.0733e-02,  ...,  1.2305e-02,  1.9483e-02,  2.3071e-02],
          ...,
          [-5.0089e-02, -1.0243e-02,  6.9451e-02,  ..., -1.4660e-01, -1.5712e-01, -1.6238e-01],
          [ 3.6658e-01,  3.5838e-01,  3.4199e-01,  ...,  4.3200e-01,  3.8841e-01,  3.6661e-01],
          [ 1.6716e-01,  8.3422e-02, -8.4061e-02,  ...,  1.7278e-01, -1.5507e-01, -3.1899e-01]],

         [[-2.0740e-01, -1.6843e-01, -9.0481e-02,  ...,  1.5850e-01,  2.3044e-01,  2.6642e-01],
          [-1.3046e-02, -2.7511e-02, -5.6441e-02,  ..., -1.2936e-01,  4.0629e-02,  1.2562e-01],
          [-1.9835e-04, -3.2644e-02, -9.7536e-02,  ...,  3.9850e-01,  3.5799e-01,  3.3773e-01],
          ...,
          [ 7.5378e-02,  8.4918e-02,  1.0400e-01,  ..., -1.7854e-01, -6.0647e-02, -1.7002e-03],
          [-1.6059e-01, -1.5231e-01, -1.3575e-01,  ...,  2.0788e-01, -1.8507e-01, -3.8155e-01],
          [-1.1611e-02,  1.6771e-02,  7.3534e-02,  ..., -1.5998e-01,  3.0191e-02,  1.2528e-01]],

         [[-2.6492e-01, -1.2315e-01,  1.6038e-01,  ...,  2.9148e-01,  2.4800e-02, -1.0854e-01],
          [-4.1821e-01, -3.4792e-01, -2.0735e-01,  ..., -1.1304e-01, -2.9202e-01, -3.8151e-01],
          [-1.3400e-02, -7.6064e-02, -2.0139e-01,  ..., -1.4290e-01, -1.9288e-01, -2.1787e-01],
          ...,
          [ 1.6844e-01,  1.2614e-01,  4.1543e-02,  ..., -2.1090e-01, -3.4841e-01, -4.1716e-01],
          [-1.7250e-02, -1.1706e-01, -3.1668e-01,  ...,  3.0498e-01,  2.9826e-01,  2.9491e-01],
          [-2.7487e-01, -2.7281e-01, -2.6868e-01,  ..., -1.8854e-01, -2.3704e-01, -2.6129e-01]],

         ...,

         [[-1.3965e-01, -1.2518e-02,  2.4175e-01,  ...,  8.2384e-02,  9.5354e-02,  1.0184e-01],
          [ 2.8751e-01,  2.7494e-01,  2.4981e-01,  ...,  1.6237e-02,  1.0113e-01,  1.4358e-01],
          [-2.3886e-01, -9.7003e-02,  1.8670e-01,  ...,  2.5849e-01,  2.2685e-01,  2.1103e-01],
          ...,
          [-3.8426e-01, -3.1630e-01, -1.8038e-01,  ..., -1.3638e-01, -1.2179e-01, -1.1449e-01],
          [ 1.3491e-01,  1.7150e-01,  2.4469e-01,  ..., -2.8900e-01, -2.0803e-01, -1.6754e-01],
          [-3.9118e-01, -2.2433e-01,  1.0939e-01,  ..., -1.8460e-01, -8.4874e-02, -3.5010e-02]],

         [[-4.5894e-02, -8.1274e-02, -1.5204e-01,  ..., -1.8077e-01, -5.3685e-02,  9.8559e-03],
          [-1.5223e-01, -4.1907e-02,  1.7874e-01,  ...,  1.7151e-01,  9.5181e-02,  5.7015e-02],
          [ 1.8551e-01,  1.3057e-01,  2.0689e-02,  ..., -9.5626e-02, -1.9605e-02,  1.8405e-02],
          ...,
          [ 3.4669e-01,  1.9884e-01, -9.6874e-02,  ..., -3.0424e-01, -2.2944e-01, -1.9204e-01],
          [ 1.6632e-01,  4.8345e-02, -1.8760e-01,  ...,  3.5742e-01,  2.8184e-01,  2.4406e-01],
          [ 4.8249e-01,  2.4705e-01, -2.2382e-01,  ...,  9.7397e-02,  9.8109e-02,  9.8466e-02]],

         [[-1.6374e-03, -4.6318e-02, -1.3568e-01,  ..., -2.6183e-01,  9.3414e-02,  2.7103e-01],
          [ 2.3509e-03, -1.1300e-01, -3.4370e-01,  ...,  3.9441e-01,  2.2517e-01,  1.4054e-01],
          [ 1.6991e-02, -1.9112e-02, -9.1317e-02,  ..., -1.0802e-01,  2.5121e-01,  4.3083e-01],
          ...,
          [-3.5002e-02, -7.4010e-02, -1.5203e-01,  ..., -9.8157e-02, -1.6473e-01, -1.9802e-01],
          [-4.1147e-01, -3.3800e-01, -1.9105e-01,  ...,  1.5605e-01, -1.2755e-02, -9.7159e-02],
          [-1.9024e-02,  3.5640e-02,  1.4497e-01,  ...,  2.0527e-02,  7.5676e-02,  1.0325e-01]]],


        [[[-7.8797e-02, -8.5400e-02, -9.8606e-02,  ..., -1.5213e-01, -4.6873e-02,  5.7537e-03],
          [-1.1916e-01, -4.1782e-02,  1.1297e-01,  ..., -9.0347e-02, -1.4228e-01, -1.6825e-01],
          [-2.0643e-01, -1.0742e-01,  9.0592e-02,  ..., -2.2693e-01, -2.2006e-02,  8.0454e-02],
          ...,
          [ 2.6952e-01,  3.1109e-01,  3.9423e-01,  ..., -1.3240e-01, -1.1483e-01, -1.0605e-01],
          [-4.5904e-01, -3.8046e-01, -2.2329e-01,  ..., -3.7461e-02, -2.0474e-01, -2.8837e-01],
          [-2.4126e-01, -1.6219e-01, -4.0441e-03,  ...,  2.9149e-01,  2.8918e-01,  2.8802e-01]],

         [[ 1.7817e-01,  1.2729e-01,  2.5537e-02,  ...,  1.8162e-01,  1.5195e-01,  1.3712e-01],
          [-2.7505e-01, -1.5071e-01,  9.7964e-02,  ..., -2.6578e-01, -4.1616e-01, -4.9135e-01],
          [ 2.2881e-01,  9.5128e-02, -1.7223e-01,  ..., -1.2071e-01,  3.4774e-02,  1.1252e-01],
          ...,
          [-2.2385e-01, -1.8741e-01, -1.1453e-01,  ..., -6.6690e-02, -2.1823e-01, -2.9401e-01],
          [-2.6064e-02, -3.8257e-02, -6.2644e-02,  ..., -5.1251e-02, -1.7510e-01, -2.3702e-01],
          [ 1.5668e-01,  2.0416e-01,  2.9911e-01,  ...,  1.7623e-01,  2.2507e-01,  2.4949e-01]],

         [[ 1.5042e-01,  2.2744e-01,  3.8147e-01,  ..., -1.3264e-01, -8.3298e-02, -5.8630e-02],
          [ 1.3874e-01,  1.9716e-01,  3.1401e-01,  ..., -3.4785e-02, -2.8139e-01, -4.0470e-01],
          [-3.7032e-01, -1.8361e-01,  1.8981e-01,  ...,  2.1900e-01,  2.3043e-01,  2.3614e-01],
          ...,
          [ 9.7446e-02,  3.3008e-02, -9.5866e-02,  ..., -2.5396e-01, -5.4762e-02,  4.4838e-02],
          [ 4.0057e-01,  3.2365e-01,  1.6981e-01,  ..., -2.1036e-01, -7.8177e-02, -1.2085e-02],
          [ 7.3062e-02,  4.1122e-02, -2.2759e-02,  ..., -3.6085e-02, -1.1975e-01, -1.6158e-01]],

         ...,

         [[ 7.3451e-02,  9.8095e-02,  1.4738e-01,  ...,  1.6987e-01,  2.1756e-01,  2.4141e-01],
          [-4.3166e-01, -4.2235e-01, -4.0375e-01,  ...,  4.2150e-01,  4.1607e-01,  4.1336e-01],
          [ 4.0997e-01,  2.6407e-01, -2.7726e-02,  ..., -2.9260e-01, -2.6025e-01, -2.4408e-01],
          ...,
          [ 1.1641e-01,  1.8391e-01,  3.1891e-01,  ...,  3.4940e-03,  1.1302e-01,  1.6778e-01],
          [-3.0487e-01, -2.5914e-01, -1.6769e-01,  ...,  1.7802e-01,  3.2449e-01,  3.9773e-01],
          [ 1.0328e-01,  2.3060e-02, -1.3737e-01,  ..., -1.5542e-01, -1.2862e-01, -1.1522e-01]],

         [[ 2.4906e-04,  3.6916e-02,  1.1025e-01,  ..., -1.5022e-01,  1.3650e-02,  9.5585e-02],
          [-8.1704e-02, -4.9969e-02,  1.3501e-02,  ...,  1.3599e-01, -1.1061e-01, -2.3390e-01],
          [ 3.0567e-01,  1.3634e-01, -2.0234e-01,  ..., -9.9082e-02,  1.9516e-01,  3.4227e-01],
          ...,
          [ 1.3726e-01,  4.3956e-02, -1.4264e-01,  ..., -1.3491e-01, -3.3885e-01, -4.4082e-01],
          [ 3.5491e-01,  2.8786e-01,  1.5377e-01,  ..., -5.7761e-02, -3.8022e-03,  2.3177e-02],
          [ 7.5458e-02,  4.2919e-02, -2.2160e-02,  ..., -1.5080e-01, -1.3314e-01, -1.2430e-01]],

         [[ 3.4882e-01,  2.1806e-01, -4.3448e-02,  ...,  1.9770e-01, -5.4120e-03, -1.0697e-01],
          [-4.6499e-01, -2.6347e-01,  1.3955e-01,  ...,  8.4253e-02,  2.5254e-01,  3.3668e-01],
          [-2.4941e-01, -1.9384e-01, -8.2699e-02,  ...,  1.2812e-01,  2.7825e-01,  3.5331e-01],
          ...,
          [ 3.9846e-01,  2.5650e-01, -2.7430e-02,  ..., -1.5878e-02, -3.2801e-01, -4.8408e-01],
          [ 1.4778e-01, -4.2385e-03, -3.0829e-01,  ..., -3.4269e-02, -2.0549e-02, -1.3690e-02],
          [-2.7590e-01, -1.0448e-01,  2.3836e-01,  ..., -8.7755e-02, -6.4740e-02, -5.3232e-02]]],


        [[[ 4.4775e-02,  1.2197e-01,  2.7637e-01,  ...,  2.2435e-01,  2.2335e-01,  2.2285e-01],
          [-1.8549e-01, -1.3132e-01, -2.2970e-02,  ..., -8.5293e-02,  7.7961e-02,  1.5959e-01],
          [ 2.4897e-01,  2.7331e-01,  3.2199e-01,  ..., -1.0553e-01, -2.9157e-01, -3.8459e-01],
          ...,
          [-1.9295e-01, -1.5599e-01, -8.2082e-02,  ..., -4.8158e-02,  1.8959e-02,  5.2518e-02],
          [-1.0078e-01, -4.2779e-02,  7.3220e-02,  ...,  2.0960e-01,  9.3705e-03, -9.0746e-02],
          [-1.6875e-01, -1.8282e-01, -2.1096e-01,  ..., -1.9423e-01,  9.2927e-02,  2.3650e-01]],

         [[-6.0625e-02, -1.0206e-01, -1.8494e-01,  ..., -7.1463e-02, -1.4566e-01, -1.8276e-01],
          [-1.8734e-01, -7.3926e-02,  1.5290e-01,  ..., -2.2299e-01, -3.9249e-01, -4.7724e-01],
          [-4.4976e-01, -2.9904e-01,  2.4064e-03,  ..., -3.1126e-02, -1.6654e-01, -2.3424e-01],
          ...,
          [-2.3879e-01, -1.2034e-01,  1.1656e-01,  ...,  1.3185e-03,  9.1887e-02,  1.3717e-01],
          [-2.8173e-01, -2.6431e-01, -2.2948e-01,  ..., -2.0512e-01,  2.2928e-01,  4.4649e-01],
          [-7.9555e-02, -3.9115e-04,  1.5794e-01,  ..., -1.6866e-01, -1.6022e-01, -1.5600e-01]],

         [[-1.8784e-01, -1.8193e-01, -1.7011e-01,  ..., -2.5536e-01, -2.7718e-01, -2.8809e-01],
          [-3.1859e-01, -2.3125e-01, -5.6573e-02,  ...,  3.2596e-01,  3.9981e-01,  4.3673e-01],
          [ 1.3532e-01,  1.1207e-01,  6.5559e-02,  ..., -9.6841e-02, -2.8091e-01, -3.7294e-01],
          ...,
          [ 4.2354e-01,  2.9535e-01,  3.8965e-02,  ...,  6.2850e-02,  2.4878e-02,  5.8919e-03],
          [ 2.5431e-01,  1.0278e-01, -2.0028e-01,  ...,  1.7860e-01,  1.4977e-01,  1.3536e-01],
          [-1.9018e-01, -1.9158e-01, -1.9437e-01,  ...,  1.1400e-01,  7.4407e-02,  5.4610e-02]],

         ...,

         [[-3.8710e-02,  4.8346e-04,  7.8871e-02,  ..., -1.2506e-01,  9.0004e-03,  7.6029e-02],
          [-4.1105e-01, -2.7409e-01, -1.5053e-04,  ...,  3.2171e-01,  5.8433e-02, -7.3207e-02],
          [ 2.7693e-03, -7.5918e-02, -2.3329e-01,  ..., -1.7883e-01,  1.8822e-01,  3.7174e-01],
          ...,
          [ 3.9746e-02, -4.5220e-02, -2.1515e-01,  ..., -1.1988e-02, -1.4721e-01, -2.1483e-01],
          [ 3.4256e-01,  1.3539e-01, -2.7896e-01,  ..., -4.3323e-01, -3.4101e-01, -2.9489e-01],
          [-3.2461e-01, -2.0245e-01,  4.1882e-02,  ...,  2.3128e-02, -9.4386e-03, -2.5722e-02]],

         [[ 2.3989e-01,  1.7305e-01,  3.9382e-02,  ..., -6.7548e-02,  1.9065e-01,  3.1974e-01],
          [-2.1431e-01, -8.3147e-02,  1.7919e-01,  ...,  8.7051e-02, -1.2512e-01, -2.3121e-01],
          [-4.1500e-01, -3.9395e-01, -3.5183e-01,  ...,  3.3128e-02, -1.8179e-01, -2.8926e-01],
          ...,
          [ 2.7525e-01,  1.9416e-01,  3.1961e-02,  ..., -5.1391e-02,  9.2832e-03,  3.9620e-02],
          [ 3.2537e-02, -9.7167e-02, -3.5658e-01,  ...,  1.8857e-02,  2.6414e-01,  3.8679e-01],
          [ 6.3515e-02,  4.6256e-02,  1.1736e-02,  ..., -2.4938e-01, -1.5659e-01, -1.1019e-01]],

         [[-1.0575e-01, -8.4452e-03,  1.8616e-01,  ...,  1.2590e-01,  1.0696e-01,  9.7484e-02],
          [ 2.8265e-01,  2.3132e-01,  1.2867e-01,  ..., -5.5107e-02, -5.0257e-02, -4.7832e-02],
          [ 3.1682e-01,  2.1070e-01, -1.5555e-03,  ...,  2.2982e-01,  1.4366e-01,  1.0058e-01],
          ...,
          [-7.3846e-02, -9.7466e-02, -1.4471e-01,  ..., -8.9478e-02, -1.6489e-01, -2.0259e-01],
          [ 3.8928e-01,  2.0382e-01, -1.6710e-01,  ...,  3.2212e-01,  3.8313e-01,  4.1363e-01],
          [ 1.1495e-02,  4.4015e-02,  1.0905e-01,  ..., -2.7968e-01, -3.3102e-01, -3.5669e-01]]],


        ...,


        [[[-1.4668e-01, -8.6111e-02,  3.5021e-02,  ...,  1.8171e-01,  1.1252e-01,  7.7919e-02],
          [-3.5993e-02,  3.4843e-02,  1.7652e-01,  ...,  3.3831e-01,  1.1328e-01,  7.6490e-04],
          [-6.9510e-02,  3.7117e-02,  2.5037e-01,  ..., -1.2956e-01, -1.9321e-01, -2.2504e-01],
          ...,
          [-2.2656e-01, -2.0566e-01, -1.6385e-01,  ..., -1.7280e-01, -1.5570e-01, -1.4716e-01],
          [ 3.9738e-01,  2.3656e-01, -8.5081e-02,  ..., -3.2412e-01, -4.3180e-01, -4.8564e-01],
          [-4.0012e-01, -3.5926e-01, -2.7753e-01,  ...,  1.8332e-01,  3.1904e-01,  3.8689e-01]],

         [[-1.3933e-01, -8.1536e-02,  3.4060e-02,  ..., -2.6613e-01, -1.8841e-01, -1.4955e-01],
          [ 3.5339e-01,  1.5124e-01, -2.5306e-01,  ..., -5.6955e-02, -2.2904e-01, -3.1508e-01],
          [ 2.0101e-02, -2.1702e-02, -1.0531e-01,  ...,  1.2590e-01,  5.3522e-02,  1.7335e-02],
          ...,
          [ 6.5354e-02,  5.5796e-02,  3.6678e-02,  ...,  2.6889e-01,  1.6169e-01,  1.0810e-01],
          [-4.9379e-02,  6.1844e-02,  2.8429e-01,  ..., -2.4287e-01, -2.0097e-01, -1.8002e-01],
          [ 1.4671e-02,  6.1214e-02,  1.5430e-01,  ...,  1.2576e-01,  1.0667e-01,  9.7132e-02]],

         [[-8.0962e-02, -9.3351e-02, -1.1813e-01,  ...,  2.7690e-01,  1.6172e-01,  1.0414e-01],
          [-7.6904e-02,  1.5487e-02,  2.0027e-01,  ...,  1.4222e-01, -2.0683e-01, -3.8135e-01],
          [ 2.6631e-01,  2.1446e-01,  1.1075e-01,  ..., -3.1377e-01, -2.5044e-01, -2.1877e-01],
          ...,
          [-1.7815e-01, -1.1299e-02,  3.2240e-01,  ..., -3.4107e-01, -1.3413e-01, -3.0659e-02],
          [ 1.8593e-02,  9.8732e-02,  2.5901e-01,  ...,  9.1919e-02,  2.0117e-01,  2.5580e-01],
          [-4.0844e-02,  3.3964e-02,  1.8358e-01,  ...,  2.2497e-01, -8.6514e-02, -2.4226e-01]],

         ...,

         [[-3.6813e-01, -3.5162e-01, -3.1860e-01,  ...,  5.9854e-02, -1.0675e-01, -1.9004e-01],
          [-3.3208e-01, -1.3721e-01,  2.5252e-01,  ...,  2.9613e-01,  2.7089e-01,  2.5827e-01],
          [ 2.8011e-01,  2.6175e-01,  2.2503e-01,  ..., -1.6111e-01, -1.4035e-01, -1.2996e-01],
          ...,
          [ 2.2065e-01,  1.6426e-01,  5.1490e-02,  ..., -4.6894e-02,  1.2031e-03,  2.5251e-02],
          [ 8.8172e-02,  1.5124e-01,  2.7737e-01,  ..., -3.8583e-01, -2.1676e-01, -1.3223e-01],
          [-1.4446e-01, -1.0575e-01, -2.8317e-02,  ..., -7.0394e-02, -1.1552e-02,  1.7869e-02]],

         [[ 2.9178e-02,  2.1256e-02,  5.4100e-03,  ...,  3.5593e-02, -2.1079e-01, -3.3399e-01],
          [ 3.6476e-01,  1.9957e-01, -1.3081e-01,  ..., -2.2866e-01,  1.7554e-01,  3.7764e-01],
          [-6.2133e-02, -1.0691e-01, -1.9647e-01,  ..., -2.2361e-01, -2.8134e-01, -3.1020e-01],
          ...,
          [-4.2559e-02, -3.1782e-02, -1.0227e-02,  ...,  1.4380e-02,  2.0009e-01,  2.9294e-01],
          [-3.9845e-01, -4.0669e-01, -4.2319e-01,  ..., -9.0850e-02, -1.1748e-01, -1.3080e-01],
          [ 2.2208e-01,  1.5067e-01,  7.8704e-03,  ...,  8.3142e-02,  3.0517e-01,  4.1618e-01]],

         [[ 1.4545e-01,  1.1596e-01,  5.6972e-02,  ...,  1.3252e-01, -7.7988e-02, -1.8324e-01],
          [ 3.7576e-01,  3.3163e-01,  2.4336e-01,  ..., -1.3882e-01,  1.6807e-02,  9.4622e-02],
          [ 3.5854e-01,  3.2659e-01,  2.6270e-01,  ..., -6.4729e-02,  1.0053e-01,  1.8316e-01],
          ...,
          [-2.8120e-01, -1.6771e-01,  5.9274e-02,  ..., -1.7860e-01, -2.0463e-01, -2.1764e-01],
          [-4.2764e-01, -4.3602e-01, -4.5280e-01,  ..., -8.8547e-02,  2.8555e-01,  4.7260e-01],
          [-1.1533e-01, -5.0628e-02,  7.8768e-02,  ...,  2.2858e-02,  2.1675e-01,  3.1369e-01]]],


        [[[-4.4768e-01, -2.9849e-01, -1.1550e-04,  ...,  1.5146e-01,  3.4189e-02, -2.4444e-02],
          [-6.1135e-02, -1.6484e-03,  1.1733e-01,  ..., -1.4640e-01,  1.6031e-01,  3.1367e-01],
          [-2.0724e-01, -1.4729e-01, -2.7394e-02,  ...,  1.2692e-01,  1.8667e-01,  2.1655e-01],
          ...,
          [ 1.2292e-01,  1.7031e-02, -1.9474e-01,  ..., -2.1153e-01, -7.8748e-02, -1.2356e-02],
          [-4.5485e-01, -2.9527e-01,  2.3903e-02,  ..., -3.1675e-01, -4.6407e-02,  8.8764e-02],
          [-2.5973e-01, -1.2313e-01,  1.5005e-01,  ...,  1.7935e-02,  3.6494e-02,  4.5774e-02]],

         [[-2.9798e-01, -2.5620e-01, -1.7264e-01,  ...,  1.2444e-01,  1.3073e-01,  1.3387e-01],
          [-6.5722e-02, -1.1274e-01, -2.0679e-01,  ..., -1.0270e-01, -2.6788e-01, -3.5046e-01],
          [ 3.2500e-02, -7.2652e-02, -2.8295e-01,  ...,  2.4972e-01,  3.8155e-01,  4.4746e-01],
          ...,
          [ 1.0253e-01,  4.0456e-02, -8.3696e-02,  ..., -3.0743e-01, -3.8478e-01, -4.2346e-01],
          [ 2.7392e-01,  2.2453e-01,  1.2573e-01,  ..., -4.1022e-01, -4.5768e-01, -4.8141e-01],
          [-1.0659e-01, -1.4822e-01, -2.3149e-01,  ...,  9.3123e-02, -6.6832e-02, -1.4681e-01]],

         [[ 3.4098e-01,  3.1991e-01,  2.7778e-01,  ..., -7.0290e-02, -1.0108e-01, -1.1647e-01],
          [-4.3116e-01, -3.2865e-01, -1.2362e-01,  ..., -8.0102e-02,  1.9243e-01,  3.2869e-01],
          [ 2.7622e-01,  1.8734e-01,  9.5878e-03,  ..., -1.9870e-01, -5.7709e-02,  1.2789e-02],
          ...,
          [-3.5811e-01, -3.1782e-01, -2.3724e-01,  ..., -1.1105e-01,  1.5626e-01,  2.8992e-01],
          [ 1.5413e-01,  3.6326e-02, -1.9929e-01,  ...,  1.2531e-01,  1.1279e-01,  1.0654e-01],
          [ 1.8553e-01,  8.2290e-02, -1.2420e-01,  ..., -8.0739e-02, -8.7591e-02, -9.1018e-02]],

         ...,

         [[ 1.0350e-01,  1.6117e-01,  2.7651e-01,  ...,  3.8810e-02, -1.2207e-01, -2.0250e-01],
          [ 5.7561e-02,  6.8766e-02,  9.1175e-02,  ...,  1.9367e-01,  1.1861e-01,  8.1071e-02],
          [-1.3455e-01, -3.0161e-02,  1.7862e-01,  ...,  1.3533e-01, -8.1504e-03, -7.9890e-02],
          ...,
          [-3.5218e-01, -3.5781e-01, -3.6909e-01,  ...,  2.8083e-02,  2.4932e-01,  3.5994e-01],
          [ 4.8199e-01,  3.5371e-01,  9.7149e-02,  ..., -1.9231e-01, -7.5210e-02, -1.6659e-02],
          [ 5.2021e-02,  2.8426e-03, -9.5514e-02,  ..., -2.0099e-02,  6.7850e-02,  1.1182e-01]],

         [[ 3.8293e-01,  2.8420e-01,  8.6745e-02,  ...,  7.2959e-02, -1.3356e-01, -2.3682e-01],
          [ 1.0161e-01,  1.2634e-01,  1.7579e-01,  ..., -1.2953e-01, -1.6290e-01, -1.7958e-01],
          [ 2.1409e-01,  1.6687e-01,  7.2426e-02,  ...,  2.3727e-01,  3.1319e-01,  3.5115e-01],
          ...,
          [-5.1892e-02, -4.0254e-02, -1.6978e-02,  ...,  1.3985e-01,  9.7494e-02,  7.6319e-02],
          [ 2.4434e-01,  1.5316e-01, -2.9215e-02,  ..., -1.8072e-01, -2.6541e-01, -3.0776e-01],
          [-2.3420e-01, -2.1563e-01, -1.7848e-01,  ...,  6.2181e-02, -1.1563e-01, -2.0453e-01]],

         [[ 2.5626e-01,  2.4388e-01,  2.1912e-01,  ...,  1.9989e-01,  1.4617e-01,  1.1931e-01],
          [-2.4618e-01, -3.0033e-01, -4.0864e-01,  ..., -1.5917e-02, -1.9033e-01, -2.7753e-01],
          [-1.6278e-01, -1.9764e-01, -2.6734e-01,  ..., -1.6384e-01,  2.1625e-02,  1.1436e-01],
          ...,
          [ 1.8390e-01,  1.7934e-01,  1.7022e-01,  ..., -7.1367e-02,  1.4565e-01,  2.5415e-01],
          [ 3.4396e-01,  1.4240e-01, -2.6074e-01,  ..., -2.3736e-01,  9.9219e-02,  2.6751e-01],
          [-8.6746e-02, -1.2121e-01, -1.9014e-01,  ...,  8.1286e-02,  3.7086e-02,  1.4987e-02]]],


        [[[-3.0627e-01, -2.8459e-01, -2.4122e-01,  ...,  2.7306e-01,  1.0177e-01,  1.6126e-02],
          [ 3.1946e-02,  1.0827e-01,  2.6093e-01,  ...,  2.9447e-01,  3.8641e-01,  4.3239e-01],
          [-8.2209e-02,  1.7045e-02,  2.1555e-01,  ...,  1.0750e-01, -2.1454e-01, -3.7556e-01],
          ...,
          [-4.3387e-01, -3.8081e-01, -2.7469e-01,  ...,  5.6404e-02, -2.1601e-01, -3.5221e-01],
          [ 4.5452e-02,  1.0155e-01,  2.1373e-01,  ...,  1.1961e-02, -1.3075e-01, -2.0211e-01],
          [-6.3450e-02, -1.4121e-01, -2.9674e-01,  ...,  3.8782e-02, -2.1507e-01, -3.4200e-01]],

         [[-2.1342e-01, -1.7503e-01, -9.8254e-02,  ...,  3.3653e-01,  1.8188e-01,  1.0455e-01],
          [-2.9635e-01, -1.8059e-01,  5.0937e-02,  ...,  3.1080e-01,  5.0167e-02, -8.0151e-02],
          [-5.4056e-02, -9.8999e-03,  7.8412e-02,  ...,  3.5152e-01,  2.2229e-01,  1.5767e-01],
          ...,
          [ 1.1682e-01,  1.0084e-01,  6.8871e-02,  ...,  6.6673e-03, -1.1022e-01, -1.6866e-01],
          [-6.5078e-02,  6.2650e-02,  3.1811e-01,  ...,  8.0646e-03,  1.0849e-01,  1.5870e-01],
          [ 3.8708e-02, -2.6041e-02, -1.5554e-01,  ..., -1.9843e-01,  2.1969e-02,  1.3217e-01]],

         [[-1.2957e-01, -3.2535e-02,  1.6153e-01,  ...,  2.5352e-01,  1.3978e-01,  8.2912e-02],
          [ 3.3329e-01,  2.4734e-01,  7.5456e-02,  ...,  1.3064e-01,  2.7539e-01,  3.4776e-01],
          [-1.4241e-01, -1.0575e-01, -3.2427e-02,  ...,  3.5086e-01,  3.5855e-01,  3.6239e-01],
          ...,
          [-5.0743e-02,  5.3888e-02,  2.6315e-01,  ...,  1.2526e-01, -2.5173e-02, -1.0039e-01],
          [-2.1708e-01, -2.7786e-01, -3.9940e-01,  ..., -2.7378e-01, -3.9230e-01, -4.5156e-01],
          [ 1.2316e-01,  1.8814e-01,  3.1809e-01,  ...,  1.1146e-01,  8.7678e-02,  7.5786e-02]],

         ...,

         [[ 7.3100e-02,  1.4914e-02, -1.0146e-01,  ...,  2.1851e-04, -2.5135e-02, -3.7812e-02],
          [-1.0556e-01,  3.4790e-02,  3.1549e-01,  ...,  4.2346e-01,  3.8855e-01,  3.7109e-01],
          [ 1.0875e-01,  1.4627e-01,  2.2131e-01,  ...,  1.2638e-01, -1.8617e-01, -3.4245e-01],
          ...,
          [ 4.3760e-01,  2.8257e-01, -2.7502e-02,  ...,  1.0584e-01,  9.1680e-02,  8.4601e-02],
          [-2.8833e-01, -1.5517e-01,  1.1117e-01,  ...,  3.2001e-01,  3.8946e-01,  4.2419e-01],
          [ 3.6589e-01,  2.5705e-01,  3.9369e-02,  ...,  1.7642e-01,  2.6740e-02, -4.8100e-02]],

         [[-2.0208e-02,  3.1552e-03,  4.9881e-02,  ..., -1.5822e-02, -1.1572e-02, -9.4471e-03],
          [-1.5494e-02,  1.0253e-02,  6.1747e-02,  ..., -2.7851e-02, -1.2051e-01, -1.6684e-01],
          [-2.5581e-01, -2.5702e-01, -2.5946e-01,  ..., -5.1976e-02, -1.8081e-01, -2.4522e-01],
          ...,
          [ 2.7171e-01,  1.7958e-01, -4.6705e-03,  ...,  8.5735e-02,  1.0862e-01,  1.2006e-01],
          [-3.1648e-01, -2.9791e-01, -2.6078e-01,  ...,  1.5539e-01,  3.1051e-01,  3.8808e-01],
          [ 1.9826e-01,  1.0603e-01, -7.8442e-02,  ...,  8.6028e-02,  4.2486e-02,  2.0716e-02]],

         [[-1.5990e-01, -6.1255e-02,  1.3604e-01,  ...,  1.9523e-02, -1.4130e-01, -2.2171e-01],
          [ 1.0935e-01,  7.1814e-02, -3.2618e-03,  ...,  8.0611e-02,  2.7033e-01,  3.6519e-01],
          [ 3.8537e-01,  2.8688e-01,  8.9895e-02,  ..., -4.6957e-02,  1.5878e-03,  2.5860e-02],
          ...,
          [ 3.9073e-01,  3.8300e-01,  3.6753e-01,  ..., -5.3803e-02, -1.8923e-01, -2.5694e-01],
          [-1.3820e-01, -2.6723e-02,  1.9622e-01,  ...,  1.3153e-01, -7.3141e-02, -1.7548e-01],
          [ 2.5642e-01,  1.5748e-01, -4.0387e-02,  ..., -1.9532e-01, -1.8254e-01, -1.7616e-01]]]])

2025-07-09 13:40:30.732226 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([16, 160, 297097, 3],"float32"), size=list[8,6,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([16, 160, 297097, 3],"float32"), size=list[8,6,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 119152 / 122880 (97.0%)
Greatest absolute difference: 0.9753652811050415 at index (9, 103, 0, 5) (up to 0.01 allowed)
Greatest relative difference: 28857.37109375 at index (5, 57, 7, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 160, 8, 6]), dtype=torch.float32)
tensor([[[[-2.8105e-01, -1.9249e-02,  2.4255e-01,  3.3114e-01,  2.4652e-01,  1.6189e-01],
          [ 2.6635e-01,  4.4400e-02, -1.7755e-01, -2.1687e-01, -7.3539e-02,  6.9789e-02],
          [ 3.3046e-01,  2.5976e-01,  1.8905e-01,  1.4284e-01,  1.2110e-01,  9.9372e-02],
          ...,
          [ 3.3553e-01,  2.7093e-01,  2.0632e-01,  1.7650e-01,  1.8144e-01,  1.8638e-01],
          [-2.2436e-01, -2.9992e-02,  1.6438e-01,  2.3809e-01,  1.9115e-01,  1.4421e-01],
          [-1.3600e-01, -1.9535e-01, -2.5469e-01, -1.9731e-01, -2.3211e-02,  1.5089e-01]],

         [[ 5.3650e-03,  9.7042e-02,  1.8872e-01,  2.7588e-01,  3.5853e-01,  4.4118e-01],
          [ 1.2570e-01,  1.1358e-01,  1.0146e-01,  9.2492e-02,  8.6680e-02,  8.0868e-02],
          [ 1.4524e-01, -7.3213e-02, -2.9167e-01, -4.0280e-01, -4.0662e-01, -4.1043e-01],
          ...,
          [-2.8955e-01, -2.3452e-01, -1.7949e-01, -1.3884e-01, -1.1258e-01, -8.6316e-02],
          [ 2.9632e-01,  2.6486e-01,  2.3339e-01,  1.4446e-01, -1.9225e-03, -1.4831e-01],
          [-1.5755e-01, -1.4776e-01, -1.3797e-01, -1.8296e-01, -2.8272e-01, -3.8249e-01]],

         [[-4.3563e-02, -6.7956e-02, -9.2349e-02, -1.3095e-02,  1.6981e-01,  3.5271e-01],
          [-1.7228e-01, -1.6576e-02,  1.3913e-01,  2.1370e-01,  2.0713e-01,  2.0056e-01],
          [ 4.7059e-02, -1.0731e-01, -2.6168e-01, -2.7636e-01, -1.5135e-01, -2.6330e-02],
          ...,
          [-4.4756e-01, -1.6043e-01,  1.2670e-01,  1.3324e-01, -1.4080e-01, -4.1485e-01],
          [ 5.1919e-02,  1.6845e-02, -1.8230e-02, -5.5314e-03,  5.4940e-02,  1.1541e-01],
          [-4.5257e-01, -2.5133e-01, -5.0085e-02,  7.2384e-02,  1.1608e-01,  1.5977e-01]],

         ...,

         [[-7.2785e-02, -1.0558e-01, -1.3837e-01, -1.3899e-01, -1.0742e-01, -7.5855e-02],
          [-1.4722e-01, -1.5878e-01, -1.7034e-01, -1.5606e-01, -1.1594e-01, -7.5822e-02],
          [ 7.9051e-02, -1.0664e-03, -8.1184e-02, -1.8379e-01, -3.0888e-01, -4.3396e-01],
          ...,
          [ 1.6230e-02, -1.5286e-01, -3.2196e-01, -3.4402e-01, -2.1906e-01, -9.4103e-02],
          [-3.5489e-01, -8.4586e-02,  1.8572e-01,  2.4411e-01,  9.0593e-02, -6.2926e-02],
          [ 3.5066e-01,  1.4073e-01, -6.9194e-02, -1.4543e-01, -8.7988e-02, -3.0543e-02]],

         [[-1.8047e-01, -1.6664e-01, -1.5280e-01, -2.0041e-01, -3.0944e-01, -4.1847e-01],
          [ 1.3032e-01,  1.8692e-01,  2.4353e-01,  2.5019e-01,  2.0692e-01,  1.6364e-01],
          [-8.1191e-02, -4.5698e-02, -1.0205e-02, -6.2208e-02, -2.0170e-01, -3.4120e-01],
          ...,
          [ 8.7810e-02,  6.6433e-02,  4.5057e-02,  7.2910e-03, -4.6864e-02, -1.0102e-01],
          [-4.4533e-01, -2.9232e-01, -1.3930e-01,  7.2088e-03,  1.4721e-01,  2.8721e-01],
          [-1.0662e-01,  6.7005e-02,  2.4063e-01,  2.8755e-01,  2.0778e-01,  1.2801e-01]],

         [[ 4.5380e-01,  1.7213e-01, -1.0953e-01, -1.6105e-01,  1.7563e-02,  1.9618e-01],
          [-2.8106e-01, -1.3577e-02,  2.5390e-01,  3.4641e-01,  2.6396e-01,  1.8150e-01],
          [ 4.2948e-02, -4.4429e-02, -1.3181e-01, -1.4401e-01, -8.1050e-02, -1.8087e-02],
          ...,
          [-1.4380e-01, -4.9501e-02,  4.4793e-02, -2.2896e-02, -2.5257e-01, -4.8224e-01],
          [-1.4212e-02,  8.6003e-02,  1.8622e-01,  1.6354e-01,  1.7973e-02, -1.2760e-01],
          [-2.0665e-02,  1.4404e-01,  3.0875e-01,  3.2229e-01,  1.8468e-01,  4.7063e-02]]],


        [[[ 2.9408e-01,  3.7266e-01,  4.5124e-01,  3.8337e-01,  1.6904e-01, -4.5297e-02],
          [ 2.2683e-02,  9.0437e-02,  1.5819e-01,  2.3277e-01,  3.1418e-01,  3.9559e-01],
          [ 2.9321e-01,  1.8115e-01,  6.9098e-02,  7.5127e-02,  1.9924e-01,  3.2335e-01],
          ...,
          [-2.8497e-01, -4.5623e-02,  1.9373e-01,  2.4288e-01,  1.0183e-01, -3.9214e-02],
          [ 1.9999e-01,  2.3411e-02, -1.5316e-01, -1.6588e-01, -1.4744e-02,  1.3639e-01],
          [ 3.2903e-01,  1.0780e-01, -1.1344e-01, -1.7078e-01, -6.4234e-02,  4.2316e-02]],

         [[-2.5629e-01,  3.1916e-02,  3.2012e-01,  3.7101e-01,  1.8460e-01, -1.8199e-03],
          [-1.5835e-01, -1.2453e-01, -9.0703e-02, -5.1501e-02, -6.9243e-03,  3.7653e-02],
          [-1.2879e-01, -5.0221e-02,  2.8345e-02,  8.9503e-02,  1.3325e-01,  1.7700e-01],
          ...,
          [ 7.6749e-02,  8.8092e-02,  9.9435e-02,  1.2428e-01,  1.6264e-01,  2.0099e-01],
          [-7.3337e-02,  5.1796e-02,  1.7693e-01,  2.0654e-01,  1.4063e-01,  7.4728e-02],
          [ 3.8442e-01,  3.4627e-01,  3.0812e-01,  1.8804e-01, -1.3956e-02, -2.1596e-01]],

         [[-2.0362e-01, -1.4423e-01, -8.4835e-02,  4.0134e-03,  1.2232e-01,  2.4062e-01],
          [-2.5169e-01, -3.0643e-01, -3.6116e-01, -3.8827e-01, -3.8776e-01, -3.8725e-01],
          [-3.8050e-02, -1.0835e-02,  1.6380e-02,  1.1038e-02, -2.6861e-02, -6.4761e-02],
          ...,
          [-1.1254e-01, -4.0287e-02,  3.1963e-02,  1.0238e-01,  1.7095e-01,  2.3953e-01],
          [ 2.3265e-01,  1.0802e-01, -1.6619e-02, -5.9618e-02, -2.0981e-02,  1.7656e-02],
          [ 3.5015e-01,  1.6887e-01, -1.2408e-02, -1.5302e-01, -2.5297e-01, -3.5292e-01]],

         ...,

         [[-2.5201e-01, -1.1392e-01,  2.4167e-02, -1.4970e-02, -2.3133e-01, -4.4769e-01],
          [-2.6107e-01, -1.2732e-02,  2.3560e-01,  2.8710e-01,  1.4174e-01, -3.6135e-03],
          [-4.0185e-02,  6.4883e-02,  1.6995e-01,  2.0344e-01,  1.6533e-01,  1.2723e-01],
          ...,
          [-1.7893e-01, -2.4547e-01, -3.1202e-01, -3.2035e-01, -2.7048e-01, -2.2061e-01],
          [ 2.2050e-01,  2.8790e-01,  3.5531e-01,  2.4743e-01, -3.5741e-02, -3.1891e-01],
          [ 3.5221e-01,  2.9646e-01,  2.4070e-01,  1.9837e-01,  1.6946e-01,  1.4055e-01]],

         [[ 2.5331e-02,  1.4534e-01,  2.6534e-01,  3.2717e-01,  3.3084e-01,  3.3450e-01],
          [-4.4017e-01, -1.7210e-01,  9.5976e-02,  2.2085e-01,  2.0252e-01,  1.8420e-01],
          [ 3.3416e-01,  8.8133e-02, -1.5789e-01, -1.6115e-01,  7.8354e-02,  3.1786e-01],
          ...,
          [-2.0519e-01, -1.7039e-01, -1.3560e-01, -6.3464e-02,  4.6012e-02,  1.5549e-01],
          [-3.3908e-01, -1.2437e-01,  9.0339e-02,  1.9792e-01,  1.9838e-01,  1.9884e-01],
          [ 4.2666e-01,  3.3756e-01,  2.4846e-01,  8.1740e-02, -1.6260e-01, -4.0695e-01]],

         [[ 3.8829e-01,  3.8108e-01,  3.7387e-01,  3.6164e-01,  3.4436e-01,  3.2709e-01],
          [-1.6406e-01, -2.5368e-01, -3.4330e-01, -3.2150e-01, -1.8829e-01, -5.5082e-02],
          [ 4.2989e-01,  1.7533e-01, -7.9220e-02, -1.7219e-01, -1.0358e-01, -3.4970e-02],
          ...,
          [-2.8774e-01, -2.6631e-01, -2.4489e-01, -2.5348e-01, -2.9207e-01, -3.3067e-01],
          [ 9.3670e-03,  6.8855e-02,  1.2834e-01,  6.6295e-02, -1.1729e-01, -3.0088e-01],
          [-3.5290e-01, -3.0406e-01, -2.5523e-01, -2.0983e-01, -1.6786e-01, -1.2590e-01]]],


        [[[ 3.6839e-01,  2.2894e-01,  8.9483e-02,  9.1455e-02,  2.3485e-01,  3.7825e-01],
          [ 2.4564e-01,  7.0232e-02, -1.0518e-01, -1.6549e-01, -1.1070e-01, -5.5913e-02],
          [-2.5591e-01, -6.4173e-02,  1.2756e-01,  1.5796e-01,  2.7036e-02, -1.0389e-01],
          ...,
          [ 1.7071e-01,  1.4916e-01,  1.2760e-01,  5.3075e-02, -7.4426e-02, -2.0193e-01],
          [ 1.3959e-03, -2.4177e-02, -4.9749e-02, -7.9642e-02, -1.1386e-01, -1.4807e-01],
          [-1.7736e-01, -1.0233e-01, -2.7294e-02,  9.8671e-02,  2.7557e-01,  4.5246e-01]],

         [[ 2.8593e-01,  3.3633e-01,  3.8673e-01,  4.2837e-01,  4.6125e-01,  4.9412e-01],
          [-1.4311e-01, -2.5002e-01, -3.5694e-01, -3.6326e-01, -2.6899e-01, -1.7473e-01],
          [-3.3036e-01, -1.1101e-01,  1.0833e-01,  2.2278e-01,  2.3234e-01,  2.4190e-01],
          ...,
          [ 3.1561e-03,  7.6723e-03,  1.2188e-02, -5.9649e-02, -2.0784e-01, -3.5603e-01],
          [-1.3906e-01, -8.0198e-02, -2.1337e-02, -4.4012e-02, -1.4822e-01, -2.5243e-01],
          [-4.3776e-02,  1.6561e-01,  3.7500e-01,  3.1042e-01, -2.8137e-02, -3.6669e-01]],

         [[-5.6491e-02, -1.0730e-01, -1.5810e-01, -9.4639e-02,  8.3096e-02,  2.6083e-01],
          [ 2.9467e-01,  1.0023e-01, -9.4208e-02, -2.0102e-01, -2.2020e-01, -2.3937e-01],
          [-8.9211e-03, -7.1521e-02, -1.3412e-01, -8.8658e-02,  6.4868e-02,  2.1839e-01],
          ...,
          [ 1.6559e-01,  5.4110e-02, -5.7373e-02, -1.1594e-01, -1.2158e-01, -1.2723e-01],
          [-1.8424e-01, -2.1049e-01, -2.3675e-01, -1.8750e-01, -6.2748e-02,  6.2002e-02],
          [-4.3484e-01, -1.7916e-01,  7.6521e-02,  9.1641e-02, -1.3380e-01, -3.5924e-01]],

         ...,

         [[ 3.7096e-02,  6.8653e-03, -2.3365e-02, -2.8296e-02, -7.9263e-03,  1.2443e-02],
          [ 1.5018e-01,  8.3779e-02,  1.7383e-02, -4.1724e-02, -9.3540e-02, -1.4536e-01],
          [ 4.5969e-02,  2.9259e-02,  1.2549e-02,  1.1741e-02,  2.6835e-02,  4.1929e-02],
          ...,
          [-2.9802e-01, -3.0964e-01, -3.2126e-01, -2.3130e-01, -3.9786e-02,  1.5173e-01],
          [ 2.3894e-01,  2.2270e-01,  2.0646e-01,  2.2051e-01,  2.6485e-01,  3.0919e-01],
          [-3.3950e-01, -2.6230e-01, -1.8510e-01, -1.8612e-02,  2.3716e-01,  4.9293e-01]],

         [[ 2.5627e-01,  9.8846e-02, -5.8576e-02, -5.0306e-02,  1.2366e-01,  2.9762e-01],
          [-6.9036e-02, -5.2609e-02, -3.6183e-02, -2.6989e-02, -2.5029e-02, -2.3068e-02],
          [-1.6962e-01, -8.6768e-02, -3.9115e-03,  4.4472e-02,  5.8383e-02,  7.2294e-02],
          ...,
          [ 2.1632e-01,  1.9845e-01,  1.8059e-01,  1.4956e-01,  1.0535e-01,  6.1147e-02],
          [ 2.9933e-03,  2.0055e-02,  3.7118e-02,  2.9975e-02, -1.3727e-03, -3.2720e-02],
          [-3.7259e-01, -2.0677e-01, -4.0955e-02, -3.0509e-02, -1.7543e-01, -3.2036e-01]],

         [[ 1.7912e-01,  2.4308e-02, -1.3050e-01, -2.5096e-01, -3.3709e-01, -4.2321e-01],
          [-9.9445e-02, -2.0897e-01, -3.1849e-01, -3.2921e-01, -2.4115e-01, -1.5308e-01],
          [ 9.1801e-03, -8.3680e-02, -1.7654e-01, -2.1605e-01, -2.0220e-01, -1.8835e-01],
          ...,
          [ 2.9646e-02,  1.0450e-01,  1.7935e-01,  1.4445e-01, -2.0352e-04, -1.4486e-01],
          [ 2.5362e-02, -5.8225e-02, -1.4181e-01, -2.0450e-01, -2.4628e-01, -2.8807e-01],
          [ 3.0250e-01,  2.1458e-01,  1.2665e-01,  6.5838e-02,  3.2133e-02, -1.5719e-03]]],


        ...,


        [[[-1.8657e-01, -1.4560e-01, -1.0462e-01, -6.1560e-04,  1.6642e-01,  3.3345e-01],
          [ 2.5149e-01,  2.6368e-01,  2.7588e-01,  1.9677e-01,  2.6372e-02, -1.4403e-01],
          [-1.9506e-01, -1.8848e-01, -1.8191e-01, -1.0940e-01,  2.9018e-02,  1.6744e-01],
          ...,
          [ 1.8406e-01, -1.1646e-02, -2.0735e-01, -1.8360e-01,  5.9604e-02,  3.0281e-01],
          [ 3.0905e-01,  2.3504e-01,  1.6104e-01,  1.1613e-01,  1.0032e-01,  8.4515e-02],
          [ 3.7881e-01,  1.9645e-01,  1.4091e-02, -1.3045e-01, -2.3719e-01, -3.4392e-01]],

         [[ 4.7798e-01,  2.8718e-01,  9.6377e-02,  9.4160e-02,  2.8053e-01,  4.6689e-01],
          [ 5.2891e-02, -9.2865e-02, -2.3862e-01, -1.7909e-01,  8.5715e-02,  3.5052e-01],
          [-1.7082e-01, -1.6195e-01, -1.5307e-01, -1.8137e-01, -2.4683e-01, -3.1229e-01],
          ...,
          [-2.7450e-02, -7.7648e-02, -1.2785e-01, -5.2209e-02,  1.4926e-01,  3.5073e-01],
          [-1.5835e-01, -1.6424e-01, -1.7014e-01, -1.5042e-01, -1.0510e-01, -5.9777e-02],
          [-2.8781e-02, -1.9802e-01, -3.6725e-01, -3.9769e-01, -2.8933e-01, -1.8098e-01]],

         [[-2.0858e-01, -1.8720e-01, -1.6582e-01, -8.1657e-02,  6.5291e-02,  2.1224e-01],
          [-2.6873e-01, -1.3622e-01, -3.7110e-03,  9.2922e-02,  1.5368e-01,  2.1443e-01],
          [ 1.0776e-01,  2.2262e-01,  3.3749e-01,  2.6761e-01,  1.2980e-02, -2.4165e-01],
          ...,
          [ 2.5167e-01,  1.0031e-01, -5.1055e-02, -1.8797e-01, -3.1043e-01, -4.3289e-01],
          [ 8.6374e-02,  1.5769e-02, -5.4836e-02, -1.2142e-01, -1.8400e-01, -2.4657e-01],
          [-3.4349e-02,  8.6786e-03,  5.1706e-02,  7.0903e-02,  6.6269e-02,  6.1635e-02]],

         ...,

         [[ 2.6493e-01,  2.6764e-01,  2.7036e-01,  1.6662e-01, -4.3584e-02, -2.5379e-01],
          [ 2.1977e-01,  1.5221e-01,  8.4656e-02,  2.7983e-02, -1.7807e-02, -6.3597e-02],
          [ 3.7835e-01,  3.4798e-01,  3.1760e-01,  2.1549e-01,  4.1628e-02, -1.3223e-01],
          ...,
          [ 2.5076e-02,  5.6583e-02,  8.8090e-02,  1.0069e-01,  9.4391e-02,  8.8089e-02],
          [-2.0734e-01, -2.3182e-01, -2.5630e-01, -2.4247e-01, -1.9033e-01, -1.3820e-01],
          [ 4.0908e-01,  3.3413e-01,  2.5918e-01,  1.3061e-01, -5.1560e-02, -2.3373e-01]],

         [[-3.7446e-01, -4.2237e-01, -4.7028e-01, -4.4577e-01, -3.4886e-01, -2.5194e-01],
          [ 1.5832e-01, -5.3508e-02, -2.6534e-01, -3.4497e-01, -2.9241e-01, -2.3985e-01],
          [-3.4398e-01, -2.2937e-01, -1.1477e-01, -7.8414e-02, -1.2031e-01, -1.6221e-01],
          ...,
          [-1.3670e-01, -7.7556e-02, -1.8413e-02,  2.7116e-02,  5.9033e-02,  9.0949e-02],
          [ 1.4420e-01,  4.2321e-02, -5.9559e-02, -1.0471e-02,  1.8959e-01,  3.8964e-01],
          [-1.9981e-01,  2.1482e-02,  2.4277e-01,  3.2056e-01,  2.5484e-01,  1.8912e-01]],

         [[-5.3126e-03, -1.6354e-01, -3.2177e-01, -2.7485e-01, -2.2787e-02,  2.2927e-01],
          [ 5.5000e-02, -6.8607e-02, -1.9221e-01, -2.5643e-01, -2.6126e-01, -2.6609e-01],
          [-1.8660e-01, -2.3594e-02,  1.3941e-01,  1.7356e-01,  7.8846e-02, -1.5863e-02],
          ...,
          [ 9.2412e-02, -1.1052e-01, -3.1345e-01, -3.3137e-01, -1.6426e-01,  2.8543e-03],
          [ 2.5620e-02,  4.0671e-02,  5.5722e-02,  2.2822e-02, -5.8029e-02, -1.3888e-01],
          [-2.9356e-01, -1.9404e-01, -9.4521e-02,  3.2734e-03,  9.9340e-02,  1.9541e-01]]],


        [[[-4.1820e-01, -2.0028e-01,  1.7631e-02,  3.1876e-02, -1.5755e-01, -3.4697e-01],
          [ 2.1612e-01,  1.7142e-01,  1.2672e-01,  1.0951e-01,  1.1982e-01,  1.3012e-01],
          [ 2.7785e-01,  4.6694e-02, -1.8446e-01, -1.6578e-01,  1.0274e-01,  3.7126e-01],
          ...,
          [ 5.2772e-02,  5.3166e-02,  5.3561e-02,  8.0372e-03, -8.3404e-02, -1.7484e-01],
          [-2.6074e-01, -6.5837e-02,  1.2906e-01,  2.5735e-01,  3.1902e-01,  3.8070e-01],
          [-1.4202e-01, -1.3402e-01, -1.2602e-01, -1.1509e-01, -1.0125e-01, -8.7411e-02]],

         [[-1.8218e-01, -3.1170e-02,  1.1983e-01,  1.9422e-01,  1.9199e-01,  1.8977e-01],
          [ 2.3560e-01,  2.4095e-02, -1.8741e-01, -2.2936e-01, -1.0178e-01,  2.5804e-02],
          [ 1.1614e-01,  4.6041e-02, -2.4061e-02,  3.7197e-03,  1.2938e-01,  2.5505e-01],
          ...,
          [ 1.5951e-01,  1.4577e-02, -1.3036e-01, -1.9220e-01, -1.7095e-01, -1.4969e-01],
          [ 1.8930e-01,  1.5882e-01,  1.2833e-01,  1.7745e-01,  3.0619e-01,  4.3492e-01],
          [ 4.9485e-01,  3.4097e-01,  1.8710e-01,  1.7651e-01,  3.0922e-01,  4.4193e-01]],

         [[-2.0338e-01, -2.3534e-01, -2.6729e-01, -2.9032e-01, -3.0442e-01, -3.1852e-01],
          [-2.9007e-01, -3.6064e-01, -4.3120e-01, -4.3693e-01, -3.7782e-01, -3.1870e-01],
          [ 1.0994e-01,  1.3723e-02, -8.2498e-02, -1.3218e-01, -1.3533e-01, -1.3848e-01],
          ...,
          [ 4.5763e-02,  4.6030e-02,  4.6296e-02,  1.3168e-02, -5.3355e-02, -1.1988e-01],
          [-1.7643e-01, -1.0602e-01, -3.5617e-02, -4.2313e-02, -1.2611e-01, -2.0991e-01],
          [-3.4991e-01, -7.5113e-02,  1.9969e-01,  2.6654e-01,  1.2544e-01, -1.5659e-02]],

         ...,

         [[-3.1131e-01, -5.8955e-02,  1.9340e-01,  2.2034e-01,  2.1883e-02, -1.7658e-01],
          [ 2.2614e-01,  2.1860e-01,  2.1105e-01,  1.9688e-01,  1.7606e-01,  1.5525e-01],
          [-2.3687e-01, -3.3643e-02,  1.6958e-01,  2.8015e-01,  2.9806e-01,  3.1597e-01],
          ...,
          [-4.4697e-02, -9.6329e-02, -1.4796e-01, -1.1349e-01,  7.0790e-03,  1.2765e-01],
          [ 1.2707e-01,  1.5288e-01,  1.7869e-01,  8.3711e-02, -1.3205e-01, -3.4782e-01],
          [-5.6436e-02,  3.0145e-02,  1.1673e-01,  2.2259e-01,  3.4773e-01,  4.7287e-01]],

         [[ 6.4993e-02, -1.2956e-01, -3.2411e-01, -4.0188e-01, -3.6286e-01, -3.2384e-01],
          [ 2.5224e-01,  1.0850e-01, -3.5234e-02, -1.4218e-01, -2.1234e-01, -2.8250e-01],
          [ 3.9115e-01,  1.5861e-01, -7.3927e-02, -2.2147e-01, -2.8402e-01, -3.4657e-01],
          ...,
          [ 2.4327e-02,  1.2953e-01,  2.3473e-01,  2.0027e-01,  2.6162e-02, -1.4795e-01],
          [-8.5163e-02, -5.1818e-02, -1.8472e-02,  1.1459e-02,  3.7976e-02,  6.4492e-02],
          [ 1.4629e-02, -1.8948e-01, -3.9358e-01, -3.3572e-01, -1.5906e-02,  3.0391e-01]],

         [[-2.1957e-02,  8.1781e-02,  1.8552e-01,  9.9373e-02, -1.7666e-01, -4.5269e-01],
          [ 1.9091e-01,  1.3564e-01,  8.0365e-02,  9.3612e-02,  1.7538e-01,  2.5714e-01],
          [ 1.1717e-01,  2.4082e-02, -6.9003e-02, -1.1874e-01, -1.2512e-01, -1.3151e-01],
          ...,
          [ 3.9207e-01,  3.4311e-01,  2.9416e-01,  2.2154e-01,  1.2528e-01,  2.9012e-02],
          [ 7.2889e-02,  7.6669e-02,  8.0449e-02,  1.4613e-01,  2.7370e-01,  4.0128e-01],
          [-2.1992e-01, -1.9082e-01, -1.6172e-01, -1.8348e-01, -2.5608e-01, -3.2868e-01]]],


        [[[-4.0365e-02,  1.5811e-02,  7.1987e-02,  3.9150e-02, -8.2700e-02, -2.0455e-01],
          [ 3.1417e-01,  1.4102e-01, -3.2138e-02, -9.7991e-03,  2.0803e-01,  4.2586e-01],
          [ 9.3427e-02,  7.4692e-02,  5.5956e-02,  4.7229e-02,  4.8510e-02,  4.9790e-02],
          ...,
          [ 6.5872e-02,  6.2190e-02,  5.8507e-02,  6.2986e-02,  7.5626e-02,  8.8266e-02],
          [-2.3437e-01, -1.2417e-01, -1.3983e-02,  5.6442e-02,  8.7101e-02,  1.1776e-01],
          [-3.1754e-01, -2.4271e-01, -1.6788e-01, -5.9468e-02,  8.2533e-02,  2.2453e-01]],

         [[ 2.6561e-01,  2.5591e-01,  2.4621e-01,  2.8732e-01,  3.7923e-01,  4.7115e-01],
          [ 5.5004e-02,  2.1184e-02, -1.2636e-02,  1.7009e-02,  1.1012e-01,  2.0323e-01],
          [ 5.3235e-02,  1.4149e-01,  2.2975e-01,  2.4465e-01,  1.8619e-01,  1.2773e-01],
          ...,
          [ 8.4950e-02,  7.1032e-03, -7.0743e-02, -3.5849e-02,  1.1179e-01,  2.5942e-01],
          [-7.1702e-02, -4.6128e-02, -2.0555e-02,  3.4953e-02,  1.2040e-01,  2.0584e-01],
          [ 2.6865e-01,  2.5632e-01,  2.4400e-01,  2.8321e-01,  3.7395e-01,  4.6470e-01]],

         [[-2.1889e-01, -3.0788e-02,  1.5731e-01,  1.6547e-01, -6.3070e-03, -1.7809e-01],
          [ 5.4388e-02, -9.2904e-02, -2.4020e-01, -2.9622e-01, -2.6099e-01, -2.2575e-01],
          [ 1.0816e-01,  1.5097e-01,  1.9378e-01,  1.3964e-01, -1.1448e-02, -1.6254e-01],
          ...,
          [ 2.5117e-01,  2.2624e-01,  2.0132e-01,  1.8253e-01,  1.6988e-01,  1.5723e-01],
          [ 1.8832e-01,  1.4340e-01,  9.8479e-02,  1.0646e-01,  1.6735e-01,  2.2824e-01],
          [ 2.4023e-01,  2.4544e-01,  2.5065e-01,  2.9446e-01,  3.7685e-01,  4.5924e-01]],

         ...,

         [[-1.4082e-01, -1.0083e-01, -6.0841e-02,  3.1466e-02,  1.7609e-01,  3.2071e-01],
          [-1.8946e-01, -1.1990e-01, -5.0330e-02,  2.5548e-02,  1.0774e-01,  1.8993e-01],
          [ 3.5641e-01,  2.3612e-01,  1.1584e-01,  3.9676e-02,  7.6375e-03, -2.4400e-02],
          ...,
          [-1.6736e-01, -1.5698e-01, -1.4660e-01, -1.1370e-01, -5.8292e-02, -2.8802e-03],
          [-2.7983e-01, -1.4449e-01, -9.1513e-03,  9.0087e-02,  1.5323e-01,  2.1636e-01],
          [ 1.3872e-01, -8.5329e-02, -3.0938e-01, -2.5900e-01,  6.5817e-02,  3.9063e-01]],

         [[ 2.1915e-01,  1.7415e-01,  1.2915e-01,  1.3636e-01,  1.9578e-01,  2.5521e-01],
          [ 1.7129e-01,  5.4360e-02, -6.2566e-02, -1.1406e-01, -1.0013e-01, -8.6197e-02],
          [-4.5468e-02,  6.4304e-02,  1.7408e-01,  1.8965e-01,  1.1104e-01,  3.2421e-02],
          ...,
          [ 8.6432e-02, -2.3164e-02, -1.3276e-01, -1.6426e-01, -1.1765e-01, -7.1046e-02],
          [-3.2267e-01, -3.5097e-01, -3.7927e-01, -2.7834e-01, -4.8203e-02,  1.8194e-01],
          [ 8.9086e-02, -9.4552e-02, -2.7819e-01, -3.8270e-01, -4.0809e-01, -4.3347e-01]],

         [[-4.5933e-01, -4.0584e-01, -3.5235e-01, -3.0460e-01, -2.6259e-01, -2.2058e-01],
          [-1.8489e-01, -1.8469e-01, -1.8450e-01, -1.4367e-01, -6.2204e-02,  1.9258e-02],
          [-2.5307e-01, -3.2846e-01, -4.0384e-01, -3.6734e-01, -2.1895e-01, -7.0560e-02],
          ...,
          [-3.6530e-01, -3.4500e-01, -3.2470e-01, -1.6074e-01,  1.4687e-01,  4.5449e-01],
          [-2.6291e-01, -2.3539e-01, -2.0787e-01, -1.1960e-01,  2.9439e-02,  1.7847e-01],
          [ 3.3333e-01,  1.2998e-01, -7.3365e-02, -1.1403e-01,  7.9755e-03,  1.2998e-01]]]])
DESIRED: (shape=torch.Size([16, 160, 8, 6]), dtype=torch.float32)
tensor([[[[-4.4301e-01, -2.7434e-01,  6.3012e-02,  1.8121e-01,  8.0261e-02,  2.9786e-02],
          [-6.3593e-02, -7.9607e-02, -1.1164e-01, -1.4103e-01, -1.6778e-01, -1.8116e-01],
          [ 1.7164e-01,  1.3870e-01,  7.2814e-02,  1.2339e-02, -4.2726e-02, -7.0258e-02],
          ...,
          [ 3.0868e-01,  2.8194e-01,  2.2845e-01,  1.9789e-01,  1.9024e-01,  1.8642e-01],
          [ 7.1979e-02,  3.7250e-02, -3.2206e-02, -4.8922e-02, -1.2895e-02,  5.1182e-03],
          [-4.0564e-01, -2.3568e-01,  1.0424e-01,  9.7244e-02, -2.5667e-01, -4.3363e-01]],

         [[-2.6450e-01, -2.9085e-01, -3.4356e-01, -2.2440e-01,  6.6612e-02,  2.1212e-01],
          [-1.1478e-02,  2.0712e-02,  8.5093e-02,  1.0707e-01,  8.6648e-02,  7.6437e-02],
          [ 2.2242e-01,  2.6941e-01,  3.6337e-01,  2.4516e-01, -8.5212e-02, -2.5040e-01],
          ...,
          [ 6.9750e-02,  3.1970e-02, -4.3590e-02, -1.0418e-01, -1.4979e-01, -1.7259e-01],
          [ 2.5745e-02,  1.2602e-01,  3.2658e-01,  2.2792e-01, -1.6996e-01, -3.6890e-01],
          [-1.4479e-01, -1.5604e-01, -1.7856e-01, -3.9237e-02,  2.6193e-01,  4.1251e-01]],

         [[-1.8888e-01, -1.9663e-01, -2.1214e-01, -7.2947e-02,  2.2094e-01,  3.6788e-01],
          [-1.7893e-01, -1.9829e-01, -2.3702e-01, -1.6636e-01,  1.3704e-02,  1.0373e-01],
          [-2.0075e-01, -1.0603e-01,  8.3407e-02,  2.1120e-01,  2.7736e-01,  3.1044e-01],
          ...,
          [-5.5190e-02, -5.7481e-02, -6.2065e-02,  1.7312e-02,  1.8065e-01,  2.6232e-01],
          [-4.2425e-02, -1.0174e-02,  5.4328e-02,  4.1336e-02, -4.9152e-02, -9.4395e-02],
          [-8.2751e-03,  3.1651e-02,  1.1150e-01,  1.0502e-01,  1.2186e-02, -3.4229e-02]],

         ...,

         [[-1.8617e-01, -2.2005e-01, -2.8780e-01, -2.2903e-01, -4.3734e-02,  4.8914e-02],
          [-1.9559e-02, -6.1305e-02, -1.4480e-01, -7.3477e-02,  1.5265e-01,  2.6571e-01],
          [ 1.8981e-01,  1.2264e-01, -1.1694e-02, -1.3923e-01, -2.5997e-01, -3.2034e-01],
          ...,
          [ 2.4249e-01,  2.7912e-01,  3.5237e-01,  3.5055e-01,  2.7366e-01,  2.3522e-01],
          [-4.4714e-01, -3.4364e-01, -1.3663e-01, -1.4366e-02,  2.3148e-02,  4.1906e-02],
          [ 3.0111e-01,  2.4060e-01,  1.1959e-01,  6.2696e-02,  6.9909e-02,  7.3515e-02]],

         [[ 6.2663e-02,  1.1484e-02, -9.0874e-02,  6.1372e-03,  3.0252e-01,  4.5071e-01],
          [ 9.3668e-03,  3.6539e-02,  9.0882e-02,  1.3493e-01,  1.6868e-01,  1.8555e-01],
          [-4.2441e-01, -2.8296e-01, -5.3229e-05,  1.0330e-01,  2.7104e-02, -1.0994e-02],
          ...,
          [ 1.3469e-01,  9.0188e-02,  1.1853e-03, -6.9928e-02, -1.2315e-01, -1.4976e-01],
          [-3.1548e-02, -1.1479e-01, -2.8126e-01, -2.6462e-01, -6.4866e-02,  3.5012e-02],
          [ 1.8469e-01,  1.0142e-01, -6.5111e-02, -1.3083e-01, -9.5747e-02, -7.8204e-02]],

         [[ 3.4232e-01,  3.6589e-01,  4.1302e-01,  3.4211e-01,  1.5315e-01,  5.8663e-02],
          [-2.0133e-01, -2.3926e-01, -3.1511e-01, -2.2042e-01,  4.4834e-02,  1.7746e-01],
          [ 1.2533e-01,  5.7483e-02, -7.8218e-02, -1.3029e-01, -9.8743e-02, -8.2968e-02],
          ...,
          [ 2.1551e-01,  1.6496e-01,  6.3858e-02, -5.6151e-02, -1.9507e-01, -2.6453e-01],
          [ 9.2716e-02,  2.4737e-02, -1.1122e-01, -1.4222e-01, -6.8263e-02, -3.1285e-02],
          [-2.9335e-01, -3.3197e-01, -4.0920e-01, -2.5695e-01,  1.2479e-01,  3.1566e-01]]],


        [[[-2.9865e-01, -3.0516e-01, -3.1819e-01, -2.7116e-01, -1.6406e-01, -1.1051e-01],
          [-3.7376e-01, -2.3843e-01,  3.2223e-02,  1.4320e-01,  9.4485e-02,  7.0129e-02],
          [ 2.8087e-02,  3.2272e-02,  4.0643e-02,  2.5127e-02, -1.4275e-02, -3.3976e-02],
          ...,
          [ 3.9001e-01,  4.0596e-01,  4.3786e-01,  2.5156e-01, -1.5296e-01, -3.5521e-01],
          [-3.1209e-01, -1.4457e-01,  1.9047e-01,  2.4777e-01,  2.7353e-02, -8.2857e-02],
          [ 3.7400e-01,  2.6519e-01,  4.7548e-02,  9.4708e-03,  1.5095e-01,  2.2169e-01]],

         [[ 1.8087e-01,  8.6976e-02, -1.0082e-01, -6.8982e-02,  1.8248e-01,  3.0821e-01],
          [-3.8957e-01, -3.4974e-01, -2.7009e-01, -2.4849e-01, -2.8495e-01, -3.0317e-01],
          [-1.5977e-01, -8.0641e-02,  7.7619e-02,  1.0096e-01, -1.0614e-02, -6.6401e-02],
          ...,
          [-2.1779e-01, -1.0379e-01,  1.2420e-01,  2.8224e-01,  3.7031e-01,  4.1435e-01],
          [ 2.0904e-01,  1.6689e-01,  8.2577e-02, -1.3476e-02, -1.2127e-01, -1.7517e-01],
          [-4.1946e-01, -3.6021e-01, -2.4172e-01, -3.7897e-02,  2.5126e-01,  3.9584e-01]],

         [[-3.2821e-01, -2.1011e-01,  2.6080e-02,  9.8325e-02,  6.6229e-03, -3.9228e-02],
          [ 3.8697e-02,  8.1984e-02,  1.6856e-01,  2.0428e-01,  1.8916e-01,  1.8160e-01],
          [-9.9893e-02, -1.2167e-02,  1.6328e-01,  2.4139e-01,  2.2215e-01,  2.1253e-01],
          ...,
          [-5.9139e-02,  4.1125e-03,  1.3062e-01,  1.1687e-01, -3.7124e-02, -1.1412e-01],
          [ 1.6185e-01,  2.2578e-02, -2.5597e-01, -2.6753e-01, -1.2121e-02,  1.1558e-01],
          [-2.7928e-01, -3.1747e-01, -3.9384e-01, -2.3151e-01,  1.6954e-01,  3.7006e-01]],

         ...,

         [[ 4.9799e-02,  1.9697e-02, -4.0508e-02, -1.5575e-01, -3.2603e-01, -4.1117e-01],
          [ 1.7138e-01,  9.9440e-02, -4.4441e-02, -1.1913e-01, -1.2463e-01, -1.2738e-01],
          [-4.1831e-02, -1.3999e-01, -3.3631e-01, -3.2521e-01, -1.0670e-01,  2.5633e-03],
          ...,
          [ 2.3539e-01,  2.1316e-01,  1.6871e-01,  2.0632e-02, -2.3106e-01, -3.5691e-01],
          [ 4.3835e-01,  2.7118e-01, -6.3168e-02, -2.1244e-01, -1.7663e-01, -1.5872e-01],
          [ 3.4606e-02,  1.2779e-01,  3.1415e-01,  4.0839e-01,  4.1051e-01,  4.1157e-01]],

         [[-1.5677e-01, -2.2059e-01, -3.4824e-01, -2.4195e-01,  9.8275e-02,  2.6839e-01],
          [ 2.1367e-02, -9.1631e-02, -3.1763e-01, -3.8932e-01, -3.0671e-01, -2.6541e-01],
          [ 4.4596e-02,  3.3584e-02,  1.1559e-02,  1.0890e-01,  3.2561e-01,  4.3396e-01],
          ...,
          [ 2.2477e-01,  6.8256e-02, -2.4477e-01, -4.0065e-01, -3.9938e-01, -3.9875e-01],
          [ 2.8531e-01,  3.0555e-01,  3.4603e-01,  1.6670e-01, -2.3244e-01, -4.3201e-01],
          [ 2.9598e-01,  1.1908e-01, -2.3474e-01, -3.4266e-01, -2.0470e-01, -1.3572e-01]],

         [[ 4.6364e-01,  3.2523e-01,  4.8396e-02,  2.1945e-02,  2.4587e-01,  3.5784e-01],
          [ 4.0395e-01,  2.1164e-01, -1.7298e-01, -3.8661e-01, -4.2925e-01, -4.5058e-01],
          [ 8.5931e-02, -4.3079e-02, -3.0110e-01, -3.2333e-01, -1.0979e-01, -3.0126e-03],
          ...,
          [-8.9085e-02, -2.9323e-02,  9.0201e-02,  1.4551e-01,  1.3661e-01,  1.3216e-01],
          [-3.8143e-01, -3.6676e-01, -3.3741e-01, -1.5938e-01,  1.6731e-01,  3.3066e-01],
          [-1.8119e-01, -2.6345e-02,  2.8334e-01,  4.0403e-01,  3.3572e-01,  3.0157e-01]]],


        [[[-4.5455e-02, -8.5970e-02, -1.6700e-01, -9.6194e-02,  1.2644e-01,  2.3776e-01],
          [ 1.6494e-01,  3.4233e-02, -2.2718e-01, -2.5681e-01, -5.4650e-02,  4.6427e-02],
          [ 2.6722e-01,  2.9154e-01,  3.4020e-01,  3.0802e-01,  1.9502e-01,  1.3852e-01],
          ...,
          [-2.3721e-01, -1.7033e-01, -3.6580e-02,  8.2243e-03, -3.5921e-02, -5.7994e-02],
          [ 2.2067e-01,  2.4817e-01,  3.0317e-01,  3.2581e-01,  3.1608e-01,  3.1121e-01],
          [ 3.6741e-01,  2.8387e-01,  1.1678e-01,  4.1457e-02,  5.7906e-02,  6.6131e-02]],

         [[-2.9302e-01, -1.7707e-01,  5.4812e-02,  6.4142e-02, -1.4908e-01, -2.5570e-01],
          [ 1.4027e-01,  1.3166e-01,  1.1445e-01,  7.3572e-03, -1.8962e-01, -2.8810e-01],
          [-7.1597e-02, -1.0491e-01, -1.7154e-01, -2.2857e-01, -2.7600e-01, -2.9972e-01],
          ...,
          [ 3.2340e-01,  1.6259e-01, -1.5903e-01, -3.2172e-01, -3.2548e-01, -3.2736e-01],
          [-4.2270e-02, -6.9425e-03,  6.3713e-02,  1.5570e-01,  2.6902e-01,  3.2568e-01],
          [-4.6822e-01, -3.2699e-01, -4.4545e-02,  1.3793e-01,  2.2043e-01,  2.6168e-01]],

         [[-7.4282e-02, -1.0846e-01, -1.7682e-01, -1.3318e-01,  2.2456e-02,  1.0027e-01],
          [-2.3247e-01, -1.9683e-01, -1.2556e-01, -2.0535e-02,  1.1823e-01,  1.8761e-01],
          [-5.3464e-02, -4.9344e-02, -4.1103e-02, -6.2751e-04,  7.2084e-02,  1.0844e-01],
          ...,
          [ 7.6274e-02,  1.6489e-02, -1.0308e-01, -1.4941e-01, -1.2249e-01, -1.0903e-01],
          [ 1.2097e-03,  1.9284e-02,  5.5432e-02, -2.1995e-02, -2.1300e-01, -3.0850e-01],
          [-1.9789e-01, -2.1077e-01, -2.3654e-01, -1.8839e-01, -6.6319e-02, -5.2859e-03]],

         ...,

         [[ 3.4458e-01,  1.8912e-01, -1.2179e-01, -1.1540e-01,  2.0831e-01,  3.7016e-01],
          [-2.5301e-01, -1.4349e-01,  7.5531e-02,  2.0443e-01,  2.4319e-01,  2.6257e-01],
          [ 3.0952e-01,  2.1661e-01,  3.0788e-02, -1.1800e-01, -2.2975e-01, -2.8563e-01],
          ...,
          [-2.6674e-01, -2.6791e-01, -2.7025e-01, -2.0151e-01, -6.1704e-02,  8.2010e-03],
          [ 2.6412e-01,  1.7507e-01, -3.0293e-03, -4.2786e-02,  5.5797e-02,  1.0509e-01],
          [-2.2258e-01, -1.2617e-01,  6.6656e-02,  1.8322e-01,  2.2351e-01,  2.4366e-01]],

         [[ 5.7680e-02,  1.4755e-01,  3.2729e-01,  2.5702e-01, -6.3243e-02, -2.2338e-01],
          [ 8.0004e-02,  6.4975e-02,  3.4916e-02, -8.2152e-02, -2.8623e-01, -3.8827e-01],
          [ 6.7979e-02,  1.1178e-01,  1.9938e-01,  2.7017e-01,  3.2415e-01,  3.5114e-01],
          ...,
          [ 3.9983e-02, -1.1703e-02, -1.1508e-01, -1.4189e-01, -9.2141e-02, -6.7267e-02],
          [-5.0136e-02,  1.0082e-02,  1.3052e-01,  2.0137e-01,  2.2263e-01,  2.3326e-01],
          [ 5.7511e-02, -4.6208e-02, -2.5365e-01, -2.2598e-01,  3.6801e-02,  1.6819e-01]],

         [[-2.2000e-01, -1.6955e-01, -6.8654e-02, -7.5408e-02, -1.8982e-01, -2.4702e-01],
          [ 3.1569e-01,  1.9370e-01, -5.0275e-02, -1.0419e-01,  3.1946e-02,  1.0002e-01],
          [-1.5690e-01, -2.3048e-01, -3.7763e-01, -2.9413e-01,  2.0020e-02,  1.7709e-01],
          ...,
          [-7.1460e-03, -6.8594e-02, -1.9149e-01, -2.6439e-01, -2.8730e-01, -2.9876e-01],
          [ 2.6176e-01,  1.8946e-01,  4.4871e-02,  5.0627e-02,  2.0673e-01,  2.8478e-01],
          [ 9.4811e-02, -3.4250e-02, -2.9237e-01, -3.3011e-01, -1.4747e-01, -5.6148e-02]]],


        ...,


        [[[ 4.2347e-01,  2.9134e-01,  2.7063e-02, -1.5098e-02,  1.6485e-01,  2.5483e-01],
          [-9.7001e-03, -3.5054e-02, -8.5762e-02,  7.7935e-03,  2.4561e-01,  3.6452e-01],
          [ 1.6605e-01,  1.1157e-01,  2.5951e-03,  5.4094e-04,  1.0540e-01,  1.5784e-01],
          ...,
          [ 3.5107e-01,  2.7273e-01,  1.1605e-01,  5.1095e-02,  7.7873e-02,  9.1262e-02],
          [ 1.1159e-01,  1.0186e-01,  8.2408e-02, -2.2250e-02, -2.1211e-01, -3.0704e-01],
          [-2.2287e-01, -2.6565e-01, -3.5121e-01, -2.9265e-01, -8.9973e-02,  1.1366e-02]],

         [[-2.0141e-01, -2.2645e-01, -2.7653e-01, -1.7374e-01,  8.1925e-02,  2.0976e-01],
          [ 3.1517e-01,  2.5683e-01,  1.4016e-01,  1.3219e-02, -1.2397e-01, -1.9257e-01],
          [-2.8099e-01, -1.8178e-01,  1.6651e-02,  7.2619e-02, -1.3874e-02, -5.7120e-02],
          ...,
          [-2.4905e-01, -1.2390e-01,  1.2640e-01,  1.7667e-01,  2.6930e-02, -4.7943e-02],
          [ 3.0168e-01,  2.8981e-01,  2.6605e-01,  1.3128e-01, -1.1451e-01, -2.3740e-01],
          [-8.7448e-02, -1.7745e-01, -3.5745e-01, -4.3559e-01, -4.1187e-01, -4.0000e-01]],

         [[ 3.9429e-01,  2.5572e-01, -2.1410e-02, -2.2732e-01, -3.6201e-01, -4.2936e-01],
          [ 3.8628e-01,  3.2692e-01,  2.0820e-01,  1.7387e-01,  2.2392e-01,  2.4894e-01],
          [-3.4781e-01, -3.0845e-01, -2.2972e-01, -1.6021e-01, -9.9927e-02, -6.9784e-02],
          ...,
          [-3.2580e-01, -3.5699e-01, -4.1938e-01, -4.0155e-01, -3.0352e-01, -2.5450e-01],
          [-2.2205e-01, -2.4088e-01, -2.7855e-01, -2.1952e-01, -6.3782e-02,  1.4086e-02],
          [-1.1860e-01, -4.7580e-02,  9.4456e-02,  2.0338e-01,  2.7920e-01,  3.1711e-01]],

         ...,

         [[-5.2536e-02, -1.4688e-01, -3.3558e-01, -2.9278e-01, -1.8491e-02,  1.1865e-01],
          [-3.8131e-02, -1.0564e-01, -2.4065e-01, -2.3616e-01, -9.2175e-02, -2.0182e-02],
          [ 2.8033e-01,  1.2672e-01, -1.8050e-01, -2.6887e-01, -1.3839e-01, -7.3146e-02],
          ...,
          [-1.3634e-01, -1.0513e-01, -4.2695e-02, -1.9467e-02, -3.5443e-02, -4.3431e-02],
          [-1.9073e-01, -2.1696e-01, -2.6942e-01, -1.8901e-01,  2.4256e-02,  1.3089e-01],
          [-8.5343e-02,  1.0091e-02,  2.0096e-01,  2.2691e-01,  8.7951e-02,  1.8471e-02]],

         [[ 7.7897e-02, -2.7084e-02, -2.3705e-01, -3.0102e-01, -2.1899e-01, -1.7798e-01],
          [-1.4572e-01, -1.6322e-01, -1.9823e-01, -2.2980e-01, -2.5792e-01, -2.7198e-01],
          [-2.9866e-02,  5.6424e-03,  7.6660e-02, -1.1999e-02, -2.6034e-01, -3.8450e-01],
          ...,
          [-3.4383e-01, -2.8979e-01, -1.8172e-01, -1.3923e-01, -1.6231e-01, -1.7386e-01],
          [-3.1740e-01, -1.7186e-01,  1.1920e-01,  2.2342e-01,  1.4079e-01,  9.9476e-02],
          [ 3.4407e-01,  2.6605e-01,  1.1001e-01, -1.3321e-02, -1.0395e-01, -1.4926e-01]],

         [[ 8.7204e-02,  1.4786e-02, -1.3005e-01, -5.0846e-02,  2.5240e-01,  4.0402e-01],
          [ 3.7634e-01,  3.3044e-01,  2.3866e-01,  1.6186e-01,  1.0003e-01,  6.9122e-02],
          [ 3.7116e-01,  3.0827e-01,  1.8249e-01,  3.1323e-02, -1.4523e-01, -2.3351e-01],
          ...,
          [ 4.6391e-01,  2.9175e-01, -5.2566e-02, -1.5581e-01, -1.7997e-02,  5.0912e-02],
          [-3.0557e-01, -3.2516e-01, -3.6433e-01, -2.2855e-01,  8.2176e-02,  2.3754e-01],
          [-1.0748e-01, -1.4465e-01, -2.1897e-01, -1.9195e-01, -6.3583e-02,  6.0035e-04]]],


        [[[-2.0432e-01, -2.1698e-01, -2.4228e-01, -3.0066e-01, -3.9209e-01, -4.3781e-01],
          [-3.1410e-01, -2.9888e-01, -2.6844e-01, -2.0933e-01, -1.2156e-01, -7.7677e-02],
          [-4.5419e-01, -2.3257e-01,  2.1068e-01,  2.7897e-01, -2.7689e-02, -1.8102e-01],
          ...,
          [-1.2260e-01, -1.8158e-01, -2.9953e-01, -2.5631e-01, -5.1906e-02,  5.0294e-02],
          [ 2.9302e-01,  2.6078e-01,  1.9628e-01,  8.4503e-02, -7.4568e-02, -1.5410e-01],
          [-2.6279e-02,  5.3589e-02,  2.1332e-01,  2.9667e-01,  3.0362e-01,  3.0709e-01]],

         [[-2.1291e-01, -1.7533e-01, -1.0016e-01, -1.3726e-01, -2.8662e-01, -3.6129e-01],
          [-4.8541e-02, -8.8372e-02, -1.6803e-01, -2.4422e-01, -3.1694e-01, -3.5330e-01],
          [ 3.9971e-01,  2.6798e-01,  4.5287e-03, -1.0160e-01, -5.0414e-02, -2.4820e-02],
          ...,
          [ 2.6572e-01,  1.6824e-01, -2.6712e-02, -4.7522e-02,  1.0581e-01,  1.8248e-01],
          [ 3.1502e-01,  2.7253e-01,  1.8755e-01,  1.6444e-01,  2.0320e-01,  2.2258e-01],
          [-4.5887e-01, -4.2602e-01, -3.6032e-01, -1.8048e-01,  1.1350e-01,  2.6049e-01]],

         [[ 2.7194e-01,  2.9184e-01,  3.3165e-01,  2.1918e-01, -4.5564e-02, -1.7793e-01],
          [-2.0612e-01, -4.3387e-02,  2.8208e-01,  3.1166e-01,  4.5370e-02, -8.7776e-02],
          [-2.2529e-02, -1.6075e-03,  4.0235e-02,  7.3276e-02,  9.7516e-02,  1.0964e-01],
          ...,
          [-1.5112e-01, -6.4516e-02,  1.0870e-01,  1.7471e-01,  1.3351e-01,  1.1291e-01],
          [-4.1318e-01, -3.9883e-01, -3.7013e-01, -3.2093e-01, -2.5123e-01, -2.1637e-01],
          [-2.0210e-02, -1.1582e-01, -3.0704e-01, -3.8834e-01, -3.5973e-01, -3.4542e-01]],

         ...,

         [[ 1.4877e-01,  8.9533e-02, -2.8946e-02, -6.5359e-02, -1.9709e-02,  3.1167e-03],
          [-3.3290e-02, -4.8637e-02, -7.9331e-02, -1.9263e-01, -3.8853e-01, -4.8649e-01],
          [-5.0908e-02, -2.7189e-02,  2.0250e-02, -4.5222e-02, -2.2361e-01, -3.1280e-01],
          ...,
          [ 3.5732e-01,  3.8357e-01,  4.3607e-01,  3.7254e-01,  1.9297e-01,  1.0318e-01],
          [-3.6199e-02, -8.5425e-02, -1.8388e-01, -1.9926e-01, -1.3158e-01, -9.7742e-02],
          [ 3.0720e-01,  2.2256e-01,  5.3263e-02, -8.2173e-02, -1.8375e-01, -2.3454e-01]],

         [[ 4.4395e-01,  4.3245e-01,  4.0945e-01,  2.1383e-01, -1.5443e-01, -3.3855e-01],
          [-2.3393e-01, -2.1686e-01, -1.8270e-01, -1.1843e-01, -2.4045e-02,  2.3146e-02],
          [ 2.3226e-01,  1.9116e-01,  1.0898e-01,  6.7193e-02,  6.5810e-02,  6.5119e-02],
          ...,
          [-1.2781e-02, -3.1529e-03,  1.6104e-02,  5.6318e-02,  1.1749e-01,  1.4807e-01],
          [-1.4188e-01, -1.0271e-01, -2.4385e-02,  6.9792e-03, -8.6193e-03, -1.6419e-02],
          [ 3.5438e-01,  2.4090e-01,  1.3950e-02, -2.8446e-02,  1.1371e-01,  1.8479e-01]],

         [[-2.8849e-01, -1.1761e-01,  2.2413e-01,  4.0001e-01,  4.1003e-01,  4.1503e-01],
          [-3.0976e-01, -1.4493e-01,  1.8473e-01,  2.0026e-01, -9.8334e-02, -2.4763e-01],
          [-2.7152e-01, -2.7104e-01, -2.7007e-01, -1.7019e-01,  2.8595e-02,  1.2799e-01],
          ...,
          [-3.4597e-01, -2.6478e-01, -1.0241e-01, -3.2470e-02, -5.4970e-02, -6.6220e-02],
          [-5.5847e-02,  1.5218e-02,  1.5735e-01,  2.4816e-01,  2.8766e-01,  3.0741e-01],
          [ 4.3174e-01,  3.2238e-01,  1.0367e-01,  1.0664e-01,  3.3130e-01,  4.4363e-01]]],


        [[[ 3.4289e-01,  2.3650e-01,  2.3703e-02, -1.0846e-01, -1.5998e-01, -1.8575e-01],
          [ 2.1409e-01,  2.3330e-01,  2.7170e-01,  1.5717e-01, -1.1031e-01, -2.4405e-01],
          [-9.1772e-02, -8.6829e-02, -7.6943e-02, -7.7103e-02, -8.7309e-02, -9.2412e-02],
          ...,
          [ 3.2144e-01,  3.0103e-01,  2.6022e-01,  1.3807e-01, -6.5420e-02, -1.6716e-01],
          [ 3.1226e-01,  2.1580e-01,  2.2894e-02, -4.6310e-02,  8.1909e-03,  3.5441e-02],
          [ 5.9782e-02, -5.7668e-02, -2.9257e-01, -2.7975e-01, -1.9221e-02,  1.1105e-01]],

         [[-3.5831e-01, -2.6307e-01, -7.2608e-02, -9.9878e-03, -7.5214e-02, -1.0783e-01],
          [-1.0822e-01, -3.2062e-02,  1.2026e-01,  1.6616e-01,  1.0563e-01,  7.5369e-02],
          [ 2.6561e-01,  2.9289e-01,  3.4746e-01,  1.9796e-01, -1.5560e-01, -3.3239e-01],
          ...,
          [ 1.2830e-01,  1.8314e-01,  2.9282e-01,  3.4576e-01,  3.4197e-01,  3.4008e-01],
          [ 3.6482e-01,  1.9619e-01, -1.4107e-01, -2.7455e-01, -2.0426e-01, -1.6912e-01],
          [-1.7575e-01, -1.1305e-01,  1.2342e-02, -1.7144e-02, -2.0151e-01, -2.9369e-01]],

         [[ 1.9921e-01,  1.0665e-01, -7.8472e-02, -1.3445e-01, -6.1281e-02, -2.4698e-02],
          [ 3.8535e-01,  3.5839e-01,  3.0449e-01,  2.4038e-01,  1.6606e-01,  1.2890e-01],
          [-3.8543e-01, -2.1606e-01,  1.2269e-01,  2.1014e-01,  4.6302e-02, -3.5619e-02],
          ...,
          [-3.9865e-01, -2.0194e-01,  1.9149e-01,  2.2164e-01, -1.1149e-01, -2.7806e-01],
          [ 1.7601e-01,  1.6064e-01,  1.2988e-01, -2.0649e-02, -2.9095e-01, -4.2610e-01],
          [-4.6223e-01, -3.1316e-01, -1.5019e-02,  1.2807e-01,  1.1612e-01,  1.1014e-01]],

         ...,

         [[-1.2858e-01, -1.0787e-01, -6.6461e-02, -4.3764e-02, -3.9781e-02, -3.7790e-02],
          [ 3.9195e-01,  2.2712e-01, -1.0256e-01, -1.9140e-01, -3.9397e-02,  3.6603e-02],
          [-1.6473e-01, -7.6427e-03,  3.0653e-01,  2.3893e-01, -2.1044e-01, -4.3513e-01],
          ...,
          [-1.5848e-01, -1.8656e-01, -2.4270e-01, -1.4213e-01,  1.1516e-01,  2.4381e-01],
          [-1.4634e-01, -7.7569e-02,  5.9981e-02,  2.0670e-01,  3.6258e-01,  4.4052e-01],
          [-3.2965e-01, -2.1510e-01,  1.4013e-02,  4.7425e-02, -1.1486e-01, -1.9600e-01]],

         [[ 1.7094e-02,  6.6815e-02,  1.6626e-01,  6.0138e-02, -2.5154e-01, -4.0739e-01],
          [ 2.0143e-02,  8.7893e-02,  2.2339e-01,  1.5472e-01, -1.1811e-01, -2.5453e-01],
          [-5.2210e-02, -1.5414e-02,  5.8180e-02,  2.3413e-02, -1.1971e-01, -1.9128e-01],
          ...,
          [-6.2890e-02, -1.3195e-02,  8.6195e-02,  6.8983e-02, -6.4831e-02, -1.3174e-01],
          [ 1.6164e-01,  5.0601e-02, -1.7148e-01, -3.1259e-01, -3.7273e-01, -4.0280e-01],
          [-4.0147e-01, -3.6281e-01, -2.8549e-01, -2.8165e-01, -3.5130e-01, -3.8613e-01]],

         [[ 3.6662e-01,  2.0804e-01, -1.0914e-01, -2.5414e-01, -2.2697e-01, -2.1338e-01],
          [-4.3819e-02, -7.5748e-02, -1.3961e-01, -8.2698e-02,  9.4975e-02,  1.8381e-01],
          [ 3.0033e-01,  3.1997e-01,  3.5925e-01,  2.8075e-01,  8.4478e-02, -1.3658e-02],
          ...,
          [-4.4089e-01, -2.7228e-01,  6.4918e-02,  2.5136e-01,  2.8703e-01,  3.0487e-01],
          [-2.2645e-01, -2.1110e-01, -1.8041e-01, -2.1484e-01, -3.1440e-01, -3.6418e-01],
          [-4.5048e-02,  1.9264e-02,  1.4789e-01,  8.4328e-02, -1.7142e-01, -2.9929e-01]]]])

2025-07-09 13:40:33.451048 GPU 4 151556 test begin: paddle.nn.functional.interpolate(Tensor([16, 160, 4, 222823],"float32"), size=list[8,6,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([16, 160, 4, 222823],"float32"), size=list[8,6,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 119165 / 122880 (97.0%)
Greatest absolute difference: 0.9816122055053711 at index (5, 121, 0, 0) (up to 0.01 allowed)
Greatest relative difference: 130039.4140625 at index (7, 13, 1, 5) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 160, 8, 6]), dtype=torch.float32)
tensor([[[[ 0.3630,  0.3707,  0.1311, -0.2796, -0.3486, -0.2004],
          [ 0.2929,  0.3940,  0.0499, -0.2374, -0.2035, -0.0777],
          [ 0.2227,  0.4173, -0.0314, -0.1953, -0.0584,  0.0451],
          ...,
          [-0.2394, -0.0323, -0.0013, -0.1526,  0.1068,  0.2138],
          [-0.2358,  0.0124, -0.1618, -0.0267,  0.0104,  0.1020],
          [-0.2323,  0.0570, -0.3224,  0.0992, -0.0860, -0.0098]],

         [[ 0.1987,  0.1425, -0.3286,  0.0281, -0.3754, -0.2323],
          [ 0.2762,  0.0850, -0.2800, -0.1569, -0.2771,  0.0557],
          [ 0.3537,  0.0275, -0.2314, -0.3418, -0.1787,  0.3436],
          ...,
          [ 0.0682, -0.1404,  0.3650,  0.3356, -0.4183, -0.3832],
          [ 0.2069,  0.0309,  0.1496,  0.2338, -0.3687, -0.3398],
          [ 0.3456,  0.2021, -0.0658,  0.1320, -0.3191, -0.2964]],

         [[ 0.3382, -0.3010,  0.2096,  0.2226,  0.1892, -0.0939],
          [ 0.0263, -0.0343,  0.0369,  0.0970,  0.1632,  0.1064],
          [-0.2856,  0.2324, -0.1358, -0.0286,  0.1372,  0.3067],
          ...,
          [-0.3033, -0.3216,  0.2830, -0.1856,  0.1730,  0.1024],
          [-0.3433, -0.2390,  0.1998, -0.1650,  0.1394,  0.0703],
          [-0.3834, -0.1564,  0.1165, -0.1445,  0.1058,  0.0382]],

         ...,

         [[ 0.2768, -0.1630, -0.0916, -0.3095,  0.1933, -0.1662],
          [ 0.2909, -0.1140,  0.1105, -0.0780,  0.2309, -0.1403],
          [ 0.3050, -0.0650,  0.3125,  0.1535,  0.2686, -0.1143],
          ...,
          [-0.2717,  0.2949, -0.2520, -0.2122,  0.2582, -0.1490],
          [-0.2059,  0.2366, -0.1574,  0.0489,  0.1597, -0.0672],
          [-0.1400,  0.1782, -0.0628,  0.3100,  0.0612,  0.0146]],

         [[-0.1125,  0.1693,  0.1841, -0.1822,  0.3793,  0.0186],
          [ 0.0365, -0.0530,  0.2817, -0.1864,  0.3278, -0.0964],
          [ 0.1855, -0.2753,  0.3793, -0.1906,  0.2762, -0.2114],
          ...,
          [-0.2164,  0.2427, -0.0917, -0.4451,  0.0139, -0.1185],
          [-0.1649,  0.0992, -0.2328, -0.4018, -0.0110,  0.0631],
          [-0.1133, -0.0443, -0.3739, -0.3584, -0.0359,  0.2447]],

         [[ 0.2078,  0.0677, -0.2148,  0.3276,  0.1958, -0.4885],
          [ 0.0693, -0.0712, -0.2587,  0.0468,  0.0323, -0.2089],
          [-0.0691, -0.2101, -0.3027, -0.2339, -0.1312,  0.0708],
          ...,
          [ 0.1294, -0.1519,  0.2267, -0.1770, -0.3227, -0.1209],
          [ 0.1845, -0.1167,  0.1163, -0.2719, -0.0437,  0.1023],
          [ 0.2395, -0.0815,  0.0058, -0.3667,  0.2353,  0.3256]]],


        [[[-0.3421, -0.0270, -0.3274, -0.0293,  0.2166,  0.3415],
          [-0.3982,  0.1426, -0.1768,  0.1669,  0.1478,  0.1435],
          [-0.4544,  0.3121, -0.0262,  0.3632,  0.0790, -0.0544],
          ...,
          [ 0.3841,  0.1956, -0.1821,  0.0253, -0.1102, -0.1860],
          [ 0.1445, -0.0660, -0.0040, -0.0209,  0.0841, -0.2618],
          [-0.0951, -0.3275,  0.1741, -0.0670,  0.2785, -0.3377]],

         [[ 0.0578,  0.0102,  0.1651, -0.1273,  0.0548,  0.0932],
          [-0.1336, -0.0405,  0.0923, -0.0532,  0.0148, -0.1507],
          [-0.3250, -0.0913,  0.0196,  0.0208, -0.0252, -0.3947],
          ...,
          [ 0.1957, -0.0179,  0.1830,  0.1392,  0.1836,  0.1079],
          [-0.0238, -0.1020, -0.1358,  0.1946,  0.1794, -0.1840],
          [-0.2432, -0.1861, -0.4546,  0.2499,  0.1752, -0.4759]],

         [[-0.2339,  0.1907,  0.0075, -0.2291, -0.1119,  0.2154],
          [-0.2656,  0.1965, -0.1232, -0.1504, -0.1422,  0.3132],
          [-0.2972,  0.2024, -0.2540, -0.0718, -0.1726,  0.4110],
          ...,
          [ 0.0827,  0.2128,  0.3053, -0.0771,  0.0820,  0.1232],
          [ 0.0321, -0.0195,  0.1486, -0.0610, -0.0285,  0.2960],
          [-0.0185, -0.2518, -0.0081, -0.0448, -0.1391,  0.4688]],

         ...,

         [[-0.4050,  0.1153,  0.3642, -0.1890, -0.4322,  0.2968],
          [-0.1316,  0.2102,  0.2302, -0.0452, -0.2256,  0.3370],
          [ 0.1418,  0.3051,  0.0963,  0.0986, -0.0190,  0.3773],
          ...,
          [ 0.1618,  0.3305,  0.1551,  0.3901, -0.4418,  0.1658],
          [-0.1183, -0.0119,  0.1281,  0.2192, -0.3077, -0.0253],
          [-0.3985, -0.3544,  0.1011,  0.0483, -0.1735, -0.2164]],

         [[ 0.4794, -0.0749,  0.1389,  0.2447,  0.3178, -0.4170],
          [ 0.4225, -0.0949,  0.1836,  0.3247,  0.2330, -0.0676],
          [ 0.3656, -0.1148,  0.2283,  0.4047,  0.1483,  0.2818],
          ...,
          [-0.4236, -0.1997,  0.0917, -0.0251,  0.2811,  0.3232],
          [-0.2460, -0.0286, -0.0535, -0.0936,  0.1025,  0.2089],
          [-0.0685,  0.1426, -0.1987, -0.1621, -0.0762,  0.0946]],

         [[ 0.4463,  0.0849,  0.3066, -0.2815,  0.1187, -0.4238],
          [ 0.2692,  0.1150,  0.3269,  0.0208,  0.0706, -0.2566],
          [ 0.0920,  0.1451,  0.3473,  0.3231,  0.0225, -0.0894],
          ...,
          [-0.3033,  0.1447, -0.1616, -0.1565, -0.0902, -0.1293],
          [-0.3527,  0.1400,  0.0387, -0.1807, -0.0305,  0.0421],
          [-0.4020,  0.1352,  0.2390, -0.2050,  0.0291,  0.2136]]],


        [[[ 0.0854,  0.3963, -0.1703, -0.3694, -0.1607, -0.2127],
          [ 0.1339,  0.2589,  0.0221, -0.2344, -0.1447, -0.0281],
          [ 0.1824,  0.1215,  0.2144, -0.0993, -0.1287,  0.1566],
          ...,
          [-0.1534, -0.3957,  0.1824,  0.3735, -0.1918, -0.1866],
          [-0.0728, -0.2047,  0.1410,  0.2212, -0.2934, -0.1781],
          [ 0.0078, -0.0137,  0.0996,  0.0690, -0.3950, -0.1696]],

         [[-0.1515, -0.3453, -0.1545, -0.3396, -0.1133,  0.1576],
          [ 0.0631, -0.1777, -0.2365, -0.1003, -0.0792, -0.0662],
          [ 0.2778, -0.0100, -0.3186,  0.1391, -0.0452, -0.2900],
          ...,
          [-0.2566, -0.1981, -0.1610,  0.1271,  0.1770,  0.2228],
          [-0.2352, -0.1756, -0.2502,  0.1681,  0.2171, -0.0083],
          [-0.2137, -0.1530, -0.3394,  0.2090,  0.2572, -0.2394]],

         [[-0.1376,  0.0484, -0.2123, -0.0594,  0.2106, -0.0630],
          [-0.0251,  0.0895, -0.2727,  0.0247,  0.1467,  0.0951],
          [ 0.0875,  0.1305, -0.3330,  0.1088,  0.0829,  0.2532],
          ...,
          [-0.0184,  0.1556, -0.2231,  0.1423, -0.0854, -0.2450],
          [-0.0283,  0.1154, -0.1282, -0.0517, -0.1349, -0.0315],
          [-0.0382,  0.0752, -0.0333, -0.2456, -0.1843,  0.1821]],

         ...,

         [[-0.3409, -0.0311, -0.1846, -0.0598,  0.1780, -0.4151],
          [-0.2941,  0.0198, -0.1238, -0.0442,  0.1789, -0.3313],
          [-0.2474,  0.0708, -0.0629, -0.0287,  0.1798, -0.2475],
          ...,
          [-0.2834, -0.1265,  0.2450, -0.0774, -0.2511, -0.2127],
          [-0.2480, -0.1876,  0.2560,  0.1515,  0.0275,  0.0100],
          [-0.2126, -0.2487,  0.2670,  0.3805,  0.3062,  0.2328]],

         [[-0.1626,  0.0084,  0.0876,  0.2562, -0.0178, -0.4261],
          [ 0.0437,  0.1511, -0.0347,  0.0601, -0.0403, -0.1604],
          [ 0.2500,  0.2938, -0.1571, -0.1361, -0.0628,  0.1052],
          ...,
          [ 0.3215,  0.2629,  0.0155,  0.1745,  0.0753, -0.2065],
          [ 0.0671, -0.0796,  0.0030,  0.1912, -0.0180, -0.3053],
          [-0.1873, -0.4220, -0.0095,  0.2079, -0.1113, -0.4041]],

         [[-0.4634,  0.0807, -0.1522, -0.2738,  0.1649, -0.4726],
          [-0.4667, -0.0434, -0.2788, -0.0433,  0.2527, -0.0987],
          [-0.4700, -0.1675, -0.4054,  0.1872,  0.3406,  0.2752],
          ...,
          [-0.0514,  0.0997, -0.0222,  0.2524, -0.2130, -0.0087],
          [-0.2316,  0.0169, -0.1562,  0.1955,  0.0061, -0.0133],
          [-0.4117, -0.0660, -0.2902,  0.1387,  0.2252, -0.0179]]],


        ...,


        [[[-0.4123, -0.1502, -0.1765,  0.1797, -0.3881,  0.3455],
          [-0.1632, -0.0348, -0.2105,  0.0528, -0.2034,  0.0666],
          [ 0.0860,  0.0805, -0.2446, -0.0741, -0.0188, -0.2122],
          ...,
          [-0.0011,  0.1323,  0.1162,  0.1288, -0.3276,  0.2202],
          [-0.2016, -0.0537,  0.0204,  0.1498, -0.0506,  0.0436],
          [-0.4021, -0.2396, -0.0754,  0.1709,  0.2265, -0.1329]],

         [[-0.0478, -0.0500, -0.1760,  0.1914,  0.1839, -0.1933],
          [-0.1100,  0.0840, -0.0102,  0.0397,  0.1224, -0.1426],
          [-0.1723,  0.2180,  0.1557, -0.1120,  0.0608, -0.0918],
          ...,
          [ 0.3607,  0.2790,  0.2624, -0.3775, -0.1178, -0.2574],
          [-0.0351,  0.3078,  0.1094, -0.2283, -0.1408,  0.0766],
          [-0.4309,  0.3367, -0.0436, -0.0791, -0.1637,  0.4106]],

         [[ 0.4817, -0.2952,  0.0842, -0.1384, -0.2331, -0.3758],
          [ 0.1552, -0.1725,  0.0034, -0.1667, -0.0713, -0.0826],
          [-0.1712, -0.0498, -0.0775, -0.1950,  0.0906,  0.2106],
          ...,
          [-0.3574, -0.0652,  0.3381, -0.2180, -0.0387,  0.2931],
          [-0.1302, -0.0015,  0.2198, -0.0787,  0.0599,  0.1566],
          [ 0.0970,  0.0621,  0.1014,  0.0605,  0.1585,  0.0201]],

         ...,

         [[-0.0260, -0.3945,  0.4597, -0.2735, -0.0924,  0.2211],
          [ 0.1044, -0.2749,  0.0894, -0.2101, -0.0284,  0.0777],
          [ 0.2349, -0.1554, -0.2809, -0.1467,  0.0356, -0.0656],
          ...,
          [ 0.1783, -0.0732,  0.0992, -0.3219,  0.1986, -0.0892],
          [ 0.1097,  0.1147,  0.2632, -0.2331,  0.0667,  0.1769],
          [ 0.0411,  0.3026,  0.4273, -0.1443, -0.0652,  0.4429]],

         [[ 0.3613,  0.3428, -0.4754, -0.0882, -0.2274,  0.1643],
          [ 0.1992,  0.1821, -0.3954,  0.0710, -0.2936,  0.2898],
          [ 0.0370,  0.0215, -0.3154,  0.2302, -0.3599,  0.4154],
          ...,
          [-0.4321, -0.1295, -0.2439, -0.2586,  0.3056, -0.2185],
          [-0.3832, -0.2382, -0.1253,  0.0131,  0.3076, -0.0135],
          [-0.3342, -0.3470, -0.0068,  0.2849,  0.3097,  0.1916]],

         [[ 0.0950, -0.2360,  0.0785,  0.1091,  0.1248,  0.3656],
          [ 0.1146,  0.0508,  0.1896,  0.1122,  0.1777,  0.1685],
          [ 0.1342,  0.3377,  0.3007,  0.1153,  0.2307, -0.0286],
          ...,
          [ 0.2250, -0.1116,  0.0314, -0.1623,  0.1185,  0.1837],
          [ 0.0462,  0.0961,  0.1830, -0.0636,  0.0431, -0.1325],
          [-0.1326,  0.3038,  0.3347,  0.0350, -0.0323, -0.4488]]],


        [[[-0.0772, -0.1365,  0.3746,  0.1521, -0.1390,  0.2044],
          [-0.0942, -0.1414,  0.2998,  0.1207, -0.0220,  0.0321],
          [-0.1112, -0.1463,  0.2251,  0.0893,  0.0950, -0.1402],
          ...,
          [ 0.1603, -0.3048,  0.1375, -0.3532, -0.2423,  0.1793],
          [-0.0246, -0.2125,  0.2631, -0.1677, -0.1454,  0.1178],
          [-0.2095, -0.1202,  0.3887,  0.0179, -0.0484,  0.0563]],

         [[ 0.3796, -0.0889,  0.2435,  0.4881,  0.2214,  0.1147],
          [ 0.2546,  0.0485,  0.2358,  0.1676,  0.1073, -0.1437],
          [ 0.1295,  0.1859,  0.2280, -0.1529, -0.0068, -0.4021],
          ...,
          [-0.0597, -0.3208, -0.0670, -0.1645,  0.1873,  0.3814],
          [-0.0481, -0.0795, -0.1155, -0.0457,  0.0197,  0.2138],
          [-0.0366,  0.1617, -0.1639,  0.0730, -0.1478,  0.0462]],

         [[ 0.1711, -0.1210, -0.1883,  0.1819, -0.2852, -0.4082],
          [ 0.1152, -0.0909, -0.1101,  0.2691, -0.1598, -0.3542],
          [ 0.0593, -0.0608, -0.0319,  0.3564, -0.0343, -0.3001],
          ...,
          [-0.2648,  0.1087,  0.2774, -0.1811, -0.1588,  0.2563],
          [-0.1547,  0.2192,  0.2439, -0.1170, -0.2082, -0.1099],
          [-0.0446,  0.3298,  0.2104, -0.0529, -0.2576, -0.4760]],

         ...,

         [[-0.3771, -0.1614, -0.2296,  0.3030, -0.0234,  0.3562],
          [-0.1563, -0.0276, -0.0828,  0.0974, -0.0266,  0.2557],
          [ 0.0645,  0.1062,  0.0641, -0.1082, -0.0298,  0.1552],
          ...,
          [-0.1040,  0.2177,  0.2526, -0.3628, -0.1647,  0.2334],
          [-0.2261, -0.0493,  0.2122, -0.2028,  0.0132,  0.3480],
          [-0.3482, -0.3163,  0.1719, -0.0427,  0.1911,  0.4626]],

         [[-0.1967,  0.1347, -0.0025,  0.3458, -0.0912, -0.0585],
          [-0.2896,  0.1427,  0.0018,  0.0614, -0.0817,  0.0799],
          [-0.3825,  0.1507,  0.0061, -0.2230, -0.0723,  0.2183],
          ...,
          [-0.0873,  0.0598,  0.0339,  0.1974,  0.1960, -0.3127],
          [ 0.0230,  0.1610, -0.1868,  0.2186,  0.2650,  0.0402],
          [ 0.1334,  0.2621, -0.4075,  0.2398,  0.3340,  0.3931]],

         [[ 0.4137, -0.2565, -0.2737,  0.3066, -0.0802, -0.2303],
          [ 0.1906, -0.1525, -0.0761,  0.2969, -0.1174, -0.1908],
          [-0.0325, -0.0484,  0.1214,  0.2872, -0.1547, -0.1512],
          ...,
          [-0.0393,  0.0317,  0.3184, -0.1772,  0.1179, -0.1515],
          [-0.0698,  0.0678,  0.2906, -0.1859, -0.0122,  0.0922],
          [-0.1003,  0.1039,  0.2628, -0.1947, -0.1423,  0.3358]]],


        [[[ 0.2416, -0.3677, -0.1150, -0.4259, -0.1875,  0.4061],
          [ 0.2934, -0.1252,  0.0946, -0.3805, -0.0038,  0.1673],
          [ 0.3452,  0.1174,  0.3043, -0.3352,  0.1799, -0.0715],
          ...,
          [-0.2992, -0.2677,  0.0611,  0.4016,  0.1672,  0.3716],
          [ 0.0736, -0.0162, -0.1463,  0.3701,  0.0046,  0.0512],
          [ 0.4464,  0.2354, -0.3536,  0.3387, -0.1580, -0.2691]],

         [[-0.3741, -0.0832,  0.0530, -0.1513, -0.3518, -0.4211],
          [-0.0555, -0.1934,  0.1537, -0.0037, -0.0713, -0.1259],
          [ 0.2632, -0.3037,  0.2544,  0.1440,  0.2093,  0.1693],
          ...,
          [ 0.0158,  0.1440,  0.0765, -0.1541, -0.0332,  0.3244],
          [-0.2246,  0.0580,  0.2157,  0.0413,  0.0736,  0.0441],
          [-0.4650, -0.0280,  0.3549,  0.2367,  0.1804, -0.2362]],

         [[ 0.3234, -0.3497,  0.1320,  0.2329,  0.2138,  0.4156],
          [ 0.0006, -0.3072,  0.1095,  0.1699,  0.0402,  0.3500],
          [-0.3223, -0.2646,  0.0871,  0.1069, -0.1334,  0.2843],
          ...,
          [ 0.4422, -0.0349, -0.1346,  0.1351, -0.3033, -0.3518],
          [ 0.2777,  0.0005, -0.0910, -0.0295, -0.2244,  0.0038],
          [ 0.1132,  0.0359, -0.0474, -0.1941, -0.1454,  0.3595]],

         ...,

         [[ 0.1452,  0.1345,  0.2340,  0.3169,  0.3286,  0.2471],
          [ 0.2166, -0.0244,  0.1469,  0.0023,  0.1064,  0.0614],
          [ 0.2880, -0.1833,  0.0598, -0.3124, -0.1158, -0.1243],
          ...,
          [-0.0354, -0.0632,  0.1961,  0.3346, -0.2626,  0.2586],
          [-0.2016,  0.0446,  0.0258,  0.3190,  0.0131,  0.0665],
          [-0.3679,  0.1523, -0.1445,  0.3034,  0.2887, -0.1257]],

         [[ 0.4108, -0.3113, -0.3952,  0.2622,  0.1853, -0.4157],
          [ 0.1430, -0.2307, -0.3657,  0.0707,  0.0988, -0.0759],
          [-0.1248, -0.1501, -0.3361, -0.1209,  0.0124,  0.2639],
          ...,
          [ 0.2561,  0.3147,  0.2373, -0.1903, -0.2389,  0.1636],
          [ 0.0082,  0.1669,  0.3226, -0.1774,  0.0148, -0.0438],
          [-0.2397,  0.0192,  0.4080, -0.1644,  0.2685, -0.2513]],

         [[-0.3219, -0.2792, -0.0006, -0.3711,  0.2652, -0.2891],
          [-0.0961, -0.1731, -0.1069, -0.1135,  0.0446, -0.1739],
          [ 0.1298, -0.0670, -0.2132,  0.1440, -0.1759, -0.0586],
          ...,
          [-0.3187, -0.1847, -0.1688,  0.2128, -0.0425, -0.2514],
          [-0.0494,  0.0377,  0.0449,  0.2774, -0.1755, -0.0185],
          [ 0.2199,  0.2601,  0.2586,  0.3420, -0.3085,  0.2144]]]])
DESIRED: (shape=torch.Size([16, 160, 8, 6]), dtype=torch.float32)
tensor([[[[ 0.3310, -0.0707, -0.0641, -0.1269,  0.4463, -0.2509],
          [ 0.1908, -0.0835, -0.0980, -0.1731,  0.3585, -0.1350],
          [-0.0894, -0.1092, -0.1659, -0.2655,  0.1829,  0.0968],
          ...,
          [-0.0525, -0.1977,  0.0439, -0.0707, -0.0814,  0.2591],
          [-0.2452, -0.0581, -0.0779, -0.0805,  0.1919,  0.2643],
          [-0.3415,  0.0116, -0.1388, -0.0855,  0.3286,  0.2670]],

         [[ 0.3735,  0.2155, -0.2691,  0.1094,  0.1757, -0.4818],
          [ 0.2319,  0.1815, -0.1622,  0.0945,  0.2423, -0.2901],
          [-0.0514,  0.1135,  0.0517,  0.0647,  0.3756,  0.0932],
          ...,
          [-0.0115, -0.0135, -0.1418, -0.2398,  0.3673, -0.0834],
          [ 0.0307,  0.0468, -0.1659, -0.1648,  0.2769,  0.2361],
          [ 0.0518,  0.0770, -0.1780, -0.1274,  0.2317,  0.3958]],

         [[ 0.4038, -0.1048, -0.2206, -0.1934,  0.1876, -0.1020],
          [ 0.2087, -0.0983, -0.1143, -0.0781,  0.0883, -0.1402],
          [-0.1814, -0.0855,  0.0983,  0.1523, -0.1105, -0.2167],
          ...,
          [ 0.2051, -0.0916,  0.0465,  0.1362,  0.0835, -0.1026],
          [ 0.1689,  0.1613,  0.1149,  0.3073, -0.2845,  0.0718],
          [ 0.1508,  0.2878,  0.1492,  0.3928, -0.4685,  0.1590]],

         ...,

         [[ 0.0564,  0.0624, -0.3603,  0.1146, -0.3765,  0.1826],
          [-0.0418, -0.0315, -0.2624,  0.1506, -0.2595,  0.2273],
          [-0.2382, -0.2195, -0.0666,  0.2225, -0.0256,  0.3167],
          ...,
          [-0.1197, -0.1290,  0.0362, -0.0318,  0.1098,  0.1325],
          [-0.2390, -0.2681,  0.1107, -0.2796, -0.1171,  0.1790],
          [-0.2987, -0.3377,  0.1479, -0.4035, -0.2305,  0.2023]],

         [[-0.1914,  0.3317,  0.2086, -0.0166,  0.3177,  0.0903],
          [-0.1892,  0.2476,  0.1194,  0.0108,  0.1751,  0.0271],
          [-0.1849,  0.0794, -0.0591,  0.0656, -0.1103, -0.0995],
          ...,
          [ 0.1671, -0.0792, -0.0551, -0.1115, -0.1510,  0.1407],
          [ 0.0314, -0.1104, -0.0060, -0.2040,  0.0054,  0.2574],
          [-0.0365, -0.1260,  0.0185, -0.2503,  0.0836,  0.3158]],

         [[ 0.0270, -0.0466, -0.3304,  0.2077,  0.1288, -0.1528],
          [-0.0477, -0.0881, -0.2265,  0.1560,  0.1217, -0.0812],
          [-0.1969, -0.1712, -0.0187,  0.0528,  0.1075,  0.0621],
          ...,
          [-0.3868, -0.1367, -0.1135, -0.1935, -0.0318,  0.2201],
          [-0.2335, -0.1041, -0.1974,  0.2064, -0.0142,  0.0828],
          [-0.1568, -0.0879, -0.2394,  0.4064, -0.0054,  0.0142]]],


        [[[-0.4559,  0.0458,  0.0717, -0.2422,  0.2467, -0.1528],
          [-0.2929, -0.0330,  0.0918, -0.1619,  0.2823, -0.1475],
          [ 0.0331, -0.1906,  0.1321, -0.0014,  0.3534, -0.1371],
          ...,
          [-0.0045,  0.2702, -0.0193,  0.2835,  0.3776,  0.0143],
          [ 0.0859,  0.0260,  0.0010, -0.0327,  0.3412,  0.1247],
          [ 0.1311, -0.0961,  0.0112, -0.1908,  0.3230,  0.1799]],

         [[ 0.2237,  0.1830,  0.2505, -0.2508,  0.2504, -0.0596],
          [ 0.1621,  0.0545,  0.2708, -0.2115,  0.2951, -0.1079],
          [ 0.0389, -0.2025,  0.3113, -0.1329,  0.3845, -0.2044],
          ...,
          [-0.1646, -0.2439,  0.1113, -0.0809,  0.0348, -0.2179],
          [-0.2018, -0.2453, -0.0015,  0.0945,  0.1905, -0.2882],
          [-0.2203, -0.2460, -0.0579,  0.1822,  0.2684, -0.3233]],

         [[-0.4031, -0.4106, -0.4458,  0.0505, -0.2973,  0.3386],
          [-0.2327, -0.2506, -0.3222,  0.0306, -0.2709,  0.3119],
          [ 0.1082,  0.0694, -0.0750, -0.0092, -0.2180,  0.2585],
          ...,
          [-0.0590,  0.3153,  0.1298, -0.0905,  0.1624,  0.3025],
          [ 0.1922,  0.3523,  0.2378, -0.1355,  0.1769,  0.1932],
          [ 0.3178,  0.3707,  0.2918, -0.1580,  0.1842,  0.1386]],

         ...,

         [[-0.2602, -0.2753, -0.0800, -0.2629,  0.0499,  0.1245],
          [-0.2742, -0.2001,  0.0146, -0.2662,  0.0774,  0.1784],
          [-0.3023, -0.0498,  0.2039, -0.2727,  0.1325,  0.2864],
          ...,
          [ 0.1089,  0.0125,  0.3128, -0.0754,  0.3060,  0.0326],
          [ 0.1418, -0.2603,  0.2835,  0.2071,  0.1671, -0.1281],
          [ 0.1582, -0.3968,  0.2689,  0.3484,  0.0977, -0.2084]],

         [[-0.4022,  0.3329,  0.1835,  0.3660, -0.3324, -0.4190],
          [-0.2815,  0.3234,  0.1035,  0.2931, -0.2317, -0.2930],
          [-0.0402,  0.3043, -0.0565,  0.1473, -0.0303, -0.0409],
          ...,
          [ 0.2958,  0.1162, -0.0877, -0.0410, -0.0763, -0.1693],
          [ 0.2448,  0.0962, -0.0972,  0.1260,  0.0384, -0.1021],
          [ 0.2192,  0.0862, -0.1020,  0.2095,  0.0957, -0.0685]],

         [[-0.1167, -0.0498,  0.0859,  0.1661, -0.4079, -0.4464],
          [-0.1695, -0.0093,  0.0375,  0.1075, -0.3687, -0.2820],
          [-0.2749,  0.0718, -0.0595, -0.0097, -0.2904,  0.0466],
          ...,
          [ 0.1136,  0.0018, -0.0366,  0.0126, -0.2550,  0.0082],
          [-0.1200,  0.2585,  0.0203,  0.1549, -0.1865,  0.1423],
          [-0.2368,  0.3868,  0.0488,  0.2261, -0.1523,  0.2093]]],


        [[[-0.3947,  0.2111,  0.3113,  0.2791,  0.1213,  0.0709],
          [-0.2084,  0.2035,  0.1617,  0.1648,  0.1706,  0.0194],
          [ 0.1642,  0.1883, -0.1374, -0.0636,  0.2693, -0.0836],
          ...,
          [ 0.1218, -0.0048, -0.1078, -0.2189,  0.2158, -0.0017],
          [ 0.0892, -0.1968,  0.0372, -0.0696,  0.1433, -0.2956],
          [ 0.0730, -0.2929,  0.1098,  0.0051,  0.1070, -0.4426]],

         [[-0.3948,  0.0370, -0.2599, -0.0082,  0.1712, -0.2640],
          [-0.1939, -0.0429, -0.1302, -0.0714,  0.1109, -0.2788],
          [ 0.2077, -0.2027,  0.1291, -0.1978, -0.0097, -0.3084],
          ...,
          [ 0.2810, -0.2063,  0.2969,  0.4294, -0.0121,  0.3441],
          [ 0.1660,  0.0324,  0.2567,  0.4054, -0.1576,  0.2232],
          [ 0.1084,  0.1518,  0.2367,  0.3934, -0.2304,  0.1627]],

         [[-0.3158,  0.0684, -0.0223, -0.1722, -0.4039, -0.3456],
          [-0.1695,  0.0096,  0.0245, -0.1434, -0.1938, -0.3042],
          [ 0.1232, -0.1080,  0.1181, -0.0859,  0.2266, -0.2214],
          ...,
          [ 0.1505, -0.0677,  0.0167,  0.2157,  0.1272,  0.1368],
          [-0.0893, -0.2543, -0.2513, -0.0890,  0.0689, -0.2397],
          [-0.2092, -0.3476, -0.3852, -0.2413,  0.0398, -0.4280]],

         ...,

         [[ 0.2915,  0.2693, -0.0306,  0.1709, -0.4005, -0.1404],
          [ 0.2908,  0.1286, -0.0129,  0.1417, -0.3824, -0.0166],
          [ 0.2895, -0.1528,  0.0226,  0.0833, -0.3461,  0.2311],
          ...,
          [ 0.0421,  0.0179, -0.1115, -0.1320,  0.1256, -0.0367],
          [-0.0095, -0.0818,  0.0708, -0.3559,  0.0025, -0.0354],
          [-0.0352, -0.1316,  0.1619, -0.4679, -0.0590, -0.0348]],

         [[-0.3912,  0.0348, -0.1352, -0.2469, -0.0956,  0.4095],
          [-0.4115,  0.1160, -0.1720, -0.1518,  0.0352,  0.3624],
          [-0.4520,  0.2783, -0.2455,  0.0384,  0.2967,  0.2681],
          ...,
          [-0.1414,  0.4294,  0.0301, -0.2865, -0.2086, -0.0013],
          [-0.1900,  0.4041,  0.0822,  0.0720, -0.0272, -0.1936],
          [-0.2143,  0.3915,  0.1083,  0.2513,  0.0635, -0.2898]],

         [[ 0.2339, -0.3772,  0.1683, -0.1863,  0.1226, -0.3979],
          [ 0.0938, -0.2644,  0.0588, -0.1958,  0.0862, -0.2710],
          [-0.1862, -0.0388, -0.1602, -0.2150,  0.0132, -0.0170],
          ...,
          [ 0.2887,  0.0741, -0.0432, -0.3131,  0.0981,  0.0229],
          [ 0.2239, -0.1069, -0.1160,  0.0524, -0.2577,  0.0891],
          [ 0.1915, -0.1974, -0.1524,  0.2351, -0.4357,  0.1223]]],


        ...,


        [[[-0.4022, -0.3563, -0.3035, -0.2216, -0.2530, -0.4082],
          [-0.1961, -0.3011, -0.2530, -0.1244, -0.1283, -0.3865],
          [ 0.2163, -0.1907, -0.1520,  0.0701,  0.1210, -0.3432],
          ...,
          [-0.0450, -0.3500,  0.2381, -0.1936,  0.2437,  0.1447],
          [ 0.2140, -0.3279,  0.2449, -0.0470, -0.0454,  0.2585],
          [ 0.3436, -0.3169,  0.2484,  0.0264, -0.1899,  0.3154]],

         [[ 0.2761, -0.3488, -0.2113, -0.0371,  0.2228, -0.3091],
          [ 0.2070, -0.2217, -0.1217, -0.1268,  0.1202, -0.2423],
          [ 0.0690,  0.0325,  0.0577, -0.3061, -0.0851, -0.1087],
          ...,
          [ 0.2469,  0.2052, -0.0057,  0.1767, -0.0741,  0.1090],
          [ 0.2682, -0.0659,  0.1682,  0.1837,  0.2309,  0.3225],
          [ 0.2789, -0.2015,  0.2552,  0.1872,  0.3834,  0.4292]],

         [[-0.3534, -0.4130,  0.0723,  0.0542, -0.1442,  0.0851],
          [-0.1617, -0.2455, -0.0034, -0.0167, -0.1667, -0.0067],
          [ 0.2218,  0.0894, -0.1549, -0.1584, -0.2117, -0.1903],
          ...,
          [ 0.1054, -0.2189,  0.1677,  0.0105,  0.0358,  0.1872],
          [ 0.0984, -0.1006,  0.1234,  0.0212,  0.0344,  0.2202],
          [ 0.0948, -0.0415,  0.1013,  0.0265,  0.0338,  0.2366]],

         ...,

         [[ 0.0581, -0.2436,  0.3766,  0.0319,  0.2892, -0.1265],
          [ 0.0936, -0.1231,  0.2271,  0.0372,  0.2303, -0.2031],
          [ 0.1646,  0.1178, -0.0718,  0.0477,  0.1124, -0.3565],
          ...,
          [ 0.1700, -0.3192,  0.1447, -0.2825, -0.2889,  0.1696],
          [-0.2125, -0.4073,  0.1704, -0.1435, -0.3216, -0.1056],
          [-0.4037, -0.4514,  0.1832, -0.0740, -0.3379, -0.2431]],

         [[-0.2365, -0.1103,  0.1004,  0.3406,  0.3944, -0.2913],
          [-0.1435,  0.0117,  0.1136,  0.3443,  0.3576, -0.1330],
          [ 0.0425,  0.2556,  0.1402,  0.3518,  0.2841,  0.1835],
          ...,
          [ 0.3636,  0.1468,  0.2658,  0.0407, -0.1175, -0.1500],
          [ 0.1960, -0.0700,  0.0525, -0.0741, -0.1979, -0.3657],
          [ 0.1122, -0.1783, -0.0542, -0.1315, -0.2380, -0.4736]],

         [[-0.3378, -0.0555,  0.2339,  0.3495,  0.0013, -0.4722],
          [-0.3539,  0.0330,  0.1354,  0.2263, -0.0738, -0.3135],
          [-0.3860,  0.2100, -0.0615, -0.0201, -0.2238,  0.0041],
          ...,
          [-0.1609,  0.1636, -0.1411, -0.2317,  0.2599, -0.1664],
          [ 0.0132,  0.0939, -0.2141, -0.2152,  0.2636,  0.1122],
          [ 0.1003,  0.0590, -0.2506, -0.2070,  0.2655,  0.2516]]],


        [[[-0.1138,  0.3915,  0.2910,  0.3653,  0.2365,  0.2525],
          [-0.1104,  0.2776,  0.2101,  0.3141,  0.1579,  0.1701],
          [-0.1035,  0.0497,  0.0484,  0.2116,  0.0007,  0.0052],
          ...,
          [ 0.0114, -0.0599,  0.1553, -0.0633,  0.1285, -0.1294],
          [-0.3163, -0.2228,  0.0007,  0.0155, -0.0575,  0.1395],
          [-0.4801, -0.3043, -0.0766,  0.0549, -0.1504,  0.2739]],

         [[-0.1586,  0.0276,  0.0187, -0.0814, -0.2679, -0.0535],
          [-0.1259,  0.0476,  0.0270, -0.0635, -0.2625, -0.0093],
          [-0.0607,  0.0876,  0.0435, -0.0276, -0.2518,  0.0791],
          ...,
          [-0.2087, -0.1139,  0.1242,  0.2502, -0.0974, -0.0756],
          [-0.0868, -0.2453,  0.0364,  0.3742,  0.1190,  0.2938],
          [-0.0259, -0.3110, -0.0075,  0.4363,  0.2272,  0.4786]],

         [[-0.1488, -0.1140,  0.2269,  0.2923, -0.2004,  0.0853],
          [-0.0701, -0.0954,  0.2490,  0.2116, -0.1191, -0.0167],
          [ 0.0874, -0.0583,  0.2932,  0.0503,  0.0435, -0.2206],
          ...,
          [-0.0674, -0.0121,  0.1356, -0.2095,  0.0917,  0.1692],
          [-0.1317,  0.2431,  0.1590, -0.3744, -0.0477, -0.0010],
          [-0.1639,  0.3706,  0.1707, -0.4569, -0.1175, -0.0861]],

         ...,

         [[-0.2579,  0.1203,  0.1919, -0.1591,  0.2754,  0.0693],
          [-0.1278, -0.0153,  0.1786, -0.1304,  0.1244,  0.1054],
          [ 0.1324, -0.2866,  0.1521, -0.0731, -0.1777,  0.1776],
          ...,
          [ 0.3888,  0.0238, -0.1560,  0.0146,  0.3403,  0.0857],
          [ 0.3089, -0.2565,  0.1710,  0.2556,  0.2987, -0.0937],
          [ 0.2690, -0.3966,  0.3345,  0.3761,  0.2779, -0.1834]],

         [[-0.2576, -0.1474,  0.2404, -0.0207,  0.2293,  0.2414],
          [-0.1310, -0.0684,  0.1989, -0.0254,  0.2080,  0.1321],
          [ 0.1223,  0.0896,  0.1158, -0.0348,  0.1655, -0.0865],
          ...,
          [-0.2208,  0.1500, -0.1063, -0.0317,  0.0204, -0.3067],
          [ 0.1704, -0.1112, -0.0499, -0.0623, -0.2587, -0.0936],
          [ 0.3661, -0.2418, -0.0217, -0.0775, -0.3982,  0.0129]],

         [[-0.0267, -0.0508, -0.3434,  0.1997,  0.2406, -0.4567],
          [ 0.0631,  0.0210, -0.2127,  0.2314,  0.2430, -0.4446],
          [ 0.2426,  0.1645,  0.0486,  0.2949,  0.2477, -0.4205],
          ...,
          [ 0.3542,  0.2926,  0.1056,  0.2984, -0.0244,  0.1580],
          [ 0.1786,  0.0582,  0.1309,  0.1034, -0.1211, -0.1668],
          [ 0.0907, -0.0590,  0.1435,  0.0060, -0.1695, -0.3292]]],


        [[[-0.1218, -0.0756, -0.2131, -0.0623, -0.0204, -0.0518],
          [ 0.0168,  0.0152, -0.2638,  0.0196, -0.0450, -0.1256],
          [ 0.2939,  0.1969, -0.3652,  0.1834, -0.0942, -0.2733],
          ...,
          [-0.2475,  0.3839, -0.3079,  0.1219,  0.0525,  0.1096],
          [ 0.0437,  0.4201, -0.3451, -0.0507, -0.1082,  0.1918],
          [ 0.1893,  0.4382, -0.3637, -0.1369, -0.1886,  0.2329]],

         [[-0.0802,  0.1859, -0.1706,  0.2646,  0.3136,  0.0586],
          [-0.0464,  0.2136, -0.1424,  0.2481,  0.2903,  0.0798],
          [ 0.0212,  0.2689, -0.0860,  0.2153,  0.2438,  0.1222],
          ...,
          [-0.1647,  0.3339, -0.2585,  0.2867, -0.2086, -0.3089],
          [-0.1926,  0.2303, -0.0941,  0.3490, -0.0555, -0.0945],
          [-0.2065,  0.1786, -0.0119,  0.3801,  0.0211,  0.0127]],

         [[ 0.0661,  0.1327,  0.3100, -0.3125,  0.4075,  0.3495],
          [ 0.0449,  0.1537,  0.2772, -0.2071,  0.2596,  0.2164],
          [ 0.0026,  0.1958,  0.2116,  0.0037, -0.0363, -0.0498],
          ...,
          [ 0.0750, -0.1778, -0.2810,  0.0479,  0.1701, -0.0399],
          [ 0.1860,  0.0859, -0.1569,  0.0270, -0.0992, -0.1457],
          [ 0.2415,  0.2177, -0.0949,  0.0165, -0.2338, -0.1986]],

         ...,

         [[-0.4383,  0.0385,  0.4237, -0.1623, -0.1181,  0.4877],
          [-0.3784, -0.0104,  0.3489, -0.0968, -0.1040,  0.3395],
          [-0.2586, -0.1084,  0.1993,  0.0341, -0.0759,  0.0430],
          ...,
          [-0.2117, -0.0141,  0.1302,  0.2579,  0.2378, -0.1980],
          [ 0.1998,  0.1060,  0.1555,  0.0537, -0.1227,  0.1901],
          [ 0.4055,  0.1660,  0.1681, -0.0485, -0.3030,  0.3841]],

         [[-0.0817, -0.2062,  0.3172,  0.1261,  0.2263, -0.2340],
          [ 0.0269, -0.1677,  0.1886,  0.0976,  0.0851, -0.1288],
          [ 0.2441, -0.0906, -0.0686,  0.0405, -0.1974,  0.0816],
          ...,
          [-0.2694, -0.3085, -0.2435, -0.0590,  0.0401,  0.3364],
          [ 0.0667, -0.0256, -0.2782, -0.0625,  0.2487,  0.3790],
          [ 0.2348,  0.1159, -0.2956, -0.0643,  0.3529,  0.4002]],

         [[ 0.4266, -0.0469, -0.0402, -0.2529, -0.3595,  0.1834],
          [ 0.2513, -0.0975, -0.0801, -0.1892, -0.2003,  0.0352],
          [-0.0993, -0.1987, -0.1598, -0.0617,  0.1182, -0.2612],
          ...,
          [ 0.2075,  0.0381,  0.2396,  0.1284,  0.2926, -0.3581],
          [ 0.0672,  0.3078,  0.1287,  0.0560,  0.3403, -0.1397],
          [-0.0030,  0.4426,  0.0732,  0.0199,  0.3642, -0.0306]]]])

2025-07-09 13:40:39.778905 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([16, 160, 8, 111412],"float32"), size=list[16,12,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([16, 160, 8, 111412],"float32"), size=list[16,12,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 476421 / 491520 (96.9%)
Greatest absolute difference: 0.9738116264343262 at index (3, 85, 0, 7) (up to 0.01 allowed)
Greatest relative difference: 261285.078125 at index (7, 115, 5, 6) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 160, 16, 12]), dtype=torch.float32)
tensor([[[[-0.2249,  0.1587,  0.2711,  ...,  0.3352, -0.2644, -0.4247],
          [-0.3225,  0.1734,  0.1341,  ...,  0.2698, -0.2221, -0.1930],
          [-0.4201,  0.1880, -0.0029,  ...,  0.2045, -0.1799,  0.0387],
          ...,
          [-0.0464,  0.0127,  0.0960,  ...,  0.0714, -0.0971, -0.0921],
          [-0.1361,  0.0337,  0.0379,  ...,  0.0207, -0.1680,  0.1032],
          [-0.2258,  0.0548, -0.0202,  ..., -0.0299, -0.2388,  0.2985]],

         [[-0.3007,  0.0136,  0.2333,  ...,  0.1682,  0.1473, -0.1785],
          [-0.0105,  0.0572,  0.2783,  ...,  0.0701,  0.0715,  0.0166],
          [ 0.2797,  0.1008,  0.3233,  ..., -0.0280, -0.0043,  0.2117],
          ...,
          [-0.1574, -0.2509,  0.0017,  ..., -0.2532, -0.1964,  0.4733],
          [-0.1926, -0.0688,  0.0177,  ...,  0.0024, -0.0634,  0.3631],
          [-0.2278,  0.1134,  0.0337,  ...,  0.2579,  0.0695,  0.2529]],

         [[-0.1884, -0.3608,  0.2782,  ..., -0.2687, -0.0557, -0.1484],
          [-0.0225, -0.1411,  0.1439,  ..., -0.1681, -0.1227,  0.0583],
          [ 0.1434,  0.0786,  0.0096,  ..., -0.0676, -0.1897,  0.2651],
          ...,
          [-0.2445,  0.1964,  0.1836,  ..., -0.1089, -0.0789, -0.1040],
          [ 0.0769,  0.0462,  0.1885,  ..., -0.2185,  0.0677, -0.0435],
          [ 0.3984, -0.1039,  0.1934,  ..., -0.3282,  0.2143,  0.0170]],

         ...,

         [[ 0.1623,  0.2124,  0.1485,  ..., -0.0846,  0.3570,  0.3207],
          [ 0.1707,  0.1375,  0.1735,  ..., -0.0531,  0.3058, -0.0508],
          [ 0.1791,  0.0625,  0.1986,  ..., -0.0217,  0.2546, -0.4224],
          ...,
          [ 0.0729, -0.3595, -0.0183,  ...,  0.1420, -0.0359,  0.0225],
          [ 0.0972, -0.2818, -0.0275,  ..., -0.1578, -0.1562,  0.1895],
          [ 0.1215, -0.2042, -0.0367,  ..., -0.4576, -0.2765,  0.3565]],

         [[ 0.4448,  0.1862, -0.2492,  ...,  0.3934, -0.0734, -0.3788],
          [ 0.3563,  0.2345, -0.1506,  ...,  0.0939, -0.0740,  0.0066],
          [ 0.2677,  0.2828, -0.0521,  ..., -0.2056, -0.0745,  0.3921],
          ...,
          [-0.0772,  0.3680,  0.0976,  ...,  0.1619,  0.0134, -0.3115],
          [-0.2081,  0.3179,  0.0203,  ..., -0.0190, -0.1040, -0.2281],
          [-0.3390,  0.2677, -0.0570,  ..., -0.1999, -0.2214, -0.1448]],

         [[ 0.4192,  0.0900,  0.0646,  ..., -0.0107, -0.1248,  0.4373],
          [ 0.2661,  0.1458,  0.0909,  ..., -0.0261, -0.0582,  0.1235],
          [ 0.1130,  0.2016,  0.1173,  ..., -0.0414,  0.0084, -0.1903],
          ...,
          [-0.2610, -0.1698,  0.2095,  ...,  0.3693, -0.1336, -0.0297],
          [-0.2583, -0.0186,  0.2187,  ...,  0.2137, -0.0454,  0.1561],
          [-0.2556,  0.1327,  0.2280,  ...,  0.0581,  0.0429,  0.3420]]],


        [[[-0.2450,  0.1157,  0.2912,  ..., -0.1756,  0.0994,  0.4050],
          [-0.2148,  0.1189,  0.2606,  ..., -0.1909,  0.0137,  0.3985],
          [-0.1846,  0.1221,  0.2301,  ..., -0.2061, -0.0721,  0.3920],
          ...,
          [ 0.2495,  0.2431, -0.1923,  ..., -0.1656,  0.1123,  0.1325],
          [ 0.1426,  0.2532, -0.0566,  ..., -0.1173,  0.2098,  0.3018],
          [ 0.0357,  0.2633,  0.0790,  ..., -0.0691,  0.3073,  0.4711]],

         [[-0.0451,  0.4256, -0.3970,  ...,  0.0339, -0.1648,  0.0892],
          [-0.0640,  0.3441, -0.1690,  ..., -0.0778, -0.2012,  0.2034],
          [-0.0829,  0.2627,  0.0589,  ..., -0.1895, -0.2376,  0.3176],
          ...,
          [ 0.4715, -0.1607,  0.3189,  ..., -0.1825,  0.1072,  0.0120],
          [ 0.4688, -0.1129,  0.0995,  ..., -0.2786, -0.0565, -0.2050],
          [ 0.4661, -0.0650, -0.1199,  ..., -0.3747, -0.2202, -0.4221]],

         [[-0.0983, -0.1385, -0.0101,  ..., -0.1410, -0.0201, -0.1618],
          [-0.1096, -0.2036,  0.0577,  ..., -0.1122,  0.1917, -0.2059],
          [-0.1208, -0.2687,  0.1255,  ..., -0.0834,  0.4036, -0.2499],
          ...,
          [-0.2804,  0.1995, -0.2032,  ...,  0.0415, -0.2042, -0.2095],
          [ 0.1057,  0.0199, -0.1036,  ..., -0.0772, -0.2599, -0.2552],
          [ 0.4918, -0.1597, -0.0039,  ..., -0.1958, -0.3157, -0.3009]],

         ...,

         [[-0.1014, -0.0680, -0.0856,  ...,  0.2994, -0.0323, -0.0965],
          [ 0.0223,  0.0743,  0.0641,  ...,  0.1492, -0.1607,  0.1004],
          [ 0.1460,  0.2166,  0.2137,  ..., -0.0010, -0.2891,  0.2974],
          ...,
          [-0.1533,  0.1360, -0.0790,  ...,  0.2729, -0.2578, -0.0534],
          [ 0.0388,  0.1497,  0.0070,  ...,  0.3224, -0.2482, -0.0577],
          [ 0.2308,  0.1634,  0.0930,  ...,  0.3720, -0.2387, -0.0619]],

         [[ 0.2644,  0.2893, -0.1271,  ...,  0.2178,  0.1955, -0.0582],
          [ 0.3035,  0.2694,  0.0021,  ...,  0.2703,  0.0495, -0.2564],
          [ 0.3426,  0.2496,  0.1314,  ...,  0.3228, -0.0965, -0.4547],
          ...,
          [ 0.1424, -0.3296, -0.0270,  ...,  0.1231,  0.0435, -0.2555],
          [-0.1092, -0.0143,  0.0226,  ...,  0.2581,  0.1607,  0.0109],
          [-0.3608,  0.3011,  0.0722,  ...,  0.3931,  0.2779,  0.2773]],

         [[ 0.1556, -0.1236,  0.1480,  ..., -0.2103, -0.0484,  0.1425],
          [ 0.2251, -0.2308,  0.1707,  ..., -0.0879, -0.0639,  0.0503],
          [ 0.2947, -0.3380,  0.1934,  ...,  0.0344, -0.0794, -0.0420],
          ...,
          [ 0.1476,  0.4222,  0.0206,  ...,  0.0119, -0.1237,  0.3251],
          [ 0.1142,  0.3762, -0.0078,  ...,  0.1266, -0.0328,  0.2531],
          [ 0.0808,  0.3302, -0.0362,  ...,  0.2413,  0.0581,  0.1810]]],


        [[[-0.4919,  0.1947,  0.0375,  ..., -0.2558, -0.1796, -0.2674],
          [-0.1737,  0.1933,  0.0313,  ..., -0.2167,  0.0107, -0.1064],
          [ 0.1444,  0.1919,  0.0252,  ..., -0.1775,  0.2011,  0.0546],
          ...,
          [-0.2978, -0.2799, -0.0174,  ...,  0.0844,  0.2009,  0.1070],
          [-0.1338, -0.0492, -0.0906,  ...,  0.1841,  0.1634,  0.0067],
          [ 0.0302,  0.1816, -0.1638,  ...,  0.2838,  0.1259, -0.0937]],

         [[-0.3417,  0.2950,  0.0571,  ..., -0.0865, -0.1656, -0.2586],
          [-0.3263,  0.1877, -0.1398,  ...,  0.0987, -0.0548, -0.1901],
          [-0.3108,  0.0803, -0.3367,  ...,  0.2839,  0.0560, -0.1216],
          ...,
          [ 0.4270,  0.2503, -0.0855,  ...,  0.0700,  0.2444, -0.1959],
          [ 0.3782,  0.2309,  0.0065,  ...,  0.2268,  0.0642, -0.1839],
          [ 0.3293,  0.2115,  0.0985,  ...,  0.3837, -0.1160, -0.1719]],

         [[ 0.2128,  0.3944, -0.2614,  ...,  0.2588,  0.4231,  0.2304],
          [ 0.1562,  0.2523, -0.1692,  ...,  0.2352,  0.3544,  0.1969],
          [ 0.0996,  0.1102, -0.0771,  ...,  0.2117,  0.2858,  0.1635],
          ...,
          [ 0.0635, -0.0664, -0.1522,  ...,  0.3346,  0.1491, -0.1673],
          [ 0.0210,  0.0324,  0.1110,  ...,  0.1707, -0.0206, -0.2165],
          [-0.0215,  0.1311,  0.3742,  ...,  0.0068, -0.1902, -0.2656]],

         ...,

         [[-0.1262, -0.0588, -0.3224,  ..., -0.0480, -0.1873, -0.4897],
          [-0.1285, -0.1537, -0.3153,  ..., -0.0704, -0.0365, -0.3656],
          [-0.1308, -0.2486, -0.3083,  ..., -0.0927,  0.1143, -0.2416],
          ...,
          [ 0.3154, -0.3821,  0.2086,  ..., -0.2649,  0.0737, -0.4774],
          [ 0.3175, -0.3466,  0.1139,  ..., -0.0374,  0.1471, -0.4235],
          [ 0.3196, -0.3110,  0.0191,  ...,  0.1900,  0.2205, -0.3695]],

         [[-0.3223, -0.1200, -0.0120,  ..., -0.0033, -0.0173,  0.2541],
          [-0.1433, -0.1816,  0.0179,  ...,  0.1173, -0.0451,  0.2991],
          [ 0.0357, -0.2432,  0.0479,  ...,  0.2379, -0.0729,  0.3441],
          ...,
          [ 0.4809,  0.1586,  0.0033,  ..., -0.0829,  0.2074, -0.1835],
          [ 0.4291, -0.0015,  0.0607,  ..., -0.1068,  0.0212,  0.0213],
          [ 0.3773, -0.1616,  0.1181,  ..., -0.1306, -0.1650,  0.2262]],

         [[-0.4373,  0.3026,  0.1307,  ...,  0.2067, -0.1517,  0.1441],
          [-0.0021,  0.2732,  0.0308,  ...,  0.2675, -0.0379, -0.0920],
          [ 0.4331,  0.2437, -0.0691,  ...,  0.3283,  0.0758, -0.3282],
          ...,
          [ 0.3591, -0.0931,  0.0427,  ...,  0.1956, -0.1204, -0.4434],
          [ 0.2937,  0.0932,  0.0943,  ...,  0.1828, -0.0371, -0.1039],
          [ 0.2282,  0.2795,  0.1458,  ...,  0.1701,  0.0461,  0.2355]]],


        ...,


        [[[ 0.3422, -0.0878, -0.1536,  ...,  0.2123,  0.2984, -0.1538],
          [ 0.2056, -0.0689, -0.0128,  ...,  0.2077,  0.2597, -0.3093],
          [ 0.0691, -0.0499,  0.1280,  ...,  0.2031,  0.2210, -0.4648],
          ...,
          [ 0.3334,  0.0480, -0.3005,  ...,  0.0141,  0.3101, -0.0160],
          [ 0.4064,  0.1037, -0.0008,  ...,  0.1075,  0.3435,  0.1617],
          [ 0.4793,  0.1593,  0.2988,  ...,  0.2008,  0.3769,  0.3393]],

         [[-0.2105,  0.1971,  0.2438,  ...,  0.0756, -0.3833,  0.1094],
          [-0.1279,  0.2355,  0.1081,  ...,  0.0588, -0.2181, -0.0340],
          [-0.0452,  0.2738, -0.0276,  ...,  0.0420, -0.0529, -0.1774],
          ...,
          [ 0.1317, -0.1678,  0.2266,  ...,  0.3038, -0.0494,  0.1532],
          [ 0.0896, -0.2302,  0.0362,  ...,  0.2206, -0.1621,  0.0017],
          [ 0.0474, -0.2926, -0.1542,  ...,  0.1375, -0.2747, -0.1498]],

         [[ 0.2684, -0.1611, -0.1333,  ..., -0.0614,  0.3871,  0.3005],
          [ 0.1260,  0.0772, -0.1980,  ...,  0.0042,  0.1491,  0.0498],
          [-0.0163,  0.3154, -0.2627,  ...,  0.0697, -0.0889, -0.2009],
          ...,
          [ 0.4711, -0.2372,  0.0793,  ..., -0.0895, -0.2113,  0.1562],
          [ 0.2927, -0.0490,  0.2448,  ...,  0.0710, -0.1178,  0.1418],
          [ 0.1144,  0.1392,  0.4102,  ...,  0.2316, -0.0242,  0.1275]],

         ...,

         [[-0.2199,  0.3405, -0.1293,  ...,  0.1815,  0.0388,  0.4505],
          [ 0.0872,  0.1779, -0.0755,  ...,  0.2593, -0.0335,  0.0329],
          [ 0.3943,  0.0153, -0.0218,  ...,  0.3371, -0.1058, -0.3847],
          ...,
          [-0.1909,  0.2445,  0.0897,  ..., -0.0162,  0.3254, -0.0950],
          [-0.2619,  0.1442, -0.1416,  ..., -0.0364,  0.3066, -0.0271],
          [-0.3330,  0.0440, -0.3729,  ..., -0.0567,  0.2878,  0.0408]],

         [[ 0.3365,  0.1164, -0.0970,  ...,  0.1684, -0.0261,  0.0719],
          [ 0.2391,  0.0757, -0.1844,  ...,  0.1252,  0.1443,  0.0333],
          [ 0.1416,  0.0349, -0.2719,  ...,  0.0821,  0.3147, -0.0053],
          ...,
          [ 0.3441,  0.3036, -0.1917,  ...,  0.1586, -0.1474, -0.4105],
          [ 0.4034,  0.0798, -0.2201,  ..., -0.0610,  0.0297, -0.4149],
          [ 0.4627, -0.1440, -0.2485,  ..., -0.2807,  0.2069, -0.4192]],

         [[ 0.3221, -0.0468, -0.0813,  ...,  0.0121, -0.0391,  0.4040],
          [ 0.1199,  0.0902,  0.0134,  ..., -0.0145, -0.0366,  0.2143],
          [-0.0824,  0.2273,  0.1080,  ..., -0.0411, -0.0341,  0.0245],
          ...,
          [-0.1473,  0.2237, -0.0917,  ..., -0.1490,  0.0725, -0.1330],
          [-0.2297,  0.0663, -0.0478,  ..., -0.1146,  0.0523, -0.0393],
          [-0.3120, -0.0911, -0.0039,  ..., -0.0803,  0.0321,  0.0544]]],


        [[[ 0.0843, -0.3269,  0.0415,  ...,  0.1610, -0.1011,  0.4041],
          [ 0.2105, -0.1704,  0.1452,  ...,  0.2688, -0.1816,  0.2151],
          [ 0.3367, -0.0139,  0.2489,  ...,  0.3765, -0.2621,  0.0261],
          ...,
          [-0.1792, -0.1692,  0.1553,  ...,  0.3601,  0.1396, -0.0857],
          [ 0.0653, -0.2462, -0.0198,  ...,  0.2007,  0.0991,  0.1274],
          [ 0.3098, -0.3231, -0.1948,  ...,  0.0413,  0.0585,  0.3405]],

         [[ 0.4867,  0.2157,  0.0797,  ...,  0.2774, -0.3141, -0.4253],
          [ 0.0852,  0.0372,  0.0133,  ...,  0.1297, -0.1815, -0.3663],
          [-0.3163, -0.1413, -0.0532,  ..., -0.0180, -0.0490, -0.3074],
          ...,
          [ 0.4096,  0.1481,  0.0833,  ...,  0.1887, -0.0018,  0.2891],
          [ 0.3721,  0.0992,  0.0336,  ...,  0.1266, -0.1671,  0.0482],
          [ 0.3345,  0.0504, -0.0161,  ...,  0.0645, -0.3325, -0.1926]],

         [[ 0.1455, -0.1529,  0.1023,  ..., -0.1144,  0.1851,  0.2323],
          [ 0.0771, -0.2702, -0.1686,  ...,  0.0593, -0.0507, -0.0998],
          [ 0.0087, -0.3876, -0.4396,  ...,  0.2329, -0.2865, -0.4318],
          ...,
          [-0.1853, -0.0315,  0.0385,  ..., -0.1136,  0.2336, -0.2876],
          [ 0.0412, -0.0120,  0.0390,  ..., -0.0871,  0.0468, -0.1930],
          [ 0.2677,  0.0075,  0.0395,  ..., -0.0606, -0.1401, -0.0985]],

         ...,

         [[ 0.2300, -0.1313,  0.0508,  ..., -0.1024, -0.0722, -0.2649],
          [ 0.2704,  0.1170, -0.0301,  ...,  0.0729, -0.0153, -0.1776],
          [ 0.3107,  0.3654, -0.1110,  ...,  0.2482,  0.0416, -0.0902],
          ...,
          [-0.2604, -0.0040, -0.2202,  ...,  0.1859,  0.1372, -0.3880],
          [-0.1589,  0.0606, -0.1332,  ...,  0.0802, -0.0227,  0.0552],
          [-0.0573,  0.1253, -0.0461,  ..., -0.0256, -0.1825,  0.4983]],

         [[-0.1748, -0.3444,  0.2440,  ..., -0.3316,  0.4538,  0.1618],
          [-0.2519, -0.2240,  0.1285,  ..., -0.0975,  0.3280, -0.1456],
          [-0.3290, -0.1036,  0.0130,  ...,  0.1366,  0.2022, -0.4529],
          ...,
          [-0.3742, -0.1625,  0.0691,  ...,  0.3401,  0.0822, -0.0221],
          [-0.3936, -0.1510, -0.0226,  ...,  0.1663, -0.0994,  0.1681],
          [-0.4129, -0.1395, -0.1143,  ..., -0.0074, -0.2809,  0.3584]],

         [[-0.2868,  0.3115,  0.1032,  ...,  0.2723, -0.0701,  0.0062],
          [-0.2511,  0.1113,  0.0474,  ...,  0.1215, -0.1452, -0.0403],
          [-0.2154, -0.0889, -0.0084,  ..., -0.0294, -0.2204, -0.0867],
          ...,
          [-0.3492, -0.3169, -0.1054,  ..., -0.1516, -0.1453,  0.3381],
          [-0.0691,  0.0592,  0.0795,  ..., -0.0202, -0.1546,  0.3131],
          [ 0.2110,  0.4352,  0.2645,  ...,  0.1111, -0.1639,  0.2882]]],


        [[[ 0.0324, -0.0168, -0.3841,  ..., -0.0880,  0.1790, -0.2881],
          [-0.0255, -0.2064, -0.2938,  ..., -0.0996,  0.0313, -0.2330],
          [-0.0835, -0.3959, -0.2034,  ..., -0.1112, -0.1165, -0.1780],
          ...,
          [-0.2461,  0.1183, -0.4296,  ..., -0.0888, -0.1365,  0.4787],
          [-0.3205,  0.0821, -0.2072,  ...,  0.1207,  0.0732,  0.3868],
          [-0.3948,  0.0458,  0.0152,  ...,  0.3302,  0.2830,  0.2949]],

         [[ 0.3137, -0.1229, -0.2924,  ..., -0.1276,  0.1259, -0.3582],
          [ 0.0992,  0.0657, -0.3019,  ..., -0.1616,  0.1198, -0.4104],
          [-0.1153,  0.2543, -0.3115,  ..., -0.1956,  0.1136, -0.4625],
          ...,
          [-0.4562, -0.4021,  0.1341,  ..., -0.0395,  0.3026, -0.2054],
          [-0.2938, -0.3993, -0.0782,  ..., -0.2362,  0.0672, -0.2225],
          [-0.1313, -0.3965, -0.2904,  ..., -0.4330, -0.1683, -0.2395]],

         [[ 0.2023,  0.1118, -0.0641,  ..., -0.2337,  0.1485,  0.0851],
          [ 0.0601,  0.0339, -0.0981,  ...,  0.0535,  0.0827,  0.0645],
          [-0.0821, -0.0440, -0.1321,  ...,  0.3407,  0.0169,  0.0440],
          ...,
          [ 0.4108,  0.1865, -0.0721,  ..., -0.0044,  0.0013,  0.3318],
          [ 0.4535, -0.1051, -0.2231,  ...,  0.0156,  0.0018,  0.3391],
          [ 0.4962, -0.3967, -0.3741,  ...,  0.0356,  0.0024,  0.3464]],

         ...,

         [[ 0.1538,  0.1270,  0.0769,  ...,  0.0097,  0.2926,  0.3371],
          [ 0.2806,  0.0135,  0.1499,  ..., -0.1237,  0.1716,  0.3090],
          [ 0.4075, -0.1001,  0.2229,  ..., -0.2571,  0.0506,  0.2809],
          ...,
          [ 0.1837, -0.0096, -0.0937,  ..., -0.2108, -0.2675,  0.1120],
          [ 0.2785,  0.1422, -0.1591,  ...,  0.0492, -0.2604,  0.2825],
          [ 0.3733,  0.2941, -0.2244,  ...,  0.3092, -0.2533,  0.4530]],

         [[-0.0794,  0.0005, -0.2881,  ..., -0.2296,  0.0302,  0.0157],
          [ 0.0158, -0.1401, -0.3280,  ..., -0.0675, -0.0523,  0.0810],
          [ 0.1111, -0.2808, -0.3680,  ...,  0.0945, -0.1348,  0.1463],
          ...,
          [-0.1447,  0.0074,  0.1018,  ...,  0.4102, -0.2334, -0.1156],
          [-0.0956,  0.0249,  0.1197,  ...,  0.3275, -0.1126, -0.0304],
          [-0.0465,  0.0423,  0.1377,  ...,  0.2448,  0.0081,  0.0548]],

         [[ 0.3344,  0.2379,  0.1155,  ...,  0.0812,  0.2471, -0.1215],
          [ 0.3771,  0.1038,  0.0558,  ..., -0.0988,  0.1481, -0.0811],
          [ 0.4198, -0.0302, -0.0039,  ..., -0.2788,  0.0491, -0.0407],
          ...,
          [ 0.3548, -0.2984,  0.0373,  ..., -0.1999,  0.0870, -0.2311],
          [ 0.4055, -0.0613,  0.0092,  ..., -0.0746, -0.0784, -0.3344],
          [ 0.4562,  0.1759, -0.0189,  ...,  0.0507, -0.2437, -0.4377]]]])
DESIRED: (shape=torch.Size([16, 160, 16, 12]), dtype=torch.float32)
tensor([[[[-1.1210e-01,  3.5417e-02,  3.8196e-01,  ...,  3.5412e-01, -2.9007e-01,  1.1686e-01],
          [-1.0976e-01, -7.6268e-02,  3.3213e-01,  ...,  3.1338e-01, -3.3393e-01,  1.5603e-01],
          [-1.0510e-01, -2.9964e-01,  2.3248e-01,  ...,  2.3192e-01, -4.2167e-01,  2.3436e-01],
          ...,
          [ 1.6559e-01, -3.5811e-01,  2.7259e-01,  ..., -1.8377e-01, -2.3802e-01,  3.3179e-01],
          [ 5.1908e-02, -2.7146e-01,  2.5388e-01,  ...,  1.4954e-01,  2.2313e-02,  5.9920e-02],
          [-4.9316e-03, -2.2813e-01,  2.4452e-01,  ...,  3.1619e-01,  1.5248e-01, -7.6012e-02]],

         [[-3.2335e-01,  3.5463e-02, -4.1026e-02,  ..., -2.7854e-01, -4.5442e-01,  4.6093e-02],
          [-2.5179e-01,  1.3199e-01, -6.6103e-02,  ..., -1.6662e-01, -4.0104e-01,  1.0573e-01],
          [-1.0867e-01,  3.2505e-01, -1.1626e-01,  ...,  5.7200e-02, -2.9430e-01,  2.2500e-01],
          ...,
          [ 1.9904e-03, -1.5869e-02, -1.6605e-01,  ..., -7.5352e-02, -1.7950e-01, -3.6993e-02],
          [-2.6010e-01, -1.2165e-01, -6.4023e-02,  ..., -2.2009e-02, -6.8168e-02,  4.2802e-02],
          [-3.9115e-01, -1.7454e-01, -1.3008e-02,  ...,  4.6626e-03, -1.2502e-02,  8.2699e-02]],

         [[ 2.4236e-01,  1.0675e-01,  2.2399e-02,  ...,  8.6793e-03,  1.0857e-01, -4.2865e-01],
          [ 2.4022e-01,  1.5118e-01, -1.2350e-02,  ..., -1.3194e-02,  4.9698e-02, -3.7695e-01],
          [ 2.3594e-01,  2.4005e-01, -8.1847e-02,  ..., -5.6940e-02, -6.8043e-02, -2.7356e-01],
          ...,
          [ 2.2489e-01, -1.5942e-01, -2.8364e-01,  ..., -3.8263e-02, -2.4342e-01, -2.5872e-01],
          [ 4.9210e-02, -1.3275e-01, -1.3008e-01,  ..., -1.1733e-01, -1.8490e-01,  1.2906e-01],
          [-3.8630e-02, -1.1942e-01, -5.3305e-02,  ..., -1.5686e-01, -1.5564e-01,  3.2295e-01]],

         ...,

         [[ 2.0888e-01, -3.4572e-02, -3.2315e-01,  ..., -6.1735e-02, -2.0308e-01,  2.6044e-01],
          [ 8.4807e-02,  5.6144e-02, -3.1398e-01,  ..., -1.0845e-01, -5.7428e-02,  1.7560e-01],
          [-1.6334e-01,  2.3757e-01, -2.9563e-01,  ..., -2.0188e-01,  2.3388e-01,  5.9111e-03],
          ...,
          [ 6.8035e-02,  1.9558e-01, -1.5701e-01,  ...,  7.4783e-02, -1.8095e-01,  5.1316e-02],
          [ 6.9812e-02, -1.4183e-01,  8.9536e-02,  ...,  5.0734e-03, -1.2845e-01,  2.2022e-01],
          [ 7.0700e-02, -3.1053e-01,  2.1281e-01,  ..., -2.9781e-02, -1.0219e-01,  3.0468e-01]],

         [[ 2.1987e-02,  3.1703e-01, -3.4370e-01,  ..., -2.8755e-01,  3.4812e-01,  1.0549e-01],
          [ 5.7114e-02,  3.3608e-01, -2.9233e-01,  ..., -1.2421e-01,  2.6031e-01,  6.9657e-02],
          [ 1.2737e-01,  3.7418e-01, -1.8959e-01,  ...,  2.0247e-01,  8.4702e-02, -2.0036e-03],
          ...,
          [ 3.3304e-01,  5.5388e-03, -5.2089e-03,  ..., -2.0418e-01,  1.3801e-01,  2.0159e-01],
          [ 2.5986e-01,  2.0839e-01,  2.0837e-02,  ..., -1.0882e-01, -5.9474e-02,  2.0565e-01],
          [ 2.2326e-01,  3.0982e-01,  3.3861e-02,  ..., -6.1143e-02, -1.5822e-01,  2.0767e-01]],

         [[ 2.1091e-01,  1.5346e-01,  2.1452e-01,  ...,  9.2741e-03, -4.4871e-01, -2.5226e-01],
          [ 1.9339e-01,  1.1540e-01,  8.7619e-02,  ...,  5.5462e-03, -3.7654e-01, -1.6507e-01],
          [ 1.5836e-01,  3.9286e-02, -1.6619e-01,  ..., -1.9094e-03, -2.3222e-01,  9.3230e-03],
          ...,
          [-2.6769e-01, -6.4955e-02, -3.3448e-02,  ...,  1.2804e-01,  2.3904e-01, -5.0587e-02],
          [-2.9590e-01, -3.3523e-01, -9.7586e-02,  ...,  2.0273e-01,  2.0589e-01, -8.6962e-02],
          [-3.1001e-01, -4.7037e-01, -1.2966e-01,  ...,  2.4007e-01,  1.8932e-01, -1.0515e-01]]],


        [[[ 9.7670e-03,  2.4790e-02,  9.6207e-03,  ..., -1.3958e-01, -1.8696e-01,  1.4929e-01],
          [ 8.1567e-02, -5.1348e-02, -2.4097e-02,  ..., -1.2725e-01, -1.2424e-01,  9.0586e-02],
          [ 2.2517e-01, -2.0363e-01, -9.1534e-02,  ..., -1.0257e-01,  1.1965e-03, -2.6814e-02],
          ...,
          [-6.2859e-02,  2.1074e-01, -1.8386e-01,  ..., -7.0941e-02, -3.8627e-01,  1.6421e-01],
          [-8.9451e-02,  6.3428e-03,  7.9217e-02,  ...,  2.7216e-02, -3.9169e-01, -6.6770e-02],
          [-1.0275e-01, -9.5858e-02,  2.1076e-01,  ...,  7.6294e-02, -3.9441e-01, -1.8226e-01]],

         [[-2.9035e-01, -4.1620e-01,  2.2160e-01,  ..., -1.3559e-01,  1.5610e-01, -2.1078e-01],
          [-3.1230e-01, -2.4587e-01,  1.0028e-01,  ..., -2.0465e-01,  4.2281e-02, -1.8432e-01],
          [-3.5622e-01,  9.4771e-02, -1.4235e-01,  ..., -3.4276e-01, -1.8536e-01, -1.3138e-01],
          ...,
          [-1.7360e-01, -1.6844e-01, -2.4722e-01,  ..., -2.0629e-01,  1.5665e-01, -4.7780e-02],
          [-7.3988e-02, -2.3103e-01, -1.8754e-01,  ...,  1.6733e-01,  1.6596e-02,  5.8792e-03],
          [-2.4182e-02, -2.6232e-01, -1.5770e-01,  ...,  3.5414e-01, -5.3429e-02,  3.2709e-02]],

         [[ 8.4559e-03, -3.5320e-01, -2.6481e-01,  ..., -6.4696e-02, -2.5213e-01, -2.8543e-01],
          [ 1.8589e-02, -3.8062e-01, -1.7218e-01,  ..., -2.1455e-02, -1.7621e-01, -2.6123e-01],
          [ 3.8856e-02, -4.3546e-01,  1.3084e-02,  ...,  6.5027e-02, -2.4367e-02, -2.1282e-01],
          ...,
          [-3.4991e-02, -3.7017e-01, -2.2321e-01,  ...,  2.3288e-01,  1.4767e-01,  1.4383e-01],
          [ 1.4085e-01, -2.6006e-01, -2.7882e-01,  ...,  4.6153e-02,  1.5764e-01,  3.1288e-01],
          [ 2.2877e-01, -2.0500e-01, -3.0662e-01,  ..., -4.7210e-02,  1.6263e-01,  3.9741e-01]],

         ...,

         [[ 4.9673e-02, -2.2481e-01,  1.9222e-01,  ..., -2.3308e-01, -2.7685e-01, -1.6949e-01],
          [-1.4304e-02, -2.0472e-01,  1.5201e-01,  ..., -2.3429e-01, -8.3307e-02, -2.1145e-01],
          [-1.4226e-01, -1.6455e-01,  7.1584e-02,  ..., -2.3673e-01,  3.0379e-01, -2.9537e-01],
          ...,
          [ 2.1609e-01,  1.5270e-01, -1.4679e-01,  ..., -3.5920e-01,  1.1679e-01, -2.2184e-01],
          [ 1.7048e-01, -2.1503e-01, -5.4866e-03,  ..., -2.3646e-01,  1.6315e-01, -1.9737e-01],
          [ 1.4768e-01, -3.9890e-01,  6.5165e-02,  ..., -1.7509e-01,  1.8633e-01, -1.8514e-01]],

         [[ 3.1873e-01, -4.0395e-01,  1.2564e-01,  ..., -8.0599e-02,  3.7330e-01, -2.7272e-01],
          [ 2.8513e-01, -1.8263e-01,  1.0847e-01,  ..., -1.3151e-01,  2.4888e-01, -1.7078e-01],
          [ 2.1793e-01,  2.6002e-01,  7.4124e-02,  ..., -2.3333e-01,  3.2037e-05,  3.3097e-02],
          ...,
          [ 4.3090e-02, -1.3084e-01,  8.1494e-02,  ...,  2.0664e-01, -6.4259e-02, -2.1319e-01],
          [-1.3120e-01, -2.4496e-01,  8.0252e-02,  ..., -4.0863e-02, -2.6120e-02, -1.3164e-01],
          [-2.1834e-01, -3.0202e-01,  7.9632e-02,  ..., -1.6461e-01, -7.0504e-03, -9.0860e-02]],

         [[-6.0268e-02, -1.1130e-01,  1.3342e-01,  ..., -1.5680e-01,  4.7166e-01, -7.3804e-02],
          [ 5.0339e-02, -6.3174e-02,  8.9859e-02,  ..., -2.2693e-02,  4.0264e-01, -2.8119e-02],
          [ 2.7155e-01,  3.3087e-02,  2.7380e-03,  ...,  2.4552e-01,  2.6460e-01,  6.3252e-02],
          ...,
          [-4.0630e-02, -1.7376e-01, -1.4610e-01,  ...,  6.5374e-03, -1.7763e-01,  2.0569e-01],
          [-1.2653e-01, -5.7037e-02, -2.7052e-01,  ..., -2.1016e-01, -1.2053e-01,  1.0330e-01],
          [-1.6949e-01,  1.3228e-03, -3.3273e-01,  ..., -3.1851e-01, -9.1983e-02,  5.2097e-02]]],


        [[[-2.2874e-01,  2.9244e-01,  1.4177e-02,  ..., -1.5179e-01, -3.3654e-01,  1.1167e-01],
          [-1.2366e-01,  1.8085e-01, -3.1484e-02,  ..., -7.4527e-02, -1.5405e-01,  1.3342e-01],
          [ 8.6502e-02, -4.2324e-02, -1.2281e-01,  ...,  8.0008e-02,  2.1094e-01,  1.7690e-01],
          ...,
          [-2.9075e-02,  2.3719e-01,  2.8632e-01,  ...,  1.3885e-01,  2.2365e-01, -1.7046e-02],
          [-1.7464e-01, -1.1831e-01,  2.8767e-01,  ...,  3.1086e-01, -2.2808e-01, -9.2577e-03],
          [-2.4742e-01, -2.9606e-01,  2.8835e-01,  ...,  3.9686e-01, -4.5395e-01, -5.3637e-03]],

         [[ 1.3707e-01,  3.1986e-01, -2.0084e-01,  ..., -2.4632e-02,  4.7139e-01,  2.2431e-01],
          [ 1.2320e-01,  2.7897e-01, -1.9190e-01,  ...,  2.0706e-02,  2.7905e-01,  1.1119e-01],
          [ 9.5445e-02,  1.9718e-01, -1.7403e-01,  ...,  1.1138e-01, -1.0563e-01, -1.1504e-01],
          ...,
          [-1.1442e-01,  2.7487e-01,  6.9161e-03,  ..., -3.8150e-01, -5.8623e-02,  8.9446e-03],
          [ 2.7819e-02,  1.9563e-01,  6.9995e-03,  ..., -2.6963e-01,  1.7699e-01, -1.4583e-01],
          [ 9.8940e-02,  1.5601e-01,  7.0412e-03,  ..., -2.1369e-01,  2.9479e-01, -2.2321e-01]],

         [[-1.1907e-01, -2.9362e-01,  5.4958e-03,  ...,  3.7008e-01, -3.4661e-01,  8.5773e-03],
          [-2.9797e-02, -1.1590e-01, -3.1933e-02,  ...,  2.9870e-01, -2.1049e-01,  6.4223e-02],
          [ 1.4875e-01,  2.3954e-01, -1.0679e-01,  ...,  1.5593e-01,  6.1750e-02,  1.7551e-01],
          ...,
          [ 2.5057e-02,  1.4545e-01,  9.4299e-02,  ..., -1.6221e-01, -2.0398e-01, -7.7865e-02],
          [ 5.8531e-02, -1.4737e-01,  1.0526e-01,  ..., -9.9372e-02, -3.5499e-01,  6.4038e-02],
          [ 7.5268e-02, -2.9379e-01,  1.1074e-01,  ..., -6.7950e-02, -4.3050e-01,  1.3499e-01]],

         ...,

         [[-3.4887e-01, -1.6278e-01,  1.0684e-01,  ...,  2.0727e-01, -2.5316e-01,  1.3332e-01],
          [-3.1548e-01, -6.5169e-02, -3.2124e-02,  ...,  1.7512e-01, -3.0136e-01, -3.2615e-03],
          [-2.4869e-01,  1.3006e-01, -3.1005e-01,  ...,  1.1083e-01, -3.9775e-01, -2.7642e-01],
          ...,
          [ 1.3295e-01, -2.7642e-01,  2.9571e-01,  ...,  3.3018e-01, -3.3906e-02,  1.1552e-02],
          [ 1.2163e-01, -1.8877e-01, -6.3970e-02,  ...,  1.7878e-01, -2.5434e-01, -2.3867e-01],
          [ 1.1597e-01, -1.4495e-01, -2.4381e-01,  ...,  1.0307e-01, -3.6455e-01, -3.6378e-01]],

         [[ 1.8848e-01, -2.9296e-01, -1.5582e-01,  ..., -1.9470e-01,  6.6630e-02, -1.5007e-01],
          [ 1.9315e-01, -2.5726e-01, -1.1957e-01,  ..., -1.9339e-01,  1.4705e-01, -5.7289e-02],
          [ 2.0250e-01, -1.8587e-01, -4.7064e-02,  ..., -1.9077e-01,  3.0789e-01,  1.2828e-01],
          ...,
          [ 4.2302e-02,  2.8959e-01, -2.7309e-01,  ..., -3.4417e-02,  1.2521e-02,  1.2142e-01],
          [-1.2514e-01,  1.0105e-01, -2.9398e-01,  ...,  1.7827e-01,  3.2948e-01, -3.0047e-02],
          [-2.0885e-01,  6.7825e-03, -3.0442e-01,  ...,  2.8462e-01,  4.8795e-01, -1.0578e-01]],

         [[ 2.1535e-01,  2.8585e-01, -1.0778e-01,  ...,  4.8010e-02, -4.4172e-01,  4.1131e-01],
          [ 1.3731e-01,  1.9513e-01, -1.1157e-01,  ...,  7.6887e-02, -3.4254e-01,  2.9827e-01],
          [-1.8756e-02,  1.3698e-02, -1.1916e-01,  ...,  1.3464e-01, -1.4419e-01,  7.2193e-02],
          ...,
          [-1.1423e-01, -1.4937e-04,  1.3487e-01,  ...,  1.5709e-01,  1.8150e-01, -2.0406e-02],
          [-3.0641e-01, -1.2136e-01,  1.7461e-01,  ..., -1.2558e-01,  2.8429e-01, -4.2870e-03],
          [-4.0250e-01, -1.8196e-01,  1.9447e-01,  ..., -2.6691e-01,  3.3568e-01,  3.7726e-03]]],


        ...,


        [[[ 1.2623e-01, -3.7891e-01, -6.9740e-02,  ..., -2.9972e-01,  4.8654e-01, -5.1436e-03],
          [ 7.5761e-02, -3.8374e-01, -6.5954e-02,  ..., -3.1740e-01,  4.3365e-01, -2.6570e-02],
          [-2.5182e-02, -3.9341e-01, -5.8383e-02,  ..., -3.5276e-01,  3.2786e-01, -6.9423e-02],
          ...,
          [ 2.9149e-01, -2.4902e-01,  2.3605e-01,  ...,  4.2221e-02, -5.1178e-02, -3.1758e-01],
          [ 1.0526e-01, -3.9858e-02,  1.6507e-01,  ..., -1.1576e-02, -3.5402e-02, -1.5714e-01],
          [ 1.2142e-02,  6.4723e-02,  1.2959e-01,  ..., -3.8475e-02, -2.7514e-02, -7.6925e-02]],

         [[-2.3858e-01, -4.8888e-01, -4.7039e-02,  ...,  1.4430e-01, -2.3661e-01, -1.9037e-01],
          [-1.3280e-01, -3.2149e-01,  2.1971e-02,  ...,  8.9065e-02, -1.9468e-01, -5.2210e-02],
          [ 7.8775e-02,  1.3281e-02,  1.5999e-01,  ..., -2.1410e-02, -1.1083e-01,  2.2412e-01],
          ...,
          [-1.5326e-01, -6.2485e-02, -7.5155e-03,  ..., -1.6192e-01, -3.3506e-01,  5.7295e-03],
          [-8.2615e-02,  1.3830e-01,  8.7202e-02,  ..., -9.2308e-02, -2.5118e-01, -4.7240e-02],
          [-4.7294e-02,  2.3869e-01,  1.3456e-01,  ..., -5.7504e-02, -2.0924e-01, -7.3724e-02]],

         [[ 6.6284e-02, -4.2229e-01, -3.8986e-01,  ...,  1.0263e-02,  2.7276e-01,  1.7662e-01],
          [ 8.5504e-02, -4.2659e-01, -1.9940e-01,  ...,  4.4187e-02,  2.2572e-01,  1.1073e-01],
          [ 1.2394e-01, -4.3518e-01,  1.8152e-01,  ...,  1.1204e-01,  1.3163e-01, -2.1059e-02],
          ...,
          [-2.3652e-01,  3.5708e-01,  2.2191e-01,  ..., -4.5376e-02,  6.0350e-02, -2.4035e-01],
          [-3.0716e-01,  8.9105e-02,  1.5260e-01,  ...,  1.3223e-01,  1.4297e-02,  6.5259e-03],
          [-3.4248e-01, -4.4885e-02,  1.1795e-01,  ...,  2.2104e-01, -8.7296e-03,  1.2996e-01]],

         ...,

         [[-4.3169e-02,  3.7462e-01, -1.5265e-01,  ...,  1.1666e-01, -3.2339e-02, -2.2720e-01],
          [-6.8936e-02,  2.2586e-01, -1.0438e-01,  ...,  8.9058e-02, -6.9206e-02, -7.3176e-02],
          [-1.2047e-01, -7.1663e-02, -7.8423e-03,  ...,  3.3860e-02, -1.4294e-01,  2.3488e-01],
          ...,
          [ 3.3966e-01, -2.4580e-01, -1.4852e-01,  ...,  4.4059e-02,  2.5826e-02, -7.6797e-02],
          [ 2.4608e-01, -3.9140e-02, -9.0737e-02,  ...,  1.6389e-01, -2.0798e-01,  8.1372e-02],
          [ 1.9929e-01,  6.4188e-02, -6.1847e-02,  ...,  2.2380e-01, -3.2489e-01,  1.6046e-01]],

         [[-3.7723e-03,  4.1019e-01, -2.3741e-01,  ...,  9.9913e-02,  3.3850e-01,  3.8402e-02],
          [-4.4672e-02,  2.0879e-01, -1.5254e-01,  ...,  1.3176e-01,  3.2686e-01,  9.4648e-02],
          [-1.2647e-01, -1.9402e-01,  1.7201e-02,  ...,  1.9546e-01,  3.0359e-01,  2.0714e-01],
          ...,
          [-1.0784e-01,  4.3701e-01,  1.7619e-01,  ..., -1.2302e-01, -3.4544e-01, -1.9178e-01],
          [ 1.2411e-01,  4.2357e-01,  3.7140e-01,  ..., -2.4331e-01, -2.0269e-01,  1.5246e-01],
          [ 2.4009e-01,  4.1684e-01,  4.6900e-01,  ..., -3.0345e-01, -1.3132e-01,  3.2458e-01]],

         [[-1.3094e-01,  4.7365e-02, -1.1450e-01,  ..., -1.2010e-01, -1.5835e-01, -5.0788e-02],
          [-1.7626e-01,  2.7664e-02,  8.1226e-03,  ..., -5.5452e-02, -1.3875e-01, -1.6352e-02],
          [-2.6688e-01, -1.1740e-02,  2.5337e-01,  ...,  7.3845e-02, -9.9560e-02,  5.2521e-02],
          ...,
          [ 2.2787e-01, -1.3700e-01,  1.2877e-01,  ...,  1.1844e-01, -3.6664e-01,  1.2276e-01],
          [ 2.0926e-01, -3.6451e-01,  4.9787e-02,  ..., -2.2447e-03, -1.5332e-01, -1.3454e-01],
          [ 1.9996e-01, -4.7827e-01,  1.0298e-02,  ..., -6.2585e-02, -4.6653e-02, -2.6318e-01]]],


        [[[-1.8029e-01, -3.2286e-01,  1.7636e-01,  ...,  1.7732e-01, -1.4061e-01,  7.9353e-04],
          [-1.0482e-01, -2.4431e-01,  1.0680e-01,  ...,  6.7535e-02, -1.2423e-01, -5.3550e-02],
          [ 4.6120e-02, -8.7197e-02, -3.2321e-02,  ..., -1.5204e-01, -9.1467e-02, -1.6224e-01],
          ...,
          [-2.5480e-01, -2.6998e-01, -1.5874e-01,  ...,  8.7727e-02,  3.4096e-02,  3.3085e-01],
          [-2.9580e-01, -3.6897e-01, -2.5125e-01,  ...,  1.0480e-03,  2.2853e-01,  9.5850e-02],
          [-3.1630e-01, -4.1847e-01, -2.9751e-01,  ..., -4.2292e-02,  3.2575e-01, -2.1651e-02]],

         [[-2.1313e-01, -3.4727e-01,  6.0382e-03,  ...,  5.4166e-02, -5.3584e-02,  4.1275e-01],
          [-1.3737e-01, -1.3996e-01, -1.8409e-02,  ..., -5.4529e-02, -3.2683e-02,  3.0887e-01],
          [ 1.4127e-02,  2.7467e-01, -6.7303e-02,  ..., -2.7192e-01,  9.1208e-03,  1.0110e-01],
          ...,
          [-1.7750e-01,  3.4467e-01,  6.3340e-02,  ...,  2.7217e-01, -1.2862e-01, -2.5964e-01],
          [-2.2027e-01,  3.0460e-01,  2.2585e-01,  ...,  2.1280e-01,  1.9728e-01, -2.2620e-01],
          [-2.4165e-01,  2.8456e-01,  3.0711e-01,  ...,  1.8312e-01,  3.6024e-01, -2.0948e-01]],

         [[ 2.3737e-01,  4.9370e-01,  2.2203e-01,  ..., -2.8139e-01,  1.1424e-02, -4.4175e-01],
          [ 2.0813e-01,  4.0826e-01,  1.8141e-01,  ..., -2.1253e-01,  7.6090e-02, -2.6257e-01],
          [ 1.4966e-01,  2.3739e-01,  1.0016e-01,  ..., -7.4826e-02,  2.0542e-01,  9.5804e-02],
          ...,
          [ 1.5781e-01, -2.8120e-01,  5.7085e-02,  ..., -2.2601e-01, -9.6084e-03, -2.6641e-01],
          [ 1.0900e-01,  1.0035e-01, -5.3792e-02,  ..., -1.6943e-01,  6.2403e-02, -1.5987e-01],
          [ 8.4593e-02,  2.9113e-01, -1.0923e-01,  ..., -1.4114e-01,  9.8409e-02, -1.0660e-01]],

         ...,

         [[-3.6953e-01,  1.4018e-01,  3.6946e-01,  ..., -2.5742e-01, -3.7264e-01, -3.1454e-02],
          [-1.9899e-01,  1.2415e-01,  3.4654e-01,  ..., -1.9575e-01, -1.9602e-01, -5.8443e-02],
          [ 1.4209e-01,  9.2068e-02,  3.0071e-01,  ..., -7.2414e-02,  1.5722e-01, -1.1242e-01],
          ...,
          [ 1.1644e-01,  5.1975e-02, -1.4446e-02,  ..., -1.1921e-01,  3.0427e-01,  1.3216e-01],
          [ 1.2569e-01,  1.7081e-01,  1.2230e-02,  ..., -2.1366e-01,  2.8397e-01,  1.9823e-01],
          [ 1.3032e-01,  2.3023e-01,  2.5568e-02,  ..., -2.6089e-01,  2.7382e-01,  2.3126e-01]],

         [[-1.0931e-01,  1.8975e-01, -1.8748e-01,  ...,  1.6773e-02,  1.6115e-01, -2.6747e-01],
          [-1.9598e-01,  2.2696e-01, -1.1564e-01,  ..., -2.5511e-02,  9.7299e-02, -2.2906e-01],
          [-3.6934e-01,  3.0138e-01,  2.8047e-02,  ..., -1.1008e-01, -3.0401e-02, -1.5225e-01],
          ...,
          [-1.2874e-01,  4.3451e-02,  1.2910e-01,  ..., -1.6101e-01,  2.2159e-01,  1.1838e-01],
          [ 2.0014e-01, -2.2039e-01,  8.6902e-03,  ..., -1.9399e-01,  3.4214e-01,  2.1563e-02],
          [ 3.6458e-01, -3.5231e-01, -5.1515e-02,  ..., -2.1048e-01,  4.0242e-01, -2.6845e-02]],

         [[ 2.4770e-01,  7.5653e-02, -2.9438e-01,  ..., -1.7325e-01, -6.1150e-03, -1.8283e-01],
          [ 1.2722e-01,  1.4363e-01, -1.5363e-01,  ..., -5.4805e-02,  4.7984e-02, -1.0004e-01],
          [-1.1375e-01,  2.7959e-01,  1.2787e-01,  ...,  1.8208e-01,  1.5618e-01,  6.5535e-02],
          ...,
          [-7.4887e-02,  3.5127e-01,  2.3913e-01,  ...,  1.3439e-02,  3.0411e-01,  9.2853e-02],
          [ 3.3046e-02,  6.0511e-02,  1.1023e-01,  ...,  2.6497e-02,  4.1673e-01, -1.1092e-01],
          [ 8.7012e-02, -8.4871e-02,  4.5776e-02,  ...,  3.3027e-02,  4.7303e-01, -2.1280e-01]]],


        [[[-3.7679e-01, -3.5630e-01, -1.5853e-01,  ..., -1.8542e-02,  4.5967e-01, -2.3047e-02],
          [-2.4425e-01, -3.1048e-01, -1.5562e-01,  ..., -4.7038e-02,  4.1253e-01, -6.5920e-02],
          [ 2.0838e-02, -2.1884e-01, -1.4980e-01,  ..., -1.0403e-01,  3.1827e-01, -1.5166e-01],
          ...,
          [ 1.1918e-01,  1.6118e-01,  4.8108e-02,  ..., -1.2312e-01,  2.1339e-01, -9.0895e-03],
          [-2.3348e-02,  1.5776e-01,  1.7506e-01,  ..., -3.1746e-01,  4.0274e-01, -1.7527e-02],
          [-9.4611e-02,  1.5606e-01,  2.3854e-01,  ..., -4.1462e-01,  4.9742e-01, -2.1746e-02]],

         [[ 1.2014e-01, -1.8670e-01, -3.1620e-03,  ...,  8.8873e-02,  2.1178e-02, -1.9439e-01],
          [ 7.7291e-02, -8.4122e-02,  6.0964e-03,  ...,  7.1263e-02, -7.6819e-02, -1.7072e-01],
          [-8.4097e-03,  1.2102e-01,  2.4613e-02,  ...,  3.6041e-02, -2.7281e-01, -1.2339e-01],
          ...,
          [-1.1880e-01,  1.1718e-01, -1.6303e-01,  ..., -4.6674e-02, -2.3515e-01, -2.1523e-01],
          [ 7.9880e-02,  2.7193e-01, -1.6035e-01,  ...,  9.2723e-02, -1.5310e-01,  1.3770e-02],
          [ 1.7922e-01,  3.4930e-01, -1.5901e-01,  ...,  1.6242e-01, -1.1208e-01,  1.2827e-01]],

         [[ 3.4348e-01, -3.5716e-01, -2.2953e-01,  ...,  2.7444e-01, -9.6619e-02,  1.2351e-01],
          [ 2.5109e-01, -3.5211e-01, -2.1859e-01,  ...,  1.9566e-01, -1.7097e-02,  1.4679e-01],
          [ 6.6294e-02, -3.4199e-01, -1.9670e-01,  ...,  3.8094e-02,  1.4195e-01,  1.9335e-01],
          ...,
          [ 2.4387e-02,  5.8536e-02, -1.3815e-01,  ..., -1.9892e-03,  5.0022e-02, -3.2963e-01],
          [ 1.6040e-02,  1.8925e-01,  1.0183e-01,  ..., -6.2921e-02, -5.5644e-02, -1.8906e-01],
          [ 1.1867e-02,  2.5461e-01,  2.2182e-01,  ..., -9.3387e-02, -1.0848e-01, -1.1877e-01]],

         ...,

         [[-2.7321e-01, -4.2496e-01,  2.0252e-01,  ...,  1.2253e-01, -2.9193e-01, -1.5801e-01],
          [-2.3122e-01, -1.9486e-01,  1.0190e-01,  ...,  7.4497e-02, -1.3371e-01, -1.9836e-01],
          [-1.4725e-01,  2.6534e-01, -9.9319e-02,  ..., -2.1569e-02,  1.8274e-01, -2.7906e-01],
          ...,
          [-6.9083e-02, -3.4029e-01,  1.6627e-01,  ...,  2.5949e-02, -1.3460e-01, -2.2076e-01],
          [-1.8487e-01, -3.0593e-01,  1.4690e-01,  ..., -1.1911e-01,  1.4761e-01,  6.3552e-02],
          [-2.4276e-01, -2.8876e-01,  1.3722e-01,  ..., -1.9164e-01,  2.8871e-01,  2.0570e-01]],

         [[-2.3187e-01,  2.4844e-01, -1.9288e-01,  ..., -1.4038e-01,  4.9904e-01,  7.1867e-02],
          [-2.2263e-01,  2.6776e-01, -1.6235e-01,  ..., -5.6159e-02,  4.0508e-01, -1.7149e-02],
          [-2.0414e-01,  3.0640e-01, -1.0130e-01,  ...,  1.1228e-01,  2.1717e-01, -1.9518e-01],
          ...,
          [-2.7805e-01,  3.4021e-01,  2.4661e-01,  ..., -1.0644e-01, -3.0586e-02, -4.7622e-03],
          [-2.6403e-01,  2.4944e-01,  2.2588e-01,  ..., -1.6730e-01, -2.2597e-01, -2.8885e-01],
          [-2.5702e-01,  2.0405e-01,  2.1552e-01,  ..., -1.9773e-01, -3.2366e-01, -4.3089e-01]],

         [[-3.1500e-01, -4.9970e-01,  2.4433e-01,  ...,  1.1841e-01,  4.9200e-01,  2.5382e-01],
          [-2.6127e-01, -3.8671e-01,  2.2470e-01,  ...,  4.9004e-02,  4.6134e-01,  2.1686e-01],
          [-1.5382e-01, -1.6075e-01,  1.8543e-01,  ..., -8.9803e-02,  4.0000e-01,  1.4294e-01],
          ...,
          [-1.6401e-01, -3.1350e-01, -2.1513e-01,  ...,  5.3110e-02,  2.0615e-01, -2.8570e-01],
          [ 1.4252e-01, -3.4870e-01, -1.5446e-01,  ...,  1.1133e-02,  2.7982e-01, -1.4340e-01],
          [ 2.9578e-01, -3.6630e-01, -1.2412e-01,  ..., -9.8558e-03,  3.1666e-01, -7.2244e-02]]]])

2025-07-09 13:40:43.507402 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([16, 356516, 20, 20],"float32"), size=list[19,19,], mode="bicubic", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([16, 356516, 20, 20],"float32"), size=list[19,19,], mode="bicubic", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 288911033 / 2059236416 (14.0%)
Greatest absolute difference: 0.04231327772140503 at index (15, 355757, 17, 17) (up to 0.01 allowed)
Greatest relative difference: 32768482.0 at index (3, 323554, 8, 17) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 356516, 19, 19]), dtype=torch.float32)
tensor([[[[-2.8105e-01,  3.8777e-01,  1.8434e-01,  ..., -2.9703e-01, -2.7959e-01,  4.0804e-02],
          [ 4.1367e-01,  9.1924e-02, -1.1457e-02,  ...,  2.2706e-01, -3.6203e-01, -3.8048e-01],
          [ 2.8379e-01,  2.1384e-01,  2.4464e-01,  ..., -4.2228e-01,  3.3476e-01, -1.3444e-01],
          ...,
          [-4.9374e-01,  3.2174e-01,  2.2844e-01,  ..., -2.5226e-01,  2.7606e-01, -4.1365e-01],
          [ 4.1541e-01,  7.5773e-02, -5.9876e-02,  ...,  3.2959e-01,  1.8699e-01,  1.7317e-01],
          [ 3.7522e-01,  4.4971e-01,  3.7763e-01,  ..., -7.9365e-02, -5.0299e-01,  4.7867e-02]],

         [[ 2.2020e-01,  4.1490e-01, -1.2185e-02,  ...,  3.0040e-01,  1.0645e-01,  1.7990e-01],
          [-2.8794e-01,  2.7988e-01, -3.6407e-01,  ...,  3.4370e-01, -3.5287e-01, -3.0999e-01],
          [ 1.2855e-01, -2.9212e-01, -1.3247e-01,  ..., -2.9014e-01,  9.1184e-02, -3.0572e-01],
          ...,
          [-4.4238e-01, -8.3288e-02,  1.4271e-01,  ..., -4.3584e-01,  2.5463e-01,  2.2515e-01],
          [-4.2420e-01,  2.1611e-01, -5.6600e-02,  ...,  2.9269e-01, -3.5971e-01,  1.6593e-01],
          [-4.7365e-01,  1.5902e-03, -2.8024e-01,  ...,  3.5445e-01,  1.1534e-01,  2.0132e-02]],

         [[-1.1604e-01,  2.9833e-01, -1.6772e-01,  ...,  1.7573e-01,  2.4907e-01, -3.9004e-01],
          [ 4.4596e-01, -4.8999e-01, -3.9912e-01,  ..., -2.7470e-01, -2.6913e-01, -2.4982e-01],
          [ 3.4618e-01, -3.4313e-01,  4.7023e-01,  ...,  4.3944e-01,  1.1606e-01, -4.3218e-01],
          ...,
          [-2.3427e-01,  2.4438e-01,  2.3781e-01,  ..., -1.2359e-01,  1.7611e-01, -2.1001e-01],
          [ 6.3566e-02, -3.0135e-01, -6.8956e-02,  ...,  6.8874e-02, -2.3642e-01, -9.5590e-02],
          [-3.9011e-01, -3.4978e-01, -1.3688e-01,  ..., -3.6530e-01, -1.9978e-01, -2.8360e-02]],

         ...,

         [[-3.2617e-01,  2.3365e-01,  4.2021e-02,  ...,  2.3047e-01,  4.3504e-01,  4.9999e-01],
          [ 6.1350e-02,  1.0131e-01,  1.4686e-01,  ...,  1.7754e-02, -4.3756e-01,  2.3005e-01],
          [ 2.8292e-01, -1.2166e-01, -3.5346e-01,  ..., -1.7174e-01,  5.0455e-02,  1.2405e-02],
          ...,
          [-3.4599e-01, -3.6767e-02,  1.0592e-01,  ..., -5.6594e-04, -3.0789e-01, -4.9685e-01],
          [ 4.7372e-01, -4.3506e-01, -2.7478e-01,  ...,  4.0365e-03,  3.1571e-01, -8.3405e-02],
          [ 1.8269e-01,  4.2607e-01, -4.3380e-02,  ...,  2.6006e-01,  3.3257e-02, -3.6531e-01]],

         [[ 2.1228e-01,  1.3957e-01, -1.4561e-01,  ..., -4.4096e-01, -5.6552e-02,  4.6203e-01],
          [-1.5379e-01, -1.1708e-01, -2.0324e-01,  ..., -3.2148e-01,  1.3202e-01, -3.9117e-01],
          [-4.2086e-01,  4.4606e-01,  2.7619e-01,  ...,  1.6064e-01, -4.9052e-01,  1.4166e-01],
          ...,
          [ 1.7434e-01, -3.6852e-01,  2.9167e-01,  ..., -3.5477e-01,  3.1659e-01, -2.6582e-01],
          [-3.0736e-01, -7.5701e-02,  3.9123e-01,  ..., -2.2353e-03, -3.9247e-01,  1.1143e-02],
          [-2.8962e-01,  2.7196e-01,  1.5947e-01,  ...,  1.4446e-01, -3.2924e-03, -7.0909e-02]],

         [[-2.5421e-01,  1.9025e-01, -4.5187e-01,  ...,  3.8368e-01, -4.7039e-01,  3.9477e-01],
          [ 2.7480e-01, -1.7117e-01, -1.7905e-01,  ..., -3.8137e-02,  4.1493e-01,  1.9750e-01],
          [ 3.3769e-01, -2.3511e-01, -2.5244e-01,  ..., -1.7246e-01,  2.6226e-01, -4.9881e-03],
          ...,
          [ 2.0123e-01, -2.8346e-02,  1.2217e-01,  ...,  2.8146e-01,  2.1458e-01,  1.9026e-01],
          [-2.6739e-02, -1.5081e-01, -4.1586e-01,  ..., -1.1868e-02, -3.0544e-01,  2.9564e-01],
          [-5.1255e-02, -3.9607e-01, -1.7058e-01,  ..., -8.5919e-02, -2.5822e-01,  4.9634e-01]]],


        [[[-2.9585e-01,  2.4430e-01, -3.7610e-01,  ..., -9.8997e-03,  2.8062e-01, -1.4448e-03],
          [ 2.7032e-01,  1.7234e-03, -1.4684e-01,  ..., -1.8265e-01, -2.0133e-01,  3.1455e-01],
          [ 5.2738e-02,  2.3609e-01, -3.4785e-01,  ..., -2.2089e-01, -1.9340e-01,  4.0360e-01],
          ...,
          [ 1.0635e-01,  1.1800e-01, -3.7316e-01,  ...,  3.4498e-01,  4.8595e-01, -1.4109e-02],
          [-3.7283e-02,  4.7978e-01,  3.0066e-01,  ..., -3.7506e-01,  3.2686e-01, -9.5230e-02],
          [-5.5942e-02, -4.0871e-01, -1.9488e-01,  ..., -7.8601e-02,  1.9098e-01,  3.8974e-01]],

         [[ 1.6635e-01,  1.9436e-02,  1.4482e-01,  ...,  2.6313e-01,  4.4199e-01, -1.4338e-01],
          [ 3.4434e-01, -5.0612e-01, -2.7948e-02,  ..., -2.9058e-01,  4.3409e-01,  2.4710e-02],
          [-1.4321e-01,  3.9084e-02,  2.8022e-01,  ...,  2.9380e-01,  1.0674e-01, -3.4134e-01],
          ...,
          [ 3.6753e-01, -2.7211e-01, -4.2425e-01,  ...,  8.4685e-02, -2.4320e-01,  1.9309e-01],
          [-1.0677e-02,  4.9501e-01,  2.1192e-01,  ...,  2.3132e-01, -2.7883e-01,  2.4834e-01],
          [-9.6839e-02, -3.3828e-01, -1.4329e-01,  ...,  7.3464e-03,  3.7467e-01, -1.7291e-01]],

         [[ 4.5337e-02,  1.7325e-01,  3.6912e-01,  ..., -9.9284e-02, -3.3834e-01, -2.1977e-01],
          [-4.4821e-01, -1.0561e-01,  1.5638e-01,  ...,  1.0390e-01, -1.4977e-01, -2.8802e-01],
          [-8.9924e-02,  2.5946e-01,  2.0719e-01,  ...,  3.7979e-01,  2.7634e-01, -1.0865e-01],
          ...,
          [-4.0218e-01,  1.0274e-02, -1.1821e-01,  ..., -1.6517e-01, -3.3928e-01,  2.8517e-01],
          [ 4.1166e-01, -2.7745e-01,  2.3911e-02,  ...,  2.1156e-01, -4.6209e-01,  2.8587e-01],
          [-7.5771e-02, -2.6302e-01,  3.6439e-01,  ..., -2.2172e-02, -2.0933e-01, -1.9589e-01]],

         ...,

         [[ 3.6437e-01, -5.2570e-01, -3.6048e-01,  ...,  8.3094e-02, -3.8835e-01,  4.2444e-01],
          [ 1.6464e-01, -3.0345e-01, -3.4523e-01,  ...,  1.1849e-03, -2.5016e-01,  3.4155e-01],
          [-2.4399e-01,  4.1576e-02,  1.8269e-02,  ..., -4.1319e-01, -1.8427e-01,  3.4789e-01],
          ...,
          [ 1.8311e-01,  4.2663e-01,  2.5626e-01,  ..., -2.2173e-02,  1.5512e-01,  9.5095e-02],
          [ 7.6018e-02,  4.8950e-01,  4.2456e-02,  ...,  3.2234e-01,  4.5735e-01,  2.1631e-02],
          [-1.4992e-02, -9.1059e-02, -2.1513e-01,  ..., -4.7195e-01, -3.1856e-02, -1.7130e-01]],

         [[ 2.3248e-01, -3.5557e-01,  5.1827e-01,  ...,  3.9275e-01,  5.0614e-01,  3.3536e-01],
          [-3.4856e-01, -1.8917e-01,  3.9435e-01,  ...,  3.2411e-01, -6.5829e-02, -2.2490e-01],
          [ 4.4349e-01, -7.7091e-02, -1.1419e-01,  ..., -9.2463e-03, -4.2832e-01, -2.7558e-01],
          ...,
          [-1.6307e-01,  6.2583e-02,  2.7765e-01,  ..., -9.5932e-02,  2.2298e-02,  2.2383e-01],
          [ 3.2911e-01,  1.1381e-01, -1.4652e-01,  ..., -2.9691e-01, -2.1268e-01, -2.1057e-01],
          [-1.6729e-01,  9.5653e-02, -2.4185e-02,  ...,  5.8008e-02, -1.3439e-01,  1.9270e-01]],

         [[ 2.5566e-01, -2.7169e-01,  3.4862e-01,  ..., -5.0236e-01,  2.0910e-01,  3.8726e-01],
          [ 2.2082e-01, -4.7535e-02,  5.1918e-01,  ..., -2.8480e-01,  1.0208e-01,  4.2009e-01],
          [-4.3511e-01,  4.5356e-01,  1.5270e-01,  ..., -2.4243e-01, -9.7910e-02, -5.0836e-01],
          ...,
          [ 1.1732e-01,  4.2164e-01,  1.3400e-02,  ..., -1.5731e-01, -1.7417e-01, -2.2955e-01],
          [-2.4160e-01,  4.9359e-02, -2.6570e-01,  ...,  2.8063e-01, -4.5845e-01, -2.1445e-02],
          [-9.5378e-02, -1.4731e-02,  1.7561e-01,  ..., -4.4431e-01,  2.6681e-01,  5.1121e-02]]],


        [[[ 1.9774e-01,  6.1456e-02, -2.7282e-01,  ..., -1.2063e-01,  2.4586e-01,  4.0683e-02],
          [ 6.8591e-02,  1.9550e-01, -1.0571e-01,  ...,  2.9743e-01,  4.0178e-01, -2.6013e-01],
          [ 3.1834e-01, -4.1261e-01, -3.0710e-01,  ...,  1.2507e-01,  7.9421e-02,  1.4179e-02],
          ...,
          [ 4.2631e-01,  1.5462e-02, -4.4311e-01,  ..., -3.3435e-01,  4.0617e-01, -4.1346e-01],
          [ 7.3172e-02,  1.2030e-01,  2.3244e-01,  ..., -2.3046e-02, -4.8960e-02, -4.5823e-02],
          [ 4.1195e-01, -5.3776e-02,  2.4006e-01,  ...,  6.6826e-02, -3.4618e-01, -4.6100e-01]],

         [[ 4.6762e-01, -3.6544e-01,  4.5597e-01,  ..., -2.4174e-01,  3.4818e-01,  1.0063e-01],
          [-2.7259e-01,  4.2474e-02,  2.1959e-01,  ..., -1.7634e-01, -4.7629e-01, -5.6990e-02],
          [-2.3891e-01,  3.3887e-01,  2.2012e-01,  ..., -2.3819e-01,  1.4981e-01,  1.9161e-01],
          ...,
          [-8.2864e-02,  3.6216e-01, -4.9903e-01,  ..., -8.1472e-02,  2.1099e-01, -3.3241e-01],
          [-1.8186e-01,  4.6042e-01,  2.9292e-01,  ..., -2.6568e-02, -4.2794e-01, -3.1721e-04],
          [-3.2782e-02, -2.9305e-01,  1.1055e-01,  ..., -4.2929e-01,  1.6019e-01,  7.7477e-02]],

         [[ 3.1903e-01,  3.2680e-02, -3.4068e-01,  ..., -1.3036e-01, -2.4970e-01,  9.0617e-02],
          [-8.2561e-02, -4.7402e-01, -1.3623e-01,  ...,  9.8329e-02,  1.6430e-01,  3.0052e-01],
          [-1.6343e-01,  3.4767e-01, -4.6784e-01,  ..., -3.7037e-01, -1.9384e-01, -6.1206e-03],
          ...,
          [-4.8116e-01,  2.6930e-01, -2.4335e-01,  ..., -3.1027e-01, -3.2398e-01, -4.1063e-01],
          [ 2.0038e-01, -2.2082e-01, -1.1092e-01,  ..., -4.9481e-01,  9.6720e-02, -4.0203e-01],
          [-4.3744e-01, -1.6578e-01, -5.0500e-01,  ...,  5.0913e-01, -4.4250e-01, -2.3453e-01]],

         ...,

         [[-3.0746e-01,  2.7663e-01, -3.6433e-02,  ..., -4.0596e-01, -4.5112e-01,  1.0847e-01],
          [ 1.4574e-01, -1.8579e-01, -7.2867e-02,  ...,  4.5890e-01,  2.8473e-01,  2.3631e-01],
          [-1.4995e-01, -1.4750e-01,  1.7475e-01,  ...,  1.8743e-01, -1.3285e-01, -4.8158e-01],
          ...,
          [ 1.6257e-01, -1.2760e-01,  3.0224e-01,  ..., -8.6106e-02,  3.8589e-01, -4.3678e-01],
          [-2.4992e-01,  1.6053e-01,  3.6406e-02,  ...,  2.6506e-01, -3.7619e-01, -3.4978e-01],
          [ 2.3240e-01, -1.9720e-01, -2.9046e-01,  ..., -1.3894e-01, -2.6559e-01,  1.2940e-02]],

         [[-1.2218e-01,  1.8380e-01,  2.0639e-02,  ...,  1.5514e-01, -2.1119e-01, -1.4726e-01],
          [-2.8358e-01, -3.4164e-01, -3.2597e-01,  ...,  1.3612e-01,  9.4422e-02, -1.2316e-01],
          [ 4.2152e-01,  4.5663e-01, -1.1395e-01,  ...,  3.0473e-01, -1.8115e-01, -5.0399e-01],
          ...,
          [ 1.7645e-01,  3.7073e-02,  1.5495e-01,  ...,  2.9027e-01,  3.7287e-01,  5.4429e-01],
          [ 6.9016e-02,  2.1962e-01,  2.4599e-01,  ..., -3.3796e-01,  7.5909e-02, -2.6229e-01],
          [ 2.9399e-01, -4.9947e-01, -2.6564e-01,  ...,  2.1672e-01,  2.4128e-01, -3.5041e-01]],

         [[-1.6677e-01,  4.9567e-01,  2.3134e-01,  ...,  1.0694e-03, -1.4418e-01, -2.3526e-01],
          [-4.5809e-01, -3.1836e-01,  4.8913e-01,  ...,  2.0280e-01,  2.3811e-01, -1.1765e-01],
          [-9.1850e-02, -3.3556e-01,  1.7441e-02,  ..., -1.8148e-01,  5.1722e-02, -2.9178e-01],
          ...,
          [-3.3214e-01,  8.8960e-02, -2.3160e-01,  ...,  5.0462e-01, -5.0289e-01,  4.4828e-01],
          [-2.7653e-01, -3.8106e-01,  3.7825e-01,  ...,  4.3751e-01, -7.9191e-02, -4.4183e-01],
          [-1.7479e-01,  2.5528e-03, -2.0385e-01,  ..., -1.9352e-01, -1.2622e-01,  9.2278e-02]]],


        ...,


        [[[ 1.1978e-02,  1.4582e-01, -2.3053e-01,  ..., -2.1263e-01,  2.9611e-01, -2.9703e-02],
          [ 1.7401e-01, -2.1447e-01, -2.8267e-01,  ..., -4.4920e-01,  1.6991e-01, -9.3581e-02],
          [ 1.5428e-01, -4.5211e-01, -1.9752e-02,  ...,  3.4299e-02,  4.0202e-01, -4.2901e-02],
          ...,
          [-3.5539e-01,  2.7356e-01, -1.6242e-01,  ...,  4.4361e-01, -3.3530e-01,  1.2338e-01],
          [-1.9540e-01,  2.2498e-01,  2.0296e-01,  ..., -4.7709e-01,  1.3861e-01, -4.2158e-01],
          [-2.5582e-02, -2.6904e-01, -2.0605e-02,  ...,  3.9357e-01, -4.8542e-01,  2.7012e-01]],

         [[-6.9597e-02,  4.2500e-01,  3.6372e-01,  ..., -4.0845e-01, -3.6577e-01,  1.2206e-01],
          [ 2.6830e-01,  1.2310e-01,  2.4632e-01,  ...,  3.0819e-01,  4.3169e-01,  1.6544e-02],
          [ 4.4730e-01,  1.4286e-01,  1.2176e-01,  ...,  2.5375e-01, -1.2967e-01,  5.2409e-02],
          ...,
          [-3.5862e-01, -1.1693e-01, -5.9001e-03,  ...,  1.2783e-01,  2.9218e-03, -6.2432e-02],
          [ 4.1250e-01, -1.2857e-01,  1.6093e-01,  ..., -4.4584e-01,  4.1165e-01,  8.1789e-02],
          [ 3.1775e-01,  3.3494e-01,  4.2363e-01,  ..., -1.4968e-01,  2.8380e-01, -4.8481e-02]],

         [[-4.7812e-01, -2.3190e-01, -1.7716e-01,  ...,  3.3368e-01, -3.4423e-01,  4.2011e-01],
          [-2.4370e-01, -1.2644e-01, -4.3534e-01,  ..., -6.2034e-02, -1.7998e-01,  1.9617e-01],
          [-4.0408e-01, -1.3784e-01, -5.9273e-03,  ...,  3.0777e-01, -3.1258e-01,  3.8257e-01],
          ...,
          [-2.5856e-01, -4.7227e-01, -3.8326e-02,  ...,  2.0767e-03,  4.3309e-01, -3.8373e-01],
          [ 1.3468e-01,  4.2872e-01,  2.8454e-01,  ..., -3.8158e-01, -4.3062e-01,  1.8044e-01],
          [-1.6763e-01,  3.3740e-01, -4.0209e-01,  ...,  2.0687e-01,  2.5005e-01,  1.3279e-01]],

         ...,

         [[-3.9700e-01, -6.5451e-03,  2.0159e-01,  ..., -3.6418e-01, -1.7109e-02, -2.2291e-02],
          [-2.8283e-01, -4.7605e-01, -2.9099e-01,  ...,  1.3876e-01,  4.5376e-01,  3.1612e-01],
          [-4.4273e-01, -4.6422e-03, -1.1254e-01,  ..., -9.7826e-02, -2.3271e-01, -2.6621e-02],
          ...,
          [ 1.0944e-01, -3.9633e-01, -2.4787e-01,  ...,  1.7136e-01, -2.9851e-01, -1.5658e-01],
          [-4.1576e-01,  1.9919e-01, -5.1918e-01,  ..., -2.3655e-02, -2.6903e-01,  8.5751e-02],
          [ 1.9570e-01,  3.5984e-01,  4.5142e-01,  ...,  8.4654e-02, -1.4174e-01, -1.8003e-01]],

         [[ 2.7691e-01,  4.4212e-01,  2.9810e-01,  ..., -1.4798e-01,  2.9169e-02, -4.1115e-01],
          [ 4.1063e-01, -1.7595e-01, -3.1710e-01,  ..., -3.1990e-01, -8.7294e-02,  1.9895e-01],
          [ 1.5354e-01, -1.5133e-01, -9.4143e-02,  ...,  1.0299e-01,  2.9144e-01,  4.7131e-01],
          ...,
          [-2.2205e-01,  1.7790e-01,  3.8110e-01,  ..., -4.0558e-01, -1.5679e-01,  2.7150e-01],
          [ 9.9409e-02, -3.8907e-01,  1.9057e-01,  ..., -1.0628e-01, -7.9510e-02,  2.4473e-01],
          [ 4.8074e-02, -1.8551e-01, -1.6400e-01,  ..., -4.7125e-01,  3.7955e-01,  3.8530e-01]],

         [[ 1.7505e-01, -1.1441e-01,  3.0770e-01,  ..., -2.8098e-02,  1.6746e-01,  2.0020e-01],
          [-4.2421e-01, -3.1792e-01, -3.0136e-01,  ..., -4.6369e-02,  8.5891e-02,  6.4366e-02],
          [-4.7312e-02, -3.5038e-01,  2.0910e-01,  ..., -3.0006e-01, -2.3910e-01, -3.8099e-01],
          ...,
          [-8.4941e-03, -9.1520e-02,  3.3987e-01,  ...,  3.5787e-01,  1.4283e-01, -2.6035e-01],
          [ 1.5737e-01, -1.4551e-01, -6.2460e-02,  ..., -4.0619e-01,  1.7008e-01, -1.0537e-01],
          [-1.3031e-01,  3.1511e-02,  3.6233e-01,  ...,  2.4413e-01, -2.5323e-01,  3.6630e-01]]],


        [[[ 4.7033e-01, -2.0419e-01, -5.1028e-01,  ...,  3.5870e-01,  2.7608e-01,  2.5425e-01],
          [ 4.1666e-01,  8.9549e-02,  4.3186e-01,  ..., -4.7423e-01,  6.2152e-02, -3.3976e-01],
          [-3.7297e-01, -4.2125e-01,  2.0147e-01,  ...,  1.0043e-02, -4.7553e-02, -4.5597e-01],
          ...,
          [-3.4574e-01,  5.0936e-02, -4.1304e-02,  ..., -3.4244e-01, -2.9722e-01,  4.3445e-01],
          [-2.7120e-01, -2.9633e-01, -3.9754e-02,  ...,  1.4944e-01, -2.0560e-01, -2.9652e-01],
          [ 3.8967e-01,  1.3308e-01, -3.4595e-01,  ...,  1.3237e-02, -2.9739e-01,  4.4903e-01]],

         [[-4.6173e-01,  1.1356e-02, -3.5734e-01,  ..., -9.6964e-04, -1.9380e-01, -4.6233e-01],
          [-4.2525e-01,  3.3571e-01,  5.0696e-01,  ...,  9.4355e-02,  1.0076e-01, -1.5365e-01],
          [ 3.6709e-01, -4.5221e-01, -3.0744e-03,  ..., -3.9909e-01, -3.2642e-01,  1.7110e-01],
          ...,
          [ 2.5039e-01,  3.3118e-01, -2.8729e-01,  ...,  1.3296e-01,  3.3396e-01, -2.8888e-01],
          [-1.3783e-01, -4.3041e-01,  3.1125e-01,  ...,  6.5877e-02,  3.1229e-01, -5.0348e-01],
          [ 7.6357e-02,  1.1356e-01,  5.4396e-02,  ..., -1.3321e-01,  3.9278e-01, -1.1992e-01]],

         [[ 4.6869e-01, -5.1732e-01, -1.3671e-01,  ..., -4.6330e-01, -4.7540e-01, -1.5562e-01],
          [ 3.6121e-01, -1.9164e-01,  4.3949e-01,  ..., -4.5795e-01, -4.3522e-01, -4.8819e-01],
          [ 8.0360e-02,  8.1563e-02, -2.9520e-02,  ..., -3.8769e-01, -3.5136e-03, -2.6376e-01],
          ...,
          [ 4.4654e-01, -5.0963e-02,  8.0473e-02,  ..., -2.9520e-01, -2.0525e-01,  3.6538e-02],
          [-1.5812e-01,  2.1553e-01,  1.5748e-01,  ..., -3.4503e-01, -1.9660e-01,  2.8828e-02],
          [ 4.2672e-01,  1.8230e-01,  1.9478e-01,  ..., -2.6482e-01,  3.5190e-02,  1.6985e-01]],

         ...,

         [[-1.3643e-01, -4.4575e-01, -3.6310e-01,  ..., -2.8042e-01, -2.5755e-01, -2.6687e-01],
          [ 1.1758e-03, -3.0151e-01,  8.9685e-02,  ...,  2.8374e-01, -3.5942e-02, -1.6416e-01],
          [ 3.2374e-01,  3.7195e-01, -3.5393e-01,  ...,  9.6669e-02, -1.8376e-01, -1.6208e-01],
          ...,
          [-7.1008e-02, -7.9295e-02,  2.9355e-01,  ...,  2.8839e-01, -4.6155e-01, -3.5486e-01],
          [-2.7168e-01,  2.0751e-01,  1.7380e-01,  ..., -4.0825e-01,  4.2322e-01, -2.1424e-01],
          [-1.9983e-01,  4.3320e-01, -2.6511e-01,  ..., -3.1378e-01,  3.2266e-01, -2.2123e-01]],

         [[-4.9403e-01,  2.9097e-01, -9.5528e-02,  ..., -4.0653e-01,  3.5424e-01,  1.6509e-01],
          [ 4.2368e-01,  3.5616e-01,  1.7133e-01,  ...,  5.4267e-01,  4.3136e-03,  4.3543e-01],
          [-6.1075e-02, -6.9913e-02, -3.5366e-01,  ...,  3.2997e-01,  1.7460e-01, -5.2452e-01],
          ...,
          [-3.6703e-02, -5.3016e-02, -9.4189e-02,  ...,  3.1223e-01,  9.1352e-02,  4.0156e-01],
          [ 1.2823e-01, -4.8203e-01,  1.6913e-01,  ...,  4.4593e-01, -2.8728e-02, -2.0294e-01],
          [-4.1970e-02, -3.8427e-01,  5.2471e-02,  ...,  4.0777e-01, -2.0053e-01, -2.7026e-01]],

         [[ 2.8651e-02, -1.8978e-01,  6.8724e-02,  ..., -2.9326e-02, -8.2959e-02,  1.5685e-01],
          [ 1.2106e-02, -3.2460e-01,  2.1645e-01,  ...,  5.2872e-02, -4.5228e-01, -6.3284e-02],
          [-9.0828e-02,  1.5362e-01,  3.9842e-01,  ...,  7.3710e-02, -2.5879e-01,  2.2624e-01],
          ...,
          [ 4.4409e-01,  8.5070e-02,  2.0892e-01,  ..., -1.6851e-01, -4.0702e-01, -4.8374e-01],
          [ 4.6078e-01, -2.3955e-01, -3.4502e-02,  ..., -1.2662e-01,  4.7773e-01,  1.6251e-01],
          [ 9.7584e-02,  2.2835e-01,  2.9687e-01,  ...,  4.9855e-01, -3.0642e-01,  3.0744e-02]]],


        [[[ 4.5997e-01, -1.8296e-01, -3.5899e-02,  ...,  2.3443e-02,  2.7795e-01,  3.4484e-01],
          [ 2.5937e-01,  3.8354e-01, -6.9317e-02,  ..., -1.3762e-01, -1.6361e-01, -1.8052e-01],
          [-3.6477e-01, -4.4314e-01, -4.5287e-01,  ...,  1.8445e-01,  1.9827e-01,  5.0244e-01],
          ...,
          [ 1.0298e-02,  4.0350e-01, -3.2339e-01,  ...,  3.7596e-01, -2.9654e-01,  4.1487e-01],
          [ 2.4995e-03,  4.4543e-02, -5.0709e-01,  ...,  1.5869e-01, -4.2028e-01,  1.6085e-01],
          [ 7.5842e-02, -3.6193e-02,  2.9088e-01,  ...,  3.9211e-01,  1.7674e-01,  2.0091e-01]],

         [[-2.9446e-01, -1.3939e-01, -1.0782e-01,  ..., -3.0988e-01, -2.0780e-01,  3.6298e-01],
          [ 2.6260e-01,  3.4831e-01, -2.5810e-01,  ..., -2.4437e-01, -2.3163e-02,  3.7618e-01],
          [-1.7357e-01, -3.3811e-02, -6.7052e-02,  ...,  5.0560e-02, -3.0929e-01, -4.2966e-01],
          ...,
          [-2.7071e-01, -4.3554e-01, -1.6060e-01,  ...,  4.2648e-01, -1.0794e-01,  4.2954e-01],
          [-4.8005e-01,  3.2201e-01,  4.6988e-01,  ..., -4.7158e-01,  4.3055e-01,  4.6908e-01],
          [-1.8399e-01, -1.7012e-01, -4.2123e-01,  ...,  2.0678e-01,  4.9152e-01,  1.1488e-01]],

         [[-1.8677e-02,  4.6381e-01, -3.1789e-01,  ..., -4.8611e-01,  3.9939e-01, -3.0579e-01],
          [ 1.3610e-02, -2.5704e-01, -3.9583e-02,  ...,  3.8230e-01,  1.6433e-01,  1.6104e-01],
          [ 4.1537e-01,  3.3245e-01,  4.3108e-01,  ...,  8.6675e-02,  2.5835e-01, -4.4319e-01],
          ...,
          [ 4.6254e-01, -2.7318e-01, -3.6189e-01,  ...,  8.4444e-03, -8.6469e-02, -1.8212e-01],
          [-1.8638e-01, -3.5208e-01, -2.7734e-01,  ..., -3.1213e-01, -4.5779e-01,  4.8923e-01],
          [-4.7043e-01, -7.3661e-02,  2.3197e-01,  ..., -5.1130e-01,  4.7287e-03, -8.7554e-02]],

         ...,

         [[-2.6202e-01,  3.1209e-01,  4.3368e-01,  ...,  2.0268e-01,  1.6140e-03,  4.8961e-02],
          [ 3.3855e-01, -1.8399e-01,  4.0870e-01,  ..., -1.9928e-01,  4.5097e-01, -3.9221e-01],
          [ 1.3338e-01, -7.3847e-02, -3.0512e-01,  ...,  1.6087e-01, -1.2017e-01,  1.7538e-01],
          ...,
          [ 2.2615e-01, -1.8039e-01,  2.0667e-01,  ...,  2.1289e-01, -3.0142e-01,  1.4125e-01],
          [ 2.9994e-01, -3.7448e-01,  3.4693e-01,  ..., -1.2427e-02,  3.4933e-01,  2.2818e-01],
          [ 4.0900e-01,  2.1417e-01,  4.1412e-01,  ...,  2.9736e-01, -2.4676e-01,  2.7822e-01]],

         [[-3.0251e-01, -6.6047e-02, -1.4680e-01,  ..., -3.1461e-01,  1.3543e-01, -3.8148e-02],
          [ 4.2463e-01, -2.8507e-02, -5.0691e-01,  ..., -4.6153e-01,  3.0324e-01, -4.1189e-01],
          [ 2.0302e-01,  1.0004e-01, -2.3642e-01,  ..., -8.5033e-02,  3.0586e-01,  4.5542e-01],
          ...,
          [-3.7414e-01, -4.3398e-01, -3.8004e-01,  ..., -2.8903e-01, -2.4703e-01, -4.1656e-02],
          [ 4.1104e-01, -3.5947e-01,  1.0190e-01,  ...,  2.1975e-01, -1.2494e-01,  3.1116e-01],
          [-4.4359e-01,  1.7563e-01,  7.2520e-02,  ..., -4.0415e-01, -2.8717e-01,  1.9653e-01]],

         [[-1.0890e-01,  7.1368e-02, -4.6793e-02,  ...,  2.9072e-01,  4.0938e-01,  2.8968e-01],
          [ 2.8429e-01,  1.5022e-01, -2.2107e-02,  ..., -2.6521e-01, -2.8562e-02, -9.1099e-02],
          [-2.2976e-01,  5.1450e-02, -5.1418e-02,  ..., -1.5240e-01, -4.6081e-01, -4.0372e-01],
          ...,
          [-8.3779e-02,  2.6025e-01,  4.0595e-01,  ..., -5.2394e-01,  4.8306e-01, -1.2620e-01],
          [-3.6178e-01, -1.8655e-01,  3.5904e-01,  ...,  1.0208e-01, -1.0848e-01,  1.1625e-01],
          [-2.5922e-01, -4.0725e-01,  4.3463e-01,  ...,  3.3606e-01, -4.4158e-01,  1.4874e-01]]]])
DESIRED: (shape=torch.Size([16, 356516, 19, 19]), dtype=torch.float32)
tensor([[[[-2.5447e-01,  3.8529e-01,  1.8638e-01,  ..., -2.8284e-01, -2.8703e-01,  2.6115e-02],
          [ 4.1620e-01,  8.7960e-02, -2.0811e-02,  ...,  2.1054e-01, -3.3367e-01, -3.8022e-01],
          [ 2.7156e-01,  2.0456e-01,  2.5318e-01,  ..., -4.0447e-01,  3.1655e-01, -1.2935e-01],
          ...,
          [-4.7793e-01,  3.2863e-01,  2.3662e-01,  ..., -2.4636e-01,  2.7821e-01, -3.8672e-01],
          [ 3.9122e-01,  6.8415e-02, -5.5443e-02,  ...,  3.1016e-01,  2.0286e-01,  1.6068e-01],
          [ 3.7863e-01,  4.4207e-01,  3.6721e-01,  ..., -6.6279e-02, -4.8937e-01,  4.0171e-02]],

         [[ 2.1441e-01,  4.0544e-01, -1.1386e-02,  ...,  2.9561e-01,  1.0146e-01,  1.6855e-01],
          [-2.7587e-01,  2.6049e-01, -3.5492e-01,  ...,  3.2229e-01, -3.3344e-01, -3.1757e-01],
          [ 1.2677e-01, -3.0173e-01, -1.3720e-01,  ..., -2.8813e-01,  8.2880e-02, -3.0141e-01],
          ...,
          [-4.3563e-01, -7.6969e-02,  1.2547e-01,  ..., -4.2692e-01,  2.4553e-01,  2.3383e-01],
          [-4.1149e-01,  2.1681e-01, -4.6210e-02,  ...,  2.8978e-01, -3.5086e-01,  1.5695e-01],
          [-4.6295e-01,  7.1851e-03, -2.7859e-01,  ...,  3.4943e-01,  1.1273e-01,  2.4142e-02]],

         [[-9.6737e-02,  2.7728e-01, -1.6571e-01,  ...,  1.5124e-01,  2.4810e-01, -3.7493e-01],
          [ 4.3371e-01, -5.1293e-01, -3.7219e-01,  ..., -2.5123e-01, -2.7281e-01, -2.5119e-01],
          [ 3.2744e-01, -3.2105e-01,  4.6366e-01,  ...,  4.2919e-01,  1.3872e-01, -4.2725e-01],
          ...,
          [-2.2338e-01,  2.5438e-01,  2.2230e-01,  ..., -1.1793e-01,  1.7731e-01, -2.0167e-01],
          [ 5.5963e-02, -2.9174e-01, -5.1347e-02,  ...,  7.7776e-02, -2.2624e-01, -1.0168e-01],
          [-3.8051e-01, -3.4522e-01, -1.3330e-01,  ..., -3.4579e-01, -2.0892e-01, -3.2526e-02]],

         ...,

         [[-3.0804e-01,  2.3532e-01,  4.2516e-02,  ...,  2.1988e-01,  4.1406e-01,  4.9369e-01],
          [ 7.3157e-02,  9.4590e-02,  1.4469e-01,  ...,  1.6142e-02, -4.4195e-01,  2.0941e-01],
          [ 2.6823e-01, -1.3062e-01, -3.5259e-01,  ..., -1.7683e-01,  5.7570e-02,  8.3408e-03],
          ...,
          [-3.3947e-01, -1.6885e-02,  1.2045e-01,  ...,  1.0288e-02, -2.9087e-01, -4.9272e-01],
          [ 4.4281e-01, -4.5147e-01, -2.6370e-01,  ..., -5.9912e-04,  3.0393e-01, -8.0110e-02],
          [ 1.9448e-01,  4.0128e-01, -4.3197e-02,  ...,  2.5252e-01,  4.9822e-02, -3.5179e-01]],

         [[ 2.0457e-01,  1.2528e-01, -1.3886e-01,  ..., -4.3477e-01, -6.7603e-02,  4.3564e-01],
          [-1.6215e-01, -1.1068e-01, -1.9562e-01,  ..., -3.2040e-01,  1.2158e-01, -3.8220e-01],
          [-4.0811e-01,  4.5927e-01,  2.7186e-01,  ...,  1.7113e-01, -4.8816e-01,  1.3480e-01],
          ...,
          [ 1.6118e-01, -3.5285e-01,  2.8162e-01,  ..., -3.5471e-01,  3.2320e-01, -2.4411e-01],
          [-2.9392e-01, -7.4494e-02,  3.8720e-01,  ..., -1.5329e-02, -3.8253e-01, -3.2933e-03],
          [-2.7970e-01,  2.7119e-01,  1.6294e-01,  ...,  1.3302e-01, -5.8732e-03, -6.8036e-02]],

         [[-2.3489e-01,  1.7662e-01, -4.4967e-01,  ...,  3.6496e-01, -4.4520e-01,  3.7351e-01],
          [ 2.7593e-01, -1.8538e-01, -1.8593e-01,  ..., -5.7873e-02,  4.2232e-01,  1.9611e-01],
          [ 3.1903e-01, -2.3044e-01, -2.3089e-01,  ..., -1.7261e-01,  2.4232e-01, -1.2704e-03],
          ...,
          [ 1.8461e-01, -3.6831e-02,  1.2322e-01,  ...,  2.6364e-01,  2.2917e-01,  1.8904e-01],
          [-2.1367e-02, -1.5262e-01, -3.9903e-01,  ..., -2.8331e-03, -2.9964e-01,  2.7926e-01],
          [-5.8042e-02, -3.9372e-01, -1.6656e-01,  ..., -9.2173e-02, -2.6493e-01,  4.7761e-01]]],


        [[[-2.7333e-01,  2.3167e-01, -3.6095e-01,  ..., -7.7295e-03,  2.6782e-01,  1.0128e-02],
          [ 2.6733e-01, -6.3285e-04, -1.5469e-01,  ..., -1.8905e-01, -2.1510e-01,  3.1248e-01],
          [ 6.2044e-02,  2.1876e-01, -3.4244e-01,  ..., -2.1191e-01, -1.9698e-01,  3.8104e-01],
          ...,
          [ 1.1590e-01,  8.9645e-02, -3.5612e-01,  ...,  3.2775e-01,  4.9382e-01, -3.8695e-03],
          [-2.5580e-02,  4.9367e-01,  2.7585e-01,  ..., -3.6156e-01,  3.2301e-01, -9.0637e-02],
          [-6.2547e-02, -3.9077e-01, -1.8788e-01,  ..., -7.9239e-02,  1.8415e-01,  3.7699e-01]],

         [[ 1.6713e-01,  9.6762e-03,  1.3756e-01,  ...,  2.4086e-01,  4.4830e-01, -1.2802e-01],
          [ 3.1974e-01, -5.0491e-01, -2.2217e-02,  ..., -2.8901e-01,  4.1944e-01,  2.6849e-02],
          [-1.4082e-01,  5.3186e-02,  2.7661e-01,  ...,  2.8141e-01,  1.1510e-01, -3.2483e-01],
          ...,
          [ 3.5152e-01, -2.7751e-01, -4.1902e-01,  ...,  9.3094e-02, -2.4083e-01,  1.8702e-01],
          [ 9.1163e-03,  4.9104e-01,  1.9561e-01,  ...,  2.2505e-01, -2.8289e-01,  2.4096e-01],
          [-1.0028e-01, -3.2251e-01, -1.2336e-01,  ...,  2.6607e-03,  3.6361e-01, -1.5394e-01]],

         [[ 3.7431e-02,  1.7566e-01,  3.5199e-01,  ..., -1.0177e-01, -3.3072e-01, -2.2394e-01],
          [-4.4169e-01, -9.2682e-02,  1.5532e-01,  ...,  1.2489e-01, -1.3362e-01, -2.8312e-01],
          [-8.0421e-02,  2.7029e-01,  1.8718e-01,  ...,  3.5412e-01,  2.8879e-01, -1.0041e-01],
          ...,
          [-4.0353e-01,  1.6615e-02, -1.2808e-01,  ..., -1.6999e-01, -3.2673e-01,  2.6741e-01],
          [ 3.8999e-01, -2.7665e-01,  2.3129e-02,  ...,  2.1263e-01, -4.6393e-01,  2.7763e-01],
          [-6.9915e-02, -2.5466e-01,  3.6329e-01,  ..., -2.3094e-02, -2.0928e-01, -1.8743e-01]],

         ...,

         [[ 3.4365e-01, -5.3267e-01, -3.4979e-01,  ...,  8.5939e-02, -3.8767e-01,  4.0632e-01],
          [ 1.4694e-01, -3.0275e-01, -3.3499e-01,  ..., -9.5005e-03, -2.4859e-01,  3.3170e-01],
          [-2.4963e-01,  5.4408e-02,  2.0065e-02,  ..., -4.1678e-01, -2.0166e-01,  3.2294e-01],
          ...,
          [ 1.9020e-01,  4.2079e-01,  2.4185e-01,  ..., -1.5848e-02,  1.3801e-01,  1.0477e-01],
          [ 8.7940e-02,  4.9391e-01,  5.2496e-02,  ...,  3.1000e-01,  4.6682e-01,  3.2461e-02],
          [-1.4467e-02, -8.3318e-02, -2.1175e-01,  ..., -4.5189e-01, -2.9727e-02, -1.6409e-01]],

         [[ 2.0774e-01, -3.4325e-01,  5.2128e-01,  ...,  3.7630e-01,  4.9875e-01,  3.2797e-01],
          [-3.3816e-01, -1.7158e-01,  3.8092e-01,  ...,  3.1746e-01, -7.1101e-02, -2.3331e-01],
          [ 4.3610e-01, -8.0804e-02, -1.1207e-01,  ..., -2.5294e-03, -4.2181e-01, -2.6758e-01],
          ...,
          [-1.5864e-01,  6.0785e-02,  2.7704e-01,  ..., -9.5613e-02,  2.0828e-02,  2.1462e-01],
          [ 3.2198e-01,  1.0679e-01, -1.3744e-01,  ..., -2.8762e-01, -2.1382e-01, -2.0567e-01],
          [-1.5177e-01,  9.5354e-02, -1.9443e-02,  ...,  6.3449e-02, -1.3902e-01,  1.7779e-01]],

         [[ 2.4477e-01, -2.6135e-01,  3.4797e-01,  ..., -4.9603e-01,  1.8977e-01,  3.8660e-01],
          [ 2.0194e-01, -2.3958e-02,  5.1897e-01,  ..., -2.8766e-01,  8.5538e-02,  3.9790e-01],
          [-4.1924e-01,  4.4412e-01,  1.4674e-01,  ..., -2.3231e-01, -9.5200e-02, -5.1099e-01],
          ...,
          [ 1.2455e-01,  4.0869e-01,  7.2152e-03,  ..., -1.4175e-01, -1.7942e-01, -2.2530e-01],
          [-2.3034e-01,  5.7645e-02, -2.6935e-01,  ...,  2.7297e-01, -4.5080e-01, -3.6859e-02],
          [-9.7264e-02, -1.0601e-02,  1.7335e-01,  ..., -4.2652e-01,  2.4031e-01,  5.4955e-02]]],


        [[[ 1.9278e-01,  5.7410e-02, -2.7505e-01,  ..., -1.2073e-01,  2.4614e-01,  3.9101e-02],
          [ 7.4248e-02,  1.8063e-01, -1.0584e-01,  ...,  3.0040e-01,  4.0640e-01, -2.4633e-01],
          [ 3.0713e-01, -4.2581e-01, -2.9512e-01,  ...,  1.0511e-01,  7.1661e-02,  1.9794e-02],
          ...,
          [ 4.2112e-01, -1.2300e-04, -4.2659e-01,  ..., -3.3693e-01,  3.9603e-01, -3.8781e-01],
          [ 7.6187e-02,  1.2328e-01,  2.1563e-01,  ..., -1.8523e-02, -3.6167e-02, -4.9245e-02],
          [ 3.9566e-01, -5.1506e-02,  2.4653e-01,  ...,  5.9567e-02, -3.3021e-01, -4.5078e-01]],

         [[ 4.3620e-01, -3.5034e-01,  4.4345e-01,  ..., -2.4989e-01,  3.2447e-01,  1.0252e-01],
          [-2.7658e-01,  6.1926e-02,  2.1448e-01,  ..., -1.6810e-01, -4.7705e-01, -6.3119e-02],
          [-2.2264e-01,  3.4722e-01,  2.0948e-01,  ..., -2.4685e-01,  1.5023e-01,  1.9535e-01],
          ...,
          [-7.9525e-02,  3.5484e-01, -4.9894e-01,  ..., -6.7643e-02,  2.2141e-01, -3.1234e-01],
          [-1.6800e-01,  4.7625e-01,  2.5993e-01,  ..., -1.9449e-02, -4.2315e-01, -1.8694e-02],
          [-4.1049e-02, -2.7557e-01,  1.2703e-01,  ..., -4.1094e-01,  1.3519e-01,  7.8797e-02]],

         [[ 3.0613e-01,  1.0994e-02, -3.3957e-01,  ..., -1.1480e-01, -2.4549e-01,  8.8573e-02],
          [-9.6580e-02, -4.6535e-01, -1.3346e-01,  ...,  7.7845e-02,  1.6263e-01,  2.9323e-01],
          [-1.5512e-01,  3.4799e-01, -4.7066e-01,  ..., -3.5661e-01, -2.0510e-01, -4.0259e-03],
          ...,
          [-4.6084e-01,  2.7067e-01, -2.4990e-01,  ..., -2.9816e-01, -3.1137e-01, -3.9594e-01],
          [ 1.8568e-01, -2.1465e-01, -1.1014e-01,  ..., -4.9613e-01,  8.5569e-02, -3.9655e-01],
          [-4.1857e-01, -1.7055e-01, -5.0063e-01,  ...,  4.9355e-01, -4.1560e-01, -2.4270e-01]],

         ...,

         [[-2.8683e-01,  2.6879e-01, -3.8127e-02,  ..., -3.7230e-01, -4.4595e-01,  1.0135e-01],
          [ 1.4039e-01, -1.9111e-01, -7.4298e-02,  ...,  4.6574e-01,  2.9119e-01,  2.2492e-01],
          [-1.5701e-01, -1.4506e-01,  1.7849e-01,  ...,  1.7744e-01, -1.2340e-01, -4.7853e-01],
          ...,
          [ 1.5000e-01, -1.1564e-01,  3.0542e-01,  ..., -8.0184e-02,  3.8439e-01, -4.1723e-01],
          [-2.3865e-01,  1.6058e-01,  4.7013e-02,  ...,  2.7395e-01, -3.5064e-01, -3.5822e-01],
          [ 2.1449e-01, -1.9738e-01, -2.8385e-01,  ..., -1.2540e-01, -2.7089e-01,  5.4388e-04]],

         [[-1.2029e-01,  1.7471e-01,  5.7593e-03,  ...,  1.6155e-01, -1.9947e-01, -1.4785e-01],
          [-2.7357e-01, -3.3423e-01, -3.1107e-01,  ...,  1.3734e-01,  9.5165e-02, -1.2634e-01],
          [ 4.2989e-01,  4.3869e-01, -1.2342e-01,  ...,  2.9579e-01, -1.5329e-01, -5.0000e-01],
          ...,
          [ 1.7642e-01,  3.3260e-02,  1.4201e-01,  ...,  2.7489e-01,  3.5190e-01,  5.4636e-01],
          [ 7.0717e-02,  2.3117e-01,  2.4329e-01,  ..., -3.2510e-01,  7.8147e-02, -2.3877e-01],
          [ 2.7419e-01, -4.9356e-01, -2.4342e-01,  ...,  2.0574e-01,  2.4460e-01, -3.3829e-01]],

         [[-1.5987e-01,  4.8338e-01,  2.3979e-01,  ...,  1.2528e-02, -1.3381e-01, -2.3088e-01],
          [-4.5349e-01, -3.1535e-01,  4.9254e-01,  ...,  1.9133e-01,  2.4375e-01, -1.1518e-01],
          [-9.4828e-02, -3.2287e-01,  2.8036e-02,  ..., -1.9852e-01,  5.9330e-02, -2.7247e-01],
          ...,
          [-3.1655e-01,  9.8291e-02, -2.3883e-01,  ...,  5.0275e-01, -4.9761e-01,  4.3571e-01],
          [-2.8317e-01, -3.6653e-01,  3.8543e-01,  ...,  4.4374e-01, -7.0701e-02, -4.2619e-01],
          [-1.7310e-01, -5.9239e-03, -1.9830e-01,  ..., -1.7036e-01, -1.3088e-01,  7.6868e-02]]],


        ...,


        [[[ 1.8136e-02,  1.3176e-01, -2.2529e-01,  ..., -2.2894e-01,  2.8909e-01, -2.4062e-02],
          [ 1.6874e-01, -2.3374e-01, -2.6974e-01,  ..., -4.3371e-01,  1.6381e-01, -8.7500e-02],
          [ 1.3947e-01, -4.4221e-01, -3.0511e-02,  ...,  3.9066e-02,  3.9098e-01, -3.2480e-02],
          ...,
          [-3.4662e-01,  2.6738e-01, -1.5987e-01,  ...,  4.3665e-01, -3.1386e-01,  1.0869e-01],
          [-1.9175e-01,  2.3982e-01,  1.9287e-01,  ..., -4.6573e-01,  1.2930e-01, -4.0688e-01],
          [-3.3452e-02, -2.5948e-01, -6.3440e-03,  ...,  3.8373e-01, -4.6629e-01,  2.4053e-01]],

         [[-5.4000e-02,  4.2785e-01,  3.4401e-01,  ..., -3.9246e-01, -3.5717e-01,  1.1090e-01],
          [ 2.7420e-01,  1.2375e-01,  2.2758e-01,  ...,  3.0188e-01,  4.3561e-01,  2.4087e-02],
          [ 4.3715e-01,  1.3500e-01,  1.1774e-01,  ...,  2.6720e-01, -1.1889e-01,  4.8611e-02],
          ...,
          [-3.6336e-01, -9.9357e-02, -1.1550e-02,  ...,  1.4200e-01,  1.0289e-02, -6.4708e-02],
          [ 3.8904e-01, -1.3631e-01,  1.4418e-01,  ..., -4.2832e-01,  3.8904e-01,  8.8717e-02],
          [ 3.2055e-01,  3.2838e-01,  4.1647e-01,  ..., -1.4996e-01,  2.8085e-01, -3.8607e-02]],

         [[-4.6854e-01, -2.2469e-01, -1.8755e-01,  ...,  3.2489e-01, -3.3710e-01,  3.9983e-01],
          [-2.4457e-01, -1.3207e-01, -4.2391e-01,  ..., -5.6644e-02, -1.8314e-01,  1.8916e-01],
          [-3.8155e-01, -1.2859e-01,  2.5877e-03,  ...,  2.8628e-01, -3.0686e-01,  3.7074e-01],
          ...,
          [-2.5470e-01, -4.7317e-01, -2.1150e-02,  ..., -7.3246e-03,  4.3643e-01, -3.5576e-01],
          [ 1.3456e-01,  4.1488e-01,  2.7764e-01,  ..., -3.7283e-01, -4.3067e-01,  1.5565e-01],
          [-1.5027e-01,  3.3089e-01, -3.8131e-01,  ...,  1.9987e-01,  2.3514e-01,  1.3669e-01]],

         ...,

         [[-3.8742e-01, -8.3366e-03,  1.9988e-01,  ..., -3.4633e-01, -1.5539e-02, -1.4467e-02],
          [-2.8837e-01, -4.7506e-01, -2.8540e-01,  ...,  1.2460e-01,  4.4519e-01,  3.1570e-01],
          [-4.3424e-01,  9.0657e-03, -1.2068e-01,  ..., -9.0971e-02, -2.3657e-01, -2.6558e-02],
          ...,
          [ 1.0566e-01, -3.9425e-01, -2.3072e-01,  ...,  1.6299e-01, -2.7723e-01, -1.6007e-01],
          [-4.0168e-01,  1.7707e-01, -5.3063e-01,  ..., -1.7619e-02, -2.7430e-01,  7.7000e-02],
          [ 1.8639e-01,  3.6152e-01,  4.3235e-01,  ...,  8.8918e-02, -1.4029e-01, -1.7418e-01]],

         [[ 2.8286e-01,  4.2926e-01,  2.8652e-01,  ..., -1.4262e-01,  2.6402e-02, -3.9102e-01],
          [ 3.9893e-01, -1.9722e-01, -3.1544e-01,  ..., -3.0489e-01, -9.0100e-02,  2.0740e-01],
          [ 1.3146e-01, -1.4942e-01, -8.0960e-02,  ...,  9.9722e-02,  2.7998e-01,  4.6937e-01],
          ...,
          [-2.1679e-01,  1.8044e-01,  3.7821e-01,  ..., -3.9206e-01, -1.7692e-01,  2.5431e-01],
          [ 8.3970e-02, -3.7396e-01,  2.0470e-01,  ..., -1.0362e-01, -9.1451e-02,  2.3882e-01],
          [ 4.4787e-02, -1.9280e-01, -1.5805e-01,  ..., -4.6976e-01,  3.5441e-01,  3.8334e-01]],

         [[ 1.5681e-01, -1.1298e-01,  2.9222e-01,  ..., -2.3036e-02,  1.6096e-01,  1.9790e-01],
          [-4.2339e-01, -3.2031e-01, -2.9038e-01,  ..., -4.3641e-02,  7.4674e-02,  5.3654e-02],
          [-4.8900e-02, -3.4934e-01,  2.0999e-01,  ..., -2.8402e-01, -2.4281e-01, -3.7594e-01],
          ...,
          [-1.0093e-02, -7.8863e-02,  3.3709e-01,  ...,  3.4876e-01,  1.4807e-01, -2.4795e-01],
          [ 1.5188e-01, -1.5138e-01, -5.2945e-02,  ..., -3.9876e-01,  1.6820e-01, -1.0949e-01],
          [-1.2193e-01,  3.5987e-02,  3.5750e-01,  ...,  2.3216e-01, -2.4352e-01,  3.4500e-01]]],


        [[[ 4.5771e-01, -2.1208e-01, -4.9462e-01,  ...,  3.3687e-01,  2.7511e-01,  2.4332e-01],
          [ 3.9437e-01,  8.7658e-02,  4.3071e-01,  ..., -4.6784e-01,  4.7944e-02, -3.4239e-01],
          [-3.8285e-01, -4.0986e-01,  2.0977e-01,  ...,  2.7436e-02, -3.4256e-02, -4.4226e-01],
          ...,
          [-3.3476e-01,  5.0465e-02, -4.3768e-02,  ..., -3.3133e-01, -3.0005e-01,  4.1635e-01],
          [-2.8272e-01, -2.8936e-01, -3.4420e-02,  ...,  1.4560e-01, -2.0099e-01, -2.8957e-01],
          [ 3.7271e-01,  1.1026e-01, -3.3466e-01,  ...,  1.7534e-02, -2.9905e-01,  4.1883e-01]],

         [[-4.5223e-01,  1.8573e-02, -3.4432e-01,  ...,  8.2314e-03, -1.8129e-01, -4.5177e-01],
          [-3.9611e-01,  3.3535e-01,  5.1183e-01,  ...,  9.2754e-02,  9.5100e-02, -1.3755e-01],
          [ 3.5832e-01, -4.4696e-01, -7.2253e-04,  ..., -3.9260e-01, -3.2321e-01,  1.5817e-01],
          ...,
          [ 2.5299e-01,  3.1991e-01, -2.9556e-01,  ...,  1.3458e-01,  3.2903e-01, -2.5723e-01],
          [-1.3911e-01, -4.1336e-01,  3.1668e-01,  ...,  6.0769e-02,  3.2163e-01, -4.9175e-01],
          [ 7.2187e-02,  1.0357e-01,  4.9215e-02,  ..., -1.2722e-01,  3.8712e-01, -1.1668e-01]],

         [[ 4.4736e-01, -5.1855e-01, -1.1507e-01,  ..., -4.6248e-01, -4.7969e-01, -1.6842e-01],
          [ 3.4568e-01, -1.7559e-01,  4.3700e-01,  ..., -4.5263e-01, -4.2621e-01, -4.8865e-01],
          [ 6.5961e-02,  7.5342e-02, -1.6779e-02,  ..., -3.8829e-01, -1.0750e-02, -2.4854e-01],
          ...,
          [ 4.4539e-01, -5.2141e-02,  9.1586e-02,  ..., -2.7903e-01, -2.0680e-01,  3.3025e-02],
          [-1.4782e-01,  2.1497e-01,  1.4739e-01,  ..., -3.5102e-01, -2.0589e-01,  2.2865e-02],
          [ 4.0992e-01,  1.8305e-01,  1.8181e-01,  ..., -2.6361e-01,  2.2154e-02,  1.6489e-01]],

         ...,

         [[-1.4020e-01, -4.4904e-01, -3.3937e-01,  ..., -2.5477e-01, -2.5632e-01, -2.6447e-01],
          [ 2.5685e-03, -2.8029e-01,  9.0888e-02,  ...,  2.7367e-01, -2.4071e-02, -1.6123e-01],
          [ 3.2977e-01,  3.4715e-01, -3.6514e-01,  ...,  9.5297e-02, -1.8156e-01, -1.6230e-01],
          ...,
          [-7.4749e-02, -6.3249e-02,  2.8188e-01,  ...,  3.1033e-01, -4.5310e-01, -3.4351e-01],
          [-2.5889e-01,  2.0526e-01,  1.7305e-01,  ..., -4.0391e-01,  4.0009e-01, -2.0673e-01],
          [-1.8826e-01,  4.2286e-01, -2.5304e-01,  ..., -3.1451e-01,  3.1948e-01, -2.0920e-01]],

         [[-4.5989e-01,  2.9375e-01, -8.1911e-02,  ..., -3.9443e-01,  3.3575e-01,  1.7606e-01],
          [ 4.2418e-01,  3.4340e-01,  1.5882e-01,  ...,  5.5800e-01,  1.0141e-02,  4.1134e-01],
          [-5.7999e-02, -7.1212e-02, -3.4814e-01,  ...,  3.1962e-01,  1.7744e-01, -5.1624e-01],
          ...,
          [-4.8213e-02, -3.6746e-02, -9.8884e-02,  ...,  3.0042e-01,  9.4535e-02,  3.9971e-01],
          [ 1.1669e-01, -4.7397e-01,  1.7231e-01,  ...,  4.3727e-01, -1.0657e-02, -1.8787e-01],
          [-4.5964e-02, -3.8136e-01,  4.9942e-02,  ...,  3.9802e-01, -1.8190e-01, -2.6930e-01]],

         [[ 2.3824e-02, -1.9224e-01,  8.0850e-02,  ..., -1.9675e-02, -9.3867e-02,  1.4734e-01],
          [ 3.4614e-03, -3.0894e-01,  2.1226e-01,  ...,  5.2464e-02, -4.5083e-01, -6.6771e-02],
          [-9.1060e-02,  1.6664e-01,  3.9825e-01,  ...,  7.3305e-02, -2.4393e-01,  2.0653e-01],
          ...,
          [ 4.3886e-01,  7.5067e-02,  2.2081e-01,  ..., -1.5765e-01, -4.1307e-01, -4.8234e-01],
          [ 4.5116e-01, -2.4381e-01, -2.9557e-02,  ..., -1.3865e-01,  4.6448e-01,  1.5783e-01],
          [ 1.0672e-01,  2.2376e-01,  2.8112e-01,  ...,  4.8362e-01, -2.7790e-01,  2.6801e-02]]],


        [[[ 4.4449e-01, -1.7591e-01, -3.8077e-02,  ...,  2.0061e-02,  2.6303e-01,  3.3279e-01],
          [ 2.4736e-01,  3.7045e-01, -9.1927e-02,  ..., -1.2744e-01, -1.6267e-01, -1.7452e-01],
          [-3.6961e-01, -4.5242e-01, -4.4457e-01,  ...,  1.7784e-01,  1.9782e-01,  5.0357e-01],
          ...,
          [ 2.5027e-02,  3.9391e-01, -3.1448e-01,  ...,  3.7307e-01, -2.8247e-01,  3.8528e-01],
          [ 2.4933e-03,  4.0670e-02, -5.0849e-01,  ...,  1.5056e-01, -4.2138e-01,  1.5611e-01],
          [ 7.1790e-02, -2.8163e-02,  2.6530e-01,  ...,  3.8668e-01,  1.6934e-01,  1.9864e-01]],

         [[-2.8004e-01, -1.2663e-01, -1.1292e-01,  ..., -2.9738e-01, -2.1608e-01,  3.5354e-01],
          [ 2.6345e-01,  3.3869e-01, -2.6424e-01,  ..., -2.3181e-01, -3.7489e-02,  3.5341e-01],
          [-1.7092e-01, -3.9217e-02, -6.5128e-02,  ...,  6.0872e-02, -3.0150e-01, -4.3287e-01],
          ...,
          [-2.6422e-01, -4.3486e-01, -1.6424e-01,  ...,  4.3891e-01, -1.0778e-01,  4.1569e-01],
          [-4.6673e-01,  3.2805e-01,  4.6699e-01,  ..., -4.6303e-01,  4.0021e-01,  4.7393e-01],
          [-1.8892e-01, -1.6618e-01, -3.9267e-01,  ...,  1.7645e-01,  4.9319e-01,  1.2926e-01]],

         [[-8.5384e-03,  4.4061e-01, -3.1977e-01,  ..., -4.6914e-01,  3.8600e-01, -2.8124e-01],
          [ 1.7760e-02, -2.5319e-01, -2.3568e-02,  ...,  3.9309e-01,  1.6697e-01,  1.5592e-01],
          [ 4.0837e-01,  3.2984e-01,  4.2030e-01,  ...,  7.2094e-02,  2.6703e-01, -4.3687e-01],
          ...,
          [ 4.5108e-01, -2.8935e-01, -3.4811e-01,  ...,  1.0903e-02, -7.8761e-02, -1.9040e-01],
          [-1.7332e-01, -3.5442e-01, -2.9008e-01,  ..., -2.9835e-01, -4.6793e-01,  4.6649e-01],
          [-4.5863e-01, -6.9434e-02,  2.3038e-01,  ..., -5.0777e-01, -1.4092e-02, -7.3378e-02]],

         ...,

         [[-2.3957e-01,  3.1206e-01,  4.3639e-01,  ...,  1.8764e-01,  1.6024e-02,  3.8697e-02],
          [ 3.3359e-01, -1.8270e-01,  3.9486e-01,  ..., -2.0122e-01,  4.4497e-01, -3.6939e-01],
          [ 1.1767e-01, -8.7806e-02, -2.9273e-01,  ...,  1.5550e-01, -1.2269e-01,  1.7014e-01],
          ...,
          [ 2.0316e-01, -1.7777e-01,  1.9866e-01,  ...,  2.1019e-01, -2.9926e-01,  1.1847e-01],
          [ 2.8573e-01, -3.7319e-01,  3.4865e-01,  ..., -8.6064e-03,  3.3820e-01,  2.3132e-01],
          [ 4.0256e-01,  2.0445e-01,  4.1151e-01,  ...,  2.9876e-01, -2.3168e-01,  2.6661e-01]],

         [[-2.8369e-01, -6.3918e-02, -1.5756e-01,  ..., -3.2510e-01,  1.3282e-01, -4.2445e-02],
          [ 4.2357e-01, -3.9054e-02, -5.1072e-01,  ..., -4.6491e-01,  3.0391e-01, -3.8442e-01],
          [ 1.9170e-01,  8.2363e-02, -2.2729e-01,  ..., -8.2574e-02,  2.8007e-01,  4.5418e-01],
          ...,
          [-3.7560e-01, -4.2827e-01, -3.8113e-01,  ..., -2.7907e-01, -2.6209e-01, -4.3191e-02],
          [ 3.9071e-01, -3.6954e-01,  8.8481e-02,  ...,  2.1828e-01, -1.2270e-01,  2.9612e-01],
          [-4.1417e-01,  1.7045e-01,  7.9560e-02,  ..., -3.7920e-01, -2.9486e-01,  1.9013e-01]],

         [[-9.7075e-02,  7.4034e-02, -5.2165e-02,  ...,  2.7872e-01,  4.0082e-01,  2.8528e-01],
          [ 2.7777e-01,  1.4509e-01, -2.5528e-02,  ..., -2.7288e-01, -4.7436e-02, -1.0336e-01],
          [-2.2943e-01,  4.8867e-02, -4.0029e-02,  ..., -1.5110e-01, -4.5257e-01, -3.9440e-01],
          ...,
          [-8.3892e-02,  2.6387e-01,  4.0088e-01,  ..., -5.2265e-01,  4.7227e-01, -1.1520e-01],
          [-3.5363e-01, -1.5954e-01,  3.5579e-01,  ...,  7.8640e-02, -8.9778e-02,  1.0685e-01],
          [-2.6565e-01, -3.8728e-01,  4.2835e-01,  ...,  3.3820e-01, -4.2915e-01,  1.3603e-01]]]])

2025-07-09 13:40:45.579265 GPU 4 151556 test begin: paddle.nn.functional.interpolate(Tensor([16, 395032, 19, 19],"float32"), size=tuple(20,20,), mode="bicubic", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([16, 395032, 19, 19],"float32"), size=tuple(20,20,), mode="bicubic", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 256953262 / 2528204800 (10.2%)
Greatest absolute difference: 0.038516461849212646 at index (13, 351725, 1, 17) (up to 0.01 allowed)
Greatest relative difference: 14752474.0 at index (8, 114933, 2, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 395032, 20, 20]), dtype=torch.float32)
tensor([[[[ 0.3630, -0.2654,  0.1917,  ...,  0.1933,  0.0720, -0.1717],
          [-0.2171,  0.0316, -0.3258,  ..., -0.3002, -0.2221, -0.3134],
          [-0.4072,  0.3052,  0.0257,  ..., -0.1053, -0.3140,  0.1560],
          ...,
          [-0.1410, -0.1481,  0.1376,  ...,  0.0260,  0.3149, -0.1200],
          [ 0.2586,  0.3468,  0.0204,  ..., -0.1226,  0.1982, -0.2445],
          [-0.4212,  0.1496,  0.4579,  ...,  0.2753, -0.1431,  0.2008]],

         [[ 0.0164,  0.3664,  0.5365,  ..., -0.0008,  0.1081,  0.2134],
          [ 0.0109,  0.1668,  0.2853,  ..., -0.3913, -0.0478, -0.1532],
          [ 0.1610,  0.1891,  0.4867,  ...,  0.1439,  0.1451,  0.2742],
          ...,
          [ 0.0110,  0.3570,  0.2656,  ..., -0.0510, -0.1132, -0.0814],
          [-0.1030, -0.3722,  0.4105,  ..., -0.2922,  0.1018,  0.3397],
          [-0.1627, -0.4849,  0.3038,  ..., -0.0113, -0.4491, -0.3863]],

         [[ 0.0521, -0.1309, -0.1241,  ...,  0.0137,  0.0957,  0.2369],
          [ 0.4776,  0.2066,  0.1900,  ..., -0.0821,  0.3326,  0.4633],
          [-0.0088,  0.1033,  0.4708,  ..., -0.1855,  0.2577,  0.2128],
          ...,
          [ 0.4117,  0.1882, -0.3944,  ..., -0.0925,  0.2625, -0.0192],
          [-0.1748, -0.2219, -0.4731,  ..., -0.2376,  0.1274, -0.2224],
          [-0.3299, -0.3300, -0.4052,  ...,  0.2820, -0.0674,  0.1001]],

         ...,

         [[ 0.3033,  0.2642, -0.3409,  ...,  0.2861, -0.2955, -0.2214],
          [ 0.0655, -0.1893, -0.4137,  ...,  0.2868, -0.1428,  0.2795],
          [-0.0978,  0.3062, -0.4329,  ..., -0.2584,  0.4483,  0.4577],
          ...,
          [ 0.1883,  0.5176,  0.1657,  ..., -0.2651,  0.2971,  0.3216],
          [ 0.3963,  0.4915, -0.3599,  ...,  0.0866, -0.2788,  0.2826],
          [-0.4538, -0.0577,  0.1185,  ..., -0.2809, -0.1687,  0.0412]],

         [[-0.2096, -0.4984,  0.1922,  ...,  0.4057,  0.3692, -0.4930],
          [-0.3290,  0.3335,  0.4378,  ...,  0.2788,  0.4177,  0.3603],
          [ 0.2676, -0.0019, -0.0199,  ..., -0.1578, -0.3639,  0.3947],
          ...,
          [ 0.5064,  0.5373, -0.1379,  ...,  0.1682, -0.3820, -0.0338],
          [ 0.3493,  0.2669, -0.2896,  ..., -0.2697, -0.1137,  0.0187],
          [-0.0450,  0.2060,  0.1186,  ...,  0.4122, -0.3287,  0.3414]],

         [[ 0.2491,  0.5158, -0.4218,  ...,  0.0112, -0.2478,  0.4850],
          [-0.1022,  0.0201, -0.2210,  ..., -0.4139, -0.4563,  0.2638],
          [-0.0939, -0.1174,  0.4278,  ..., -0.2380, -0.4884,  0.3553],
          ...,
          [-0.0995, -0.4537,  0.4228,  ..., -0.2222,  0.3946, -0.3174],
          [ 0.1546,  0.1283,  0.0164,  ...,  0.0195,  0.3526, -0.3679],
          [-0.0750,  0.0765, -0.1918,  ..., -0.0190, -0.4459, -0.3515]]],


        [[[ 0.1334, -0.1284,  0.1045,  ..., -0.1132,  0.0634,  0.0302],
          [-0.1875, -0.4592, -0.3340,  ..., -0.3442, -0.0347,  0.2160],
          [ 0.0487,  0.0039,  0.3695,  ..., -0.3150, -0.1320,  0.2552],
          ...,
          [ 0.0849, -0.3015,  0.2448,  ..., -0.1436,  0.0517, -0.3939],
          [-0.2874, -0.3556,  0.4655,  ..., -0.4431,  0.0013, -0.0286],
          [-0.4990, -0.3313,  0.4804,  ...,  0.0418, -0.0066,  0.0794]],

         [[-0.4097, -0.4101, -0.1463,  ...,  0.4586,  0.1569, -0.0062],
          [ 0.1666,  0.3615, -0.3118,  ..., -0.0904, -0.2540,  0.3381],
          [ 0.1595, -0.2614, -0.1750,  ..., -0.4436, -0.3397, -0.2675],
          ...,
          [-0.1198, -0.3262,  0.4762,  ...,  0.1406, -0.2405, -0.4385],
          [-0.4363,  0.0291,  0.3685,  ...,  0.0159, -0.3513, -0.2328],
          [ 0.3272, -0.3456,  0.0401,  ...,  0.3910,  0.0015,  0.2933]],

         [[-0.4705, -0.2422,  0.0023,  ..., -0.4101, -0.3395,  0.2470],
          [-0.1703,  0.2204, -0.3235,  ..., -0.1743,  0.4905,  0.4673],
          [-0.2837,  0.4881, -0.1679,  ..., -0.3103, -0.0070, -0.1892],
          ...,
          [ 0.3758, -0.3782, -0.0903,  ..., -0.4068, -0.4411,  0.1504],
          [-0.2016, -0.2050, -0.3634,  ...,  0.1470,  0.0456,  0.3198],
          [-0.2129,  0.0579,  0.2957,  ...,  0.3474,  0.1283, -0.3598]],

         ...,

         [[ 0.2261,  0.1895,  0.0855,  ..., -0.1958,  0.0846,  0.0106],
          [ 0.1163, -0.4727,  0.2920,  ...,  0.3049,  0.1184, -0.4349],
          [-0.3062,  0.0386, -0.3058,  ...,  0.0574, -0.3937, -0.1862],
          ...,
          [-0.3280,  0.2289, -0.0557,  ...,  0.1529, -0.3316,  0.0913],
          [ 0.3714, -0.4357, -0.4497,  ...,  0.0151,  0.1826, -0.4753],
          [ 0.1768, -0.0787, -0.3849,  ..., -0.4374,  0.4135,  0.3741]],

         [[-0.0975, -0.0023,  0.2159,  ..., -0.4273, -0.4324,  0.2522],
          [-0.1794,  0.2558, -0.1072,  ...,  0.2089, -0.0045, -0.3969],
          [ 0.1627, -0.1708,  0.3414,  ...,  0.3530, -0.5239, -0.2899],
          ...,
          [ 0.1469,  0.2053, -0.1800,  ...,  0.3676, -0.1230,  0.1787],
          [ 0.3291,  0.1757, -0.2365,  ..., -0.0107, -0.3458, -0.0272],
          [ 0.0443, -0.1387, -0.3443,  ...,  0.0320, -0.1367, -0.0553]],

         [[ 0.2771, -0.0361,  0.1333,  ...,  0.3953,  0.2850,  0.4992],
          [-0.4051, -0.2516, -0.2153,  ...,  0.2218,  0.2176,  0.0042],
          [-0.3647,  0.2125,  0.4136,  ..., -0.1210,  0.2709,  0.4110],
          ...,
          [ 0.0898,  0.3601,  0.1162,  ..., -0.3055, -0.0010,  0.2732],
          [-0.3617,  0.2287, -0.2537,  ..., -0.3486, -0.3971, -0.2039],
          [-0.3817, -0.4130, -0.2561,  ...,  0.3639,  0.0678,  0.0414]]],


        [[[-0.2131, -0.2314, -0.4127,  ..., -0.1232,  0.3115, -0.1996],
          [-0.4731,  0.2666, -0.2956,  ...,  0.0433, -0.0768,  0.3699],
          [ 0.3248, -0.3742,  0.2002,  ..., -0.3673,  0.0473, -0.0311],
          ...,
          [-0.4680,  0.2251, -0.3717,  ...,  0.3224,  0.1020,  0.4302],
          [-0.3453, -0.0249, -0.4310,  ...,  0.1773, -0.2524, -0.4023],
          [-0.3853,  0.3527, -0.2884,  ..., -0.4222, -0.2403,  0.4933]],

         [[ 0.3498, -0.1179, -0.2603,  ...,  0.2913, -0.1962,  0.3197],
          [ 0.4839, -0.1930, -0.0451,  ..., -0.1950,  0.1175, -0.3560],
          [ 0.3562, -0.1477,  0.3446,  ...,  0.1600, -0.4002, -0.2948],
          ...,
          [-0.2429, -0.1648,  0.3143,  ..., -0.4328,  0.1278, -0.4018],
          [-0.2468, -0.1761, -0.3338,  ..., -0.1936,  0.2294,  0.1827],
          [-0.4758, -0.4098,  0.1525,  ...,  0.3864,  0.1784, -0.0054]],

         [[-0.3715,  0.0762,  0.2176,  ...,  0.4574,  0.3523, -0.0530],
          [-0.0985,  0.1751, -0.0750,  ...,  0.4286, -0.4150,  0.0719],
          [ 0.1584,  0.3378, -0.1981,  ..., -0.2787, -0.4039,  0.0074],
          ...,
          [ 0.0284, -0.4685,  0.1435,  ...,  0.4903,  0.4368,  0.3858],
          [ 0.3481, -0.3175,  0.2733,  ..., -0.1887,  0.0968,  0.1874],
          [-0.1387, -0.0640,  0.2116,  ..., -0.4913, -0.4095, -0.3153]],

         ...,

         [[-0.3144, -0.0195, -0.4563,  ..., -0.3772,  0.1708, -0.3399],
          [ 0.0926,  0.2653, -0.0174,  ...,  0.4734,  0.4698, -0.3837],
          [ 0.1943, -0.2717,  0.4642,  ..., -0.3170, -0.1907,  0.3389],
          ...,
          [ 0.1674,  0.3575, -0.3639,  ..., -0.1444, -0.4196,  0.1477],
          [ 0.3650,  0.1890,  0.1282,  ...,  0.0452,  0.1722,  0.2135],
          [-0.1321, -0.3331,  0.1157,  ..., -0.4313,  0.0241, -0.2769]],

         [[ 0.4442,  0.4218,  0.1167,  ..., -0.2829,  0.1270,  0.2273],
          [ 0.2659, -0.1075,  0.3159,  ...,  0.0459,  0.0926, -0.4300],
          [ 0.1654, -0.3896, -0.2529,  ...,  0.1534,  0.1498,  0.1514],
          ...,
          [ 0.2335,  0.0839,  0.2149,  ...,  0.0806,  0.2261, -0.4475],
          [-0.2560, -0.2212,  0.2254,  ...,  0.0660, -0.3948, -0.1493],
          [-0.3077,  0.1896,  0.0347,  ...,  0.1279, -0.3471, -0.1229]],

         [[-0.3784,  0.2168,  0.4130,  ..., -0.1416, -0.2928, -0.2427],
          [ 0.0939,  0.1523,  0.1870,  ..., -0.1746,  0.1533, -0.2435],
          [-0.0132,  0.3353,  0.4866,  ...,  0.3153,  0.0055, -0.1586],
          ...,
          [-0.1332,  0.0361,  0.3389,  ..., -0.2892,  0.1273, -0.0090],
          [ 0.3654, -0.3313, -0.4795,  ...,  0.4019,  0.4464, -0.1616],
          [-0.2861,  0.0349, -0.4082,  ..., -0.0349, -0.3985, -0.4313]]],


        ...,


        [[[ 0.3078,  0.0306,  0.2026,  ...,  0.1881, -0.2698,  0.2320],
          [ 0.2898,  0.3885, -0.0288,  ..., -0.3908, -0.0625, -0.3765],
          [ 0.3798,  0.0539,  0.2875,  ...,  0.3445,  0.0709, -0.1184],
          ...,
          [-0.4094, -0.3451, -0.0852,  ...,  0.2043,  0.0134,  0.2621],
          [ 0.4604, -0.3049, -0.3905,  ..., -0.3945, -0.2626,  0.4622],
          [-0.2044, -0.2220, -0.2043,  ...,  0.3320, -0.1139, -0.0871]],

         [[-0.1910, -0.1533, -0.4928,  ...,  0.2609,  0.1508, -0.4517],
          [-0.3919,  0.3220, -0.0641,  ...,  0.4513, -0.1846,  0.2645],
          [-0.2249,  0.0820, -0.3240,  ...,  0.4345, -0.2713, -0.2605],
          ...,
          [ 0.4537,  0.3172,  0.0620,  ...,  0.0908,  0.3114, -0.1554],
          [-0.1861,  0.2263,  0.0547,  ...,  0.2869, -0.2883, -0.2377],
          [-0.0425,  0.3501,  0.2662,  ..., -0.0140, -0.2459,  0.1884]],

         [[ 0.4085,  0.4873, -0.4359,  ...,  0.0681,  0.2826, -0.4412],
          [ 0.0084, -0.0018, -0.0779,  ...,  0.1390, -0.2943,  0.0984],
          [ 0.2751, -0.2545, -0.1219,  ..., -0.1865, -0.2923, -0.0877],
          ...,
          [ 0.4110, -0.0294,  0.2549,  ..., -0.4024, -0.4110,  0.2103],
          [-0.2778, -0.4505, -0.0583,  ...,  0.1552,  0.0301, -0.3079],
          [-0.3563, -0.3441, -0.1779,  ..., -0.0311, -0.3961,  0.3045]],

         ...,

         [[-0.0429, -0.3365,  0.3508,  ..., -0.2423, -0.3057,  0.2144],
          [ 0.0200, -0.0931,  0.2409,  ..., -0.2174, -0.0895,  0.3043],
          [ 0.4559,  0.1666,  0.1113,  ...,  0.4769,  0.3534, -0.4484],
          ...,
          [ 0.2876, -0.4557,  0.4067,  ..., -0.4216, -0.2819,  0.2890],
          [-0.3822,  0.1251,  0.3972,  ...,  0.5086, -0.0456, -0.3006],
          [ 0.3931,  0.4482,  0.1337,  ...,  0.1379, -0.3718,  0.2161]],

         [[ 0.3315, -0.1406, -0.3388,  ...,  0.3046,  0.0225,  0.3563],
          [ 0.2453, -0.4964,  0.1449,  ...,  0.1821, -0.3936,  0.3678],
          [ 0.1304,  0.3032, -0.3809,  ..., -0.3357,  0.1081,  0.0893],
          ...,
          [-0.4913, -0.0468,  0.4659,  ...,  0.3753, -0.3731,  0.0910],
          [-0.0890, -0.1003,  0.0874,  ..., -0.2203, -0.2619,  0.0367],
          [-0.1356, -0.4624,  0.2509,  ..., -0.2041, -0.4446,  0.1577]],

         [[ 0.0295,  0.4869, -0.3522,  ..., -0.4236,  0.0224, -0.1665],
          [-0.4826,  0.1762, -0.0776,  ...,  0.4460,  0.2196, -0.4071],
          [ 0.2980,  0.3476, -0.2231,  ..., -0.3521,  0.2026, -0.3633],
          ...,
          [ 0.2485,  0.2033,  0.0982,  ...,  0.4112,  0.3677,  0.3435],
          [ 0.4217,  0.1264, -0.3649,  ..., -0.4239,  0.3645, -0.3191],
          [-0.2694, -0.3588,  0.3719,  ...,  0.1592,  0.4449, -0.4948]]],


        [[[ 0.1529, -0.0754,  0.3538,  ...,  0.3553,  0.0768,  0.0545],
          [-0.1674, -0.3015, -0.1241,  ...,  0.1671,  0.2817, -0.3585],
          [-0.1109, -0.0220, -0.3364,  ...,  0.4471, -0.0347, -0.1248],
          ...,
          [ 0.1790,  0.0402, -0.2409,  ..., -0.1543, -0.2863,  0.2367],
          [ 0.4393,  0.1762, -0.2405,  ..., -0.1049,  0.3400, -0.1611],
          [-0.1219, -0.2683, -0.0160,  ...,  0.3150,  0.3590, -0.3088]],

         [[ 0.1562,  0.2496, -0.0588,  ..., -0.0368, -0.1028,  0.2836],
          [-0.4423,  0.3169,  0.1699,  ...,  0.3849, -0.2246,  0.2226],
          [-0.0927,  0.0959, -0.3865,  ...,  0.3300, -0.3090, -0.4634],
          ...,
          [ 0.3144, -0.3802,  0.3760,  ...,  0.3408, -0.3060,  0.1257],
          [-0.2164,  0.2496,  0.1584,  ..., -0.0187, -0.4130,  0.1581],
          [ 0.1924,  0.4881, -0.3386,  ..., -0.0722, -0.4550,  0.3339]],

         [[-0.2496, -0.0179, -0.3910,  ...,  0.1708,  0.3336,  0.4757],
          [-0.0718,  0.2715, -0.0338,  ...,  0.4725,  0.3292,  0.1187],
          [ 0.3483, -0.0273, -0.2237,  ...,  0.4291, -0.4164, -0.2863],
          ...,
          [-0.1337,  0.4049, -0.2129,  ..., -0.1728, -0.2866, -0.1052],
          [-0.4281,  0.0383, -0.4249,  ...,  0.1718,  0.3912,  0.2509],
          [ 0.4630, -0.3739, -0.0571,  ..., -0.2411, -0.1636,  0.4437]],

         ...,

         [[-0.3150,  0.4692, -0.0650,  ...,  0.4583,  0.4067, -0.0416],
          [-0.1480,  0.2746,  0.3755,  ..., -0.3420,  0.1554, -0.2804],
          [-0.3816, -0.3087, -0.0355,  ..., -0.5211, -0.4355,  0.1173],
          ...,
          [-0.0776,  0.4968, -0.3612,  ...,  0.3691, -0.4732, -0.1423],
          [ 0.0971,  0.3948,  0.5157,  ..., -0.0734, -0.2111,  0.2115],
          [-0.1751,  0.1594,  0.4987,  ...,  0.2654,  0.1753, -0.2316]],

         [[ 0.0408,  0.3244,  0.0060,  ...,  0.3141, -0.2845, -0.3824],
          [-0.1805,  0.1732,  0.0684,  ..., -0.0934, -0.0285,  0.1841],
          [ 0.1258, -0.2269,  0.3527,  ..., -0.2159, -0.2753, -0.1112],
          ...,
          [ 0.4006, -0.1436, -0.0989,  ..., -0.4643, -0.3673, -0.4428],
          [ 0.4970, -0.2520,  0.0069,  ...,  0.0387, -0.3588,  0.0560],
          [ 0.3031, -0.1408,  0.2676,  ..., -0.4559,  0.3600, -0.1493]],

         [[ 0.4074,  0.0491,  0.1800,  ...,  0.4293, -0.1713, -0.1802],
          [-0.3468,  0.1242, -0.4094,  ..., -0.1168, -0.2431, -0.0853],
          [-0.4300,  0.4119, -0.0392,  ..., -0.4887, -0.4860,  0.1818],
          ...,
          [-0.2128, -0.0768,  0.0971,  ...,  0.4674, -0.0813,  0.2252],
          [-0.3973,  0.4454,  0.0416,  ...,  0.2523, -0.0288, -0.1662],
          [-0.0972, -0.1104, -0.1266,  ...,  0.1683,  0.1875, -0.0515]]],


        [[[-0.4577,  0.2856, -0.2909,  ...,  0.2362, -0.1529, -0.3761],
          [-0.4169, -0.2435,  0.3505,  ..., -0.4246, -0.2907, -0.4822],
          [ 0.3794, -0.3372,  0.0539,  ...,  0.3020,  0.3278, -0.0783],
          ...,
          [ 0.2922,  0.2402, -0.2047,  ..., -0.2454,  0.4842, -0.3016],
          [-0.3441, -0.1174,  0.3168,  ..., -0.1122,  0.1301,  0.2742],
          [-0.4763,  0.1369, -0.0149,  ..., -0.0439, -0.2223, -0.2819]],

         [[-0.2951, -0.4510, -0.3548,  ...,  0.0965,  0.4462, -0.2343],
          [ 0.2789, -0.2504, -0.5230,  ..., -0.3956,  0.5181, -0.4423],
          [-0.0075,  0.0934,  0.4637,  ...,  0.0032, -0.3438,  0.1642],
          ...,
          [-0.0530,  0.2255,  0.1328,  ..., -0.0821,  0.3565,  0.2756],
          [ 0.1382,  0.2835, -0.0928,  ...,  0.2554, -0.0754,  0.1310],
          [-0.2017,  0.1937, -0.2058,  ...,  0.4503,  0.2119, -0.4203]],

         [[-0.4521, -0.0055, -0.2633,  ...,  0.4563,  0.1536, -0.3435],
          [-0.2987, -0.2458, -0.4511,  ...,  0.3939,  0.3272, -0.3651],
          [-0.3072,  0.0213, -0.3126,  ...,  0.0701, -0.3159,  0.0935],
          ...,
          [-0.4086, -0.0346, -0.1064,  ...,  0.0318, -0.2339,  0.2471],
          [-0.0989, -0.0494, -0.0583,  ..., -0.1679,  0.0776, -0.4921],
          [ 0.0741, -0.1792,  0.2039,  ...,  0.1599, -0.4390,  0.3774]],

         ...,

         [[ 0.0764, -0.2960, -0.5084,  ...,  0.4644,  0.2732, -0.2176],
          [ 0.4576,  0.3241,  0.3436,  ..., -0.3187, -0.2883, -0.1312],
          [ 0.0271,  0.4771, -0.2862,  ..., -0.3345,  0.3624,  0.0930],
          ...,
          [-0.0791, -0.2293,  0.4006,  ...,  0.4767, -0.2886, -0.1398],
          [-0.4823,  0.1594, -0.3960,  ...,  0.4344, -0.2076, -0.2676],
          [-0.1025,  0.3534, -0.3336,  ...,  0.0253, -0.2116,  0.2736]],

         [[-0.2075, -0.2767, -0.1346,  ...,  0.4951,  0.0143,  0.1619],
          [-0.2050,  0.1864, -0.0383,  ..., -0.0875,  0.3257, -0.0245],
          [ 0.4112,  0.0651, -0.2860,  ...,  0.2322, -0.2779,  0.0110],
          ...,
          [ 0.2514, -0.3638,  0.0497,  ..., -0.2373, -0.4433,  0.2610],
          [-0.0991,  0.3965,  0.0250,  ..., -0.2250, -0.3578,  0.2287],
          [-0.2607,  0.1310, -0.2489,  ...,  0.2248,  0.0784,  0.1076]],

         [[-0.3883, -0.0071,  0.4457,  ..., -0.2006,  0.4081,  0.1472],
          [ 0.1857, -0.1929, -0.3497,  ..., -0.3367,  0.0436,  0.0333],
          [ 0.2212, -0.0706, -0.1933,  ..., -0.4007, -0.1616, -0.2949],
          ...,
          [-0.0121, -0.2223,  0.0659,  ..., -0.2608,  0.1161, -0.1284],
          [ 0.4612,  0.4157, -0.4500,  ..., -0.2125, -0.2974,  0.0030],
          [-0.3438, -0.3090, -0.1769,  ..., -0.0854,  0.2779, -0.1472]]]])
DESIRED: (shape=torch.Size([16, 395032, 20, 20]), dtype=torch.float32)
tensor([[[[ 0.3855, -0.2655,  0.1907,  ...,  0.2038,  0.0713, -0.1740],
          [-0.2081,  0.0228, -0.3129,  ..., -0.2960, -0.2165, -0.3187],
          [-0.4267,  0.2922,  0.0258,  ..., -0.1127, -0.3082,  0.1612],
          ...,
          [-0.1318, -0.1471,  0.1260,  ...,  0.0244,  0.3143, -0.1311],
          [ 0.2498,  0.3525,  0.0355,  ..., -0.1127,  0.1865, -0.2455],
          [-0.4445,  0.1315,  0.4694,  ...,  0.2805, -0.1481,  0.2154]],

         [[ 0.0098,  0.3619,  0.5487,  ...,  0.0119,  0.1148,  0.2220],
          [ 0.0059,  0.1663,  0.2886,  ..., -0.3852, -0.0450, -0.1537],
          [ 0.1578,  0.1881,  0.4822,  ...,  0.1362,  0.1410,  0.2709],
          ...,
          [ 0.0045,  0.3451,  0.2828,  ..., -0.0601, -0.1115, -0.0698],
          [-0.1010, -0.3899,  0.3961,  ..., -0.2848,  0.1038,  0.3366],
          [-0.1584, -0.4923,  0.2883,  ..., -0.0158, -0.4645, -0.3986]],

         [[ 0.0479, -0.1342, -0.1358,  ...,  0.0156,  0.0950,  0.2356],
          [ 0.4817,  0.2074,  0.1772,  ..., -0.0709,  0.3380,  0.4659],
          [ 0.0024,  0.1048,  0.4687,  ..., -0.1851,  0.2692,  0.2235],
          ...,
          [ 0.4069,  0.1994, -0.3966,  ..., -0.0966,  0.2639, -0.0315],
          [-0.1849, -0.2254, -0.4746,  ..., -0.2229,  0.1206, -0.2254],
          [-0.3332, -0.3315, -0.4044,  ...,  0.2832, -0.0729,  0.1089]],

         ...,

         [[ 0.3089,  0.2816, -0.3277,  ...,  0.2811, -0.3052, -0.2299],
          [ 0.0770, -0.1796, -0.4117,  ...,  0.2896, -0.1512,  0.2756],
          [-0.1057,  0.3013, -0.4275,  ..., -0.2392,  0.4560,  0.4619],
          ...,
          [ 0.1881,  0.5247,  0.1624,  ..., -0.2566,  0.2967,  0.3268],
          [ 0.3818,  0.4910, -0.3454,  ...,  0.0763, -0.2791,  0.2880],
          [-0.4770, -0.0781,  0.1265,  ..., -0.2924, -0.1621,  0.0405]],

         [[-0.2026, -0.5179,  0.1715,  ...,  0.4152,  0.3520, -0.5251],
          [-0.3470,  0.3092,  0.4446,  ...,  0.2950,  0.4279,  0.3425],
          [ 0.2650,  0.0160, -0.0126,  ..., -0.1521, -0.3450,  0.4141],
          ...,
          [ 0.5120,  0.5506, -0.1313,  ...,  0.1446, -0.3851, -0.0269],
          [ 0.3421,  0.2717, -0.2777,  ..., -0.2677, -0.1106,  0.0284],
          [-0.0571,  0.2014,  0.1328,  ...,  0.4125, -0.3299,  0.3596]],

         [[ 0.2513,  0.5329, -0.4121,  ...,  0.0114, -0.2337,  0.5024],
          [-0.0976,  0.0324, -0.2329,  ..., -0.4120, -0.4391,  0.2801],
          [-0.0928, -0.1220,  0.4189,  ..., -0.2523, -0.4779,  0.3700],
          ...,
          [-0.0862, -0.4530,  0.4079,  ..., -0.2111,  0.3927, -0.3337],
          [ 0.1545,  0.1373,  0.0100,  ...,  0.0274,  0.3279, -0.3814],
          [-0.0818,  0.0762, -0.1967,  ..., -0.0282, -0.4649, -0.3496]]],


        [[[ 0.1439, -0.1212,  0.1040,  ..., -0.1056,  0.0670,  0.0261],
          [-0.1799, -0.4561, -0.3382,  ..., -0.3417, -0.0235,  0.2165],
          [ 0.0425, -0.0118,  0.3529,  ..., -0.3228, -0.1262,  0.2610],
          ...,
          [ 0.0874, -0.3086,  0.2430,  ..., -0.1517,  0.0477, -0.4005],
          [-0.2959, -0.3665,  0.4582,  ..., -0.4367,  0.0059, -0.0220],
          [-0.5069, -0.3448,  0.4754,  ...,  0.0533, -0.0056,  0.0833]],

         [[-0.4205, -0.4282, -0.1519,  ...,  0.4683,  0.1574, -0.0157],
          [ 0.1524,  0.3605, -0.3004,  ..., -0.0854, -0.2367,  0.3507],
          [ 0.1669, -0.2470, -0.1913,  ..., -0.4492, -0.3368, -0.2552],
          ...,
          [-0.1220, -0.3311,  0.4744,  ...,  0.1272, -0.2528, -0.4442],
          [-0.4346,  0.0133,  0.3577,  ...,  0.0163, -0.3487, -0.2182],
          [ 0.3537, -0.3449,  0.0216,  ...,  0.3927,  0.0084,  0.3084]],

         [[-0.4803, -0.2586,  0.0069,  ..., -0.4221, -0.3430,  0.2545],
          [-0.1813,  0.2084, -0.3119,  ..., -0.1660,  0.4909,  0.4727],
          [-0.2958,  0.4864, -0.1550,  ..., -0.3106,  0.0058, -0.1790],
          ...,
          [ 0.3807, -0.3664, -0.1083,  ..., -0.4101, -0.4248,  0.1655],
          [-0.2099, -0.1959, -0.3545,  ...,  0.1588,  0.0574,  0.3139],
          [-0.2188,  0.0549,  0.3110,  ...,  0.3526,  0.1178, -0.3818]],

         ...,

         [[ 0.2290,  0.2038,  0.0855,  ..., -0.1995,  0.0871,  0.0176],
          [ 0.1342, -0.4661,  0.2877,  ...,  0.3009,  0.1123, -0.4408],
          [-0.3116,  0.0267, -0.2924,  ...,  0.0595, -0.3863, -0.1939],
          ...,
          [-0.3316,  0.2100, -0.0603,  ...,  0.1408, -0.3212,  0.0875],
          [ 0.3920, -0.4232, -0.4630,  ...,  0.0124,  0.1846, -0.4791],
          [ 0.1784, -0.0642, -0.3866,  ..., -0.4384,  0.4290,  0.3895]],

         [[-0.0980, -0.0111,  0.2245,  ..., -0.4498, -0.4275,  0.2771],
          [-0.1903,  0.2529, -0.1030,  ...,  0.1952, -0.0160, -0.3937],
          [ 0.1658, -0.1678,  0.3265,  ...,  0.3444, -0.5305, -0.2951],
          ...,
          [ 0.1500,  0.2134, -0.1742,  ...,  0.3589, -0.1283,  0.1865],
          [ 0.3294,  0.1782, -0.2334,  ..., -0.0265, -0.3440, -0.0245],
          [ 0.0426, -0.1385, -0.3458,  ...,  0.0283, -0.1340, -0.0546]],

         [[ 0.2955, -0.0290,  0.1322,  ...,  0.3969,  0.2889,  0.5122],
          [-0.3953, -0.2569, -0.2179,  ...,  0.2334,  0.2146,  0.0046],
          [-0.3776,  0.1932,  0.4034,  ..., -0.1116,  0.2811,  0.4124],
          ...,
          [ 0.0779,  0.3628,  0.1102,  ..., -0.3107, -0.0042,  0.2694],
          [-0.3788,  0.2103, -0.2539,  ..., -0.3366, -0.3913, -0.2025],
          [-0.3818, -0.4271, -0.2658,  ...,  0.3783,  0.0719,  0.0449]]],


        [[[-0.2081, -0.2373, -0.4132,  ..., -0.1173,  0.3147, -0.2194],
          [-0.4925,  0.2598, -0.2909,  ...,  0.0435, -0.0648,  0.3732],
          [ 0.3250, -0.3592,  0.1809,  ..., -0.3527,  0.0445, -0.0185],
          ...,
          [-0.4817,  0.2158, -0.3666,  ...,  0.3185,  0.1004,  0.4237],
          [-0.3504, -0.0217, -0.4226,  ...,  0.1561, -0.2656, -0.3994],
          [-0.3994,  0.3539, -0.2781,  ..., -0.4351, -0.2237,  0.5235]],

         [[ 0.3562, -0.1057, -0.2632,  ...,  0.2884, -0.1987,  0.3418],
          [ 0.4963, -0.1815, -0.0568,  ..., -0.1866,  0.1140, -0.3527],
          [ 0.3745, -0.1494,  0.3375,  ...,  0.1388, -0.3977, -0.2993],
          ...,
          [-0.2476, -0.1769,  0.3019,  ..., -0.4253,  0.1297, -0.3996],
          [-0.2527, -0.1807, -0.3341,  ..., -0.1711,  0.2351,  0.1869],
          [-0.4817, -0.4231,  0.1557,  ...,  0.3951,  0.1711, -0.0121]],

         [[-0.3852,  0.0640,  0.2260,  ...,  0.4620,  0.3576, -0.0632],
          [-0.1119,  0.1692, -0.0656,  ...,  0.4243, -0.4038,  0.0788],
          [ 0.1559,  0.3444, -0.1871,  ..., -0.2800, -0.4069,  0.0182],
          ...,
          [ 0.0442, -0.4705,  0.1288,  ...,  0.4921,  0.4346,  0.3877],
          [ 0.3551, -0.3062,  0.2680,  ..., -0.2000,  0.0883,  0.1775],
          [-0.1493, -0.0641,  0.2124,  ..., -0.4975, -0.4163, -0.3229]],

         ...,

         [[-0.3273, -0.0239, -0.4562,  ..., -0.3812,  0.1640, -0.3485],
          [ 0.0805,  0.2682, -0.0268,  ...,  0.4744,  0.4574, -0.4085],
          [ 0.2041, -0.2611,  0.4546,  ..., -0.3102, -0.1667,  0.3420],
          ...,
          [ 0.1684,  0.3619, -0.3443,  ..., -0.1479, -0.4082,  0.1583],
          [ 0.3617,  0.1809,  0.1369,  ...,  0.0430,  0.1797,  0.2055],
          [-0.1377, -0.3456,  0.1063,  ..., -0.4302,  0.0224, -0.2914]],

         [[ 0.4481,  0.4366,  0.1192,  ..., -0.2882,  0.1350,  0.2415],
          [ 0.2775, -0.0930,  0.3115,  ...,  0.0420,  0.0837, -0.4349],
          [ 0.1809, -0.3842, -0.2538,  ...,  0.1566,  0.1499,  0.1419],
          ...,
          [ 0.2327,  0.0820,  0.2143,  ...,  0.0840,  0.2059, -0.4621],
          [-0.2645, -0.2240,  0.2183,  ...,  0.0603, -0.4039, -0.1409],
          [-0.3183,  0.1895,  0.0329,  ...,  0.1185, -0.3491, -0.1184]],

         [[-0.3984,  0.2037,  0.4158,  ..., -0.1441, -0.3021, -0.2418],
          [ 0.0851,  0.1495,  0.1899,  ..., -0.1724,  0.1446, -0.2517],
          [-0.0181,  0.3243,  0.4938,  ...,  0.3021,  0.0061, -0.1638],
          ...,
          [-0.1238,  0.0238,  0.3286,  ..., -0.2761,  0.1433, -0.0162],
          [ 0.3731, -0.3147, -0.4966,  ...,  0.4098,  0.4247, -0.1803],
          [-0.3039,  0.0408, -0.4036,  ..., -0.0536, -0.4200, -0.4371]]],


        ...,


        [[[ 0.3133,  0.0275,  0.2083,  ...,  0.1874, -0.2706,  0.2524],
          [ 0.2878,  0.3899, -0.0245,  ..., -0.3902, -0.0702, -0.3743],
          [ 0.3898,  0.0621,  0.2803,  ...,  0.3326,  0.0616, -0.1317],
          ...,
          [-0.4005, -0.3498, -0.0987,  ...,  0.1961,  0.0123,  0.2791],
          [ 0.4735, -0.2877, -0.3985,  ..., -0.3913, -0.2483,  0.4687],
          [-0.2161, -0.2206, -0.2007,  ...,  0.3386, -0.1173, -0.0971]],

         [[-0.1877, -0.1581, -0.4991,  ...,  0.2614,  0.1442, -0.4763],
          [-0.4038,  0.3087, -0.0555,  ...,  0.4407, -0.1772,  0.2659],
          [-0.2397,  0.0866, -0.3140,  ...,  0.4302, -0.2817, -0.2501],
          ...,
          [ 0.4485,  0.3242,  0.0629,  ...,  0.1082,  0.2948, -0.1674],
          [-0.1996,  0.2224,  0.0666,  ...,  0.2770, -0.3025, -0.2300],
          [-0.0475,  0.3462,  0.2744,  ..., -0.0257, -0.2404,  0.2043]],

         [[ 0.4149,  0.5077, -0.4313,  ...,  0.0766,  0.2827, -0.4647],
          [ 0.0125,  0.0119, -0.0849,  ...,  0.1346, -0.2826,  0.0970],
          [ 0.2805, -0.2464, -0.1296,  ..., -0.1847, -0.2959, -0.0834],
          ...,
          [ 0.4069, -0.0337,  0.2478,  ..., -0.3970, -0.3969,  0.2126],
          [-0.2861, -0.4573, -0.0774,  ...,  0.1623,  0.0199, -0.3096],
          [-0.3587, -0.3454, -0.1873,  ..., -0.0428, -0.3953,  0.3286]],

         ...,

         [[-0.0393, -0.3453,  0.3406,  ..., -0.2504, -0.3015,  0.2229],
          [ 0.0150, -0.1039,  0.2426,  ..., -0.2300, -0.0906,  0.3206],
          [ 0.4583,  0.1665,  0.1180,  ...,  0.4725,  0.3288, -0.4522],
          ...,
          [ 0.2890, -0.4507,  0.3981,  ..., -0.4116, -0.2650,  0.2891],
          [-0.3863,  0.1261,  0.3975,  ...,  0.5138, -0.0604, -0.3039],
          [ 0.4063,  0.4583,  0.1358,  ...,  0.1208, -0.3734,  0.2361]],

         [[ 0.3420, -0.1230, -0.3492,  ...,  0.3068,  0.0327,  0.3622],
          [ 0.2621, -0.4951,  0.1291,  ...,  0.1770, -0.3867,  0.3853],
          [ 0.1303,  0.3005, -0.3655,  ..., -0.3249,  0.1019,  0.0955],
          ...,
          [-0.5001, -0.0584,  0.4582,  ...,  0.3561, -0.3728,  0.0940],
          [-0.0846, -0.1097,  0.0848,  ..., -0.2287, -0.2586,  0.0435],
          [-0.1306, -0.4728,  0.2420,  ..., -0.2094, -0.4397,  0.1710]],

         [[ 0.0306,  0.4956, -0.3439,  ..., -0.4385,  0.0211, -0.1651],
          [-0.4956,  0.1714, -0.0728,  ...,  0.4416,  0.2016, -0.4152],
          [ 0.2858,  0.3559, -0.2166,  ..., -0.3351,  0.2008, -0.3799],
          ...,
          [ 0.2565,  0.2124,  0.0887,  ...,  0.4090,  0.3734,  0.3310],
          [ 0.4170,  0.1287, -0.3521,  ..., -0.4102,  0.3642, -0.3442],
          [-0.2810, -0.3763,  0.3767,  ...,  0.1769,  0.4324, -0.5159]]],


        [[[ 0.1628, -0.0730,  0.3556,  ...,  0.3588,  0.0695,  0.0616],
          [-0.1598, -0.3017, -0.1217,  ...,  0.1705,  0.2715, -0.3656],
          [-0.1160, -0.0266, -0.3431,  ...,  0.4365, -0.0398, -0.1317],
          ...,
          [ 0.1858,  0.0536, -0.2380,  ..., -0.1597, -0.2693,  0.2421],
          [ 0.4372,  0.1804, -0.2320,  ..., -0.0886,  0.3452, -0.1785],
          [-0.1296, -0.2772, -0.0139,  ...,  0.3299,  0.3480, -0.3242]],

         [[ 0.1657,  0.2514, -0.0562,  ..., -0.0492, -0.0942,  0.2923],
          [-0.4503,  0.3065,  0.1802,  ...,  0.3722, -0.2206,  0.2406],
          [-0.1100,  0.1011, -0.3768,  ...,  0.3325, -0.3219, -0.4632],
          ...,
          [ 0.3174, -0.3709,  0.3639,  ...,  0.3283, -0.3093,  0.1360],
          [-0.2247,  0.2553,  0.1511,  ..., -0.0326, -0.4099,  0.1726],
          [ 0.1948,  0.4986, -0.3377,  ..., -0.0884, -0.4467,  0.3516]],

         [[-0.2572, -0.0224, -0.3923,  ...,  0.1660,  0.3391,  0.4854],
          [-0.0871,  0.2677, -0.0323,  ...,  0.4705,  0.3336,  0.1270],
          [ 0.3505, -0.0122, -0.2176,  ...,  0.4267, -0.4202, -0.2810],
          ...,
          [-0.1543,  0.4040, -0.2053,  ..., -0.1675, -0.2704, -0.0966],
          [-0.4236,  0.0232, -0.4178,  ...,  0.1724,  0.3908,  0.2572],
          [ 0.4950, -0.3701, -0.0590,  ..., -0.2555, -0.1612,  0.4590]],

         ...,

         [[-0.3323,  0.4661, -0.0575,  ...,  0.4740,  0.4025, -0.0458],
          [-0.1567,  0.2773,  0.3760,  ..., -0.3209,  0.1658, -0.2893],
          [-0.3864, -0.3037, -0.0233,  ..., -0.5268, -0.4192,  0.1215],
          ...,
          [-0.0847,  0.4980, -0.3424,  ...,  0.3429, -0.4816, -0.1265],
          [ 0.0887,  0.3817,  0.5268,  ..., -0.0805, -0.1951,  0.2158],
          [-0.1865,  0.1444,  0.5008,  ...,  0.2774,  0.1740, -0.2475]],

         [[ 0.0397,  0.3268,  0.0137,  ...,  0.3180, -0.2989, -0.3951],
          [-0.1871,  0.1765,  0.0676,  ..., -0.0804, -0.0249,  0.1810],
          [ 0.1275, -0.2221,  0.3475,  ..., -0.2182, -0.2681, -0.1057],
          ...,
          [ 0.4204, -0.1353, -0.1000,  ..., -0.4592, -0.3702, -0.4350],
          [ 0.5090, -0.2410,  0.0052,  ...,  0.0261, -0.3430,  0.0663],
          [ 0.3075, -0.1359,  0.2685,  ..., -0.4527,  0.3747, -0.1620]],

         [[ 0.4283,  0.0529,  0.1902,  ...,  0.4367, -0.1779, -0.1827],
          [-0.3400,  0.1177, -0.3959,  ..., -0.1034, -0.2379, -0.0882],
          [-0.4534,  0.4019, -0.0412,  ..., -0.4919, -0.4758,  0.1901],
          ...,
          [-0.2248, -0.0684,  0.0973,  ...,  0.4618, -0.0853,  0.2287],
          [-0.4099,  0.4323,  0.0493,  ...,  0.2433, -0.0309, -0.1719],
          [-0.0913, -0.1201, -0.1345,  ...,  0.1738,  0.1879, -0.0541]]],


        [[[-0.4726,  0.2895, -0.2900,  ...,  0.2439, -0.1604, -0.3789],
          [-0.4324, -0.2443,  0.3337,  ..., -0.4234, -0.2989, -0.4896],
          [ 0.3833, -0.3334,  0.0498,  ...,  0.2941,  0.3095, -0.0969],
          ...,
          [ 0.2844,  0.2413, -0.1936,  ..., -0.2307,  0.4828, -0.3081],
          [-0.3600, -0.1276,  0.3160,  ..., -0.1033,  0.1256,  0.2744],
          [-0.4906,  0.1320, -0.0153,  ..., -0.0448, -0.2328, -0.2931]],

         [[-0.3028, -0.4535, -0.3535,  ...,  0.1136,  0.4374, -0.2432],
          [ 0.2818, -0.2455, -0.5361,  ..., -0.3729,  0.5226, -0.4642],
          [-0.0045,  0.0841,  0.4538,  ..., -0.0157, -0.3309,  0.1635],
          ...,
          [-0.0495,  0.2251,  0.1327,  ..., -0.0712,  0.3534,  0.2743],
          [ 0.1323,  0.2850, -0.0946,  ...,  0.2576, -0.0770,  0.1219],
          [-0.2150,  0.1899, -0.2024,  ...,  0.4548,  0.2016, -0.4428]],

         [[-0.4632, -0.0068, -0.2598,  ...,  0.4589,  0.1376, -0.3527],
          [-0.3027, -0.2437, -0.4491,  ...,  0.4061,  0.3185, -0.3840],
          [-0.3158,  0.0104, -0.3186,  ...,  0.0625, -0.3087,  0.0946],
          ...,
          [-0.4178, -0.0437, -0.1094,  ...,  0.0212, -0.2237,  0.2397],
          [-0.0927, -0.0531, -0.0533,  ..., -0.1607,  0.0645, -0.4962],
          [ 0.0820, -0.1822,  0.2012,  ...,  0.1549, -0.4408,  0.4082]],

         ...,

         [[ 0.0767, -0.2983, -0.5304,  ...,  0.4836,  0.2715, -0.2288],
          [ 0.4592,  0.3129,  0.3346,  ..., -0.3037, -0.2834, -0.1334],
          [ 0.0295,  0.4829, -0.2542,  ..., -0.3313,  0.3555,  0.0833],
          ...,
          [-0.0857, -0.2324,  0.3804,  ...,  0.4742, -0.2948, -0.1404],
          [-0.4925,  0.1636, -0.3981,  ...,  0.4231, -0.2157, -0.2604],
          [-0.1037,  0.3577, -0.3260,  ...,  0.0119, -0.2055,  0.2926]],

         [[-0.2066, -0.2859, -0.1389,  ...,  0.5031,  0.0055,  0.1679],
          [-0.2208,  0.1750, -0.0323,  ..., -0.0702,  0.3274, -0.0275],
          [ 0.4079,  0.0808, -0.2830,  ...,  0.2121, -0.2710,  0.0162],
          ...,
          [ 0.2624, -0.3433,  0.0378,  ..., -0.2460, -0.4390,  0.2762],
          [-0.1158,  0.3978,  0.0268,  ..., -0.2207, -0.3399,  0.2368],
          [-0.2709,  0.1240, -0.2518,  ...,  0.2293,  0.0848,  0.1055]],

         [[-0.4066, -0.0170,  0.4610,  ..., -0.1913,  0.4181,  0.1450],
          [ 0.1816, -0.1829, -0.3388,  ..., -0.3327,  0.0581,  0.0395],
          [ 0.2322, -0.0709, -0.1992,  ..., -0.3990, -0.1570, -0.2980],
          ...,
          [ 0.0051, -0.2134,  0.0511,  ..., -0.2546,  0.1121, -0.1265],
          [ 0.4542,  0.4226, -0.4442,  ..., -0.2173, -0.2880,  0.0077],
          [-0.3592, -0.3251, -0.1775,  ..., -0.0722,  0.2855, -0.1578]]]])

2025-07-09 13:40:47.125355 GPU 6 151383 test begin: paddle.nn.functional.interpolate(Tensor([16, 40, 148549, 24],"float32"), size=list[64,48,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([16, 40, 148549, 24],"float32"), size=list[64,48,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1903150 / 1966080 (96.8%)
Greatest absolute difference: 0.9920206069946289 at index (6, 27, 63, 0) (up to 0.01 allowed)
Greatest relative difference: 8034132.0 at index (4, 28, 39, 23) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 40, 64, 48]), dtype=torch.float32)
tensor([[[[ 0.0063,  0.0369,  0.0675,  ...,  0.2814, -0.0552, -0.3919],
          [-0.2096, -0.2440, -0.2784,  ...,  0.3533,  0.0766, -0.2000],
          [ 0.2972,  0.2704,  0.2436,  ..., -0.0533, -0.0535, -0.0537],
          ...,
          [ 0.2290,  0.2476,  0.2661,  ..., -0.0600, -0.0513, -0.0427],
          [ 0.1332, -0.0746, -0.2824,  ...,  0.1063,  0.0217, -0.0629],
          [ 0.3662,  0.0742, -0.2179,  ...,  0.4608,  0.2670,  0.0731]],

         [[-0.4467, -0.0338,  0.3792,  ...,  0.4443,  0.2674,  0.0905],
          [-0.2951, -0.0756,  0.1440,  ..., -0.2815, -0.1657, -0.0498],
          [-0.3256, -0.1138,  0.0979,  ..., -0.2179, -0.0601,  0.0977],
          ...,
          [ 0.0728, -0.1003, -0.2734,  ..., -0.3427, -0.1760, -0.0093],
          [-0.0831, -0.1262, -0.1693,  ..., -0.3430, -0.2256, -0.1082],
          [ 0.4346,  0.1407, -0.1532,  ...,  0.0662, -0.0869, -0.2400]],

         [[-0.2630,  0.0392,  0.3413,  ...,  0.1257,  0.0060, -0.1137],
          [ 0.3055,  0.2707,  0.2358,  ..., -0.3310, -0.3384, -0.3459],
          [-0.2227, -0.0455,  0.1317,  ..., -0.2108, -0.1710, -0.1313],
          ...,
          [ 0.2336,  0.1570,  0.0804,  ...,  0.1583,  0.3259,  0.4936],
          [ 0.3279,  0.0535, -0.2209,  ...,  0.3970,  0.3958,  0.3946],
          [-0.0900,  0.0041,  0.0981,  ...,  0.1276, -0.1704, -0.4684]],

         ...,

         [[-0.0370, -0.1211, -0.2052,  ...,  0.3786,  0.3039,  0.2293],
          [ 0.4147,  0.0322, -0.3503,  ...,  0.1366,  0.2820,  0.4274],
          [-0.0201, -0.0233, -0.0265,  ..., -0.0386, -0.0583, -0.0781],
          ...,
          [ 0.4580,  0.4175,  0.3771,  ...,  0.3462,  0.0461, -0.2541],
          [-0.3816, -0.2481, -0.1146,  ..., -0.1444, -0.0847, -0.0251],
          [-0.1544, -0.2905, -0.4266,  ...,  0.4818,  0.0819, -0.3180]],

         [[ 0.0029,  0.0565,  0.1102,  ...,  0.4241,  0.4155,  0.4069],
          [ 0.0855,  0.1860,  0.2865,  ..., -0.2312,  0.0390,  0.3092],
          [ 0.1869, -0.1209, -0.4287,  ..., -0.1821,  0.0813,  0.3447],
          ...,
          [ 0.0109,  0.1821,  0.3532,  ...,  0.0339,  0.1570,  0.2801],
          [ 0.0417,  0.0305,  0.0193,  ..., -0.1563,  0.1236,  0.4036],
          [-0.0424,  0.2029,  0.4482,  ..., -0.1750, -0.2010, -0.2269]],

         [[ 0.3102,  0.3453,  0.3805,  ..., -0.2171,  0.1385,  0.4940],
          [ 0.1703, -0.0605, -0.2913,  ...,  0.3586,  0.0102, -0.3383],
          [-0.0965, -0.0503, -0.0041,  ...,  0.2191, -0.0752, -0.3694],
          ...,
          [-0.3046, -0.1191,  0.0664,  ..., -0.3803, -0.0331,  0.3141],
          [ 0.0914, -0.1037, -0.2988,  ...,  0.2937,  0.3291,  0.3645],
          [-0.3051, -0.2212, -0.1373,  ...,  0.1513,  0.3140,  0.4768]]],


        [[[-0.4269, -0.2839, -0.1409,  ...,  0.1183, -0.0974, -0.3131],
          [ 0.3639,  0.1356, -0.0927,  ..., -0.0270,  0.1309,  0.2889],
          [-0.0956, -0.1917, -0.2879,  ...,  0.2489,  0.1577,  0.0665],
          ...,
          [ 0.1429, -0.0802, -0.3033,  ...,  0.2500,  0.0578, -0.1343],
          [-0.0413, -0.1718, -0.3023,  ..., -0.0910,  0.0469,  0.1848],
          [-0.1167, -0.1200, -0.1234,  ...,  0.4267,  0.3381,  0.2496]],

         [[-0.0356, -0.1922, -0.3487,  ...,  0.4539,  0.2591,  0.0644],
          [ 0.2510,  0.2331,  0.2151,  ..., -0.1832, -0.0473,  0.0885],
          [-0.3561, -0.3113, -0.2665,  ..., -0.0172,  0.0080,  0.0331],
          ...,
          [-0.0044,  0.1888,  0.3820,  ..., -0.2093, -0.0932,  0.0229],
          [-0.3141,  0.0306,  0.3752,  ..., -0.1793,  0.1413,  0.4620],
          [ 0.1333,  0.1244,  0.1156,  ...,  0.0172, -0.2335, -0.4841]],

         [[-0.0977,  0.1192,  0.3362,  ..., -0.2566, -0.3159, -0.3751],
          [ 0.0801,  0.0735,  0.0669,  ..., -0.0257,  0.0855,  0.1967],
          [-0.0116,  0.1017,  0.2151,  ...,  0.0396, -0.0585, -0.1566],
          ...,
          [ 0.1568, -0.0082, -0.1733,  ...,  0.4220,  0.0839, -0.2543],
          [-0.3067, -0.0926,  0.1215,  ..., -0.2982, -0.1825, -0.0668],
          [ 0.3542,  0.2703,  0.1864,  ...,  0.4828,  0.2793,  0.0758]],

         ...,

         [[ 0.2743,  0.0277, -0.2189,  ..., -0.4188, -0.2263, -0.0337],
          [ 0.1092, -0.1682, -0.4456,  ..., -0.1683,  0.1275,  0.4232],
          [ 0.0090,  0.0255,  0.0420,  ..., -0.0734, -0.0082,  0.0570],
          ...,
          [ 0.3590,  0.0457, -0.2676,  ..., -0.1272, -0.0996, -0.0720],
          [ 0.2924,  0.1697,  0.0471,  ..., -0.1552, -0.0790, -0.0028],
          [ 0.4926,  0.1090, -0.2745,  ..., -0.2926, -0.1694, -0.0462]],

         [[-0.1769, -0.2252, -0.2734,  ...,  0.1485,  0.0181, -0.1123],
          [-0.3973, -0.0170,  0.3634,  ..., -0.0208, -0.2349, -0.4490],
          [ 0.2712,  0.1777,  0.0842,  ...,  0.0851, -0.0014, -0.0879],
          ...,
          [-0.0037, -0.1614, -0.3190,  ...,  0.1702, -0.0078, -0.1859],
          [ 0.0152, -0.1090, -0.2332,  ...,  0.0996,  0.1826,  0.2656],
          [ 0.2944,  0.1540,  0.0136,  ..., -0.3622, -0.3326, -0.3029]],

         [[ 0.2093,  0.1868,  0.1644,  ..., -0.4556, -0.0062,  0.4432],
          [ 0.1214,  0.2819,  0.4423,  ...,  0.1323,  0.0703,  0.0083],
          [ 0.1587,  0.1706,  0.1826,  ..., -0.3334, -0.0175,  0.2984],
          ...,
          [-0.1829, -0.2218, -0.2606,  ...,  0.1311,  0.1670,  0.2029],
          [ 0.4185,  0.2383,  0.0580,  ...,  0.2603,  0.2413,  0.2223],
          [ 0.3558,  0.1324, -0.0910,  ..., -0.4115, -0.4441, -0.4766]]],


        [[[ 0.1736,  0.2842,  0.3947,  ..., -0.1967, -0.2100, -0.2233],
          [-0.2884, -0.2482, -0.2081,  ...,  0.1247, -0.0249, -0.1746],
          [-0.2316, -0.2112, -0.1908,  ..., -0.2002,  0.0734,  0.3469],
          ...,
          [ 0.2620,  0.1463,  0.0307,  ..., -0.0875, -0.1723, -0.2571],
          [ 0.4749,  0.2619,  0.0490,  ...,  0.3700,  0.1184, -0.1333],
          [-0.2104, -0.0619,  0.0865,  ..., -0.0286, -0.1998, -0.3710]],

         [[ 0.0842, -0.0576, -0.1995,  ..., -0.0973, -0.0220,  0.0534],
          [ 0.1961, -0.1065, -0.4091,  ...,  0.2008, -0.1051, -0.4110],
          [-0.0821, -0.0721, -0.0621,  ..., -0.0820, -0.0065,  0.0690],
          ...,
          [-0.3891, -0.2568, -0.1246,  ..., -0.3746, -0.3259, -0.2771],
          [-0.1964, -0.0712,  0.0540,  ...,  0.1510, -0.0313, -0.2137],
          [-0.1870,  0.1351,  0.4573,  ..., -0.1220,  0.0290,  0.1801]],

         [[ 0.0795,  0.1646,  0.2496,  ..., -0.3629, -0.0711,  0.2206],
          [-0.1452, -0.2350, -0.3248,  ...,  0.1148, -0.1065, -0.3279],
          [ 0.1189,  0.2156,  0.3123,  ..., -0.2995, -0.1090,  0.0815],
          ...,
          [-0.3699, -0.1187,  0.1326,  ..., -0.3515, -0.1593,  0.0328],
          [ 0.3770,  0.4124,  0.4477,  ..., -0.3613, -0.3311, -0.3009],
          [-0.1061, -0.1615, -0.2169,  ..., -0.1978,  0.0948,  0.3875]],

         ...,

         [[ 0.3966,  0.0347, -0.3273,  ...,  0.4052, -0.0395, -0.4841],
          [ 0.1000, -0.0666, -0.2332,  ...,  0.3937,  0.3310,  0.2683],
          [ 0.2779,  0.1204, -0.0371,  ...,  0.3268,  0.0390, -0.2488],
          ...,
          [-0.3068, -0.1712, -0.0357,  ...,  0.2133,  0.2683,  0.3233],
          [-0.0741, -0.1964, -0.3186,  ..., -0.3629, -0.2512, -0.1395],
          [-0.0512,  0.2133,  0.4778,  ..., -0.0731, -0.0395, -0.0059]],

         [[ 0.1316,  0.2452,  0.3587,  ...,  0.1236, -0.1230, -0.3695],
          [-0.1730, -0.0507,  0.0716,  ..., -0.3238, -0.2775, -0.2313],
          [-0.2665, -0.2398, -0.2130,  ...,  0.3188, -0.0226, -0.3640],
          ...,
          [-0.3757, -0.0866,  0.2025,  ..., -0.2899, -0.0123,  0.2653],
          [-0.3166,  0.0482,  0.4129,  ...,  0.2440, -0.0054, -0.2547],
          [ 0.0470, -0.0661, -0.1791,  ...,  0.2014,  0.3252,  0.4490]],

         [[-0.0566,  0.0913,  0.2393,  ...,  0.3200,  0.3106,  0.3012],
          [ 0.0731,  0.2050,  0.3369,  ...,  0.3945,  0.1770, -0.0405],
          [-0.0682,  0.1360,  0.3402,  ..., -0.0326,  0.1326,  0.2978],
          ...,
          [-0.1535, -0.2916, -0.4298,  ...,  0.1602,  0.1204,  0.0806],
          [-0.2618, -0.1189,  0.0241,  ..., -0.2974, -0.0232,  0.2509],
          [ 0.1060,  0.0842,  0.0623,  ..., -0.4749, -0.1801,  0.1148]]],


        ...,


        [[[-0.1436,  0.0171,  0.1777,  ..., -0.1244, -0.0907, -0.0570],
          [ 0.3222,  0.3447,  0.3673,  ...,  0.3125,  0.2112,  0.1100],
          [-0.0533, -0.2042, -0.3551,  ..., -0.1542, -0.1423, -0.1305],
          ...,
          [ 0.1153, -0.0220, -0.1593,  ...,  0.2250,  0.1972,  0.1694],
          [ 0.3652, -0.0291, -0.4234,  ..., -0.1517, -0.2030, -0.2542],
          [ 0.0173,  0.2327,  0.4480,  ..., -0.2730,  0.0441,  0.3612]],

         [[-0.0975, -0.2662, -0.4349,  ...,  0.1610, -0.1505, -0.4619],
          [-0.4079, -0.0056,  0.3968,  ...,  0.1283,  0.1543,  0.1803],
          [ 0.1121,  0.1241,  0.1362,  ..., -0.0451, -0.1240, -0.2029],
          ...,
          [-0.3208, -0.1515,  0.0178,  ...,  0.3353,  0.0020, -0.3312],
          [-0.1676, -0.1857, -0.2038,  ...,  0.0079, -0.2386, -0.4850],
          [ 0.4599,  0.0189, -0.4221,  ..., -0.0689, -0.1188, -0.1688]],

         [[ 0.3961,  0.2281,  0.0601,  ...,  0.0777,  0.0080, -0.0618],
          [-0.3958, -0.1539,  0.0879,  ...,  0.4151,  0.0273, -0.3605],
          [ 0.3338,  0.2005,  0.0671,  ..., -0.3590, -0.2555, -0.1520],
          ...,
          [-0.1236, -0.2960, -0.4685,  ...,  0.3764,  0.1661, -0.0441],
          [-0.0020,  0.1832,  0.3685,  ..., -0.1685,  0.0759,  0.3203],
          [-0.1186, -0.1717, -0.2248,  ..., -0.3189,  0.0628,  0.4445]],

         ...,

         [[ 0.2606, -0.0500, -0.3606,  ..., -0.3954, -0.0584,  0.2786],
          [-0.1003,  0.1272,  0.3547,  ...,  0.0160, -0.1052, -0.2264],
          [ 0.3870,  0.1413, -0.1045,  ..., -0.0875, -0.1300, -0.1726],
          ...,
          [ 0.4591,  0.2078, -0.0434,  ...,  0.2982,  0.0846, -0.1290],
          [-0.3921, -0.0489,  0.2944,  ...,  0.3562,  0.2000,  0.0437],
          [-0.2218, -0.0860,  0.0499,  ...,  0.4136,  0.1451, -0.1235]],

         [[ 0.0046, -0.0286, -0.0619,  ..., -0.4348, -0.2744, -0.1141],
          [ 0.4059,  0.3448,  0.2837,  ...,  0.1038,  0.0215, -0.0607],
          [-0.1287, -0.1691, -0.2096,  ...,  0.1346,  0.2380,  0.3414],
          ...,
          [ 0.2056,  0.2557,  0.3059,  ...,  0.3664,  0.3511,  0.3359],
          [ 0.4147,  0.0162, -0.3824,  ..., -0.1773, -0.2695, -0.3617],
          [-0.2347, -0.1265, -0.0182,  ..., -0.0081, -0.0070, -0.0058]],

         [[ 0.3004, -0.0470, -0.3944,  ...,  0.3587,  0.0494, -0.2599],
          [-0.3049, -0.2145, -0.1241,  ...,  0.2420,  0.1268,  0.0116],
          [-0.3021, -0.2402, -0.1783,  ..., -0.4269, -0.3251, -0.2232],
          ...,
          [-0.3513, -0.2291, -0.1070,  ..., -0.2090, -0.1265, -0.0439],
          [-0.3363, -0.2810, -0.2258,  ..., -0.0639, -0.2005, -0.3371],
          [ 0.1887,  0.0635, -0.0617,  ..., -0.3096,  0.0408,  0.3913]]],


        [[[-0.2848, -0.0172,  0.2504,  ...,  0.1489,  0.2943,  0.4397],
          [-0.4113, -0.1758,  0.0596,  ..., -0.1733,  0.0027,  0.1787],
          [ 0.0561,  0.1593,  0.2624,  ...,  0.1145, -0.0585, -0.2316],
          ...,
          [ 0.2391,  0.2527,  0.2663,  ...,  0.3338,  0.2462,  0.1586],
          [ 0.2122,  0.2871,  0.3619,  ..., -0.0424, -0.0852, -0.1280],
          [-0.4845, -0.3750, -0.2655,  ..., -0.2732, -0.3036, -0.3339]],

         [[ 0.1464, -0.1248, -0.3959,  ..., -0.1659, -0.1091, -0.0523],
          [ 0.3643,  0.1038, -0.1567,  ...,  0.0107,  0.0541,  0.0976],
          [-0.1934, -0.0675,  0.0584,  ...,  0.3019,  0.1483, -0.0053],
          ...,
          [-0.3072,  0.0288,  0.3647,  ..., -0.1180,  0.1124,  0.3428],
          [ 0.0026, -0.0564, -0.1154,  ...,  0.0925,  0.1867,  0.2810],
          [-0.1614,  0.1415,  0.4445,  ..., -0.0892,  0.1304,  0.3500]],

         [[-0.4394, -0.2274, -0.0155,  ...,  0.0506, -0.0820, -0.2145],
          [ 0.0119,  0.1849,  0.3579,  ..., -0.2423, -0.2528, -0.2634],
          [-0.1431, -0.1620, -0.1808,  ...,  0.1708,  0.2258,  0.2808],
          ...,
          [ 0.1660,  0.2109,  0.2558,  ...,  0.3923,  0.3298,  0.2674],
          [ 0.0998,  0.0737,  0.0476,  ..., -0.2212, -0.1396, -0.0580],
          [ 0.1388, -0.1039, -0.3467,  ..., -0.3808,  0.0471,  0.4749]],

         ...,

         [[-0.4326, -0.1812,  0.0702,  ..., -0.1903, -0.2244, -0.2586],
          [-0.0814,  0.0947,  0.2708,  ...,  0.2797,  0.3082,  0.3366],
          [-0.1176, -0.1154, -0.1131,  ..., -0.3935, -0.2566, -0.1197],
          ...,
          [ 0.2023,  0.1410,  0.0797,  ...,  0.1899,  0.0166, -0.1566],
          [-0.2581, -0.1698, -0.0815,  ...,  0.3314, -0.0478, -0.4269],
          [ 0.0460, -0.0464, -0.1388,  ..., -0.4318, -0.1647,  0.1024]],

         [[-0.4333, -0.3028, -0.1723,  ..., -0.4030, -0.3941, -0.3852],
          [-0.3538, -0.3336, -0.3134,  ...,  0.1179,  0.1997,  0.2816],
          [ 0.0560, -0.0912, -0.2384,  ...,  0.0706, -0.1720, -0.4146],
          ...,
          [ 0.1384,  0.1604,  0.1824,  ...,  0.0888, -0.0255, -0.1399],
          [-0.4485, -0.2890, -0.1294,  ...,  0.0763, -0.0495, -0.1753],
          [ 0.1760, -0.0031, -0.1822,  ..., -0.2716, -0.2703, -0.2690]],

         [[-0.4580, -0.3850, -0.3120,  ..., -0.1414, -0.0989, -0.0564],
          [ 0.2635,  0.2679,  0.2723,  ...,  0.2361,  0.2106,  0.1852],
          [-0.2694,  0.0140,  0.2974,  ..., -0.3893, -0.0043,  0.3807],
          ...,
          [-0.4080, -0.0783,  0.2515,  ...,  0.2375,  0.1249,  0.0123],
          [-0.0892, -0.0134,  0.0624,  ..., -0.0645, -0.0738, -0.0832],
          [-0.3748, -0.3145, -0.2542,  ...,  0.2258,  0.2928,  0.3598]]],


        [[[-0.0359,  0.1974,  0.4307,  ..., -0.4693, -0.3616, -0.2539],
          [-0.3390, -0.1608,  0.0174,  ...,  0.2840,  0.1814,  0.0788],
          [ 0.3486,  0.2917,  0.2348,  ..., -0.3830, -0.0746,  0.2337],
          ...,
          [ 0.4072,  0.0884, -0.2304,  ..., -0.1586,  0.0446,  0.2477],
          [-0.0161,  0.0162,  0.0485,  ..., -0.0406, -0.2265, -0.4125],
          [ 0.3695,  0.0214, -0.3267,  ...,  0.4745,  0.0186, -0.4373]],

         [[-0.4865, -0.1895,  0.1075,  ...,  0.1080, -0.1275, -0.3629],
          [-0.2696, -0.1146,  0.0404,  ..., -0.3657, -0.0224,  0.3209],
          [-0.1827, -0.0716,  0.0395,  ...,  0.4088,  0.1956, -0.0176],
          ...,
          [-0.2922,  0.0091,  0.3103,  ...,  0.0757, -0.1230, -0.3216],
          [-0.1730, -0.0878, -0.0025,  ...,  0.0396, -0.0947, -0.2291],
          [-0.2606, -0.1524, -0.0441,  ..., -0.1027,  0.1897,  0.4821]],

         [[-0.0062,  0.0749,  0.1560,  ..., -0.2020, -0.0378,  0.1264],
          [-0.1083, -0.0918, -0.0754,  ...,  0.4708,  0.3191,  0.1675],
          [-0.0237, -0.0366, -0.0494,  ..., -0.3009,  0.0333,  0.3674],
          ...,
          [ 0.3625,  0.2386,  0.1147,  ..., -0.3582, -0.1950, -0.0318],
          [ 0.1735,  0.0842, -0.0050,  ...,  0.3217,  0.3512,  0.3808],
          [-0.0619,  0.1675,  0.3969,  ...,  0.1336, -0.1616, -0.4569]],

         ...,

         [[-0.1602, -0.2137, -0.2671,  ..., -0.0538, -0.0655, -0.0773],
          [ 0.2613,  0.3224,  0.3835,  ..., -0.1762, -0.1693, -0.1624],
          [ 0.3280,  0.0260, -0.2760,  ..., -0.0097, -0.0734, -0.1371],
          ...,
          [ 0.0990,  0.1686,  0.2382,  ...,  0.0568,  0.2149,  0.3731],
          [ 0.0962,  0.2756,  0.4550,  ...,  0.0872,  0.0749,  0.0625],
          [-0.4740, -0.1165,  0.2410,  ...,  0.3798,  0.1771, -0.0255]],

         [[-0.1107, -0.1698, -0.2288,  ..., -0.3575, -0.0038,  0.3500],
          [-0.1330,  0.1181,  0.3691,  ...,  0.2620,  0.3125,  0.3629],
          [-0.3256, -0.2826, -0.2396,  ...,  0.2190,  0.0477, -0.1235],
          ...,
          [ 0.0102,  0.0748,  0.1395,  ..., -0.2866, -0.2634, -0.2403],
          [-0.2848, -0.2916, -0.2984,  ..., -0.0452, -0.0829, -0.1205],
          [ 0.3943,  0.0341, -0.3261,  ...,  0.3846,  0.0243, -0.3361]],

         [[ 0.0399,  0.0731,  0.1062,  ...,  0.4769,  0.1636, -0.1496],
          [-0.1370, -0.1748, -0.2126,  ...,  0.3273,  0.0746, -0.1781],
          [-0.2312, -0.0173,  0.1967,  ..., -0.3622, -0.1732,  0.0157],
          ...,
          [ 0.0736, -0.0772, -0.2280,  ..., -0.3376, -0.2640, -0.1904],
          [-0.4075,  0.0239,  0.4553,  ..., -0.2066, -0.2279, -0.2492],
          [ 0.0414, -0.0555, -0.1524,  ..., -0.1186, -0.2649, -0.4111]]]])
DESIRED: (shape=torch.Size([16, 40, 64, 48]), dtype=torch.float32)
tensor([[[[ 2.8861e-01,  2.2334e-01,  9.2793e-02,  ..., -1.4260e-01, -2.5053e-01, -3.0450e-01],
          [-1.9835e-01, -9.5328e-02,  1.1071e-01,  ...,  1.6329e-01,  2.9896e-01,  3.6680e-01],
          [-2.8083e-01, -9.2247e-02,  2.8492e-01,  ..., -1.5932e-01, -1.0970e-01, -8.4888e-02],
          ...,
          [ 2.0099e-01,  1.7187e-01,  1.1362e-01,  ...,  3.2801e-01,  3.8710e-01,  4.1664e-01],
          [ 2.6006e-01,  2.1821e-01,  1.3451e-01,  ...,  2.5003e-02,  1.0924e-01,  1.5136e-01],
          [ 2.3634e-01,  2.2407e-01,  1.9953e-01,  ..., -1.4360e-01, -3.6146e-01, -4.7040e-01]],

         [[ 4.1830e-01,  3.5824e-01,  2.3812e-01,  ...,  3.8126e-01,  2.3484e-01,  1.6163e-01],
          [-3.0074e-01, -2.6777e-01, -2.0182e-01,  ...,  2.2437e-01,  5.5353e-02, -2.9158e-02],
          [ 1.6294e-01,  5.5683e-02, -1.5882e-01,  ...,  8.6260e-02,  2.3375e-01,  3.0750e-01],
          ...,
          [ 1.0540e-01,  1.3375e-01,  1.9044e-01,  ..., -6.6943e-02, -2.7345e-01, -3.7670e-01],
          [ 1.6533e-03,  5.2925e-03,  1.2571e-02,  ..., -1.8119e-01, -2.2386e-01, -2.4520e-01],
          [-3.8983e-01, -3.0849e-01, -1.4580e-01,  ..., -2.3254e-01,  1.4851e-02,  1.3855e-01]],

         [[ 3.3654e-01,  2.5639e-01,  9.6091e-02,  ..., -2.6281e-01, -6.7230e-02,  3.0558e-02],
          [ 4.2140e-01,  2.6156e-01, -5.8123e-02,  ..., -8.9387e-04, -8.4448e-02, -1.2623e-01],
          [-2.1491e-01, -2.3170e-01, -2.6527e-01,  ..., -1.5031e-01,  1.2714e-01,  2.6587e-01],
          ...,
          [-3.9775e-02, -7.4196e-02, -1.4304e-01,  ...,  2.4963e-01,  1.1860e-01,  5.3082e-02],
          [-2.8626e-01, -2.3163e-01, -1.2237e-01,  ...,  1.0523e-01,  2.4830e-01,  3.1984e-01],
          [ 1.4852e-01,  1.5728e-02, -2.4985e-01,  ...,  1.9443e-01,  2.6364e-01,  2.9825e-01]],

         ...,

         [[-1.5795e-01, -2.0798e-01, -3.0804e-01,  ...,  4.4885e-01,  4.2065e-01,  4.0655e-01],
          [-2.0994e-01, -1.5871e-01, -5.6261e-02,  ...,  3.5465e-01,  4.1596e-01,  4.4661e-01],
          [ 1.5291e-01,  9.3546e-02, -2.5179e-02,  ..., -5.0898e-02, -7.1061e-02, -8.1142e-02],
          ...,
          [-1.9676e-01, -1.2516e-01,  1.8036e-02,  ...,  1.7000e-01,  3.2505e-01,  4.0257e-01],
          [ 4.4687e-01,  2.7755e-01, -6.1103e-02,  ...,  6.4258e-03,  3.5325e-02,  4.9774e-02],
          [-1.0703e-01, -9.8355e-03,  1.8456e-01,  ...,  1.7631e-01,  8.3506e-02,  3.7106e-02]],

         [[-4.1108e-01, -2.4006e-01,  1.0199e-01,  ..., -4.5618e-02,  2.7125e-01,  4.2968e-01],
          [-2.6010e-01, -2.7617e-01, -3.0830e-01,  ...,  1.3451e-01, -2.6467e-01, -4.6426e-01],
          [-3.4726e-01, -2.4114e-01, -2.8893e-02,  ...,  9.7262e-02,  4.9209e-03, -4.1250e-02],
          ...,
          [ 4.5234e-01,  2.4995e-01, -1.5483e-01,  ...,  2.7296e-01,  1.5106e-01,  9.0106e-02],
          [-2.7322e-03, -8.5163e-02, -2.5003e-01,  ..., -1.0848e-01,  9.1477e-02,  1.9146e-01],
          [ 1.1837e-01, -1.3654e-02, -2.7769e-01,  ...,  1.4630e-01,  1.4015e-01,  1.3708e-01]],

         [[ 3.2077e-01,  3.1838e-01,  3.1360e-01,  ..., -2.4581e-01, -2.0207e-01, -1.8020e-01],
          [-3.9787e-02, -6.8269e-02, -1.2523e-01,  ...,  2.3328e-01,  3.5109e-01,  4.1000e-01],
          [-2.0567e-01, -9.2993e-02,  1.3236e-01,  ...,  3.7346e-01,  3.0879e-01,  2.7646e-01],
          ...,
          [-1.7331e-01, -1.8166e-01, -1.9836e-01,  ..., -5.9869e-02, -1.2915e-01, -1.6379e-01],
          [ 1.7116e-01,  6.5913e-02, -1.4459e-01,  ..., -2.9684e-01, -2.5446e-03,  1.4460e-01],
          [-7.8299e-02, -1.1407e-01, -1.8561e-01,  ...,  2.2478e-01,  3.1487e-01,  3.5992e-01]]],


        [[[-1.5058e-01, -1.1849e-02,  2.6561e-01,  ..., -1.2120e-01, -2.3657e-01, -2.9425e-01],
          [-2.9311e-01, -2.5111e-01, -1.6712e-01,  ...,  3.3444e-01,  2.9044e-01,  2.6844e-01],
          [-3.3691e-01, -1.7019e-01,  1.6325e-01,  ..., -2.9103e-01, -2.7861e-01, -2.7239e-01],
          ...,
          [-1.9689e-01, -1.8090e-01, -1.4893e-01,  ...,  2.9815e-01,  3.7714e-01,  4.1663e-01],
          [ 3.7771e-01,  1.9005e-01, -1.8529e-01,  ..., -1.7159e-01, -1.9641e-01, -2.0882e-01],
          [-3.0465e-01, -2.6840e-01, -1.9588e-01,  ...,  2.4362e-01, -2.3493e-01, -4.7421e-01]],

         [[ 2.4026e-01,  1.5292e-01, -2.1740e-02,  ..., -1.7789e-01, -7.9489e-03,  7.7023e-02],
          [-9.6216e-02, -3.2692e-02,  9.4357e-02,  ...,  2.2784e-01,  8.1743e-03, -1.0166e-01],
          [-1.2190e-01, -1.6138e-01, -2.4032e-01,  ..., -3.7760e-01, -2.7588e-01, -2.2502e-01],
          ...,
          [ 6.1106e-02, -1.1666e-02, -1.5721e-01,  ..., -8.5987e-02,  1.1242e-01,  2.1163e-01],
          [ 2.1669e-01,  2.6508e-01,  3.6184e-01,  ...,  1.5235e-01, -2.0989e-01, -3.9101e-01],
          [ 1.7754e-01,  7.8915e-02, -1.1834e-01,  ...,  2.9771e-01,  9.3303e-02, -8.8980e-03]],

         [[-1.3956e-01, -3.4502e-02,  1.7561e-01,  ..., -3.3774e-01, -4.2951e-01, -4.7540e-01],
          [-7.7379e-03,  9.0646e-02,  2.8741e-01,  ..., -1.3183e-04, -8.2303e-02, -1.2339e-01],
          [ 1.4676e-01,  1.3836e-01,  1.2155e-01,  ..., -1.5668e-01, -2.0729e-01, -2.3260e-01],
          ...,
          [-4.0201e-01, -2.2258e-01,  1.3629e-01,  ..., -1.4977e-03,  1.8019e-01,  2.7103e-01],
          [-3.6597e-01, -2.1245e-01,  9.4582e-02,  ...,  9.5679e-02, -2.0612e-01, -3.5701e-01],
          [ 1.3224e-01,  6.7995e-02, -6.0497e-02,  ...,  2.7043e-01,  2.2918e-01,  2.0855e-01]],

         ...,

         [[ 9.0760e-02,  1.7987e-02, -1.2756e-01,  ..., -2.7110e-01, -8.8325e-02,  3.0620e-03],
          [ 7.7042e-02,  5.7840e-03, -1.3673e-01,  ..., -2.7224e-01, -3.4779e-01, -3.8556e-01],
          [-2.6937e-01, -2.5623e-01, -2.2997e-01,  ..., -6.5166e-03,  2.8369e-01,  4.2879e-01],
          ...,
          [-1.1467e-01, -1.1968e-01, -1.2972e-01,  ...,  3.2402e-01,  3.1131e-01,  3.0496e-01],
          [ 1.9326e-01,  2.1494e-01,  2.5830e-01,  ..., -1.7093e-01,  9.4102e-02,  2.2662e-01],
          [-3.4898e-01, -3.0712e-01, -2.2340e-01,  ..., -3.0242e-01, -1.0786e-01, -1.0581e-02]],

         [[ 6.0733e-02, -5.1884e-02, -2.7712e-01,  ..., -4.3360e-01, -3.4274e-01, -2.9730e-01],
          [ 2.8630e-01,  2.7354e-01,  2.4801e-01,  ..., -1.9135e-01, -5.6996e-02,  1.0180e-02],
          [-1.1201e-01, -8.9066e-02, -4.3169e-02,  ..., -1.3338e-01,  1.5353e-01,  2.9699e-01],
          ...,
          [-1.2671e-01, -1.5451e-01, -2.1009e-01,  ...,  1.2982e-01, -5.0094e-02, -1.4005e-01],
          [-3.3918e-02, -9.5501e-02, -2.1867e-01,  ...,  7.3921e-02, -1.5153e-01, -2.6425e-01],
          [ 2.5684e-01,  1.8794e-01,  5.0154e-02,  ...,  9.9412e-02,  1.8298e-01,  2.2476e-01]],

         [[ 4.7010e-01,  2.2904e-01, -2.5307e-01,  ...,  2.8269e-01, -2.0454e-02, -1.7203e-01],
          [-2.3865e-01, -1.0281e-01,  1.6888e-01,  ..., -1.3661e-01,  2.0485e-02,  9.9031e-02],
          [ 3.4892e-03, -8.6960e-03, -3.3066e-02,  ...,  3.2381e-01,  1.6879e-01,  9.1285e-02],
          ...,
          [ 1.8502e-01,  1.2814e-01,  1.4400e-02,  ..., -2.2318e-01, -1.1806e-01, -6.5506e-02],
          [-3.1677e-01, -2.4195e-01, -9.2306e-02,  ..., -2.1595e-01, -1.6289e-01, -1.3636e-01],
          [-1.5424e-01, -2.2989e-01, -3.8119e-01,  ...,  6.1166e-02,  1.0445e-01,  1.2609e-01]]],


        [[[-2.2336e-01, -2.1252e-01, -1.9084e-01,  ..., -1.8274e-01, -2.1662e-01, -2.3356e-01],
          [-3.4401e-01, -1.5058e-01,  2.3629e-01,  ...,  3.8963e-01,  3.3879e-01,  3.1338e-01],
          [ 3.1650e-01,  2.0167e-01, -2.7978e-02,  ...,  1.0237e-01,  4.4043e-02,  1.4878e-02],
          ...,
          [-1.6933e-01, -5.1785e-02,  1.8330e-01,  ...,  2.3099e-01,  1.1921e-01,  6.3321e-02],
          [-6.3334e-02, -4.5195e-02, -8.9184e-03,  ..., -2.0052e-01, -1.0942e-01, -6.3870e-02],
          [-4.5692e-01, -3.4151e-01, -1.1069e-01,  ..., -1.7836e-01,  2.1060e-02,  1.2077e-01]],

         [[-3.9817e-01, -2.2809e-01,  1.1208e-01,  ..., -4.0661e-01, -3.5281e-01, -3.2591e-01],
          [ 1.5727e-01,  6.7222e-02, -1.1288e-01,  ..., -1.4250e-01,  3.5521e-02,  1.2453e-01],
          [ 2.6602e-01,  2.5837e-01,  2.4305e-01,  ...,  1.2690e-01,  2.1528e-01,  2.5948e-01],
          ...,
          [-3.6579e-01, -1.8322e-01,  1.8194e-01,  ...,  2.3111e-01, -1.0666e-01, -2.7554e-01],
          [ 3.9722e-01,  2.1087e-01, -1.6184e-01,  ...,  2.2609e-01, -9.3009e-02, -2.5256e-01],
          [ 4.3501e-01,  3.2425e-01,  1.0274e-01,  ...,  1.7220e-01,  7.1557e-02,  2.1235e-02]],

         [[-4.4037e-02,  3.1268e-02,  1.8188e-01,  ...,  4.8988e-02, -3.3838e-02, -7.5251e-02],
          [ 3.4332e-01,  1.5396e-01, -2.2475e-01,  ..., -1.5600e-01,  1.9478e-01,  3.7018e-01],
          [-9.3701e-02,  1.6379e-02,  2.3654e-01,  ...,  4.7242e-02,  1.0229e-01,  1.2981e-01],
          ...,
          [-2.3834e-01, -1.6606e-01, -2.1483e-02,  ..., -5.0343e-02, -1.9005e-02, -3.3364e-03],
          [-3.3480e-01, -2.8992e-01, -2.0018e-01,  ...,  3.8463e-01,  3.0292e-01,  2.6206e-01],
          [ 2.5501e-01,  1.6995e-01, -1.5397e-04,  ...,  9.3668e-02, -1.9646e-02, -7.6303e-02]],

         ...,

         [[-7.8895e-02,  5.4995e-02,  3.2277e-01,  ..., -1.3465e-01, -2.5577e-01, -3.1633e-01],
          [-2.5113e-02, -1.3526e-02,  9.6490e-03,  ..., -3.7147e-02,  2.5994e-01,  4.0848e-01],
          [-3.2306e-01, -2.1684e-01, -4.4055e-03,  ...,  1.2098e-01,  1.6223e-01,  1.8285e-01],
          ...,
          [-2.3441e-01, -2.9213e-01, -4.0756e-01,  ..., -3.3204e-02,  1.2217e-01,  1.9985e-01],
          [ 4.2758e-02, -9.1195e-03, -1.1288e-01,  ..., -4.2599e-02, -9.6615e-02, -1.2362e-01],
          [ 2.4883e-02,  9.7520e-02,  2.4279e-01,  ...,  1.5845e-01,  1.6753e-01,  1.7207e-01]],

         [[-4.0250e-01, -2.5317e-01,  4.5489e-02,  ...,  1.9621e-02, -1.4377e-01, -2.2547e-01],
          [ 4.1451e-02, -3.2949e-02, -1.8175e-01,  ...,  6.5558e-02, -2.1612e-01, -3.5697e-01],
          [ 3.4247e-01,  1.5079e-01, -2.3256e-01,  ..., -3.6399e-01, -2.3887e-01, -1.7631e-01],
          ...,
          [ 1.1176e-01,  9.3333e-02,  5.6470e-02,  ...,  6.2388e-03,  2.1318e-01,  3.1665e-01],
          [ 5.8229e-02,  1.0710e-01,  2.0485e-01,  ..., -1.1614e-01,  3.7004e-02,  1.1358e-01],
          [-1.5772e-01, -7.8160e-03,  2.9199e-01,  ...,  2.4490e-01,  2.9680e-01,  3.2275e-01]],

         [[-6.8269e-04,  1.4783e-02,  4.5715e-02,  ..., -2.2899e-01, -2.3455e-01, -2.3733e-01],
          [ 3.0774e-01,  3.2139e-01,  3.4870e-01,  ..., -3.9069e-02, -7.0251e-02, -8.5842e-02],
          [-2.4930e-01, -1.3058e-01,  1.0687e-01,  ...,  3.1786e-02, -4.7112e-03, -2.2960e-02],
          ...,
          [-4.2450e-01, -3.8709e-01, -3.1227e-01,  ..., -2.8381e-01, -3.6248e-02,  8.7533e-02],
          [ 2.0776e-01,  1.9358e-01,  1.6522e-01,  ..., -1.5141e-01, -1.3857e-01, -1.3215e-01],
          [ 1.4025e-02,  6.1389e-02,  1.5612e-01,  ...,  2.5746e-01, -1.3446e-01, -3.3043e-01]]],


        ...,


        [[[-1.6022e-01, -2.4068e-02,  2.4824e-01,  ...,  1.9247e-01,  2.1054e-01,  2.1958e-01],
          [-9.7635e-02, -1.1357e-01, -1.4545e-01,  ..., -2.0112e-01, -2.6105e-01, -2.9102e-01],
          [ 8.5282e-02,  4.3001e-02, -4.1561e-02,  ...,  1.9442e-01,  2.6288e-01,  2.9711e-01],
          ...,
          [ 4.1145e-02, -2.5381e-02, -1.5843e-01,  ...,  1.8033e-01,  2.0231e-01,  2.1330e-01],
          [ 3.9283e-01,  1.9368e-01, -2.0463e-01,  ...,  9.7518e-03,  2.5572e-01,  3.7870e-01],
          [-2.4693e-01, -1.8224e-01, -5.2860e-02,  ..., -6.9894e-03,  2.1478e-01,  3.2566e-01]],

         [[-4.6976e-01, -3.4356e-01, -9.1172e-02,  ..., -2.4243e-01,  1.4786e-02,  1.4339e-01],
          [-1.0418e-01, -1.4965e-02,  1.6346e-01,  ...,  7.9537e-02,  1.5383e-01,  1.9098e-01],
          [-8.1996e-02, -1.1762e-01, -1.8888e-01,  ...,  1.4941e-01, -2.9948e-02, -1.1963e-01],
          ...,
          [ 5.8416e-02,  1.8916e-02, -6.0084e-02,  ...,  9.9858e-02, -1.1795e-01, -2.2686e-01],
          [-1.0083e-01, -1.4952e-01, -2.4692e-01,  ...,  6.5697e-02, -1.8545e-01, -3.1103e-01],
          [-4.1756e-01, -2.2969e-01,  1.4607e-01,  ..., -2.2187e-01, -1.9724e-01, -1.8492e-01]],

         [[ 1.8176e-01,  5.8607e-02, -1.8770e-01,  ..., -1.9891e-01, -1.3141e-02,  7.9742e-02],
          [ 2.3801e-01,  1.2058e-01, -1.1430e-01,  ...,  2.1960e-01,  1.7423e-01,  1.5154e-01],
          [ 1.6327e-01,  1.5020e-01,  1.2407e-01,  ..., -1.0844e-03, -1.1357e-01, -1.6981e-01],
          ...,
          [-1.6455e-02, -5.0686e-02, -1.1915e-01,  ..., -3.7157e-01, -2.4137e-01, -1.7627e-01],
          [ 3.6625e-01,  2.9605e-01,  1.5567e-01,  ..., -3.0666e-01, -2.8234e-01, -2.7018e-01],
          [ 4.7077e-01,  2.3903e-01, -2.2445e-01,  ...,  1.5006e-01,  1.9967e-01,  2.2448e-01]],

         ...,

         [[-1.8983e-01, -2.1201e-01, -2.5638e-01,  ...,  2.6951e-01,  3.0322e-01,  3.2008e-01],
          [ 3.1365e-01,  2.9228e-01,  2.4954e-01,  ..., -2.3398e-01, -2.6214e-01, -2.7622e-01],
          [ 2.3241e-01,  2.1301e-01,  1.7420e-01,  ..., -1.4577e-01, -2.0871e-01, -2.4018e-01],
          ...,
          [ 4.0690e-02, -8.6444e-03, -1.0731e-01,  ..., -2.8292e-01, -2.5036e-01, -2.3408e-01],
          [-2.8066e-01, -2.4382e-01, -1.7014e-01,  ..., -1.1431e-02,  7.7654e-02,  1.2220e-01],
          [ 2.5855e-01,  1.0019e-01, -2.1654e-01,  ..., -3.2002e-01, -1.3647e-01, -4.4692e-02]],

         [[ 1.9121e-01,  2.0643e-01,  2.3687e-01,  ..., -1.6334e-01, -8.1082e-02, -3.9953e-02],
          [-9.5914e-02, -1.3605e-01, -2.1633e-01,  ..., -1.5500e-02,  5.7290e-02,  9.3685e-02],
          [ 1.0162e-01,  1.1812e-01,  1.5112e-01,  ..., -2.9310e-01, -2.3418e-01, -2.0472e-01],
          ...,
          [-2.6151e-01, -2.3695e-01, -1.8781e-01,  ..., -1.0686e-01, -2.3525e-01, -2.9945e-01],
          [-1.6577e-02, -8.4919e-02, -2.2160e-01,  ..., -1.1225e-01, -2.8551e-01, -3.7215e-01],
          [-1.4404e-01, -7.5905e-02,  6.0361e-02,  ..., -3.2669e-03, -1.0997e-01, -1.6332e-01]],

         [[-1.2509e-01, -1.9670e-01, -3.3992e-01,  ..., -4.2947e-01, -3.9809e-01, -3.8241e-01],
          [ 2.8479e-01,  1.2774e-01, -1.8637e-01,  ...,  1.6026e-01, -1.7679e-01, -3.4532e-01],
          [-3.2830e-01, -2.5627e-01, -1.1221e-01,  ...,  3.2547e-02,  2.8921e-01,  4.1755e-01],
          ...,
          [ 1.3207e-01,  4.4403e-02, -1.3093e-01,  ...,  2.8281e-02,  1.5106e-01,  2.1245e-01],
          [ 1.9257e-01,  7.5169e-02, -1.5963e-01,  ...,  1.1428e-01,  2.6013e-01,  3.3306e-01],
          [-6.7370e-02, -6.4656e-03,  1.1534e-01,  ..., -2.2347e-02,  1.5873e-02,  3.4983e-02]]],


        [[[-2.0919e-01, -1.3472e-01,  1.4215e-02,  ...,  1.1493e-01,  2.4443e-01,  3.0918e-01],
          [ 2.6746e-02, -7.4405e-02, -2.7671e-01,  ..., -2.0637e-02, -1.9255e-01, -2.7851e-01],
          [ 2.3988e-01,  1.1794e-01, -1.2594e-01,  ...,  1.8535e-01, -2.2155e-02, -1.2591e-01],
          ...,
          [-1.8488e-01, -1.0963e-01,  4.0877e-02,  ..., -1.2316e-01, -2.6536e-01, -3.3646e-01],
          [ 1.6384e-01,  1.0218e-01, -2.1145e-02,  ...,  2.8090e-01,  3.7621e-01,  4.2387e-01],
          [-3.5634e-01, -3.3988e-01, -3.0696e-01,  ..., -3.8055e-01, -2.7648e-01, -2.2445e-01]],

         [[-2.6777e-01, -1.2619e-01,  1.5699e-01,  ..., -6.6856e-02, -1.7289e-01, -2.2590e-01],
          [-2.3888e-01, -2.5815e-01, -2.9669e-01,  ...,  3.4786e-01,  1.2980e-01,  2.0772e-02],
          [-2.0799e-01, -1.7690e-01, -1.1472e-01,  ...,  1.3791e-01,  2.8306e-01,  3.5563e-01],
          ...,
          [ 2.2021e-01,  2.5721e-01,  3.3121e-01,  ...,  1.9947e-01,  1.3938e-01,  1.0934e-01],
          [ 1.5961e-01,  1.2170e-01,  4.5891e-02,  ..., -2.9476e-01, -3.0837e-01, -3.1518e-01],
          [ 4.6491e-01,  3.7310e-01,  1.8949e-01,  ..., -4.6603e-02,  1.1000e-01,  1.8831e-01]],

         [[-1.1343e-01, -2.0894e-01, -3.9995e-01,  ..., -1.0559e-01,  1.0266e-01,  2.0678e-01],
          [-1.4892e-01, -1.7463e-01, -2.2605e-01,  ..., -1.5300e-01, -3.2405e-01, -4.0958e-01],
          [ 4.7751e-01,  2.8338e-01, -1.0488e-01,  ..., -3.1037e-01, -2.1160e-01, -1.6221e-01],
          ...,
          [-4.5432e-01, -4.1636e-01, -3.4044e-01,  ...,  1.2591e-01,  2.3487e-02, -2.7723e-02],
          [ 3.4612e-01,  3.0294e-01,  2.1660e-01,  ...,  1.6330e-01,  1.1402e-01,  8.9381e-02],
          [ 2.7710e-01,  3.2461e-01,  4.1964e-01,  ...,  1.9809e-01, -6.9136e-02, -2.0275e-01]],

         ...,

         [[ 4.2791e-01,  4.1595e-01,  3.9202e-01,  ...,  3.0869e-01,  1.7134e-01,  1.0266e-01],
          [-1.7727e-01, -5.7771e-02,  1.8123e-01,  ...,  1.8130e-01,  3.3837e-01,  4.1691e-01],
          [-6.7073e-03, -7.5819e-02, -2.1404e-01,  ..., -1.6034e-01,  9.2370e-02,  2.1872e-01],
          ...,
          [-9.9419e-02, -1.1226e-01, -1.3793e-01,  ..., -3.3367e-01, -1.9710e-01, -1.2882e-01],
          [-1.0915e-01, -6.7372e-02,  1.6186e-02,  ..., -1.6320e-01,  1.2859e-01,  2.7448e-01],
          [ 1.8228e-01,  8.1194e-02, -1.2098e-01,  ..., -1.1534e-01,  2.6206e-01,  4.5076e-01]],

         [[-2.1126e-01, -2.1901e-01, -2.3452e-01,  ..., -2.0233e-01, -2.0699e-01, -2.0933e-01],
          [-2.1734e-01, -2.3447e-01, -2.6872e-01,  ..., -2.4196e-01, -3.3844e-01, -3.8668e-01],
          [-3.4956e-02, -1.1035e-01, -2.6113e-01,  ..., -2.2651e-01,  1.0610e-01,  2.7240e-01],
          ...,
          [ 1.7106e-01,  6.7594e-02, -1.3934e-01,  ..., -2.0732e-01, -3.0535e-01, -3.5437e-01],
          [-3.4376e-01, -2.7433e-01, -1.3547e-01,  ..., -2.4843e-02, -2.4908e-01, -3.6119e-01],
          [ 2.3281e-01,  2.7893e-01,  3.7116e-01,  ...,  7.3135e-02,  3.4528e-02,  1.5225e-02]],

         [[-1.9400e-01, -1.5479e-01, -7.6350e-02,  ..., -1.0372e-01, -1.0591e-01, -1.0700e-01],
          [ 2.8017e-01,  2.0928e-01,  6.7510e-02,  ..., -3.5587e-01, -3.0502e-01, -2.7960e-01],
          [ 1.4892e-01,  1.3383e-01,  1.0366e-01,  ...,  1.4615e-01,  1.1040e-01,  9.2534e-02],
          ...,
          [-1.5183e-01, -8.3832e-02,  5.2164e-02,  ...,  3.2702e-01,  1.3602e-01,  4.0514e-02],
          [-4.0713e-01, -4.0599e-01, -4.0371e-01,  ..., -1.1237e-01,  1.3417e-01,  2.5744e-01],
          [-1.7922e-01, -2.6811e-02,  2.7800e-01,  ..., -1.4636e-01, -2.1972e-01, -2.5640e-01]]],


        [[[ 2.6206e-01,  2.4881e-01,  2.2232e-01,  ...,  2.4350e-01, -1.8625e-01, -4.0113e-01],
          [-1.0310e-01, -1.6667e-01, -2.9382e-01,  ...,  5.7431e-02,  7.2687e-02,  8.0315e-02],
          [ 1.3370e-01,  1.2809e-01,  1.1688e-01,  ..., -1.2158e-01,  3.3930e-02,  1.1168e-01],
          ...,
          [ 1.6837e-01,  1.1827e-01,  1.8070e-02,  ...,  2.1916e-01,  2.5825e-01,  2.7779e-01],
          [-1.0111e-01, -1.1452e-01, -1.4135e-01,  ...,  8.4667e-03,  3.7518e-02,  5.2044e-02],
          [ 2.3092e-01,  1.6758e-01,  4.0901e-02,  ...,  1.5428e-01,  1.4708e-01,  1.4348e-01]],

         [[ 4.0970e-01,  1.9558e-01, -2.3264e-01,  ..., -1.5853e-02, -2.5682e-01, -3.7731e-01],
          [ 2.0697e-01,  1.0150e-01, -1.0944e-01,  ...,  1.3293e-01,  1.7464e-01,  1.9549e-01],
          [-2.6863e-01, -1.7817e-01,  2.7562e-03,  ..., -1.9773e-01,  6.0156e-02,  1.8910e-01],
          ...,
          [ 5.6607e-02,  9.3456e-02,  1.6715e-01,  ..., -9.6440e-02,  1.4906e-02,  7.0579e-02],
          [ 2.7172e-01,  1.6602e-01, -4.5391e-02,  ..., -4.1238e-01, -3.1388e-01, -2.6464e-01],
          [ 2.8257e-01,  2.2057e-01,  9.6555e-02,  ..., -1.9012e-01,  7.7414e-02,  2.1118e-01]],

         [[ 3.7516e-01,  2.0565e-01, -1.3335e-01,  ...,  3.6220e-01,  1.9102e-01,  1.0544e-01],
          [ 2.8234e-01,  2.9783e-01,  3.2880e-01,  ..., -1.8517e-01, -3.3295e-01, -4.0683e-01],
          [-3.0671e-01, -1.9265e-01,  3.5492e-02,  ..., -3.4101e-01, -1.1196e-01,  2.5723e-03],
          ...,
          [-3.1809e-01, -2.7454e-01, -1.8744e-01,  ...,  2.0307e-01,  1.3330e-01,  9.8418e-02],
          [-8.8220e-02, -9.2390e-02, -1.0073e-01,  ..., -2.4278e-01,  1.1467e-01,  2.9340e-01],
          [ 4.2650e-01,  2.0488e-01, -2.3834e-01,  ...,  2.7492e-01,  3.2017e-01,  3.4280e-01]],

         ...,

         [[ 1.6597e-01,  1.0619e-01, -1.3385e-02,  ..., -1.3361e-02,  1.1658e-01,  1.8155e-01],
          [-1.7251e-01, -1.8450e-01, -2.0847e-01,  ...,  4.9854e-02, -1.2034e-01, -2.0544e-01],
          [ 4.1328e-01,  4.0505e-01,  3.8857e-01,  ..., -4.2588e-01, -4.5573e-01, -4.7065e-01],
          ...,
          [-3.5732e-03,  5.3030e-02,  1.6624e-01,  ...,  1.3922e-01, -4.2346e-02, -1.3313e-01],
          [-2.0280e-01, -2.0129e-01, -1.9827e-01,  ..., -2.1244e-01,  1.5039e-01,  3.3181e-01],
          [ 4.0724e-01,  2.3601e-01, -1.0645e-01,  ..., -1.1499e-02, -2.6802e-02, -3.4454e-02]],

         [[ 2.6575e-01,  2.0799e-01,  9.2482e-02,  ..., -1.6917e-01, -1.1580e-01, -8.9116e-02],
          [-4.1280e-01, -1.8872e-01,  2.5943e-01,  ..., -2.9197e-02, -2.3140e-01, -3.3250e-01],
          [-4.0726e-01, -1.9538e-01,  2.2837e-01,  ...,  1.0372e-01,  1.5726e-01,  1.8402e-01],
          ...,
          [-4.2102e-01, -2.0618e-01,  2.2349e-01,  ...,  2.7920e-01,  3.5820e-01,  3.9771e-01],
          [ 3.6366e-01,  2.4651e-01,  1.2228e-02,  ..., -3.1284e-01, -3.0383e-01, -2.9933e-01],
          [-1.4099e-01, -1.6132e-01, -2.0199e-01,  ..., -3.6223e-01, -4.4850e-01, -4.9163e-01]],

         [[-3.7919e-01, -1.7879e-01,  2.2201e-01,  ...,  1.8323e-01,  8.4749e-02,  3.5506e-02],
          [-5.6667e-02, -9.5135e-02, -1.7207e-01,  ...,  2.4808e-02,  3.3101e-02,  3.7248e-02],
          [ 2.5028e-01,  1.5706e-01, -2.9379e-02,  ..., -3.4827e-01, -2.5897e-01, -2.1433e-01],
          ...,
          [-1.6329e-01, -1.4335e-01, -1.0346e-01,  ..., -3.1188e-01, -1.5207e-01, -7.2167e-02],
          [ 8.7718e-02,  1.0953e-01,  1.5316e-01,  ..., -1.3279e-02,  1.3744e-01,  2.1280e-01],
          [-4.2458e-01, -4.0327e-01, -3.6063e-01,  ..., -1.4533e-03, -2.1495e-01, -3.2170e-01]]]])

2025-07-09 13:40:47.799617 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([16, 40, 16, 222823],"float32"), size=list[32,24,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([16, 40, 16, 222823],"float32"), size=list[32,24,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 476023 / 491520 (96.8%)
Greatest absolute difference: 0.9444066882133484 at index (6, 8, 0, 11) (up to 0.01 allowed)
Greatest relative difference: 239130.578125 at index (15, 18, 23, 22) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 40, 32, 24]), dtype=torch.float32)
tensor([[[[ 1.8170e-01,  3.8177e-01, -5.3739e-02,  ..., -3.7299e-01, -7.6733e-02,  5.5861e-02],
          [-1.3947e-01,  1.1857e-01,  3.6705e-02,  ..., -3.8356e-01,  3.2633e-02, -7.6748e-02],
          [-4.6063e-01, -1.4463e-01,  1.2715e-01,  ..., -3.9412e-01,  1.4200e-01, -2.0936e-01],
          ...,
          [-3.0268e-01, -1.9804e-01, -2.0256e-01,  ...,  3.0884e-01, -3.5222e-01, -1.5567e-01],
          [-3.9633e-02, -8.5058e-02, -1.2676e-01,  ...,  2.9364e-01,  3.6052e-02, -3.6339e-02],
          [ 2.2341e-01,  2.7925e-02, -5.0958e-02,  ...,  2.7843e-01,  4.2433e-01,  8.2991e-02]],

         [[-1.9454e-01,  3.9124e-01, -4.0085e-01,  ...,  1.5078e-01, -2.1274e-01, -4.5284e-01],
          [-1.1902e-01,  9.6784e-02, -2.6156e-01,  ...,  6.7839e-02,  8.5275e-02, -5.7222e-02],
          [-4.3495e-02, -1.9767e-01, -1.2228e-01,  ..., -1.5103e-02,  3.8329e-01,  3.3840e-01],
          ...,
          [-1.7475e-01, -2.8800e-01,  1.8888e-01,  ...,  4.3675e-01,  4.7289e-01,  1.8805e-02],
          [-2.8479e-01, -9.2792e-02, -3.4876e-02,  ...,  4.1441e-01,  4.1363e-01,  1.8656e-01],
          [-3.9482e-01,  1.0241e-01, -2.5864e-01,  ...,  3.9207e-01,  3.5436e-01,  3.5431e-01]],

         [[-3.1120e-01,  2.5382e-01, -3.2387e-01,  ..., -4.0190e-01, -4.3978e-01, -6.1063e-02],
          [-1.4285e-02, -4.4793e-02, -1.8334e-01,  ..., -3.5360e-01, -3.6698e-01, -2.4163e-01],
          [ 2.8263e-01, -3.4341e-01, -4.2817e-02,  ..., -3.0530e-01, -2.9418e-01, -4.2219e-01],
          ...,
          [-3.5384e-01, -1.5507e-01, -1.8241e-01,  ...,  2.5896e-02,  3.6247e-01, -3.8858e-01],
          [-1.6695e-01, -2.1423e-01,  7.0492e-02,  ...,  1.5280e-01,  1.5844e-01,  2.1492e-02],
          [ 1.9940e-02, -2.7339e-01,  3.2339e-01,  ...,  2.7971e-01, -4.5585e-02,  4.3157e-01]],

         ...,

         [[ 3.1052e-01,  3.9921e-01,  3.0397e-01,  ..., -2.8636e-01,  9.3714e-02,  4.8331e-01],
          [ 1.0697e-01,  3.6746e-01, -1.2882e-02,  ..., -3.2137e-01, -1.1420e-01,  5.3314e-02],
          [-9.6581e-02,  3.3570e-01, -3.2974e-01,  ..., -3.5637e-01, -3.2211e-01, -3.7668e-01],
          ...,
          [ 8.4647e-02,  3.2218e-01,  6.0305e-02,  ..., -1.0956e-01, -1.6168e-01,  3.0497e-01],
          [ 2.3049e-01,  1.6320e-01,  1.1199e-01,  ...,  1.1380e-02, -1.7213e-01,  2.8504e-01],
          [ 3.7633e-01,  4.2270e-03,  1.6368e-01,  ...,  1.3232e-01, -1.8258e-01,  2.6511e-01]],

         [[ 1.1917e-01, -4.7209e-02,  3.3795e-01,  ...,  4.4149e-01,  2.2612e-01, -1.4459e-01],
          [ 1.8523e-02, -2.3743e-01,  1.6345e-01,  ...,  1.0139e-01,  9.3558e-02, -1.1353e-01],
          [-8.2120e-02, -4.2765e-01, -1.1056e-02,  ..., -2.3871e-01, -3.9005e-02, -8.2472e-02],
          ...,
          [-1.1463e-01,  3.2717e-04, -1.3695e-01,  ...,  2.5475e-01, -2.7276e-01, -3.0418e-01],
          [ 7.0037e-02, -9.2526e-02,  8.7555e-02,  ...,  1.9151e-01, -2.1998e-01, -8.5724e-02],
          [ 2.5470e-01, -1.8538e-01,  3.1206e-01,  ...,  1.2828e-01, -1.6720e-01,  1.3273e-01]],

         [[ 1.6563e-01, -9.2925e-02, -3.3774e-01,  ..., -2.9741e-01,  4.4995e-01, -5.2305e-02],
          [ 2.2117e-01,  7.4444e-02, -1.4278e-01,  ..., -2.6635e-01,  2.9486e-01,  1.5033e-01],
          [ 2.7671e-01,  2.4181e-01,  5.2184e-02,  ..., -2.3529e-01,  1.3977e-01,  3.5296e-01],
          ...,
          [-1.9059e-01,  2.1505e-01,  4.7116e-02,  ..., -3.5432e-01,  1.3765e-02, -3.1522e-01],
          [-1.9040e-01, -1.2166e-01, -2.9643e-02,  ..., -2.4303e-02,  8.2880e-02, -3.4093e-01],
          [-1.9022e-01, -4.5836e-01, -1.0640e-01,  ...,  3.0571e-01,  1.5200e-01, -3.6664e-01]]],


        [[[-4.6939e-01, -3.3354e-01,  2.4986e-01,  ..., -2.0209e-01,  1.7944e-01,  3.5852e-01],
          [-4.1678e-01, -3.3355e-01,  3.4751e-01,  ..., -3.0447e-01, -8.5529e-02,  1.2506e-01],
          [-3.6417e-01, -3.3356e-01,  4.4516e-01,  ..., -4.0685e-01, -3.5050e-01, -1.0839e-01],
          ...,
          [-3.4973e-01,  3.0826e-02, -3.8223e-02,  ..., -2.1031e-01, -9.9652e-02, -1.7365e-01],
          [-1.1837e-01, -4.1479e-02, -2.0900e-01,  ...,  3.9740e-02, -1.6493e-02,  9.0858e-02],
          [ 1.1299e-01, -1.1379e-01, -3.7979e-01,  ...,  2.8979e-01,  6.6666e-02,  3.5537e-01]],

         [[ 1.1522e-01,  9.8643e-02, -2.7254e-02,  ..., -3.1479e-01,  3.6151e-01,  1.0106e-01],
          [ 1.5144e-01,  2.3323e-01, -1.3117e-01,  ..., -1.9506e-01,  1.6668e-01,  6.8366e-02],
          [ 1.8766e-01,  3.6781e-01, -2.3510e-01,  ..., -7.5320e-02, -2.8149e-02,  3.5668e-02],
          ...,
          [-1.7937e-01, -2.5110e-01, -1.8493e-01,  ...,  9.7173e-03,  1.0238e-01, -8.0126e-02],
          [ 8.4043e-02, -1.6245e-01, -8.1096e-03,  ..., -6.5138e-02,  1.7676e-01,  1.1153e-01],
          [ 3.4746e-01, -7.3800e-02,  1.6871e-01,  ..., -1.3999e-01,  2.5114e-01,  3.0319e-01]],

         [[-1.6989e-01, -4.5106e-01,  1.4071e-01,  ..., -3.6350e-02,  3.1850e-01, -4.8587e-01],
          [-1.8313e-01, -3.6982e-01,  1.6665e-01,  ..., -1.7578e-02,  1.6668e-01, -1.7869e-01],
          [-1.9637e-01, -2.8859e-01,  1.9258e-01,  ...,  1.1944e-03,  1.4866e-02,  1.2849e-01],
          ...,
          [-2.4533e-01,  2.6452e-01, -1.7345e-01,  ..., -8.2180e-03, -1.1522e-01,  3.2844e-02],
          [-7.4511e-02,  1.8609e-01,  7.3761e-02,  ...,  1.4255e-01, -1.3604e-01,  1.5395e-01],
          [ 9.6308e-02,  1.0766e-01,  3.2097e-01,  ...,  2.9331e-01, -1.5687e-01,  2.7506e-01]],

         ...,

         [[-1.2456e-01, -3.1088e-01, -5.4863e-02,  ..., -6.9325e-02, -1.7360e-01, -4.7986e-01],
          [-3.2412e-03, -3.1460e-01,  1.7701e-01,  ..., -1.2788e-01, -1.9413e-02, -3.7843e-01],
          [ 1.1807e-01, -3.1831e-01,  4.0888e-01,  ..., -1.8643e-01,  1.3477e-01, -2.7700e-01],
          ...,
          [-3.7922e-01, -2.8378e-01,  1.2921e-01,  ..., -6.7990e-02,  2.3591e-01, -6.7305e-02],
          [-1.8561e-02,  3.6933e-02,  1.6706e-01,  ..., -2.5126e-01,  3.2224e-01, -1.0788e-01],
          [ 3.4210e-01,  3.5764e-01,  2.0490e-01,  ..., -4.3453e-01,  4.0857e-01, -1.4845e-01]],

         [[-2.1685e-01,  2.5394e-01, -3.3925e-01,  ..., -3.1648e-01,  2.1815e-01,  3.8619e-02],
          [-1.6844e-01, -3.4060e-02,  1.8651e-02,  ..., -3.1749e-01,  2.7860e-01, -7.8424e-02],
          [-1.2003e-01, -3.2206e-01,  3.7655e-01,  ..., -3.1850e-01,  3.3905e-01, -1.9547e-01],
          ...,
          [-5.5644e-02,  1.8240e-03, -3.5374e-01,  ...,  3.9401e-02,  2.0385e-01, -2.5649e-01],
          [ 9.8334e-03,  2.0888e-01,  2.9840e-02,  ...,  1.8036e-01,  2.5944e-01, -3.1979e-01],
          [ 7.5311e-02,  4.1594e-01,  4.1342e-01,  ...,  3.2131e-01,  3.1503e-01, -3.8310e-01]],

         [[ 2.2820e-02,  1.5716e-01, -2.8613e-01,  ..., -3.6307e-01,  2.5685e-01,  7.2995e-02],
          [-1.2565e-01, -8.6165e-02, -1.1405e-01,  ..., -5.1422e-02,  1.6654e-01,  1.9967e-01],
          [-2.7412e-01, -3.2949e-01,  5.8036e-02,  ...,  2.6023e-01,  7.6229e-02,  3.2634e-01],
          ...,
          [ 1.2386e-01, -2.7444e-01,  2.0763e-01,  ...,  1.3367e-01, -2.5712e-01,  1.5149e-01],
          [ 1.6537e-01, -8.0387e-02,  6.1745e-02,  ...,  2.0355e-02,  5.9162e-03,  1.3085e-01],
          [ 2.0689e-01,  1.1366e-01, -8.4138e-02,  ..., -9.2959e-02,  2.6895e-01,  1.1020e-01]]],


        [[[ 3.6543e-01, -2.8943e-01, -1.2294e-01,  ...,  5.5943e-02,  1.5085e-01, -3.5325e-01],
          [ 3.1175e-01, -2.1053e-01,  1.6646e-01,  ...,  1.5399e-01,  2.2866e-01,  1.4807e-02],
          [ 2.5807e-01, -1.3164e-01,  4.5587e-01,  ...,  2.5203e-01,  3.0647e-01,  3.8286e-01],
          ...,
          [ 3.8543e-01,  4.2010e-01,  3.2183e-01,  ...,  1.9035e-01, -4.0285e-01, -9.1691e-03],
          [ 4.0872e-01,  3.5055e-01, -2.3387e-02,  ...,  1.5115e-01, -2.4284e-01,  1.1879e-01],
          [ 4.3200e-01,  2.8099e-01, -3.6860e-01,  ...,  1.1194e-01, -8.2835e-02,  2.4676e-01]],

         [[ 3.8468e-01, -2.9085e-01, -1.1727e-01,  ...,  2.0057e-01,  3.0796e-01,  4.0348e-01],
          [ 3.9712e-01, -1.4408e-01, -6.7242e-02,  ...,  4.0423e-02,  3.1859e-01,  3.3625e-01],
          [ 4.0956e-01,  2.6843e-03, -1.7215e-02,  ..., -1.1972e-01,  3.2922e-01,  2.6902e-01],
          ...,
          [ 3.0142e-01, -4.2250e-01, -1.0939e-01,  ..., -2.1191e-01, -2.0587e-01, -3.7960e-01],
          [ 1.2975e-01, -5.0677e-02, -8.4324e-02,  ..., -1.3117e-01, -3.0322e-01, -1.8945e-01],
          [-4.1914e-02,  3.2114e-01, -5.9262e-02,  ..., -5.0432e-02, -4.0057e-01,  6.8926e-04]],

         [[ 4.4120e-01, -1.8947e-01, -1.6299e-01,  ..., -4.1706e-01, -4.3589e-01,  4.6347e-01],
          [ 4.9672e-02, -3.0171e-01, -1.4023e-01,  ..., -4.0080e-01, -3.1686e-02,  3.4631e-01],
          [-3.4186e-01, -4.1395e-01, -1.1748e-01,  ..., -3.8454e-01,  3.7251e-01,  2.2916e-01],
          ...,
          [ 1.8432e-01,  2.7900e-01,  1.8508e-01,  ..., -2.9198e-01, -8.2102e-02, -7.2692e-02],
          [ 3.0360e-01,  2.0738e-01,  1.7364e-01,  ..., -3.0643e-01,  7.9634e-03, -2.1439e-01],
          [ 4.2289e-01,  1.3576e-01,  1.6219e-01,  ..., -3.2088e-01,  9.8029e-02, -3.5609e-01]],

         ...,

         [[-4.2427e-01, -3.7766e-01,  2.7665e-01,  ..., -5.9816e-02, -1.6805e-01, -1.2504e-01],
          [-3.8234e-01, -2.3194e-01,  2.6616e-01,  ...,  1.5956e-01, -3.0024e-01,  4.6133e-02],
          [-3.4040e-01, -8.6218e-02,  2.5567e-01,  ...,  3.7894e-01, -4.3243e-01,  2.1731e-01],
          ...,
          [-3.1984e-01, -3.9570e-01, -1.5008e-02,  ...,  8.2420e-02, -1.9142e-01, -4.2906e-02],
          [-4.2456e-02, -4.7374e-02, -2.8280e-03,  ..., -1.4282e-02, -1.0751e-02,  2.2069e-01],
          [ 2.3493e-01,  3.0096e-01,  9.3525e-03,  ..., -1.1098e-01,  1.6992e-01,  4.8428e-01]],

         [[ 4.0940e-01,  9.8846e-02,  4.3742e-01,  ..., -7.1698e-02, -2.9206e-01,  7.1160e-03],
          [ 1.8002e-01, -1.4479e-01,  2.1817e-01,  ..., -1.4917e-01, -1.8032e-01, -1.4881e-01],
          [-4.9360e-02, -3.8844e-01, -1.0797e-03,  ..., -2.2665e-01, -6.8592e-02, -3.0474e-01],
          ...,
          [-1.5524e-01, -3.5070e-01,  1.6319e-01,  ...,  1.9746e-01,  4.5750e-01,  3.3748e-01],
          [-5.6641e-02, -1.2498e-01,  1.4031e-01,  ..., -6.5336e-02,  3.7848e-01,  3.2088e-01],
          [ 4.1955e-02,  1.0075e-01,  1.1744e-01,  ..., -3.2813e-01,  2.9946e-01,  3.0427e-01]],

         [[-3.7500e-01, -7.8035e-02, -3.5979e-01,  ...,  4.1900e-02,  1.6416e-01, -1.6521e-01],
          [-3.5834e-01,  8.3939e-02, -5.4792e-02,  ..., -9.3778e-03,  2.1266e-02, -2.4359e-01],
          [-3.4167e-01,  2.4591e-01,  2.5021e-01,  ..., -6.0656e-02, -1.2163e-01, -3.2198e-01],
          ...,
          [-1.9208e-01,  7.1691e-02, -2.6905e-02,  ..., -6.6845e-02, -1.8575e-01,  3.1039e-01],
          [-1.8565e-01, -1.2027e-01, -3.2510e-02,  ..., -2.2447e-01, -6.5875e-02,  1.4365e-01],
          [-1.7922e-01, -3.1223e-01, -3.8115e-02,  ..., -3.8210e-01,  5.3997e-02, -2.3089e-02]]],


        ...,


        [[[-4.3303e-01, -6.2703e-02, -2.2464e-01,  ..., -1.8389e-01, -2.2669e-01, -6.7974e-02],
          [-4.1813e-01,  7.1317e-02, -2.8690e-01,  ...,  1.3236e-01, -2.8949e-01, -1.0787e-01],
          [-4.0322e-01,  2.0534e-01, -3.4915e-01,  ...,  4.4862e-01, -3.5229e-01, -1.4777e-01],
          ...,
          [-7.2967e-02,  2.6925e-01,  2.3361e-01,  ..., -2.3957e-01,  1.6163e-01, -3.8412e-01],
          [ 1.4326e-01,  2.7150e-01,  2.8052e-01,  ...,  2.0491e-02,  3.2863e-02, -3.5738e-01],
          [ 3.5948e-01,  2.7375e-01,  3.2744e-01,  ...,  2.8055e-01, -9.5903e-02, -3.3065e-01]],

         [[-7.2716e-02,  1.5884e-01,  2.5849e-01,  ..., -2.0852e-01, -3.7013e-01, -3.6478e-01],
          [ 1.9661e-01, -2.7226e-02,  2.5077e-02,  ..., -2.0832e-01, -3.8441e-01, -3.6222e-01],
          [ 4.6593e-01, -2.1329e-01, -2.0834e-01,  ..., -2.0812e-01, -3.9868e-01, -3.5967e-01],
          ...,
          [-4.5816e-01, -9.1474e-02,  5.0076e-02,  ..., -3.4662e-01, -1.6705e-01, -2.0420e-01],
          [-3.9492e-02, -1.6732e-01,  2.2237e-01,  ..., -3.3358e-01, -2.3628e-01, -1.6437e-01],
          [ 3.7918e-01, -2.4316e-01,  3.9467e-01,  ..., -3.2054e-01, -3.0551e-01, -1.2454e-01]],

         [[ 1.6799e-02, -1.8208e-01, -3.8566e-01,  ..., -2.8212e-01,  2.9228e-02, -1.2432e-01],
          [-9.5453e-02, -1.8597e-01, -1.0293e-01,  ..., -2.6090e-01, -8.7525e-03, -1.2014e-01],
          [-2.0771e-01, -1.8985e-01,  1.7979e-01,  ..., -2.3968e-01, -4.6733e-02, -1.1595e-01],
          ...,
          [-3.0977e-01,  3.7849e-01,  3.0024e-01,  ...,  2.1003e-01, -2.0265e-01,  1.6017e-01],
          [-4.0019e-01,  3.0909e-01,  1.0996e-02,  ..., -6.2639e-02, -1.0651e-01,  7.7817e-02],
          [-4.9061e-01,  2.3970e-01, -2.7825e-01,  ..., -3.3531e-01, -1.0365e-02, -4.5416e-03]],

         ...,

         [[-2.9844e-01,  2.7703e-01,  3.3198e-01,  ..., -1.6923e-02,  2.9601e-01,  3.2922e-01],
          [-3.0802e-01,  1.8899e-01,  3.2858e-02,  ..., -6.0892e-02, -1.4997e-02,  2.1947e-01],
          [-3.1760e-01,  1.0096e-01, -2.6627e-01,  ..., -1.0486e-01, -3.2600e-01,  1.0972e-01],
          ...,
          [-2.8947e-01, -2.4472e-01,  5.6831e-02,  ...,  3.5265e-01,  2.5721e-01,  4.0305e-01],
          [ 6.8501e-02, -3.2432e-01,  1.3685e-01,  ...,  1.3752e-01, -8.1526e-02,  8.2245e-02],
          [ 4.2648e-01, -4.0391e-01,  2.1687e-01,  ..., -7.7617e-02, -4.2026e-01, -2.3856e-01]],

         [[-2.4466e-01, -3.5095e-01, -3.6260e-01,  ..., -5.2349e-04, -4.8408e-03,  5.8670e-02],
          [-3.3824e-01, -6.3199e-02, -8.0064e-02,  ...,  1.2780e-01, -1.6530e-01,  1.7486e-01],
          [-4.3181e-01,  2.2456e-01,  2.0247e-01,  ...,  2.5613e-01, -3.2576e-01,  2.9104e-01],
          ...,
          [ 4.1564e-01, -2.2551e-01, -7.5264e-02,  ...,  2.8862e-01,  2.6225e-01,  1.8378e-01],
          [ 4.0516e-01, -1.8493e-01,  1.2174e-01,  ..., -5.9986e-02,  3.4508e-02,  5.6554e-05],
          [ 3.9469e-01, -1.4436e-01,  3.1874e-01,  ..., -4.0859e-01, -1.9324e-01, -1.8367e-01]],

         [[-1.4519e-01, -2.5078e-01,  1.1753e-01,  ...,  6.9570e-02,  3.2087e-01,  3.0533e-01],
          [-7.6441e-02, -1.2099e-01,  2.4258e-01,  ...,  3.1166e-03,  1.2533e-01,  8.4151e-02],
          [-7.6887e-03,  8.8006e-03,  3.6763e-01,  ..., -6.3337e-02, -7.0215e-02, -1.3703e-01],
          ...,
          [ 2.3737e-01, -1.8406e-01,  2.5482e-01,  ...,  4.0484e-01, -2.3831e-01, -4.4585e-01],
          [-1.2786e-01, -2.9079e-01,  4.1028e-02,  ...,  3.1756e-01, -3.1840e-01, -2.4708e-01],
          [-4.9308e-01, -3.9753e-01, -1.7276e-01,  ...,  2.3028e-01, -3.9850e-01, -4.8304e-02]]],


        [[[-6.6901e-02,  1.1978e-01,  4.1843e-01,  ...,  1.7908e-01, -3.6732e-01,  4.0575e-01],
          [-6.6695e-02,  2.2001e-02,  3.6228e-01,  ..., -4.2151e-02, -1.0089e-01,  2.3665e-01],
          [-6.6489e-02, -7.5775e-02,  3.0612e-01,  ..., -2.6339e-01,  1.6553e-01,  6.7556e-02],
          ...,
          [-4.4863e-01,  3.4461e-01,  2.5928e-01,  ...,  3.1863e-01, -1.2457e-01, -4.5907e-01],
          [-2.1042e-01,  2.0092e-01,  1.5862e-01,  ...,  2.6532e-01,  1.2044e-01, -2.9642e-01],
          [ 2.7780e-02,  5.7234e-02,  5.7951e-02,  ...,  2.1200e-01,  3.6546e-01, -1.3377e-01]],

         [[-1.6576e-01,  3.8729e-02, -1.5921e-01,  ..., -1.5375e-02,  1.5531e-01, -4.9120e-01],
          [ 7.6716e-02,  1.3351e-01, -1.8234e-01,  ...,  6.2700e-02, -9.5047e-02, -6.8355e-02],
          [ 3.1920e-01,  2.2828e-01, -2.0548e-01,  ...,  1.4078e-01, -3.4540e-01,  3.5449e-01],
          ...,
          [ 4.6030e-01,  8.9355e-02,  1.3685e-02,  ...,  1.4247e-01,  3.3284e-01, -2.2055e-01],
          [ 3.1696e-01, -7.5441e-02, -1.3688e-01,  ..., -5.2047e-02,  7.9153e-02,  7.8203e-02],
          [ 1.7362e-01, -2.4024e-01, -2.8744e-01,  ..., -2.4657e-01, -1.7453e-01,  3.7696e-01]],

         [[-1.7531e-01, -2.0999e-01,  6.1558e-02,  ..., -1.0739e-02,  3.1586e-01,  3.1835e-01],
          [-2.3948e-01, -1.8110e-01,  2.2049e-01,  ...,  1.0972e-01, -1.7534e-03, -1.0692e-02],
          [-3.0366e-01, -1.5221e-01,  3.7941e-01,  ...,  2.3019e-01, -3.1936e-01, -3.3973e-01],
          ...,
          [-3.3472e-01, -7.0577e-02, -4.2622e-02,  ..., -3.6716e-01,  1.8533e-01, -4.6221e-01],
          [-1.3180e-01, -2.0772e-01,  1.8382e-02,  ..., -2.2035e-01, -5.2506e-02,  3.9683e-03],
          [ 7.1119e-02, -3.4486e-01,  7.9386e-02,  ..., -7.3547e-02, -2.9034e-01,  4.7015e-01]],

         ...,

         [[-2.3836e-01, -1.7424e-01,  2.1245e-01,  ..., -2.5575e-01, -3.1356e-01, -4.5946e-01],
          [ 4.9229e-02,  1.1193e-01, -6.1695e-02,  ..., -1.0139e-01, -2.0351e-01, -2.4045e-01],
          [ 3.3682e-01,  3.9810e-01, -3.3584e-01,  ...,  5.2966e-02, -9.3458e-02, -2.1436e-02],
          ...,
          [ 3.7452e-01,  2.5544e-01,  1.5131e-02,  ..., -1.1531e-01, -3.5331e-01,  3.5138e-01],
          [ 1.8588e-01,  2.2615e-01,  6.6082e-02,  ...,  7.0024e-02, -2.6689e-01,  2.8480e-01],
          [-2.7585e-03,  1.9686e-01,  1.1703e-01,  ...,  2.5536e-01, -1.8046e-01,  2.1822e-01]],

         [[-1.7205e-01, -2.9714e-01, -3.2566e-01,  ...,  3.5530e-01,  2.9972e-01,  2.8019e-01],
          [-1.7696e-01, -2.9572e-01, -1.7346e-01,  ...,  1.6761e-01,  2.7803e-01,  3.1165e-01],
          [-1.8187e-01, -2.9431e-01, -2.1264e-02,  ..., -2.0091e-02,  2.5633e-01,  3.4310e-01],
          ...,
          [ 3.3536e-01, -6.6705e-02,  3.1027e-01,  ..., -6.0049e-02, -7.2102e-02, -8.8871e-02],
          [ 1.2435e-01,  3.4940e-02,  3.3157e-01,  ..., -1.8432e-01, -7.6760e-02, -2.7844e-01],
          [-8.6663e-02,  1.3658e-01,  3.5288e-01,  ..., -3.0860e-01, -8.1417e-02, -4.6802e-01]],

         [[ 3.4649e-01, -1.6343e-01,  1.6120e-01,  ..., -1.1076e-02, -4.8421e-01, -4.7954e-01],
          [ 3.4973e-03,  1.2326e-01,  3.5187e-02,  ...,  4.9522e-02, -8.7223e-02, -5.6663e-02],
          [-3.3950e-01,  4.0996e-01, -9.0824e-02,  ...,  1.1012e-01,  3.0976e-01,  3.6621e-01],
          ...,
          [-2.8170e-01,  7.0526e-02, -3.2832e-01,  ..., -3.7747e-01,  2.5737e-01, -5.7221e-02],
          [-3.0835e-01, -9.0063e-02, -6.8442e-02,  ..., -2.0548e-01,  1.1533e-01,  7.3642e-02],
          [-3.3499e-01, -2.5065e-01,  1.9144e-01,  ..., -3.3482e-02, -2.6713e-02,  2.0451e-01]]],


        [[[-4.8512e-01, -1.1536e-01, -8.6736e-02,  ...,  3.5584e-01, -3.3085e-01,  2.9160e-01],
          [-1.0963e-01, -1.1453e-01, -2.2283e-01,  ...,  4.2179e-02,  2.3879e-02, -1.8016e-02],
          [ 2.6585e-01, -1.1370e-01, -3.5893e-01,  ..., -2.7148e-01,  3.7861e-01, -3.2763e-01],
          ...,
          [ 4.0547e-01,  3.0240e-01, -9.5785e-02,  ...,  1.3477e-02,  3.6471e-01, -7.3260e-02],
          [ 2.9596e-01, -3.8501e-02, -1.6297e-01,  ..., -2.0037e-01,  1.5603e-01, -4.8837e-02],
          [ 1.8644e-01, -3.7940e-01, -2.3015e-01,  ..., -4.1422e-01, -5.2650e-02, -2.4414e-02]],

         [[ 7.1003e-02,  1.2660e-01, -1.7818e-02,  ..., -3.4070e-01,  2.4330e-02,  6.1107e-02],
          [ 1.0415e-01,  6.9433e-03,  5.1646e-02,  ..., -3.0867e-02, -1.0471e-01, -2.0817e-01],
          [ 1.3729e-01, -1.1272e-01,  1.2111e-01,  ...,  2.7897e-01, -2.3375e-01, -4.7744e-01],
          ...,
          [ 2.2317e-01, -2.1647e-01, -2.0140e-01,  ...,  3.1597e-01,  6.1622e-02,  5.2108e-02],
          [-5.7001e-02, -2.3229e-01,  1.1975e-02,  ...,  3.4935e-01,  2.1990e-01, -2.0166e-01],
          [-3.3717e-01, -2.4810e-01,  2.2535e-01,  ...,  3.8272e-01,  3.7818e-01, -4.5542e-01]],

         [[-2.8711e-02, -3.4389e-01, -2.1995e-01,  ...,  1.9711e-01, -4.4350e-01, -2.4127e-01],
          [-7.1241e-02, -1.3076e-01, -3.2602e-01,  ...,  2.2851e-01, -2.8269e-01, -2.9567e-01],
          [-1.1377e-01,  8.2367e-02, -4.3208e-01,  ...,  2.5990e-01, -1.2187e-01, -3.5008e-01],
          ...,
          [-2.9478e-01, -1.9423e-01,  5.0903e-02,  ..., -3.3413e-01,  1.5272e-01,  9.5547e-02],
          [-3.2106e-01, -2.1346e-01, -3.8958e-02,  ..., -3.3098e-01,  1.7697e-01,  2.3274e-01],
          [-3.4733e-01, -2.3269e-01, -1.2882e-01,  ..., -3.2783e-01,  2.0121e-01,  3.6993e-01]],

         ...,

         [[-3.7004e-01, -5.6110e-02,  4.7896e-02,  ..., -1.6359e-01, -2.3465e-01,  2.0522e-01],
          [-1.6394e-01, -1.2551e-02, -6.6212e-02,  ..., -9.7610e-02, -6.9692e-02,  2.6390e-01],
          [ 4.2169e-02,  3.1008e-02, -1.8032e-01,  ..., -3.1629e-02,  9.5269e-02,  3.2258e-01],
          ...,
          [-4.0714e-01,  2.2094e-01, -1.5078e-02,  ...,  4.8548e-02,  2.5843e-01,  1.4943e-01],
          [-3.0784e-02,  2.2244e-01,  8.4419e-02,  ...,  1.9410e-01, -8.3212e-04, -1.2955e-01],
          [ 3.4558e-01,  2.2394e-01,  1.8392e-01,  ...,  3.3965e-01, -2.6010e-01, -4.0852e-01]],

         [[ 4.9292e-01, -3.3180e-01,  1.3633e-01,  ...,  3.0686e-01,  3.8543e-01,  2.2947e-01],
          [ 1.7212e-01, -2.4847e-01,  8.1760e-02,  ...,  3.1053e-01,  2.4697e-01, -6.5437e-02],
          [-1.4867e-01, -1.6514e-01,  2.7193e-02,  ...,  3.1420e-01,  1.0851e-01, -3.6035e-01],
          ...,
          [-3.3138e-01, -3.4892e-01,  2.9533e-01,  ...,  3.9165e-01,  1.4046e-01, -9.0862e-02],
          [-3.3973e-01, -6.0909e-02,  3.5150e-01,  ...,  3.2050e-01, -7.4643e-02, -1.8380e-01],
          [-3.4808e-01,  2.2710e-01,  4.0766e-01,  ...,  2.4936e-01, -2.8975e-01, -2.7673e-01]],

         [[-1.6096e-01, -1.4414e-01,  3.4196e-01,  ..., -1.2206e-01, -2.5987e-01,  6.7999e-02],
          [ 9.7998e-03, -2.7867e-01,  3.3444e-01,  ...,  3.7506e-02, -3.5621e-02,  7.0039e-03],
          [ 1.8056e-01, -4.1320e-01,  3.2692e-01,  ...,  1.9708e-01,  1.8863e-01, -5.3991e-02],
          ...,
          [-4.1634e-01, -5.0860e-02,  1.0674e-01,  ...,  2.3584e-01,  3.9135e-01, -9.8003e-02],
          [ 9.8218e-03,  1.6145e-01,  8.1699e-02,  ...,  1.8532e-01,  2.4863e-01, -2.0793e-01],
          [ 4.3598e-01,  3.7376e-01,  5.6659e-02,  ...,  1.3480e-01,  1.0591e-01, -3.1786e-01]]]])
DESIRED: (shape=torch.Size([16, 40, 32, 24]), dtype=torch.float32)
tensor([[[[-1.8699e-02,  4.7227e-01, -2.7215e-01,  ...,  2.5498e-01,  1.6625e-01, -1.5470e-01],
          [-8.1091e-02,  4.2992e-01, -2.0711e-01,  ...,  1.6955e-01,  1.1032e-01, -1.7032e-01],
          [-2.0588e-01,  3.4521e-01, -7.7022e-02,  ..., -1.3085e-03, -1.5438e-03, -2.0157e-01],
          ...,
          [ 1.1493e-01, -1.5538e-01,  3.0760e-01,  ..., -1.4302e-01,  3.2835e-02,  6.4901e-02],
          [-1.5160e-02, -2.6279e-02,  1.1366e-01,  ..., -8.8331e-02,  1.1895e-02,  6.9020e-02],
          [-8.0206e-02,  3.8269e-02,  1.6692e-02,  ..., -6.0986e-02,  1.4248e-03,  7.1080e-02]],

         [[-9.9406e-02,  4.5275e-01, -3.3241e-03,  ..., -3.2649e-01, -4.1301e-01, -7.2551e-02],
          [-3.9788e-02,  3.3037e-01, -7.4735e-02,  ..., -2.5261e-01, -3.1688e-01, -1.0267e-01],
          [ 7.9446e-02,  8.5621e-02, -2.1756e-01,  ..., -1.0486e-01, -1.2463e-01, -1.6290e-01],
          ...,
          [ 3.0015e-01, -1.0811e-02, -1.8602e-01,  ...,  7.4599e-02,  1.4837e-01, -1.2842e-01],
          [ 3.7638e-01,  2.3169e-01, -3.4137e-01,  ...,  8.6437e-02, -1.1441e-01, -1.8495e-01],
          [ 4.1450e-01,  3.5295e-01, -4.1905e-01,  ...,  9.2356e-02, -2.4580e-01, -2.1321e-01]],

         [[-1.3596e-01,  3.0991e-01, -2.4143e-01,  ...,  4.5782e-01,  1.5827e-01, -2.3705e-01],
          [-7.9830e-02,  2.4792e-01, -2.2576e-01,  ...,  3.8483e-01,  2.3332e-02, -1.9456e-01],
          [ 3.2424e-02,  1.2393e-01, -1.9442e-01,  ...,  2.3886e-01, -2.4654e-01, -1.0959e-01],
          ...,
          [-1.1603e-03,  3.2626e-02,  3.9039e-01,  ..., -2.8185e-01,  3.7074e-01,  2.5032e-02],
          [ 2.8385e-01, -2.8048e-01,  2.3532e-01,  ..., -2.6319e-01,  3.1715e-01, -1.6093e-01],
          [ 4.2636e-01, -4.3704e-01,  1.5779e-01,  ..., -2.5386e-01,  2.9036e-01, -2.5391e-01]],

         ...,

         [[-9.3318e-03, -4.0044e-01,  1.0138e-01,  ...,  3.2736e-01,  4.5264e-01, -3.3976e-01],
          [ 8.1365e-02, -1.9324e-01, -7.6660e-03,  ...,  1.8771e-01,  4.3599e-01, -2.6821e-01],
          [ 2.6276e-01,  2.2114e-01, -2.2577e-01,  ..., -9.1571e-02,  4.0268e-01, -1.2513e-01],
          ...,
          [-2.3713e-01,  1.0193e-01,  3.0731e-02,  ...,  4.9044e-02,  3.7144e-01,  2.1924e-01],
          [-3.0886e-01,  4.4764e-02, -1.6170e-01,  ...,  1.5444e-01,  3.7741e-01, -3.5758e-03],
          [-3.4473e-01,  1.6178e-02, -2.5791e-01,  ...,  2.0714e-01,  3.8039e-01, -1.1498e-01]],

         [[ 3.6283e-01,  1.6622e-01,  1.6121e-01,  ...,  2.1136e-01,  1.4123e-01, -4.4069e-01],
          [ 3.0763e-01,  2.3220e-01,  7.9269e-02,  ...,  1.7443e-01,  8.5497e-02, -2.8270e-01],
          [ 1.9723e-01,  3.6416e-01, -8.4607e-02,  ...,  1.0058e-01, -2.5968e-02,  3.3287e-02],
          ...,
          [ 9.8409e-02,  7.6469e-02,  2.9547e-01,  ..., -1.7255e-02, -1.7476e-01, -7.9147e-02],
          [ 2.9520e-01, -1.0863e-01,  1.4281e-02,  ...,  7.1757e-02, -5.2608e-02,  2.8086e-02],
          [ 3.9360e-01, -2.0118e-01, -1.2631e-01,  ...,  1.1626e-01,  8.4656e-03,  8.1702e-02]],

         [[-5.4400e-02, -3.8033e-01, -2.0144e-01,  ..., -2.4632e-01,  2.5636e-01,  1.9153e-01],
          [-6.8515e-02, -2.4266e-01, -2.3566e-01,  ..., -1.1345e-01,  9.7500e-02,  2.1827e-01],
          [-9.6747e-02,  3.2699e-02, -3.0409e-01,  ...,  1.5229e-01, -2.2021e-01,  2.7175e-01],
          ...,
          [ 4.2835e-02,  3.2293e-01,  4.0243e-02,  ...,  1.0935e-02,  2.5929e-01, -2.6532e-01],
          [ 5.4614e-02,  1.6513e-01, -5.5431e-02,  ...,  1.4772e-01,  2.6450e-01, -1.5278e-01],
          [ 6.0503e-02,  8.6233e-02, -1.0327e-01,  ...,  2.1611e-01,  2.6711e-01, -9.6505e-02]]],


        [[[ 1.3671e-01, -1.2483e-01,  5.3588e-02,  ..., -1.9463e-01,  2.3883e-01, -9.7897e-02],
          [ 2.2712e-02, -4.7308e-02, -3.2324e-04,  ..., -2.0519e-01,  1.1006e-01, -9.7475e-02],
          [-2.0528e-01,  1.0774e-01, -1.0815e-01,  ..., -2.2631e-01, -1.4748e-01, -9.6633e-02],
          ...,
          [-2.0465e-01,  2.1786e-01,  3.3740e-03,  ...,  8.9105e-02,  6.1276e-02, -2.1274e-01],
          [-3.1818e-01, -1.9529e-01,  2.5300e-01,  ...,  2.6248e-01,  3.3573e-01, -3.7725e-01],
          [-3.7494e-01, -4.0186e-01,  3.7781e-01,  ...,  3.4917e-01,  4.7296e-01, -4.5950e-01]],

         [[ 2.9442e-01,  2.6280e-01,  4.5807e-02,  ...,  3.7441e-01, -3.6148e-01,  5.4628e-02],
          [ 1.6814e-01,  2.9847e-01,  4.5078e-02,  ...,  2.4690e-01, -1.8377e-01,  5.9326e-02],
          [-8.4425e-02,  3.6982e-01,  4.3620e-02,  ..., -8.1151e-03,  1.7165e-01,  6.8723e-02],
          ...,
          [ 4.5135e-02,  1.2193e-01,  4.5408e-02,  ...,  2.4219e-04, -1.7561e-01,  1.9533e-01],
          [-4.7022e-02,  1.8043e-01, -2.2954e-01,  ..., -8.6856e-02, -1.4735e-01, -8.4404e-02],
          [-9.3100e-02,  2.0968e-01, -3.6701e-01,  ..., -1.3041e-01, -1.3322e-01, -2.2427e-01]],

         [[-1.8857e-01, -4.2372e-01,  1.2576e-01,  ...,  3.5999e-01, -3.4225e-01, -2.2301e-01],
          [-1.8677e-01, -2.3755e-01,  1.8035e-01,  ...,  2.9871e-01, -2.2294e-01, -1.2463e-01],
          [-1.8317e-01,  1.3479e-01,  2.8954e-01,  ...,  1.7616e-01,  1.5685e-02,  7.2132e-02],
          ...,
          [-2.4797e-01, -2.1082e-01,  3.4402e-01,  ...,  1.7354e-01,  1.9035e-01,  1.2030e-01],
          [-2.5225e-01,  2.0239e-01,  3.4942e-01,  ..., -8.6730e-02, -1.0021e-02,  2.9981e-01],
          [-2.5440e-01,  4.0899e-01,  3.5212e-01,  ..., -2.1687e-01, -1.1021e-01,  3.8957e-01]],

         ...,

         [[-5.6992e-02, -2.5329e-01, -3.0278e-01,  ..., -1.1105e-01,  4.6053e-01, -2.6698e-01],
          [ 5.8127e-05, -2.0615e-01, -2.4853e-01,  ...,  1.7064e-02,  2.9809e-01, -2.9352e-01],
          [ 1.1416e-01, -1.1186e-01, -1.4001e-01,  ...,  2.7330e-01, -2.6783e-02, -3.4660e-01],
          ...,
          [ 2.7493e-01, -6.2669e-02,  2.4892e-01,  ...,  1.5536e-01, -9.9700e-02, -1.1844e-01],
          [ 2.5001e-01,  1.2544e-01,  2.0971e-01,  ..., -5.1399e-02, -1.1168e-01, -9.4467e-02],
          [ 2.3756e-01,  2.1949e-01,  1.9010e-01,  ..., -1.5478e-01, -1.1767e-01, -8.2481e-02]],

         [[ 2.3524e-01, -1.6217e-01, -1.6208e-01,  ..., -2.8579e-01, -2.2324e-01,  2.7174e-02],
          [ 1.4872e-01, -1.6653e-01, -1.4453e-01,  ..., -2.2739e-01, -1.0937e-01, -3.2954e-02],
          [-2.4333e-02, -1.7523e-01, -1.0943e-01,  ..., -1.1059e-01,  1.1837e-01, -1.5321e-01],
          ...,
          [ 2.0281e-01,  1.7240e-01, -1.3267e-01,  ..., -2.2116e-01, -3.6238e-01,  1.7141e-02],
          [-3.8797e-02,  3.0190e-01,  1.5500e-01,  ..., -1.2545e-02, -4.2983e-01, -1.9221e-01],
          [-1.5960e-01,  3.6665e-01,  2.9883e-01,  ...,  9.1761e-02, -4.6356e-01, -2.9689e-01]],

         [[ 2.5800e-01, -3.9216e-01,  3.4703e-01,  ..., -3.4299e-01, -3.9924e-02, -1.2796e-01],
          [ 2.2439e-01, -3.7001e-01,  1.8135e-01,  ..., -2.0603e-01,  4.5847e-02, -1.8779e-02],
          [ 1.5715e-01, -3.2572e-01, -1.5000e-01,  ...,  6.7910e-02,  2.1739e-01,  1.9959e-01],
          ...,
          [ 6.3784e-02,  1.3624e-01, -1.6696e-01,  ...,  3.9769e-02, -9.7113e-02,  4.7791e-03],
          [ 2.1028e-01, -2.3512e-01,  7.4970e-02,  ..., -3.4989e-02, -3.1575e-01, -1.8794e-01],
          [ 2.8352e-01, -4.2080e-01,  1.9594e-01,  ..., -7.2367e-02, -4.2508e-01, -2.8430e-01]]],


        [[[-2.0463e-02,  2.3945e-01, -3.1390e-01,  ...,  2.7361e-01, -4.2219e-01,  2.0404e-01],
          [ 5.0080e-02,  7.8469e-02, -2.4433e-01,  ...,  2.2880e-01, -3.0687e-01,  1.2330e-01],
          [ 1.9117e-01, -2.4350e-01, -1.0519e-01,  ...,  1.3917e-01, -7.6251e-02, -3.8190e-02],
          ...,
          [-1.8944e-02,  1.4486e-01,  6.6955e-02,  ...,  1.1773e-01, -6.2193e-03, -1.4486e-01],
          [-1.7207e-01, -8.6694e-02,  1.4124e-01,  ...,  1.4391e-01, -2.9649e-01, -4.6657e-02],
          [-2.4864e-01, -2.0247e-01,  1.7839e-01,  ...,  1.5699e-01, -4.4163e-01,  2.4470e-03]],

         [[-1.9793e-01,  4.3298e-01, -2.4958e-01,  ...,  4.0956e-01,  2.1677e-01,  2.7047e-02],
          [-1.3008e-01,  4.0132e-01, -1.6406e-01,  ...,  2.4816e-01,  2.3895e-01, -5.8344e-03],
          [ 5.6390e-03,  3.3799e-01,  6.9906e-03,  ..., -7.4653e-02,  2.8331e-01, -7.1597e-02],
          ...,
          [ 1.8024e-01, -7.6082e-02,  2.4613e-01,  ...,  8.9319e-02,  2.8971e-01, -7.4679e-02],
          [ 3.0068e-03,  2.7082e-01,  1.6030e-01,  ...,  2.0659e-01,  1.9444e-01,  6.6000e-03],
          [-8.5609e-02,  4.4427e-01,  1.1738e-01,  ...,  2.6522e-01,  1.4680e-01,  4.7239e-02]],

         [[ 2.0258e-02, -2.6080e-01,  5.5998e-03,  ...,  1.3564e-01,  3.1160e-01, -2.4409e-01],
          [ 4.5094e-02, -2.1680e-01, -5.6410e-02,  ...,  1.7433e-01,  1.6057e-01, -1.7837e-01],
          [ 9.4767e-02, -1.2879e-01, -1.8043e-01,  ...,  2.5170e-01, -1.4149e-01, -4.6932e-02],
          ...,
          [-1.2574e-01,  1.8133e-01, -6.9771e-02,  ..., -1.7152e-02, -2.6315e-01, -1.4731e-01],
          [ 2.4105e-02,  3.1875e-01,  1.0560e-01,  ..., -1.0662e-01, -5.4026e-02, -2.5520e-02],
          [ 9.9026e-02,  3.8747e-01,  1.9328e-01,  ..., -1.5135e-01,  5.0536e-02,  3.5377e-02]],

         ...,

         [[ 3.9181e-02, -4.1938e-02, -4.1095e-01,  ..., -1.2698e-02, -3.3493e-01,  3.9125e-01],
          [ 1.0132e-01, -6.6772e-02, -2.9987e-01,  ..., -8.8047e-02, -3.3335e-01,  3.5661e-01],
          [ 2.2559e-01, -1.1644e-01, -7.7703e-02,  ..., -2.3875e-01, -3.3020e-01,  2.8735e-01],
          ...,
          [ 6.8956e-02,  2.1893e-01, -3.4392e-01,  ..., -8.7944e-02, -6.1716e-02,  2.0755e-02],
          [ 6.6780e-02, -6.4044e-02, -5.5964e-02,  ..., -1.8462e-01, -9.5158e-02,  6.8016e-02],
          [ 6.5692e-02, -2.0553e-01,  8.8014e-02,  ..., -2.3295e-01, -1.1188e-01,  9.1647e-02]],

         [[ 2.7312e-01,  8.8851e-03, -1.9448e-01,  ...,  1.5321e-01, -1.2684e-01,  1.4242e-02],
          [ 2.3156e-01,  8.6905e-02, -1.9434e-01,  ...,  2.3252e-01, -8.2870e-03,  6.4117e-02],
          [ 1.4844e-01,  2.4295e-01, -1.9405e-01,  ...,  3.9114e-01,  2.2882e-01,  1.6387e-01],
          ...,
          [ 2.3647e-01, -3.8625e-01, -8.1842e-02,  ..., -1.1734e-01,  2.3404e-01, -8.3308e-04],
          [ 2.2285e-01, -1.9562e-01, -7.1620e-03,  ...,  1.2898e-01,  2.4798e-01,  8.7584e-02],
          [ 2.1605e-01, -1.0030e-01,  3.0178e-02,  ...,  2.5214e-01,  2.5494e-01,  1.3179e-01]],

         [[-5.9891e-02,  5.4211e-02, -2.2619e-01,  ...,  2.4017e-01,  2.8529e-01, -1.5252e-01],
          [-6.8285e-02, -9.0469e-03, -2.2178e-01,  ...,  2.0016e-01,  2.8795e-01, -5.4858e-02],
          [-8.5073e-02, -1.3556e-01, -2.1298e-01,  ...,  1.2012e-01,  2.9327e-01,  1.4046e-01],
          ...,
          [ 9.7758e-03,  2.0487e-01,  1.3305e-01,  ..., -1.1378e-01,  1.1200e-01,  9.8106e-03],
          [-2.2305e-02,  1.9744e-01, -1.5258e-01,  ..., -2.3784e-01,  3.3352e-01,  1.3557e-01],
          [-3.8345e-02,  1.9373e-01, -2.9540e-01,  ..., -2.9987e-01,  4.4429e-01,  1.9845e-01]]],


        ...,


        [[[-1.0922e-01, -6.6145e-02,  2.5456e-01,  ...,  5.4329e-02,  3.7854e-01,  1.3994e-01],
          [-2.0355e-02, -8.8776e-02,  1.7207e-01,  ..., -1.0307e-02,  2.6179e-01,  4.2146e-02],
          [ 1.5738e-01, -1.3404e-01,  7.0742e-03,  ..., -1.3958e-01,  2.8287e-02, -1.5344e-01],
          ...,
          [ 1.2144e-01, -3.4699e-01, -1.9742e-01,  ..., -4.3693e-01, -2.7722e-01, -4.3671e-02],
          [ 1.0220e-01, -2.2915e-01, -1.8676e-01,  ..., -3.8226e-01, -2.0015e-01,  2.1218e-02],
          [ 9.2582e-02, -1.7023e-01, -1.8143e-01,  ..., -3.5493e-01, -1.6161e-01,  5.3663e-02]],

         [[-7.1261e-03,  3.5366e-01, -2.5608e-01,  ...,  2.6109e-01,  7.2654e-03, -3.5569e-01],
          [-3.9958e-02,  1.5284e-01, -2.2855e-01,  ...,  2.4360e-01,  1.1129e-01, -2.9706e-01],
          [-1.0562e-01, -2.4879e-01, -1.7351e-01,  ...,  2.0863e-01,  3.1935e-01, -1.7981e-01],
          ...,
          [ 2.2088e-02,  2.8743e-01,  1.5092e-01,  ..., -2.1369e-01, -1.2803e-01, -1.0931e-01],
          [-1.6123e-01,  3.2207e-01, -5.8117e-02,  ..., -2.6636e-01, -3.0612e-01,  9.7187e-02],
          [-2.5289e-01,  3.3938e-01, -1.6264e-01,  ..., -2.9270e-01, -3.9517e-01,  2.0044e-01]],

         [[ 3.7812e-01, -1.3152e-01,  3.6313e-01,  ...,  3.7054e-01,  7.7559e-03, -2.3432e-02],
          [ 2.6180e-01, -1.3808e-01,  2.4781e-01,  ...,  3.2641e-01, -8.2186e-02,  3.8117e-02],
          [ 2.9151e-02, -1.5120e-01,  1.7191e-02,  ...,  2.3816e-01, -2.6207e-01,  1.6122e-01],
          ...,
          [ 2.2940e-01,  2.0604e-01,  7.6938e-02,  ..., -5.8471e-02, -7.2827e-02,  1.8026e-01],
          [-6.9807e-02,  6.8613e-02, -5.6772e-02,  ..., -5.6657e-02,  2.4295e-01,  5.4063e-02],
          [-2.1941e-01, -9.9158e-05, -1.2363e-01,  ..., -5.5749e-02,  4.0084e-01, -9.0370e-03]],

         ...,

         [[ 2.3530e-02,  2.1916e-02, -2.5930e-01,  ...,  1.8154e-01, -1.6368e-01,  2.2797e-01],
          [-4.9095e-02, -3.1452e-02, -8.3050e-02,  ...,  3.9094e-02, -2.0440e-01,  1.2654e-01],
          [-1.9434e-01, -1.3819e-01,  2.6944e-01,  ..., -2.4579e-01, -2.8585e-01, -7.6320e-02],
          ...,
          [-1.5052e-02, -1.1039e-01,  2.1650e-01,  ...,  1.4057e-01, -1.6787e-01,  2.0426e-01],
          [ 7.4508e-02,  6.7210e-02, -7.4227e-02,  ..., -1.4494e-01,  1.5081e-01,  9.4204e-02],
          [ 1.1929e-01,  1.5601e-01, -2.1959e-01,  ..., -2.8770e-01,  3.1015e-01,  3.9175e-02]],

         [[-1.2210e-01, -4.5056e-01, -1.6480e-01,  ...,  2.6396e-01, -6.7416e-02,  7.2214e-02],
          [-6.8511e-02, -3.0422e-01, -1.9023e-01,  ...,  2.7800e-01, -1.0155e-01,  1.0099e-01],
          [ 3.8675e-02, -1.1548e-02, -2.4111e-01,  ...,  3.0607e-01, -1.6982e-01,  1.5855e-01],
          ...,
          [ 1.2354e-01,  2.8130e-01,  3.4321e-01,  ..., -1.7138e-01,  6.2688e-02, -1.8550e-01],
          [ 3.2747e-01,  2.8843e-01,  4.0152e-01,  ..., -2.5093e-01,  1.9032e-01, -9.4804e-02],
          [ 4.2944e-01,  2.9200e-01,  4.3067e-01,  ..., -2.9070e-01,  2.5413e-01, -4.9456e-02]],

         [[-1.8939e-01,  4.4109e-01, -2.9578e-02,  ..., -2.2042e-01,  3.2550e-01,  2.2934e-01],
          [-1.7111e-01,  3.3500e-01, -2.6419e-02,  ..., -2.4384e-01,  1.7477e-01,  7.7516e-02],
          [-1.3454e-01,  1.2282e-01, -2.0100e-02,  ..., -2.9069e-01, -1.2670e-01, -2.2613e-01],
          ...,
          [-2.1812e-01, -6.1852e-02,  2.2643e-01,  ...,  9.5702e-02, -2.4050e-01, -2.0337e-01],
          [-3.6873e-01, -1.5347e-01,  1.4799e-01,  ..., -3.2004e-02, -3.6823e-01, -8.6499e-02],
          [-4.4404e-01, -1.9928e-01,  1.0877e-01,  ..., -9.5856e-02, -4.3210e-01, -2.8066e-02]]],


        [[[ 7.6423e-02,  2.7130e-01,  1.2162e-02,  ...,  1.6288e-01, -2.2629e-01, -1.3931e-01],
          [ 1.0734e-01,  2.5054e-01,  2.5024e-03,  ...,  7.8027e-02, -6.7581e-02, -1.5854e-01],
          [ 1.6917e-01,  2.0902e-01, -1.6818e-02,  ..., -9.1678e-02,  2.4984e-01, -1.9701e-01],
          ...,
          [ 1.4796e-01,  3.3012e-01,  3.6502e-01,  ...,  6.5632e-02, -8.1398e-02,  1.1396e-01],
          [ 4.9917e-02,  3.7069e-01,  3.7461e-01,  ..., -7.2523e-02,  5.7333e-02, -3.3852e-02],
          [ 8.9467e-04,  3.9097e-01,  3.7940e-01,  ..., -1.4160e-01,  1.2670e-01, -1.0776e-01]],

         [[ 1.8511e-01, -3.9059e-01,  3.9383e-01,  ...,  1.0803e-01, -2.8028e-02, -2.8818e-01],
          [ 1.1074e-01, -2.6272e-01,  3.2530e-01,  ...,  3.9634e-02,  2.4945e-02, -1.9344e-01],
          [-3.7996e-02, -6.9958e-03,  1.8824e-01,  ..., -9.7158e-02,  1.3089e-01, -3.9482e-03],
          ...,
          [ 1.1729e-01, -9.7927e-02,  2.2467e-01,  ..., -3.0102e-01,  1.8352e-01,  1.4614e-01],
          [-7.4159e-02,  2.7088e-01, -1.7193e-01,  ..., -3.8588e-01,  2.5254e-01, -1.4094e-01],
          [-1.6988e-01,  4.5528e-01, -3.7022e-01,  ..., -4.2831e-01,  2.8705e-01, -2.8449e-01]],

         [[-2.3814e-02, -4.5212e-01, -4.1826e-02,  ...,  8.1323e-02, -4.3918e-01,  1.7090e-01],
          [-7.4021e-02, -3.3922e-01,  1.5955e-02,  ...,  7.2927e-02, -2.9117e-01,  1.3290e-01],
          [-1.7444e-01, -1.1342e-01,  1.3152e-01,  ...,  5.6134e-02,  4.8476e-03,  5.6889e-02],
          ...,
          [ 6.3737e-02, -2.6547e-01,  1.6254e-01,  ..., -2.3574e-01, -1.0748e-01,  5.1553e-02],
          [ 1.6748e-01, -3.2031e-01, -1.3606e-01,  ..., -3.2439e-01, -2.0576e-01,  9.1676e-02],
          [ 2.1935e-01, -3.4773e-01, -2.8536e-01,  ..., -3.6872e-01, -2.5490e-01,  1.1174e-01]],

         ...,

         [[-4.5535e-02,  2.4546e-01,  3.2443e-02,  ...,  3.2210e-01, -1.7699e-01, -6.8329e-02],
          [ 1.2919e-02,  2.9081e-01, -1.1059e-02,  ...,  2.5888e-01, -2.8421e-02, -7.2324e-02],
          [ 1.2983e-01,  3.8150e-01, -9.8063e-02,  ...,  1.3242e-01,  2.6871e-01, -8.0314e-02],
          ...,
          [-7.5757e-02, -1.7519e-01,  7.3637e-02,  ..., -2.6443e-01,  1.3538e-01, -3.2155e-01],
          [ 2.1612e-01, -1.4273e-01, -1.7846e-01,  ..., -6.6760e-02, -2.0930e-01, -1.8812e-01],
          [ 3.6206e-01, -1.2650e-01, -3.0451e-01,  ...,  3.2073e-02, -3.8163e-01, -1.2141e-01]],

         [[ 3.0627e-01, -2.7800e-01, -8.9871e-02,  ..., -2.8409e-01, -1.8246e-01, -3.9737e-02],
          [ 2.6841e-01, -2.1904e-01, -9.5035e-03,  ..., -3.1475e-01, -1.6354e-01, -9.1035e-02],
          [ 1.9269e-01, -1.0113e-01,  1.5123e-01,  ..., -3.7606e-01, -1.2571e-01, -1.9363e-01],
          ...,
          [ 2.0131e-01, -1.8677e-01, -4.0297e-02,  ..., -2.6748e-02,  9.4160e-03,  1.5107e-01],
          [-1.1840e-01, -2.3385e-01, -2.4080e-01,  ..., -2.4391e-02, -1.8420e-01,  4.4815e-02],
          [-2.7826e-01, -2.5738e-01, -3.4105e-01,  ..., -2.3212e-02, -2.8100e-01, -8.3123e-03]],

         [[ 3.4459e-01,  1.9460e-01, -3.4197e-01,  ...,  2.3927e-01,  1.9800e-02,  2.2895e-02],
          [ 2.5086e-01,  2.2471e-01, -3.1810e-01,  ...,  2.3491e-01,  9.2184e-02,  2.5626e-02],
          [ 6.3398e-02,  2.8493e-01, -2.7035e-01,  ...,  2.2618e-01,  2.3695e-01,  3.1086e-02],
          ...,
          [-2.9834e-01,  1.7397e-01,  5.5401e-02,  ...,  2.6911e-01,  4.0895e-01, -5.8740e-02],
          [-2.8135e-02,  1.2439e-01, -1.6652e-01,  ...,  2.2170e-01,  2.6495e-01,  1.0820e-01],
          [ 1.0697e-01,  9.9601e-02, -2.7747e-01,  ...,  1.9799e-01,  1.9294e-01,  1.9166e-01]]],


        [[[ 1.4694e-01, -5.6133e-02, -1.7752e-01,  ...,  1.2076e-01, -3.3419e-01, -9.7422e-02],
          [ 1.6128e-01, -9.4765e-02, -9.0839e-02,  ...,  3.4711e-02, -3.4001e-01, -1.0523e-01],
          [ 1.8995e-01, -1.7203e-01,  8.2524e-02,  ..., -1.3740e-01, -3.5167e-01, -1.2086e-01],
          ...,
          [-2.9045e-01,  8.8955e-02,  1.1763e-01,  ..., -4.4275e-02, -1.5452e-01,  2.8223e-01],
          [-8.0993e-02, -1.8442e-01, -2.0543e-01,  ..., -1.2852e-01, -7.5315e-03,  1.6417e-01],
          [ 2.3736e-02, -3.2111e-01, -3.6697e-01,  ..., -1.7064e-01,  6.5964e-02,  1.0514e-01]],

         [[-3.2872e-01,  1.5951e-01, -4.0336e-01,  ..., -2.3806e-01, -3.0013e-01, -1.1985e-01],
          [-3.2834e-01,  2.1567e-01, -3.4811e-01,  ..., -1.2808e-01, -2.4970e-01,  2.8960e-03],
          [-3.2760e-01,  3.2798e-01, -2.3760e-01,  ...,  9.1895e-02, -1.4883e-01,  2.4838e-01],
          ...,
          [ 1.3534e-01,  2.3326e-01, -1.6670e-01,  ..., -4.9157e-02,  6.4304e-02,  1.2427e-01],
          [ 4.7280e-02, -9.7579e-02, -8.0114e-02,  ..., -1.1763e-01, -2.5352e-01,  1.9143e-01],
          [ 3.2477e-03, -2.6300e-01, -3.6822e-02,  ..., -1.5187e-01, -4.1243e-01,  2.2502e-01]],

         [[ 1.6870e-01, -3.6132e-01,  1.1695e-01,  ..., -2.7914e-01, -3.2718e-01,  1.3497e-01],
          [ 1.7795e-01, -2.1407e-01,  2.0254e-01,  ..., -2.6027e-01, -2.5368e-01,  8.9838e-02],
          [ 1.9646e-01,  8.0447e-02,  3.7372e-01,  ..., -2.2253e-01, -1.0669e-01, -4.2106e-04],
          ...,
          [-2.6396e-01, -1.4850e-01, -2.4809e-01,  ...,  6.1649e-02, -3.1398e-01, -1.1645e-01],
          [-1.0864e-01,  2.4851e-02, -1.9271e-01,  ...,  1.9023e-01, -1.9429e-01,  4.6856e-02],
          [-3.0984e-02,  1.1153e-01, -1.6502e-01,  ...,  2.5453e-01, -1.3445e-01,  1.2851e-01]],

         ...,

         [[ 1.8481e-01,  3.4268e-01,  3.0187e-01,  ..., -9.3327e-02,  3.3858e-01,  1.9101e-01],
          [ 6.6545e-02,  2.3832e-01,  2.2863e-01,  ..., -3.9960e-02,  3.0897e-01,  1.6550e-01],
          [-1.6998e-01,  2.9595e-02,  8.2163e-02,  ...,  6.6774e-02,  2.4975e-01,  1.1446e-01],
          ...,
          [ 1.1862e-01,  1.9973e-01, -6.4764e-02,  ...,  1.7296e-01, -5.8381e-04, -2.4925e-01],
          [-1.4506e-02,  8.0511e-02, -6.2975e-02,  ...,  3.4563e-02, -1.3130e-01, -1.5135e-01],
          [-8.1068e-02,  2.0903e-02, -6.2080e-02,  ..., -3.4633e-02, -1.9666e-01, -1.0240e-01]],

         [[-1.1299e-01,  1.3065e-01,  1.5992e-01,  ...,  9.9875e-02, -1.6222e-01,  1.5957e-01],
          [-1.5637e-01,  1.6757e-01,  1.9759e-02,  ..., -1.7076e-02, -7.1625e-02,  1.1309e-01],
          [-2.4311e-01,  2.4142e-01, -2.6056e-01,  ..., -2.5098e-01,  1.0957e-01,  2.0144e-02],
          ...,
          [ 1.3892e-01,  3.5093e-01,  1.7653e-01,  ...,  3.4107e-03,  1.3945e-01,  1.3891e-01],
          [-7.6161e-02,  2.6509e-01, -9.0880e-02,  ..., -1.3654e-01, -2.0534e-01,  1.5350e-01],
          [-1.8370e-01,  2.2216e-01, -2.2458e-01,  ..., -2.0651e-01, -3.7774e-01,  1.6079e-01]],

         [[ 7.0234e-03,  9.9358e-02,  2.2074e-01,  ..., -6.1196e-02,  3.2576e-01, -3.5695e-02],
          [ 4.5952e-02,  9.0487e-02,  2.1239e-01,  ..., -1.2676e-01,  1.2942e-01,  3.2704e-02],
          [ 1.2381e-01,  7.2745e-02,  1.9569e-01,  ..., -2.5788e-01, -2.6326e-01,  1.6950e-01],
          ...,
          [-1.2440e-01,  3.1509e-02, -3.1450e-01,  ..., -7.7399e-02,  3.1531e-01,  1.2034e-01],
          [ 2.6617e-02, -1.9186e-02, -3.0080e-01,  ...,  1.2924e-01,  2.0302e-01,  1.1885e-01],
          [ 1.0212e-01, -4.4534e-02, -2.9395e-01,  ...,  2.3256e-01,  1.4687e-01,  1.1811e-01]]]])

2025-07-09 13:40:53.237302 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([16, 40, 297097, 12],"float32"), size=list[32,24,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([16, 40, 297097, 12],"float32"), size=list[32,24,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 475913 / 491520 (96.8%)
Greatest absolute difference: 0.952296793460846 at index (10, 2, 3, 23) (up to 0.01 allowed)
Greatest relative difference: 88469.8671875 at index (0, 31, 8, 6) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 40, 32, 24]), dtype=torch.float32)
tensor([[[[-2.2491e-01, -2.8655e-01, -3.4820e-01,  ...,  1.2203e-01,  1.1315e-01,  1.0427e-01],
          [ 1.8975e-02,  9.7615e-02,  1.7626e-01,  ...,  3.0500e-01,  5.0860e-02, -2.0328e-01],
          [-3.6855e-01, -9.0383e-02,  1.8779e-01,  ...,  3.4404e-01,  1.4026e-02, -3.1599e-01],
          ...,
          [ 2.2290e-01,  3.0734e-01,  3.9178e-01,  ...,  3.3709e-01,  1.6684e-01, -3.4033e-03],
          [ 2.2254e-01,  1.1486e-01,  7.1732e-03,  ..., -2.2784e-01, -1.9327e-01, -1.5869e-01],
          [-1.7846e-01, -2.6090e-01, -3.4334e-01,  ...,  4.2046e-02,  2.5235e-01,  4.6265e-01]],

         [[ 2.5052e-01,  2.8769e-01,  3.2487e-01,  ..., -1.9380e-01, -3.1230e-01, -4.3080e-01],
          [-4.1221e-01, -1.8136e-01,  4.9484e-02,  ...,  1.7265e-02, -2.2049e-03, -2.1675e-02],
          [ 1.8725e-01,  2.7963e-01,  3.7201e-01,  ...,  6.4076e-02, -1.9224e-01, -4.4855e-01],
          ...,
          [ 2.3565e-01,  3.3251e-02, -1.6914e-01,  ...,  2.3851e-02,  7.9395e-02,  1.3494e-01],
          [ 1.3977e-01,  2.2072e-01,  3.0166e-01,  ...,  8.5361e-02,  1.7484e-01,  2.6433e-01],
          [ 7.3852e-02,  2.4640e-01,  4.1896e-01,  ..., -1.5344e-01,  1.3123e-01,  4.1590e-01]],

         [[ 4.1007e-01,  8.2306e-02, -2.4546e-01,  ..., -2.0085e-01,  2.6875e-03,  2.0622e-01],
          [ 3.4355e-02,  1.9155e-01,  3.4875e-01,  ...,  8.3854e-02,  2.1574e-01,  3.4762e-01],
          [ 2.9900e-01,  1.7585e-01,  5.2700e-02,  ...,  1.5003e-01,  1.5084e-01,  1.5166e-01],
          ...,
          [-1.7837e-01, -1.0372e-01, -2.9067e-02,  ...,  3.3974e-02,  1.5066e-01,  2.6734e-01],
          [ 2.4106e-01,  2.0878e-01,  1.7649e-01,  ...,  3.3762e-01,  7.7688e-02, -1.8224e-01],
          [-2.7068e-01, -2.3360e-01, -1.9652e-01,  ...,  3.0497e-01,  2.4777e-01,  1.9058e-01]],

         ...,

         [[ 2.0253e-01,  1.3491e-01,  6.7286e-02,  ...,  3.9875e-01,  6.0776e-05, -3.9863e-01],
          [ 1.1803e-01,  2.5079e-01,  3.8355e-01,  ..., -2.8786e-01,  1.3254e-02,  3.1437e-01],
          [ 3.3586e-02,  1.1958e-01,  2.0557e-01,  ...,  1.5115e-01,  1.1080e-01,  7.0452e-02],
          ...,
          [ 2.3993e-01,  1.5303e-01,  6.6125e-02,  ..., -1.2767e-01,  2.1423e-02,  1.7052e-01],
          [-1.8620e-01, -6.9897e-02,  4.6410e-02,  ..., -1.1656e-01,  6.1780e-02,  2.4012e-01],
          [ 2.0118e-01,  1.3637e-03, -1.9846e-01,  ...,  4.0861e-01,  4.9209e-02, -3.1019e-01]],

         [[-2.1239e-01, -4.5701e-02,  1.2098e-01,  ..., -1.7959e-01,  1.2475e-01,  4.2909e-01],
          [-2.0817e-02, -1.5471e-01, -2.8860e-01,  ..., -4.6461e-02, -3.2943e-02, -1.9426e-02],
          [ 3.6006e-01,  1.1712e-01, -1.2581e-01,  ..., -4.1876e-01, -1.4219e-01,  1.3437e-01],
          ...,
          [-1.1916e-01, -8.7223e-02, -5.5282e-02,  ..., -3.6207e-01, -1.6516e-01,  3.1747e-02],
          [ 6.8954e-02, -2.3619e-02, -1.1619e-01,  ..., -3.8331e-01, -2.2996e-01, -7.6620e-02],
          [ 4.6720e-01,  4.3501e-01,  4.0283e-01,  ..., -2.2120e-01, -4.0904e-03,  2.1302e-01]],

         [[ 4.9083e-01,  3.1699e-01,  1.4315e-01,  ...,  3.5164e-01,  2.2054e-01,  8.9431e-02],
          [ 1.3462e-01, -3.8440e-02, -2.1149e-01,  ...,  6.5543e-02,  1.0249e-01,  1.3944e-01],
          [-7.7780e-02, -9.8838e-02, -1.1990e-01,  ...,  9.0346e-02, -1.6036e-03, -9.3553e-02],
          ...,
          [-4.2465e-02, -1.8548e-01, -3.2849e-01,  ..., -4.2242e-02, -1.0606e-01, -1.6987e-01],
          [-2.3647e-01, -8.8688e-02,  5.9099e-02,  ...,  2.0843e-01,  4.3821e-02, -1.2079e-01],
          [-3.6281e-01, -2.3846e-01, -1.1410e-01,  ..., -9.6030e-02,  1.5076e-01,  3.9754e-01]]],


        [[[-6.0294e-02,  5.7424e-02,  1.7514e-01,  ..., -4.3186e-02, -1.6731e-01, -2.9144e-01],
          [-3.2896e-01, -1.0847e-01,  1.1202e-01,  ..., -1.3537e-01, -2.5999e-01, -3.8461e-01],
          [ 3.0154e-01,  1.7964e-01,  5.7730e-02,  ...,  1.8794e-01,  2.7366e-02, -1.3321e-01],
          ...,
          [ 1.0055e-01,  1.2245e-01,  1.4435e-01,  ..., -1.3123e-01, -1.9196e-01, -2.5269e-01],
          [-3.7628e-01, -3.3294e-01, -2.8961e-01,  ...,  1.6512e-01,  2.6412e-02, -1.1230e-01],
          [ 1.2500e-01,  2.0004e-01,  2.7507e-01,  ..., -2.1330e-01, -2.8824e-01, -3.6319e-01]],

         [[-1.0884e-01,  1.5685e-01,  4.2254e-01,  ..., -3.4655e-01,  1.2212e-02,  3.7098e-01],
          [-1.8108e-01,  5.1847e-02,  2.8477e-01,  ..., -3.7602e-01, -2.7101e-01, -1.6599e-01],
          [ 2.6163e-01, -7.7680e-02, -4.1699e-01,  ...,  2.4046e-02, -1.5857e-01, -3.4118e-01],
          ...,
          [-2.2732e-01, -1.9102e-01, -1.5471e-01,  ...,  2.9534e-01,  1.2355e-01, -4.8234e-02],
          [ 2.9705e-01,  2.1815e-01,  1.3926e-01,  ..., -2.7570e-01,  4.1731e-02,  3.5916e-01],
          [-1.0813e-01, -7.6166e-02, -4.4201e-02,  ...,  7.3422e-02,  2.1376e-01,  3.5409e-01]],

         [[ 3.1917e-01,  1.5930e-01, -5.6077e-04,  ..., -1.2869e-01,  3.8171e-02,  2.0504e-01],
          [ 1.5709e-01, -3.0367e-02, -2.1782e-01,  ...,  2.5132e-01,  1.6544e-01,  7.9557e-02],
          [ 2.1887e-01,  4.2285e-02, -1.3430e-01,  ...,  3.3491e-02,  1.0499e-02, -1.2494e-02],
          ...,
          [-3.7934e-01, -1.2794e-01,  1.2346e-01,  ...,  1.2077e-01, -9.5800e-02, -3.1237e-01],
          [-3.9304e-01, -1.8505e-01,  2.2938e-02,  ..., -9.0691e-02, -3.3258e-02,  2.4174e-02],
          [ 1.4702e-01,  5.4842e-02, -3.7338e-02,  ..., -3.8420e-01, -1.4358e-01,  9.7034e-02]],

         ...,

         [[ 2.2017e-01, -4.7963e-02, -3.1610e-01,  ...,  3.3934e-02,  1.9924e-01,  3.6455e-01],
          [ 4.1121e-01,  3.8566e-01,  3.6011e-01,  ...,  8.1557e-02,  1.4959e-02, -5.1640e-02],
          [-3.9817e-02,  9.4674e-03,  5.8752e-02,  ..., -1.5277e-01, -2.0007e-01, -2.4738e-01],
          ...,
          [ 4.4939e-02,  1.0129e-02, -2.4682e-02,  ...,  1.2986e-01,  8.7656e-02,  4.5448e-02],
          [ 1.6651e-01, -1.4145e-02, -1.9480e-01,  ...,  8.6870e-02, -2.7720e-02, -1.4231e-01],
          [-3.8004e-01, -3.7799e-01, -3.7594e-01,  ..., -3.6123e-02, -9.4873e-02, -1.5362e-01]],

         [[-4.4441e-01, -1.1777e-01,  2.0886e-01,  ...,  3.5526e-01,  3.1666e-01,  2.7807e-01],
          [ 8.1445e-02, -1.7573e-01, -4.3291e-01,  ..., -1.1359e-01, -1.4508e-01, -1.7657e-01],
          [-3.4335e-01, -7.1018e-02,  2.0132e-01,  ...,  3.2809e-02, -3.6946e-02, -1.0670e-01],
          ...,
          [-2.4599e-01, -1.3096e-01, -1.5934e-02,  ...,  4.7457e-02,  2.1443e-01,  3.8141e-01],
          [ 1.6880e-01,  1.2445e-01,  8.0103e-02,  ...,  1.6671e-01,  2.0195e-01,  2.3720e-01],
          [-3.3271e-01, -3.4988e-01, -3.6705e-01,  ..., -3.1622e-01, -3.6477e-01, -4.1331e-01]],

         [[-4.7794e-01, -3.1885e-01, -1.5976e-01,  ...,  4.5148e-01,  4.6671e-01,  4.8195e-01],
          [ 4.8522e-02, -3.4136e-03, -5.5349e-02,  ..., -1.5641e-01, -7.1726e-02,  1.2963e-02],
          [-2.4689e-02, -7.7846e-02, -1.3100e-01,  ...,  1.3972e-01,  7.4587e-02,  9.4581e-03],
          ...,
          [ 3.2175e-01, -4.5441e-02, -4.1264e-01,  ...,  1.3567e-01,  1.4981e-01,  1.6395e-01],
          [-1.4152e-01, -1.1913e-01, -9.6734e-02,  ...,  5.6646e-02, -1.1838e-01, -2.9341e-01],
          [-4.7266e-02, -5.4831e-02, -6.2396e-02,  ...,  3.0381e-01, -1.6901e-02, -3.3761e-01]]],


        [[[-8.5977e-02, -1.0599e-01, -1.2601e-01,  ...,  1.7874e-02, -4.5311e-02, -1.0850e-01],
          [ 4.2088e-01,  4.1780e-01,  4.1473e-01,  ...,  3.0162e-01,  2.1637e-01,  1.3112e-01],
          [ 2.8001e-01,  2.7560e-01,  2.7119e-01,  ...,  2.1609e-02,  1.3751e-01,  2.5341e-01],
          ...,
          [-3.3688e-02, -3.6517e-03,  2.6385e-02,  ...,  1.2032e-01,  3.0979e-02, -5.8363e-02],
          [ 5.3523e-02, -2.0933e-01, -4.7218e-01,  ...,  2.1768e-01,  6.3549e-02, -9.0580e-02],
          [-1.7628e-01, -1.6030e-01, -1.4432e-01,  ...,  2.2546e-01,  2.5643e-01,  2.8740e-01]],

         [[ 1.0635e-01,  2.2259e-01,  3.3882e-01,  ...,  1.1100e-01,  5.3183e-02, -4.6326e-03],
          [ 2.6145e-02, -1.6149e-01, -3.4913e-01,  ...,  8.9475e-02, -7.9043e-02, -2.4756e-01],
          [ 1.1458e-02, -2.8099e-02, -6.7655e-02,  ..., -2.1510e-01, -6.2368e-02,  9.0366e-02],
          ...,
          [ 2.2246e-01, -5.5042e-02, -3.3254e-01,  ...,  1.7522e-01,  1.6731e-01,  1.5941e-01],
          [ 4.9921e-02, -1.0816e-01, -2.6625e-01,  ..., -2.6961e-01, -9.9635e-02,  7.0343e-02],
          [ 4.8653e-01,  4.7015e-01,  4.5378e-01,  ..., -3.7184e-02,  1.5270e-01,  3.4258e-01]],

         [[ 3.7464e-01,  2.7598e-02, -3.1944e-01,  ..., -4.4412e-01, -3.3589e-01, -2.2766e-01],
          [ 8.2054e-02, -6.3731e-02, -2.0951e-01,  ...,  4.2265e-01,  2.0081e-01, -2.1036e-02],
          [-3.7754e-01, -3.0927e-01, -2.4100e-01,  ...,  7.0096e-02,  6.8177e-02,  6.6258e-02],
          ...,
          [-2.3247e-01, -8.6552e-02,  5.9363e-02,  ...,  1.8047e-01, -1.7776e-02, -2.1602e-01],
          [-2.8516e-01, -1.0178e-01,  8.1597e-02,  ..., -1.5485e-01,  4.7157e-02,  2.4917e-01],
          [ 1.0013e-02, -1.6955e-01, -3.4912e-01,  ...,  1.8457e-01,  7.0798e-02, -4.2974e-02]],

         ...,

         [[ 4.7945e-01,  9.7770e-02, -2.8391e-01,  ...,  5.9379e-02,  2.7257e-01,  4.8576e-01],
          [-3.0414e-01, -9.4134e-02,  1.1588e-01,  ..., -2.8266e-01, -1.1360e-02,  2.5994e-01],
          [-2.6965e-01, -2.0955e-01, -1.4944e-01,  ...,  1.8846e-02,  1.7426e-01,  3.2967e-01],
          ...,
          [ 3.3927e-01,  1.1555e-01, -1.0817e-01,  ...,  2.4617e-02,  2.0083e-02,  1.5549e-02],
          [ 1.1333e-01, -6.2138e-02, -2.3761e-01,  ...,  4.7780e-02,  2.3361e-01,  4.1944e-01],
          [-4.3057e-02, -5.0261e-02, -5.7464e-02,  ...,  3.8644e-02,  1.8258e-01,  3.2653e-01]],

         [[-9.2181e-03, -1.7433e-01, -3.3945e-01,  ..., -2.3261e-01,  4.7901e-02,  3.2841e-01],
          [ 1.8085e-01,  1.5505e-02, -1.4984e-01,  ...,  3.2192e-01,  4.2229e-02, -2.3746e-01],
          [-2.8688e-01, -1.0621e-01,  7.4467e-02,  ..., -1.5323e-01, -1.9758e-01, -2.4194e-01],
          ...,
          [-4.0163e-01, -3.2838e-01, -2.5512e-01,  ...,  3.6002e-01,  1.8286e-01,  5.6960e-03],
          [-2.5333e-01, -1.5825e-02,  2.2168e-01,  ...,  7.7881e-02,  1.6733e-01,  2.5678e-01],
          [-2.5443e-02, -1.4744e-01, -2.6943e-01,  ..., -1.8761e-02, -7.5653e-02, -1.3255e-01]],

         [[-2.6652e-01,  9.4229e-02,  4.5498e-01,  ...,  2.5803e-01,  3.5921e-01,  4.6039e-01],
          [ 3.0389e-01,  2.3953e-01,  1.7517e-01,  ..., -1.0400e-01, -2.4206e-02,  5.5593e-02],
          [-1.1922e-01, -1.6782e-01, -2.1642e-01,  ...,  1.7084e-02, -2.3025e-02, -6.3134e-02],
          ...,
          [-3.9532e-01, -1.6045e-01,  7.4423e-02,  ...,  7.4172e-02, -9.3603e-02, -2.6138e-01],
          [-3.6992e-01, -2.0692e-01, -4.3923e-02,  ...,  7.5502e-02,  1.6774e-01,  2.5997e-01],
          [-2.0901e-01, -3.4163e-01, -4.7425e-01,  ...,  8.1261e-02, -1.5626e-01, -3.9379e-01]]],


        ...,


        [[[-5.4200e-02,  1.0060e-01,  2.5539e-01,  ..., -4.3128e-01, -1.8031e-01,  7.0656e-02],
          [-3.6453e-01, -2.1443e-01, -6.4335e-02,  ...,  4.7501e-02, -7.6037e-02, -1.9958e-01],
          [-9.8072e-02, -1.0677e-01, -1.1547e-01,  ..., -2.7865e-01, -5.6944e-02,  1.6476e-01],
          ...,
          [-1.3059e-01,  1.1757e-01,  3.6572e-01,  ...,  1.2047e-01,  1.1229e-01,  1.0411e-01],
          [-3.6621e-01, -3.1308e-01, -2.5995e-01,  ..., -9.1414e-02, -8.1825e-02, -7.2237e-02],
          [ 2.4300e-01,  1.4141e-01,  3.9817e-02,  ...,  4.1773e-01,  3.3739e-01,  2.5706e-01]],

         [[ 2.4730e-01, -4.2122e-02, -3.3155e-01,  ..., -3.7975e-01, -1.2501e-01,  1.2973e-01],
          [ 4.7646e-01,  4.5407e-02, -3.8565e-01,  ...,  3.8453e-01,  1.0171e-01, -1.8112e-01],
          [-1.5653e-01, -1.7327e-01, -1.9000e-01,  ..., -1.2967e-01, -2.1526e-01, -3.0084e-01],
          ...,
          [ 3.3624e-02, -2.6245e-02, -8.6115e-02,  ..., -8.1999e-02,  7.3170e-02,  2.2834e-01],
          [ 1.0381e-01, -8.3019e-03, -1.2041e-01,  ...,  1.2876e-01,  2.8010e-02, -7.2739e-02],
          [ 3.9270e-01,  2.8816e-02, -3.3507e-01,  ...,  4.0250e-01,  3.6471e-01,  3.2692e-01]],

         [[ 1.1619e-01,  5.4869e-02, -6.4529e-03,  ...,  2.1499e-01, -5.5753e-02, -3.2650e-01],
          [ 3.0475e-01,  2.9192e-01,  2.7909e-01,  ...,  1.9859e-01,  1.9493e-01,  1.9128e-01],
          [-3.9629e-01, -2.8028e-01, -1.6427e-01,  ..., -1.8439e-01, -2.3985e-01, -2.9530e-01],
          ...,
          [ 1.7847e-01, -8.3049e-03, -1.9508e-01,  ...,  7.8332e-02,  7.1834e-02,  6.5336e-02],
          [ 4.1995e-01,  2.9954e-01,  1.7913e-01,  ...,  2.9653e-02,  1.3329e-01,  2.3694e-01],
          [-3.8982e-01, -2.3789e-01, -8.5970e-02,  ...,  1.6456e-01,  2.2796e-01,  2.9136e-01]],

         ...,

         [[ 3.6623e-01,  9.9319e-03, -3.4637e-01,  ...,  1.3731e-01,  1.3552e-01,  1.3372e-01],
          [-1.0863e-01, -1.9010e-01, -2.7156e-01,  ..., -1.8692e-01, -2.1049e-03,  1.8271e-01],
          [-5.5564e-02, -2.5104e-01, -4.4651e-01,  ..., -9.7344e-02,  1.7886e-01,  4.5506e-01],
          ...,
          [ 3.0665e-01,  1.5246e-01, -1.7284e-03,  ...,  3.8084e-02, -6.4488e-03, -5.0982e-02],
          [ 1.4935e-01,  1.0065e-02, -1.2922e-01,  ..., -1.7532e-01,  6.6120e-02,  3.0755e-01],
          [ 4.8698e-01,  4.5766e-01,  4.2834e-01,  ..., -1.3332e-01,  1.7280e-01,  4.7892e-01]],

         [[ 2.3416e-01, -7.6256e-03, -2.4941e-01,  ..., -1.6711e-01, -1.4675e-01, -1.2639e-01],
          [-1.2285e-01, -1.3928e-01, -1.5570e-01,  ..., -1.1741e-01, -8.5355e-02, -5.3295e-02],
          [ 1.4062e-01,  1.8613e-01,  2.3165e-01,  ..., -1.2685e-01, -2.1216e-01, -2.9746e-01],
          ...,
          [-2.3735e-01, -1.1591e-01,  5.5241e-03,  ..., -5.5193e-02, -5.8331e-02, -6.1468e-02],
          [ 2.7398e-01, -4.0865e-02, -3.5571e-01,  ...,  6.1631e-02, -1.6383e-01, -3.8930e-01],
          [ 2.1060e-01,  1.3959e-01,  6.8587e-02,  ...,  5.9161e-03,  2.2248e-01,  4.3905e-01]],

         [[-2.7182e-01, -1.3298e-01,  5.8658e-03,  ...,  2.5314e-01,  2.2327e-01,  1.9340e-01],
          [ 1.4881e-01, -3.6002e-02, -2.2082e-01,  ..., -1.9413e-01, -1.9221e-01, -1.9029e-01],
          [-3.3616e-01, -2.1552e-01, -9.4878e-02,  ...,  3.2354e-01,  1.8664e-01,  4.9733e-02],
          ...,
          [-3.3549e-01, -3.2587e-01, -3.1626e-01,  ...,  4.5952e-02, -1.9280e-02, -8.4512e-02],
          [-2.9622e-02,  1.2984e-02,  5.5590e-02,  ...,  4.0478e-01,  2.8549e-01,  1.6620e-01],
          [ 9.1802e-02,  2.3813e-02, -4.4177e-02,  ...,  3.7656e-01,  4.2105e-01,  4.6555e-01]]],


        [[[ 5.0602e-02,  2.0970e-01,  3.6880e-01,  ...,  3.6952e-02, -4.8287e-02, -1.3353e-01],
          [ 2.2033e-01, -3.9956e-02, -3.0024e-01,  ...,  6.3408e-02,  1.4100e-01,  2.1860e-01],
          [ 5.9716e-02,  4.6734e-02,  3.3753e-02,  ..., -2.0448e-01, -2.0391e-01, -2.0334e-01],
          ...,
          [ 8.7157e-02,  1.3975e-01,  1.9234e-01,  ..., -3.2292e-01, -1.4994e-01,  2.3050e-02],
          [-4.3353e-01, -3.6777e-01, -3.0201e-01,  ..., -1.2473e-01,  4.6681e-02,  2.1809e-01],
          [-3.9423e-01, -2.0552e-01, -1.6806e-02,  ..., -2.1308e-01, -2.8448e-01, -3.5589e-01]],

         [[ 4.8975e-01,  3.0800e-01,  1.2626e-01,  ...,  1.9329e-01,  5.5802e-02, -8.1685e-02],
          [-1.1805e-01, -5.2557e-02,  1.2933e-02,  ...,  4.4749e-01,  2.7164e-01,  9.5791e-02],
          [ 1.0581e-01, -1.6002e-02, -1.3782e-01,  ..., -5.1490e-02, -7.9501e-02, -1.0751e-01],
          ...,
          [-3.3762e-01, -3.7125e-01, -4.0487e-01,  ...,  7.4562e-02, -6.6833e-02, -2.0823e-01],
          [ 1.8522e-01,  7.9897e-02, -2.5427e-02,  ..., -9.6175e-02, -1.1036e-01, -1.2454e-01],
          [ 4.7664e-01,  1.3486e-01, -2.0691e-01,  ..., -2.8396e-01,  3.5174e-03,  2.9099e-01]],

         [[ 2.0066e-01,  3.1351e-01,  4.2636e-01,  ..., -3.5321e-01, -2.9985e-01, -2.4649e-01],
          [ 8.4289e-03,  2.0421e-01,  3.9999e-01,  ..., -1.7468e-01, -9.3469e-02, -1.2260e-02],
          [ 1.0699e-01,  1.7734e-01,  2.4769e-01,  ...,  1.8596e-01,  7.7034e-02, -3.1888e-02],
          ...,
          [-4.2065e-01, -1.6503e-01,  9.0599e-02,  ...,  2.8290e-01,  1.9331e-01,  1.0372e-01],
          [ 3.2340e-01,  2.2932e-01,  1.3524e-01,  ..., -3.1544e-01,  2.7776e-03,  3.2099e-01],
          [-4.3111e-01, -1.1913e-01,  1.9285e-01,  ...,  4.3181e-01,  1.2661e-01, -1.7859e-01]],

         ...,

         [[-4.1186e-01, -3.2478e-01, -2.3771e-01,  ...,  3.2501e-01,  3.6531e-01,  4.0561e-01],
          [-3.2102e-01, -2.0493e-01, -8.8834e-02,  ...,  1.5683e-01,  1.3868e-01,  1.2053e-01],
          [-1.9293e-01, -1.9676e-01, -2.0058e-01,  ..., -9.2251e-02,  1.7500e-01,  4.4225e-01],
          ...,
          [ 1.9264e-01,  2.2093e-01,  2.4922e-01,  ..., -2.2240e-01,  1.0918e-01,  4.4076e-01],
          [-1.5951e-02, -1.2619e-01, -2.3644e-01,  ...,  8.2103e-02,  2.1421e-01,  3.4632e-01],
          [ 3.5387e-02, -1.9636e-01, -4.2811e-01,  ..., -8.4012e-02,  1.7046e-02,  1.1810e-01]],

         [[ 5.2915e-02, -3.4133e-02, -1.2118e-01,  ...,  3.5342e-01,  5.7708e-02, -2.3801e-01],
          [ 9.9192e-03, -2.0812e-01, -4.2616e-01,  ..., -1.1716e-01,  6.0540e-02,  2.3824e-01],
          [ 1.8347e-01,  4.4169e-02, -9.5132e-02,  ..., -8.3937e-02, -1.4689e-01, -2.0985e-01],
          ...,
          [-7.2114e-02,  1.5031e-01,  3.7273e-01,  ..., -5.0389e-03,  1.0768e-01,  2.2040e-01],
          [ 2.5319e-01,  2.0532e-01,  1.5746e-01,  ..., -2.6012e-01, -1.3465e-01, -9.1820e-03],
          [ 1.4741e-01, -1.0207e-01, -3.5154e-01,  ...,  3.5517e-01,  2.2458e-01,  9.3980e-02]],

         [[-1.9475e-01, -3.1551e-01, -4.3627e-01,  ...,  2.3555e-01,  2.2044e-01,  2.0534e-01],
          [ 4.3192e-01,  2.2022e-01,  8.5291e-03,  ..., -5.3413e-02,  8.6023e-02,  2.2546e-01],
          [-3.6764e-01, -8.8413e-02,  1.9082e-01,  ..., -1.2656e-01, -9.9525e-02, -7.2489e-02],
          ...,
          [-5.1934e-02, -8.1856e-02, -1.1178e-01,  ..., -5.1217e-02,  9.9522e-03,  7.1121e-02],
          [ 9.3516e-02,  1.7544e-02, -5.8427e-02,  ..., -2.2116e-01, -1.7006e-02,  1.8715e-01],
          [-2.0066e-01, -2.0775e-02,  1.5911e-01,  ...,  3.4669e-01,  3.9429e-01,  4.4189e-01]]],


        [[[-3.4274e-02,  9.5331e-02,  2.2493e-01,  ..., -2.3170e-01, -9.0515e-02,  5.0670e-02],
          [ 4.1418e-01,  3.1662e-01,  2.1907e-01,  ..., -2.6107e-01, -9.8817e-02,  6.3437e-02],
          [-1.1249e-03,  7.6278e-02,  1.5368e-01,  ...,  1.4485e-01,  4.3384e-02, -5.8077e-02],
          ...,
          [ 1.4834e-01,  3.7644e-02, -7.3052e-02,  ...,  2.0661e-01,  1.7136e-01,  1.3610e-01],
          [ 1.6194e-01, -1.5873e-02, -1.9368e-01,  ...,  2.8484e-01,  2.1235e-01,  1.3985e-01],
          [ 3.4946e-01, -2.8236e-02, -4.0593e-01,  ...,  3.1403e-01,  1.5446e-01, -5.1053e-03]],

         [[ 1.9805e-01, -2.4915e-02, -2.4788e-01,  ...,  1.8617e-01,  1.2213e-01,  5.8086e-02],
          [ 4.2979e-02, -1.7386e-01, -3.9069e-01,  ..., -7.4307e-02, -4.5385e-02, -1.6462e-02],
          [ 1.3530e-01,  2.6576e-02, -8.2144e-02,  ..., -3.7395e-01, -1.9799e-01, -2.2026e-02],
          ...,
          [-2.1757e-01,  5.5616e-02,  3.2880e-01,  ..., -9.6160e-02, -3.4057e-02,  2.8045e-02],
          [ 1.3477e-01,  9.2750e-02,  5.0733e-02,  ...,  8.5966e-03, -1.3937e-01, -2.8733e-01],
          [ 2.7474e-01,  2.1999e-01,  1.6525e-01,  ..., -4.5338e-01, -2.7571e-01, -9.8049e-02]],

         [[-3.9575e-01, -1.2569e-01,  1.4438e-01,  ...,  4.7976e-01,  2.7910e-01,  7.8447e-02],
          [ 3.6472e-01,  2.1473e-01,  6.4737e-02,  ..., -4.3772e-01, -6.9620e-02,  2.9848e-01],
          [-1.2964e-01, -6.5189e-02, -7.3305e-04,  ..., -6.3928e-02, -3.6527e-02, -9.1265e-03],
          ...,
          [-1.0948e-01, -1.6320e-03,  1.0621e-01,  ...,  2.5670e-01,  1.0184e-01, -5.3013e-02],
          [ 3.3945e-01,  2.5117e-01,  1.6288e-01,  ...,  2.3999e-01,  2.6990e-02, -1.8601e-01],
          [ 1.7479e-01,  1.8593e-01,  1.9707e-01,  ..., -4.0890e-01, -3.3739e-01, -2.6588e-01]],

         ...,

         [[ 3.3616e-01, -6.3559e-02, -4.6328e-01,  ..., -1.0803e-01, -1.6013e-01, -2.1223e-01],
          [ 1.4509e-02,  8.2319e-02,  1.5013e-01,  ..., -2.9949e-01, -9.6269e-02,  1.0695e-01],
          [ 3.3337e-01,  2.2233e-01,  1.1130e-01,  ..., -1.4509e-01, -1.6161e-01, -1.7812e-01],
          ...,
          [-2.0296e-01, -1.5367e-01, -1.0438e-01,  ..., -4.0972e-01, -1.9154e-01,  2.6634e-02],
          [ 1.9641e-01,  2.4245e-01,  2.8849e-01,  ..., -2.3024e-01, -3.2340e-01, -4.1657e-01],
          [ 6.0197e-02, -6.0000e-02, -1.8020e-01,  ...,  1.7681e-01,  6.7113e-02, -4.2582e-02]],

         [[ 3.4025e-01,  4.0466e-01,  4.6907e-01,  ..., -2.0840e-01, -1.0540e-01, -2.4045e-03],
          [ 1.8151e-01,  2.0877e-01,  2.3603e-01,  ...,  2.8519e-01,  1.3208e-01, -2.1033e-02],
          [ 2.3895e-01,  1.5388e-02, -2.0817e-01,  ...,  1.2020e-02,  4.0286e-02,  6.8552e-02],
          ...,
          [-3.2541e-01, -2.3397e-01, -1.4253e-01,  ..., -9.9631e-02, -2.6897e-02,  4.5837e-02],
          [ 6.7671e-02,  1.8239e-01,  2.9711e-01,  ..., -3.9284e-02,  1.1055e-02,  6.1393e-02],
          [ 2.5093e-01,  3.6873e-02, -1.7718e-01,  ...,  2.5451e-01,  1.8297e-01,  1.1144e-01]],

         [[-4.1211e-01, -3.1654e-01, -2.2097e-01,  ..., -1.4373e-01, -1.8742e-01, -2.3111e-01],
          [ 2.2142e-01,  8.8889e-02, -4.3641e-02,  ...,  6.6941e-02,  1.7997e-01,  2.9300e-01],
          [-3.1235e-01, -6.9619e-02,  1.7312e-01,  ..., -3.2189e-01, -1.6722e-01, -1.2551e-02],
          ...,
          [-1.5064e-01, -1.4532e-01, -1.4000e-01,  ...,  3.2985e-01,  2.7587e-01,  2.2189e-01],
          [-2.3728e-01, -2.2053e-01, -2.0378e-01,  ..., -1.3172e-01, -1.5901e-01, -1.8629e-01],
          [-4.2108e-01, -2.5562e-01, -9.0169e-02,  ...,  1.3342e-01,  3.0640e-01,  4.7938e-01]]]])
DESIRED: (shape=torch.Size([16, 40, 32, 24]), dtype=torch.float32)
tensor([[[[ 2.3456e-02,  1.0165e-01,  2.5803e-01,  ..., -1.3396e-01, -9.2244e-02, -7.1387e-02],
          [-2.7181e-01, -3.0105e-01, -3.5955e-01,  ...,  8.8018e-03, -2.1748e-01, -3.3062e-01],
          [ 4.2191e-01,  3.9739e-01,  3.4834e-01,  ...,  1.7849e-01,  1.4258e-02, -6.7858e-02],
          ...,
          [-2.7949e-01, -2.3895e-01, -1.5786e-01,  ..., -1.7677e-01, -1.0048e-01, -6.2335e-02],
          [ 1.3848e-01,  3.3705e-02, -1.7584e-01,  ..., -1.9408e-01, -8.5491e-02, -3.1195e-02],
          [ 3.7431e-01,  3.2582e-01,  2.2884e-01,  ...,  2.2875e-01, -3.9469e-02, -1.7358e-01]],

         [[ 9.9667e-02,  3.3451e-02, -9.8980e-02,  ...,  8.4022e-02,  2.0127e-01,  2.5989e-01],
          [-1.0343e-02,  4.8447e-02,  1.6603e-01,  ...,  1.8198e-01,  6.5742e-02,  7.6202e-03],
          [ 2.2767e-01,  1.4060e-01, -3.3539e-02,  ..., -1.4873e-01,  1.2109e-01,  2.5599e-01],
          ...,
          [-1.7011e-01, -4.0374e-02,  2.1910e-01,  ..., -5.0581e-02,  2.2803e-01,  3.6734e-01],
          [-9.5242e-02, -1.4532e-01, -2.4548e-01,  ...,  1.4530e-01, -1.4071e-01, -2.8371e-01],
          [-8.8428e-02, -6.5277e-02, -1.8976e-02,  ..., -3.1358e-02,  1.5809e-01,  2.5282e-01]],

         [[ 1.3897e-01,  1.0245e-01,  2.9421e-02,  ..., -6.2223e-02,  1.5925e-01,  2.6999e-01],
          [ 1.5378e-01,  1.2707e-01,  7.3652e-02,  ...,  1.7907e-02,  3.0603e-01,  4.5009e-01],
          [-7.7499e-02, -4.5487e-02,  1.8537e-02,  ..., -2.4054e-01, -9.1338e-02, -1.6738e-02],
          ...,
          [ 1.3752e-01,  1.8042e-01,  2.6623e-01,  ..., -5.1259e-02, -2.9782e-01, -4.2110e-01],
          [ 9.4121e-02,  1.7501e-01,  3.3678e-01,  ...,  3.7391e-01,  3.0320e-01,  2.6784e-01],
          [ 1.1647e-01,  1.9816e-01,  3.6153e-01,  ..., -1.7140e-01, -1.1102e-02,  6.9046e-02]],

         ...,

         [[-1.3586e-01, -6.6262e-02,  7.2935e-02,  ...,  2.1423e-02, -1.6785e-01, -2.6249e-01],
          [-2.2030e-02, -9.3227e-02, -2.3562e-01,  ..., -2.9113e-01, -9.5528e-02,  2.2728e-03],
          [ 1.0820e-01,  1.2915e-01,  1.7104e-01,  ...,  2.5410e-01, -7.0967e-02, -2.3350e-01],
          ...,
          [-2.2891e-01, -1.4305e-01,  2.8654e-02,  ..., -2.3492e-01, -1.6753e-01, -1.3384e-01],
          [-4.0579e-01, -3.3915e-01, -2.0587e-01,  ...,  1.5274e-02,  1.1340e-01,  1.6247e-01],
          [-1.1353e-01, -1.4243e-01, -2.0023e-01,  ..., -2.0295e-01, -1.6126e-01, -1.4042e-01]],

         [[-1.4642e-01, -5.1188e-02,  1.3929e-01,  ..., -1.2341e-01,  1.1323e-01,  2.3155e-01],
          [ 3.8901e-01,  1.8706e-01, -2.1682e-01,  ...,  2.6086e-01,  1.7277e-01,  1.2872e-01],
          [ 2.6410e-01,  8.5182e-02, -2.7266e-01,  ...,  1.2990e-02, -2.0354e-01, -3.1180e-01],
          ...,
          [-9.5128e-02, -1.5955e-01, -2.8841e-01,  ...,  1.7068e-01, -1.2306e-01, -2.6993e-01],
          [ 1.4487e-01,  2.0673e-01,  3.3047e-01,  ..., -5.0634e-02, -8.1916e-02, -9.7558e-02],
          [-3.4055e-01, -2.0735e-01,  5.9046e-02,  ...,  9.3555e-02, -1.9556e-01, -3.4012e-01]],

         [[ 4.0660e-01,  2.4571e-01, -7.6063e-02,  ..., -2.9903e-01,  2.0206e-02,  1.7982e-01],
          [ 1.7181e-01,  1.9577e-01,  2.4369e-01,  ..., -1.2300e-01, -5.5779e-02, -2.2169e-02],
          [ 1.8980e-01,  1.4661e-01,  6.0234e-02,  ..., -2.7342e-02,  5.5056e-02,  9.6254e-02],
          ...,
          [ 2.4117e-01,  8.8332e-02, -2.1734e-01,  ..., -3.7308e-02, -2.6200e-01, -3.7434e-01],
          [ 4.6415e-01,  4.0477e-01,  2.8603e-01,  ..., -9.7838e-02, -4.5534e-02, -1.9382e-02],
          [ 4.8273e-02,  1.1837e-01,  2.5858e-01,  ...,  1.0289e-01, -5.0783e-02, -1.2762e-01]]],


        [[[-4.4290e-01, -4.0590e-01, -3.3191e-01,  ..., -2.5976e-01, -1.8779e-01, -1.5180e-01],
          [ 4.7737e-02,  4.6648e-02,  4.4472e-02,  ..., -1.9893e-01,  1.1488e-01,  2.7179e-01],
          [-4.0017e-01, -3.3192e-01, -1.9542e-01,  ..., -7.1861e-02, -1.1892e-01, -1.4245e-01],
          ...,
          [ 1.3199e-02, -4.7387e-02, -1.6856e-01,  ..., -3.5097e-02,  7.7311e-02,  1.3352e-01],
          [-1.6087e-01, -9.2283e-02,  4.4893e-02,  ...,  2.3968e-02,  1.3971e-01,  1.9759e-01],
          [ 1.2595e-01,  8.8567e-02,  1.3798e-02,  ..., -3.5943e-01, -1.4263e-01, -3.4228e-02]],

         [[-8.3818e-02, -2.9029e-02,  8.0549e-02,  ..., -1.9641e-01, -3.1041e-01, -3.6740e-01],
          [ 4.0659e-02, -4.2880e-02, -2.0996e-01,  ...,  1.5971e-01, -2.0475e-01, -3.8698e-01],
          [-3.9976e-01, -2.3244e-01,  1.0219e-01,  ...,  1.1259e-01, -1.5228e-01, -2.8472e-01],
          ...,
          [-1.8586e-01, -1.5834e-01, -1.0329e-01,  ...,  1.1751e-01,  3.3021e-01,  4.3656e-01],
          [ 8.6714e-02,  2.4168e-02, -1.0093e-01,  ..., -2.1852e-01, -7.3149e-02, -4.6344e-04],
          [ 2.7314e-01,  2.6486e-01,  2.4831e-01,  ..., -2.5029e-01, -2.7572e-01, -2.8843e-01]],

         [[-7.7385e-02, -1.5513e-02,  1.0823e-01,  ...,  2.9193e-01,  2.1369e-01,  1.7457e-01],
          [-3.5886e-01, -3.6858e-01, -3.8801e-01,  ...,  5.3888e-02, -1.7628e-01, -2.9136e-01],
          [-1.2432e-02, -9.6205e-02, -2.6375e-01,  ..., -1.8704e-02, -3.0689e-02, -3.6682e-02],
          ...,
          [ 3.6881e-02, -6.8679e-02, -2.7980e-01,  ...,  3.7238e-02, -2.2942e-01, -3.6275e-01],
          [-4.5862e-01, -3.9798e-01, -2.7669e-01,  ..., -1.7827e-01, -6.3595e-02, -6.2587e-03],
          [-1.6241e-01, -5.6407e-02,  1.5560e-01,  ...,  1.6442e-01,  4.9017e-02, -8.6866e-03]],

         ...,

         [[-2.8256e-01, -1.4795e-01,  1.2126e-01,  ...,  3.5292e-01,  1.4473e-01,  4.0633e-02],
          [-1.7691e-01, -2.5574e-01, -4.1341e-01,  ...,  1.9204e-01, -1.2600e-01, -2.8501e-01],
          [-2.3754e-01, -1.0077e-01,  1.7277e-01,  ..., -4.0396e-02,  2.3196e-01,  3.6814e-01],
          ...,
          [-6.8701e-03, -1.5106e-02, -3.1578e-02,  ...,  9.8366e-02,  6.1191e-02,  4.2603e-02],
          [ 4.3718e-01,  2.7912e-01, -3.7012e-02,  ..., -1.8076e-02, -5.8127e-02, -7.8152e-02],
          [ 2.7589e-01,  1.9179e-01,  2.3592e-02,  ...,  3.3735e-02,  1.2358e-02,  1.6696e-03]],

         [[-1.1267e-01, -1.5656e-01, -2.4435e-01,  ...,  1.7089e-02,  1.4596e-01,  2.1039e-01],
          [-4.3814e-01, -2.5027e-01,  1.2549e-01,  ..., -4.7975e-02, -3.2399e-01, -4.6200e-01],
          [-3.3956e-01, -2.2917e-01, -8.3910e-03,  ..., -5.5956e-02, -1.9410e-01, -2.6317e-01],
          ...,
          [-2.1183e-01, -1.5307e-01, -3.5550e-02,  ..., -7.1415e-02, -1.1769e-01, -1.4083e-01],
          [-2.0711e-01, -1.7965e-01, -1.2472e-01,  ..., -2.6978e-03, -3.5446e-02, -5.1820e-02],
          [ 2.7064e-01,  2.1113e-01,  9.2106e-02,  ..., -3.6296e-02, -6.7684e-02, -8.3377e-02]],

         [[-2.6271e-01, -1.6415e-01,  3.2950e-02,  ..., -1.1683e-01,  1.7570e-01,  3.2196e-01],
          [ 4.5872e-01,  3.2626e-01,  6.1319e-02,  ...,  7.9741e-02,  2.4852e-01,  3.3291e-01],
          [ 2.9424e-02,  1.7514e-02, -6.3057e-03,  ...,  2.2658e-02,  2.0792e-01,  3.0056e-01],
          ...,
          [-2.1848e-01, -1.4551e-01,  4.2189e-04,  ..., -3.3016e-02, -5.7846e-02, -7.0261e-02],
          [-2.8617e-01, -2.0657e-01, -4.7354e-02,  ..., -2.4076e-01, -3.6128e-01, -4.2154e-01],
          [-2.0911e-01, -2.0785e-01, -2.0532e-01,  ..., -1.2134e-01, -8.0245e-02, -5.9699e-02]]],


        [[[ 1.0865e-02,  6.0000e-02,  1.5827e-01,  ..., -3.3149e-01, -1.5492e-01, -6.6630e-02],
          [ 7.8430e-02,  1.0652e-01,  1.6270e-01,  ...,  2.3896e-01, -1.8866e-01, -4.0247e-01],
          [-1.8860e-02, -4.2004e-02, -8.8291e-02,  ..., -1.4079e-01,  1.8891e-01,  3.5376e-01],
          ...,
          [-2.1513e-01, -1.1100e-01,  9.7262e-02,  ..., -2.6128e-03, -1.6236e-01, -2.4223e-01],
          [ 3.4503e-01,  2.0477e-01, -7.5743e-02,  ...,  3.5097e-01,  3.4018e-01,  3.3478e-01],
          [ 1.3405e-01,  1.2444e-01,  1.0522e-01,  ..., -1.4090e-01, -2.5009e-03,  6.6700e-02]],

         [[ 3.6528e-02, -6.1668e-02, -2.5806e-01,  ...,  1.3883e-02,  2.2180e-01,  3.2576e-01],
          [-3.1626e-01, -3.1128e-01, -3.0132e-01,  ...,  3.4004e-02, -1.9664e-01, -3.1197e-01],
          [ 1.5552e-01,  1.2241e-01,  5.6195e-02,  ..., -4.3566e-03, -7.9015e-03, -9.6739e-03],
          ...,
          [-2.0682e-01, -2.4271e-01, -3.1450e-01,  ..., -2.8518e-01, -1.2817e-01, -4.9658e-02],
          [ 2.3049e-01,  1.3624e-01, -5.2241e-02,  ...,  2.8095e-01,  2.2852e-01,  2.0231e-01],
          [-1.5494e-01, -1.9449e-01, -2.7357e-01,  ..., -1.2012e-01, -1.0089e-01, -9.1274e-02]],

         [[ 4.1886e-01,  3.2826e-01,  1.4707e-01,  ...,  2.4079e-01, -2.5920e-02, -1.5928e-01],
          [ 4.2105e-01,  3.8348e-01,  3.0835e-01,  ..., -2.1936e-01, -2.7106e-01, -2.9691e-01],
          [ 4.0095e-02,  7.8000e-02,  1.5381e-01,  ...,  9.0766e-02,  3.4588e-02,  6.4994e-03],
          ...,
          [ 2.5433e-01,  1.7322e-01,  1.1000e-02,  ..., -9.4620e-02,  2.2781e-01,  3.8903e-01],
          [ 3.5025e-01,  1.5491e-01, -2.3577e-01,  ..., -8.1590e-02,  2.9546e-02,  8.5115e-02],
          [ 1.9020e-01,  1.0750e-01, -5.7893e-02,  ...,  5.7451e-02,  1.3392e-01,  1.7215e-01]],

         ...,

         [[-1.3085e-01, -1.5628e-01, -2.0713e-01,  ...,  1.6139e-02,  1.7938e-01,  2.6100e-01],
          [-2.0341e-02,  8.8271e-03,  6.7164e-02,  ...,  1.3482e-01,  1.5129e-01,  1.5952e-01],
          [ 4.1113e-01,  3.0136e-01,  8.1820e-02,  ..., -2.0115e-01,  1.5880e-01,  3.3877e-01],
          ...,
          [-3.6718e-01, -3.6663e-01, -3.6551e-01,  ...,  1.6370e-01,  8.3316e-02,  4.3126e-02],
          [ 3.4263e-01,  1.6535e-01, -1.8921e-01,  ...,  2.5932e-01,  1.9775e-01,  1.6696e-01],
          [-2.4590e-01, -2.1095e-01, -1.4105e-01,  ..., -7.5116e-02,  1.7574e-01,  3.0116e-01]],

         [[-1.4752e-01, -4.1142e-02,  1.7161e-01,  ...,  8.8576e-02, -7.3298e-02, -1.5423e-01],
          [ 2.8808e-01,  1.8137e-01, -3.2048e-02,  ..., -6.6467e-02, -8.2704e-02, -9.0823e-02],
          [ 1.6600e-01,  2.0193e-01,  2.7379e-01,  ...,  4.2573e-02, -1.3435e-01, -2.2281e-01],
          ...,
          [ 1.3620e-01, -2.1070e-03, -2.7872e-01,  ...,  4.0958e-01,  3.5951e-01,  3.3448e-01],
          [ 5.1313e-02,  1.1510e-01,  2.4268e-01,  ..., -2.5881e-01,  1.0585e-01,  2.8818e-01],
          [-3.3996e-02, -4.5441e-02, -6.8331e-02,  ...,  2.2526e-01, -6.3540e-03, -1.2216e-01]],

         [[-1.8434e-01, -2.5927e-02,  2.9090e-01,  ...,  8.4392e-02, -2.8093e-01, -4.6359e-01],
          [-7.3375e-03,  3.6362e-02,  1.2376e-01,  ..., -1.6206e-01, -2.0414e-01, -2.2518e-01],
          [-7.2597e-02, -2.6590e-02,  6.5426e-02,  ...,  3.0250e-01,  2.6550e-02, -1.1143e-01],
          ...,
          [-3.0367e-01, -1.8267e-01,  5.9343e-02,  ..., -5.4361e-02,  1.7797e-01,  2.9413e-01],
          [-3.2250e-01, -2.9195e-01, -2.3085e-01,  ..., -7.5690e-02, -5.1171e-02, -3.8912e-02],
          [ 1.0455e-01,  5.5379e-02, -4.2956e-02,  ...,  1.3180e-01,  1.1261e-02, -4.9007e-02]]],


        ...,


        [[[ 3.6125e-01,  2.2538e-01, -4.6375e-02,  ..., -1.3507e-01, -1.5666e-02,  4.4035e-02],
          [-4.1729e-02,  4.0012e-02,  2.0349e-01,  ..., -3.5089e-02,  3.0247e-01,  4.7125e-01],
          [-3.6691e-01, -2.6841e-01, -7.1410e-02,  ..., -6.7841e-02, -4.8569e-02, -3.8933e-02],
          ...,
          [-2.7720e-01, -1.3515e-01,  1.4895e-01,  ..., -5.5671e-02,  2.5579e-02,  6.6205e-02],
          [ 2.3214e-01,  6.5681e-02, -2.6723e-01,  ...,  1.7737e-01, -4.0640e-02, -1.4964e-01],
          [ 3.5288e-01,  1.6653e-01, -2.0616e-01,  ...,  1.2737e-01,  1.3038e-01,  1.3189e-01]],

         [[ 8.2109e-02, -5.7207e-02, -3.3584e-01,  ..., -5.6383e-02,  1.2409e-01,  2.1433e-01],
          [ 1.1790e-01,  1.7741e-01,  2.9643e-01,  ...,  1.3562e-01,  2.2878e-01,  2.7536e-01],
          [ 3.8301e-01,  3.4174e-01,  2.5919e-01,  ..., -2.9458e-01, -2.0367e-01, -1.5821e-01],
          ...,
          [-2.4150e-01, -1.1407e-01,  1.4080e-01,  ..., -2.0920e-01, -1.5387e-01, -1.2621e-01],
          [ 1.9815e-01,  1.6954e-01,  1.1232e-01,  ..., -6.9692e-02, -2.9597e-01, -4.0911e-01],
          [ 1.4481e-01,  1.6173e-01,  1.9557e-01,  ..., -1.6523e-02, -2.4686e-01, -3.6203e-01]],

         [[ 1.8801e-01,  1.1368e-01, -3.4990e-02,  ..., -3.5748e-02,  2.4002e-01,  3.7791e-01],
          [ 3.4056e-01,  2.9889e-01,  2.1556e-01,  ..., -2.2613e-01, -1.2070e-01, -6.7985e-02],
          [ 3.5589e-01,  2.0037e-01, -1.1067e-01,  ...,  9.7848e-02,  2.3423e-02, -1.3789e-02],
          ...,
          [-1.0439e-01, -9.7726e-02, -8.4402e-02,  ..., -7.1296e-02, -8.0972e-02, -8.5810e-02],
          [-4.1912e-01, -3.5811e-01, -2.3609e-01,  ...,  8.0815e-02, -1.3098e-01, -2.3688e-01],
          [ 4.5551e-01,  3.3928e-01,  1.0684e-01,  ..., -2.3958e-01, -9.3322e-02, -2.0195e-02]],

         ...,

         [[-2.3692e-01, -2.1026e-01, -1.5692e-01,  ..., -1.1917e-01, -2.7345e-01, -3.5059e-01],
          [-4.0027e-01, -3.5961e-01, -2.7828e-01,  ..., -2.3021e-02,  2.5719e-01,  3.9730e-01],
          [-4.2002e-02, -7.2803e-02, -1.3440e-01,  ..., -3.3988e-02,  1.1995e-01,  1.9692e-01],
          ...,
          [-1.4607e-01, -9.2762e-02,  1.3847e-02,  ...,  1.3973e-01,  8.5829e-02,  5.8878e-02],
          [-1.4759e-01, -1.9202e-02,  2.3756e-01,  ..., -5.2676e-02,  9.3682e-02,  1.6686e-01],
          [ 1.3732e-01,  1.2145e-01,  8.9712e-02,  ..., -1.7088e-01, -2.6088e-02,  4.6309e-02]],

         [[-2.8248e-01, -2.2801e-01, -1.1907e-01,  ...,  3.8355e-02, -4.0104e-02, -7.9333e-02],
          [ 6.6836e-02, -3.3501e-02, -2.3417e-01,  ...,  9.7724e-02,  3.3078e-01,  4.4730e-01],
          [ 2.5169e-01,  8.9721e-02, -2.3422e-01,  ..., -1.9634e-01,  1.4185e-01,  3.1095e-01],
          ...,
          [ 8.7215e-02, -2.3280e-02, -2.4427e-01,  ...,  6.6173e-02, -2.0133e-01, -3.3508e-01],
          [-2.7743e-01, -2.9876e-01, -3.4141e-01,  ...,  2.2570e-01, -1.3419e-01, -3.1414e-01],
          [ 7.6201e-02,  1.1749e-01,  2.0008e-01,  ..., -1.2017e-01,  2.8314e-01,  4.8480e-01]],

         [[ 4.0539e-01,  2.1286e-01, -1.7221e-01,  ..., -3.3493e-02,  4.9995e-02,  9.1740e-02],
          [ 4.2470e-01,  2.3816e-01, -1.3494e-01,  ...,  3.7854e-01,  3.3552e-01,  3.1402e-01],
          [-2.7339e-01, -1.8558e-01, -9.9776e-03,  ...,  4.3271e-01,  4.4870e-01,  4.5670e-01],
          ...,
          [ 4.2993e-01,  2.1626e-01, -2.1107e-01,  ..., -2.6193e-01, -1.1511e-01, -4.1700e-02],
          [-2.3615e-01, -1.6289e-01, -1.6369e-02,  ..., -5.4553e-02,  1.4842e-01,  2.4990e-01],
          [-2.9589e-01, -1.6553e-01,  9.5192e-02,  ...,  1.8462e-01,  2.3971e-01,  2.6726e-01]]],


        [[[ 3.2602e-01,  1.8399e-01, -1.0008e-01,  ...,  1.3459e-01, -5.6214e-02, -1.5162e-01],
          [-1.5043e-01,  5.3798e-03,  3.1700e-01,  ...,  7.8952e-02, -3.0618e-01, -4.9875e-01],
          [ 1.6864e-01,  2.0252e-01,  2.7027e-01,  ...,  5.3523e-02, -2.2755e-01, -3.6808e-01],
          ...,
          [ 4.2186e-01,  2.7952e-01, -5.1603e-03,  ...,  6.9168e-02, -1.8678e-01, -3.1475e-01],
          [-3.5949e-01, -3.6696e-01, -3.8190e-01,  ...,  2.5406e-01,  2.4104e-02, -9.0872e-02],
          [-1.1611e-01, -6.4967e-02,  3.7322e-02,  ..., -6.6293e-02, -8.2815e-03,  2.0724e-02]],

         [[-9.3196e-02, -6.1317e-02,  2.4401e-03,  ..., -1.4246e-01, -2.1255e-01, -2.4759e-01],
          [ 3.6712e-01,  2.8680e-01,  1.2617e-01,  ..., -6.4252e-02, -4.0604e-02, -2.8780e-02],
          [-1.0770e-01, -6.4527e-02,  2.1811e-02,  ...,  2.8132e-03,  3.3597e-02,  4.8989e-02],
          ...,
          [-4.4407e-01, -3.4107e-01, -1.3505e-01,  ...,  1.8386e-01, -1.2549e-02, -1.1075e-01],
          [ 3.9533e-01,  2.7058e-01,  2.1100e-02,  ...,  1.1563e-01, -1.3261e-01, -2.5673e-01],
          [-3.5096e-01, -2.8764e-01, -1.6100e-01,  ...,  5.1572e-02, -1.9489e-01, -3.1812e-01]],

         [[ 3.3911e-02, -3.9220e-02, -1.8548e-01,  ..., -2.4902e-01, -2.5153e-01, -2.5278e-01],
          [ 3.1637e-01,  1.5383e-01, -1.7124e-01,  ..., -3.2651e-02,  2.6294e-01,  4.1074e-01],
          [ 1.2891e-02,  7.6147e-02,  2.0266e-01,  ..., -2.7952e-02, -1.8688e-02, -1.4057e-02],
          ...,
          [-2.8186e-01, -1.8074e-01,  2.1497e-02,  ...,  5.6023e-02,  3.5740e-02,  2.5599e-02],
          [ 1.7307e-01,  1.2466e-01,  2.7859e-02,  ..., -5.4627e-02, -3.8928e-02, -3.1078e-02],
          [ 2.8946e-01,  2.5706e-01,  1.9225e-01,  ...,  7.3791e-02, -7.0677e-02, -1.4291e-01]],

         ...,

         [[-5.7031e-02, -7.5972e-02, -1.1385e-01,  ...,  1.0204e-01,  2.6818e-01,  3.5124e-01],
          [ 3.6407e-01,  1.7366e-01, -2.0716e-01,  ...,  1.5243e-01,  2.8145e-01,  3.4596e-01],
          [ 2.8349e-02, -3.4661e-02, -1.6068e-01,  ..., -1.1030e-01, -7.5702e-02, -5.8402e-02],
          ...,
          [ 2.6425e-01,  1.8187e-01,  1.7112e-02,  ..., -2.3093e-02, -3.0063e-01, -4.3939e-01],
          [ 4.7595e-01,  2.3867e-01, -2.3590e-01,  ...,  4.4383e-02,  2.7031e-01,  3.8327e-01],
          [ 4.0563e-01,  2.3084e-01, -1.1875e-01,  ..., -2.0258e-01, -2.2648e-01, -2.3843e-01]],

         [[-3.5005e-02, -8.4234e-02, -1.8269e-01,  ...,  1.4766e-01, -1.0811e-01, -2.3599e-01],
          [-1.9921e-02, -9.9252e-02, -2.5791e-01,  ...,  2.3452e-01,  9.2856e-02,  2.2026e-02],
          [ 2.7061e-02, -8.3414e-02, -3.0437e-01,  ..., -3.6168e-01, -2.9788e-01, -2.6598e-01],
          ...,
          [ 2.5504e-01,  2.1224e-01,  1.2665e-01,  ...,  3.0366e-01,  1.8479e-01,  1.2536e-01],
          [-3.6707e-01, -2.0358e-01,  1.2340e-01,  ..., -1.0789e-01, -2.8778e-01, -3.7772e-01],
          [-8.4555e-02, -5.7454e-02, -3.2498e-03,  ..., -1.3829e-01, -7.2612e-02, -3.9773e-02]],

         [[-2.6954e-01, -2.7044e-01, -2.7224e-01,  ...,  1.0769e-01,  1.0575e-02, -3.7983e-02],
          [-5.4067e-03,  6.4766e-02,  2.0511e-01,  ..., -2.0820e-01, -1.7040e-01, -1.5150e-01],
          [ 2.6633e-01,  1.7886e-01,  3.9317e-03,  ..., -1.0903e-01, -2.8945e-01, -3.7966e-01],
          ...,
          [ 2.2466e-01,  1.3942e-01, -3.1067e-02,  ..., -2.2215e-01, -4.4400e-02,  4.4476e-02],
          [-1.2124e-01,  3.8039e-03,  2.5389e-01,  ...,  2.7964e-02,  6.5704e-02,  8.4574e-02],
          [-1.1316e-01, -5.9400e-02,  4.8127e-02,  ..., -7.5727e-02,  2.0258e-01,  3.4173e-01]]],


        [[[ 1.3776e-02,  6.6141e-02,  1.7087e-01,  ..., -8.8095e-02,  1.2694e-01,  2.3445e-01],
          [ 2.6363e-01,  2.9345e-01,  3.5308e-01,  ...,  1.2562e-01, -1.0687e-01, -2.2311e-01],
          [ 1.0998e-01,  1.0362e-01,  9.0899e-02,  ..., -1.6020e-02, -1.3242e-01, -1.9062e-01],
          ...,
          [ 1.1151e-01,  1.0060e-02, -1.9284e-01,  ..., -1.3570e-02, -2.0778e-01, -3.0489e-01],
          [-4.8061e-01, -3.2934e-01, -2.6796e-02,  ...,  3.8223e-01,  2.5766e-01,  1.9538e-01],
          [ 4.3301e-01,  3.6923e-01,  2.4167e-01,  ..., -4.8363e-02,  1.4074e-01,  2.3528e-01]],

         [[-1.9234e-01, -1.7222e-01, -1.3200e-01,  ...,  1.1922e-01,  2.1252e-01,  2.5916e-01],
          [ 2.6262e-01,  2.0465e-01,  8.8730e-02,  ..., -1.2397e-01, -9.4390e-02, -7.9600e-02],
          [ 3.1275e-01,  1.4658e-01, -1.8574e-01,  ...,  1.3684e-01,  2.8050e-01,  3.5233e-01],
          ...,
          [-1.8509e-01, -1.9694e-01, -2.2063e-01,  ..., -3.6099e-01, -1.2783e-01, -1.1244e-02],
          [ 2.5225e-01,  1.5526e-01, -3.8729e-02,  ...,  3.9801e-01,  3.7081e-01,  3.5722e-01],
          [ 1.0714e-01, -4.4196e-02, -3.4688e-01,  ...,  8.2751e-02, -1.7630e-01, -3.0583e-01]],

         [[ 1.7469e-01,  2.1484e-01,  2.9513e-01,  ..., -8.7126e-02, -2.3129e-01, -3.0337e-01],
          [ 1.1212e-01,  8.3721e-02,  2.6932e-02,  ..., -2.5697e-01, -8.9954e-02, -6.4438e-03],
          [ 1.4698e-01,  1.0135e-01,  1.0084e-02,  ..., -3.3182e-01, -2.5671e-01, -2.1916e-01],
          ...,
          [ 5.3590e-02,  1.1000e-01,  2.2282e-01,  ...,  3.9635e-01,  3.3171e-01,  2.9939e-01],
          [ 2.2951e-01,  8.6430e-02, -1.9973e-01,  ...,  1.2571e-01, -2.4894e-01, -4.3627e-01],
          [ 7.2366e-02,  8.4453e-02,  1.0863e-01,  ...,  1.4943e-02, -7.9570e-02, -1.2683e-01]],

         ...,

         [[-1.7575e-01, -1.0792e-01,  2.7731e-02,  ...,  3.4781e-01,  2.8058e-01,  2.4697e-01],
          [ 6.7889e-02,  5.2626e-03, -1.1999e-01,  ..., -5.9157e-02,  1.7886e-01,  2.9787e-01],
          [ 3.9848e-01,  2.1179e-01, -1.6159e-01,  ...,  8.8938e-03, -1.9368e-01, -2.9496e-01],
          ...,
          [ 3.9123e-01,  2.2413e-01, -1.1008e-01,  ...,  2.5567e-01,  2.5928e-01,  2.6109e-01],
          [ 2.1102e-01,  1.6519e-01,  7.3534e-02,  ..., -1.7111e-01,  2.2155e-01,  4.1788e-01],
          [-1.7905e-01, -1.8219e-01, -1.8848e-01,  ...,  1.5775e-01,  2.9966e-01,  3.7062e-01]],

         [[ 1.2144e-01,  1.2168e-01,  1.2216e-01,  ..., -1.8655e-01, -1.0135e-01, -5.8747e-02],
          [ 2.7612e-01,  2.1228e-01,  8.4589e-02,  ..., -6.5574e-02,  6.3844e-02,  1.2855e-01],
          [-3.3887e-02,  9.7876e-02,  3.6140e-01,  ..., -6.3102e-02, -2.0738e-01, -2.7952e-01],
          ...,
          [ 2.2393e-01,  2.2396e-01,  2.2401e-01,  ..., -6.5054e-02, -2.9413e-01, -4.0867e-01],
          [ 3.5952e-01,  3.7978e-01,  4.2030e-01,  ...,  1.1721e-01,  1.4048e-01,  1.5211e-01],
          [ 2.9863e-01,  3.0102e-01,  3.0578e-01,  ..., -1.2808e-01, -8.2170e-02, -5.9217e-02]],

         [[ 2.3506e-01,  1.3657e-01, -6.0404e-02,  ...,  1.7464e-01,  1.6872e-01,  1.6577e-01],
          [ 4.3067e-01,  4.2323e-01,  4.0836e-01,  ...,  3.2553e-01,  3.5970e-01,  3.7679e-01],
          [-4.2974e-01, -4.1369e-01, -3.8157e-01,  ..., -4.8761e-02, -1.8186e-01, -2.4841e-01],
          ...,
          [ 8.6941e-02,  1.2963e-01,  2.1502e-01,  ..., -1.6172e-01, -3.7415e-01, -4.8036e-01],
          [ 3.8891e-01,  1.8819e-01, -2.1325e-01,  ...,  2.9373e-01,  1.3525e-01,  5.6012e-02],
          [ 1.7914e-01,  3.8139e-02, -2.4387e-01,  ..., -1.0514e-01,  1.7377e-01,  3.1323e-01]]]])

2025-07-09 13:41:00.241357 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([16, 40, 32, 111412],"float32"), size=list[64,48,], mode="bilinear", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([16, 40, 32, 111412],"float32"), size=list[64,48,], mode="bilinear", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1903340 / 1966080 (96.8%)
Greatest absolute difference: 0.9827291965484619 at index (13, 1, 0, 0) (up to 0.01 allowed)
Greatest relative difference: 3314481.75 at index (8, 5, 33, 30) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([16, 40, 64, 48]), dtype=torch.float32)
tensor([[[[ 1.8170e-01,  4.1408e-02,  3.9740e-01,  ..., -2.3021e-01,  7.1378e-02,  4.5622e-01],
          [ 1.8439e-01,  1.2874e-01,  2.5836e-01,  ..., -1.4249e-01,  2.2839e-01, -5.4664e-03],
          [ 1.8708e-01,  2.1607e-01,  1.1932e-01,  ..., -5.4772e-02,  3.8539e-01, -4.6715e-01],
          ...,
          [ 1.4391e-01,  2.0670e-01, -1.4769e-01,  ..., -4.0184e-01, -1.6324e-03, -4.7759e-01],
          [-7.5417e-02,  2.5589e-01,  6.0054e-02,  ..., -2.0815e-01, -2.1163e-01, -4.2016e-01],
          [-2.9474e-01,  3.0509e-01,  2.6780e-01,  ..., -1.4470e-02, -4.2162e-01, -3.6273e-01]],

         [[ 4.3186e-01, -2.8778e-01, -3.1621e-01,  ..., -7.6083e-02, -2.8792e-01,  2.7852e-01],
          [-2.3081e-02, -1.5342e-01, -2.7925e-01,  ..., -5.1692e-02, -1.5305e-01, -7.6976e-02],
          [-4.7803e-01, -1.9055e-02, -2.4230e-01,  ..., -2.7300e-02, -1.8172e-02, -4.3247e-01],
          ...,
          [-4.0990e-04, -2.1893e-01, -2.9607e-01,  ..., -2.6486e-01, -4.0097e-01,  4.4762e-01],
          [ 2.4345e-01, -4.5755e-02, -6.0307e-02,  ..., -3.4166e-01, -3.5867e-01,  4.1400e-01],
          [ 4.8731e-01,  1.2742e-01,  1.7545e-01,  ..., -4.1846e-01, -3.1636e-01,  3.8038e-01]],

         [[-1.7556e-02,  1.0719e-01, -1.9130e-01,  ...,  2.1288e-01, -2.6021e-01, -2.4303e-01],
          [-1.9949e-01, -9.9302e-02, -1.5624e-01,  ...,  5.7857e-02, -4.2708e-02, -1.7426e-01],
          [-3.8141e-01, -3.0580e-01, -1.2118e-01,  ..., -9.7163e-02,  1.7480e-01, -1.0549e-01],
          ...,
          [-2.4530e-01,  8.4551e-02,  1.0083e-01,  ...,  1.3001e-01,  1.9887e-01,  4.0345e-01],
          [-3.7232e-01,  1.2283e-01,  1.8709e-01,  ...,  2.9278e-01,  1.0909e-01,  4.2858e-01],
          [-4.9934e-01,  1.6111e-01,  2.7335e-01,  ...,  4.5556e-01,  1.9320e-02,  4.5370e-01]],

         ...,

         [[ 7.1401e-02, -2.0472e-01, -4.9134e-02,  ..., -3.5469e-02,  4.4087e-02, -3.8469e-01],
          [ 1.5521e-01,  5.4862e-02, -2.0089e-01,  ..., -3.2524e-02,  8.7536e-02, -5.5078e-02],
          [ 2.3903e-01,  3.1444e-01, -3.5265e-01,  ..., -2.9579e-02,  1.3099e-01,  2.7453e-01],
          ...,
          [ 3.4413e-01, -8.3775e-02, -1.6806e-01,  ..., -2.8330e-01,  1.1615e-01, -4.3466e-01],
          [-2.1604e-02, -2.1624e-02,  9.9493e-02,  ..., -2.9491e-01,  1.2403e-01, -4.5157e-01],
          [-3.8734e-01,  4.0526e-02,  3.6704e-01,  ..., -3.0653e-01,  1.3191e-01, -4.6849e-01]],

         [[ 3.5687e-01, -1.8816e-01, -4.8766e-01,  ...,  3.0296e-01, -1.6749e-01, -2.2775e-01],
          [ 2.0685e-01,  4.4574e-02, -3.7874e-01,  ...,  1.8257e-01, -2.9659e-01, -7.1962e-02],
          [ 5.6828e-02,  2.7731e-01, -2.6982e-01,  ...,  6.2175e-02, -4.2569e-01,  8.3823e-02],
          ...,
          [ 2.5485e-01,  1.1005e-02,  3.1893e-01,  ...,  2.5532e-01,  8.9293e-02,  4.1766e-01],
          [ 1.6024e-01, -9.0501e-03,  3.3812e-02,  ..., -3.9342e-02, -6.9457e-03, -3.2370e-03],
          [ 6.5630e-02, -2.9105e-02, -2.5131e-01,  ..., -3.3401e-01, -1.0318e-01, -4.2413e-01]],

         [[ 4.6028e-01, -4.3297e-02,  1.9332e-01,  ..., -1.5130e-01, -4.2085e-02,  4.5939e-01],
          [ 1.0604e-01, -1.5253e-01,  3.0427e-01,  ..., -3.7604e-02, -1.9463e-02,  1.8774e-01],
          [-2.4821e-01, -2.6175e-01,  4.1523e-01,  ...,  7.6093e-02,  3.1591e-03, -8.3915e-02],
          ...,
          [ 2.5225e-01,  1.3933e-01, -2.2631e-01,  ...,  4.4179e-01, -1.0144e-01,  1.3771e-01],
          [ 2.5271e-01,  7.9656e-02, -2.7486e-01,  ...,  1.5944e-01,  3.1981e-02,  3.0746e-02],
          [ 2.5317e-01,  1.9978e-02, -3.2340e-01,  ..., -1.2291e-01,  1.6540e-01, -7.6215e-02]]],


        [[[-2.5289e-01,  8.3428e-02, -3.1433e-01,  ..., -9.4150e-02,  5.0793e-02, -1.9998e-01],
          [-4.9954e-02,  7.2570e-02,  1.3647e-02,  ..., -2.8701e-02,  1.2195e-02, -1.8944e-01],
          [ 1.5298e-01,  6.1712e-02,  3.4162e-01,  ...,  3.6747e-02, -2.6403e-02, -1.7891e-01],
          ...,
          [-1.9694e-01,  1.7144e-01, -4.4081e-01,  ...,  2.7618e-01, -8.0107e-02,  3.3120e-01],
          [-9.0371e-02,  8.9518e-02, -1.6066e-01,  ...,  1.3018e-01, -7.0136e-02,  3.3615e-01],
          [ 1.6197e-02,  7.5927e-03,  1.1949e-01,  ..., -1.5813e-02, -6.0164e-02,  3.4111e-01]],

         [[ 2.4689e-01, -2.2650e-02, -1.0268e-02,  ...,  3.3562e-01,  7.7132e-02,  4.2695e-02],
          [ 3.1452e-01,  5.9049e-02,  4.8651e-02,  ..., -5.8100e-02,  5.5705e-02,  8.5229e-02],
          [ 3.8215e-01,  1.4075e-01,  1.0757e-01,  ..., -4.5182e-01,  3.4277e-02,  1.2776e-01],
          ...,
          [ 1.1586e-01, -2.2805e-01, -1.2680e-01,  ...,  3.9421e-01,  9.4208e-02,  3.6114e-01],
          [ 2.0814e-01, -2.2255e-01, -2.1466e-01,  ...,  4.2221e-01,  4.6417e-02,  6.6425e-02],
          [ 3.0042e-01, -2.1704e-01, -3.0252e-01,  ...,  4.5020e-01, -1.3743e-03, -2.2829e-01]],

         [[ 1.8053e-02, -2.0311e-01,  3.8144e-01,  ..., -3.0261e-01, -3.4777e-01,  9.3913e-02],
          [ 7.7309e-02, -2.0336e-01,  2.9405e-01,  ...,  2.1520e-02, -2.5459e-01,  9.0385e-02],
          [ 1.3657e-01, -2.0361e-01,  2.0667e-01,  ...,  3.4565e-01, -1.6142e-01,  8.6856e-02],
          ...,
          [ 3.1683e-01,  7.9702e-03,  2.6435e-01,  ...,  1.3746e-01, -1.5921e-01, -4.3554e-01],
          [ 2.5884e-02, -1.2743e-01,  3.1664e-01,  ..., -2.4751e-02, -4.7700e-02, -4.4948e-01],
          [-2.6507e-01, -2.6283e-01,  3.6892e-01,  ..., -1.8696e-01,  6.3810e-02, -4.6342e-01]],

         ...,

         [[-4.7041e-01,  4.5969e-01, -1.1716e-01,  ..., -1.9401e-01, -1.5863e-01, -1.5701e-01],
          [-2.6465e-01,  3.2776e-01, -7.7453e-02,  ..., -2.5076e-01, -2.7044e-02, -2.7588e-01],
          [-5.8883e-02,  1.9582e-01, -3.7743e-02,  ..., -3.0751e-01,  1.0455e-01, -3.9476e-01],
          ...,
          [ 3.7985e-01,  4.5822e-02,  1.1778e-01,  ...,  3.8501e-01, -5.6807e-02, -2.1505e-01],
          [ 3.0645e-01,  6.2355e-02,  3.9046e-03,  ...,  1.5435e-01,  1.5943e-03, -1.1836e-01],
          [ 2.3304e-01,  7.8888e-02, -1.0997e-01,  ..., -7.6304e-02,  5.9995e-02, -2.1671e-02]],

         [[ 3.0848e-01, -3.0079e-01, -3.1606e-01,  ...,  3.1365e-01, -2.2873e-01,  3.3704e-02],
          [ 1.9833e-01, -3.1542e-01, -2.4744e-01,  ...,  1.9642e-01, -1.0114e-01,  2.7152e-03],
          [ 8.8175e-02, -3.3006e-01, -1.7882e-01,  ...,  7.9190e-02,  2.6452e-02, -2.8274e-02],
          ...,
          [ 3.4138e-01,  2.9953e-01, -1.0118e-01,  ..., -2.5232e-01, -5.3188e-02,  2.5161e-01],
          [ 2.8304e-01,  1.4205e-01, -2.7495e-01,  ..., -1.5367e-01, -8.5587e-02,  2.1313e-01],
          [ 2.2469e-01, -1.5430e-02, -4.4873e-01,  ..., -5.5034e-02, -1.1799e-01,  1.7466e-01]],

         [[-9.7873e-02, -7.6530e-02,  4.1291e-01,  ...,  4.4845e-01, -2.0294e-01,  3.3782e-01],
          [-3.0118e-02, -8.7714e-02,  3.6427e-01,  ...,  1.8529e-01,  5.7111e-02,  1.1023e-01],
          [ 3.7638e-02, -9.8898e-02,  3.1562e-01,  ..., -7.7875e-02,  3.1716e-01, -1.1737e-01],
          ...,
          [ 1.0146e-01,  1.0655e-01,  1.3531e-01,  ..., -3.9257e-01, -1.2845e-01, -4.1829e-01],
          [-1.2216e-01,  1.4701e-02,  1.4307e-01,  ..., -3.4200e-01, -1.2815e-01, -8.8545e-02],
          [-3.4578e-01, -7.7153e-02,  1.5082e-01,  ..., -2.9143e-01, -1.2785e-01,  2.4120e-01]]],


        [[[-2.1651e-01, -3.1280e-01, -3.6782e-01,  ..., -1.6895e-01, -2.9907e-01, -2.6862e-01],
          [-2.4477e-01, -1.7673e-01, -8.9342e-03,  ...,  8.5642e-03, -1.0770e-01, -2.4185e-01],
          [-2.7302e-01, -4.0652e-02,  3.4995e-01,  ...,  1.8608e-01,  8.3672e-02, -2.1509e-01],
          ...,
          [-8.9870e-02, -3.7974e-02,  8.2580e-02,  ..., -2.9805e-01, -8.5114e-02,  1.5521e-01],
          [ 1.0365e-01,  7.9184e-03,  2.6897e-01,  ..., -1.0220e-01, -1.5295e-01,  9.3437e-02],
          [ 2.9718e-01,  5.3810e-02,  4.5537e-01,  ...,  9.3644e-02, -2.2079e-01,  3.1662e-02]],

         [[-2.8811e-01,  3.1164e-02, -5.2839e-02,  ...,  2.2106e-01, -7.9374e-03, -3.8043e-01],
          [ 2.2666e-06, -1.4196e-02, -2.2706e-02,  ...,  2.0683e-01,  3.7997e-02, -2.4495e-01],
          [ 2.8811e-01, -5.9556e-02,  7.4274e-03,  ...,  1.9261e-01,  8.3930e-02, -1.0947e-01],
          ...,
          [-4.8230e-02, -1.0877e-01,  1.3662e-01,  ...,  3.1176e-01, -4.5996e-02, -2.6180e-01],
          [ 3.7974e-02, -1.6054e-01,  1.0281e-01,  ..., -2.1459e-02, -6.9679e-02, -1.9794e-01],
          [ 1.2418e-01, -2.1231e-01,  6.9004e-02,  ..., -3.5467e-01, -9.3363e-02, -1.3408e-01]],

         [[ 8.7157e-02, -2.3856e-01,  2.4627e-01,  ..., -4.0959e-01,  4.1463e-02, -2.4209e-01],
          [-1.0091e-01, -1.0442e-01,  3.2555e-01,  ..., -1.6790e-01,  1.0024e-01,  1.2248e-01],
          [-2.8897e-01,  2.9717e-02,  4.0482e-01,  ...,  7.3782e-02,  1.5901e-01,  4.8705e-01],
          ...,
          [-3.0218e-01,  1.1095e-01, -3.9486e-01,  ...,  3.5922e-01, -1.7103e-01, -1.8051e-01],
          [-9.2871e-02,  1.1170e-01, -1.8570e-01,  ...,  2.4036e-01,  5.3852e-02, -4.8050e-03],
          [ 1.1644e-01,  1.1246e-01,  2.3469e-02,  ...,  1.2149e-01,  2.7874e-01,  1.7090e-01]],

         ...,

         [[ 1.0023e-01,  5.9728e-02,  1.5701e-01,  ..., -4.4920e-01,  1.9136e-02, -1.9243e-01],
          [ 9.9981e-02,  1.3138e-01, -3.1190e-02,  ..., -9.9109e-04,  1.1010e-02, -1.6147e-01],
          [ 9.9734e-02,  2.0303e-01, -2.1939e-01,  ...,  4.4722e-01,  2.8838e-03, -1.3051e-01],
          ...,
          [-3.1161e-01, -1.2323e-01, -2.0910e-01,  ..., -1.2307e-01,  3.8603e-02, -1.3031e-01],
          [-2.5933e-01, -1.5507e-01, -2.8118e-01,  ...,  1.3825e-01,  5.7635e-02, -3.1404e-01],
          [-2.0704e-01, -1.8692e-01, -3.5325e-01,  ...,  3.9957e-01,  7.6667e-02, -4.9777e-01]],

         [[ 3.2169e-01,  2.1744e-01, -1.2916e-01,  ..., -2.4895e-01, -6.8363e-03,  3.6055e-02],
          [ 9.2241e-02, -1.0094e-01,  1.4853e-01,  ..., -2.0349e-01,  8.7819e-02, -1.2249e-01],
          [-1.3721e-01, -4.1932e-01,  4.2621e-01,  ..., -1.5802e-01,  1.8247e-01, -2.8103e-01],
          ...,
          [ 7.3439e-02,  2.2655e-01,  3.1638e-01,  ..., -7.2523e-02,  1.3172e-01, -4.8282e-01],
          [-7.2859e-02,  1.3720e-01,  2.7549e-01,  ..., -2.0490e-01,  2.1124e-01, -5.9325e-02],
          [-2.1916e-01,  4.7846e-02,  2.3459e-01,  ..., -3.3728e-01,  2.9075e-01,  3.6417e-01]],

         [[ 9.4396e-02,  2.4556e-01,  2.4848e-01,  ...,  2.4640e-01,  1.3196e-01, -4.0832e-01],
          [-1.1304e-01,  2.5344e-01,  2.7744e-01,  ..., -7.9840e-02, -7.7966e-02, -4.5321e-01],
          [-3.2047e-01,  2.6131e-01,  3.0640e-01,  ..., -4.0608e-01, -2.8790e-01, -4.9809e-01],
          ...,
          [-7.3672e-02, -2.1278e-01,  2.1137e-01,  ..., -3.6444e-01, -1.3050e-01,  3.1866e-01],
          [-3.5973e-02, -2.3935e-01,  3.8661e-02,  ..., -8.0242e-02, -1.7783e-01,  2.9738e-01],
          [ 1.7251e-03, -2.6593e-01, -1.3405e-01,  ...,  2.0396e-01, -2.2516e-01,  2.7609e-01]]],


        ...,


        [[[ 1.6017e-01,  4.4977e-02,  3.1034e-01,  ..., -2.9086e-01, -1.7967e-01,  1.5650e-01],
          [-8.6622e-02, -1.2352e-01,  1.2130e-01,  ...,  7.3888e-02,  8.6098e-03,  2.4805e-01],
          [-3.3342e-01, -2.9202e-01, -6.7746e-02,  ...,  4.3863e-01,  1.9689e-01,  3.3961e-01],
          ...,
          [ 1.9940e-01,  2.6328e-02, -2.2375e-01,  ..., -2.8870e-01, -1.0445e-01,  4.3595e-01],
          [ 9.8887e-02,  1.0191e-01, -2.5241e-01,  ..., -2.0064e-01,  1.3444e-03,  3.7730e-01],
          [-1.6310e-03,  1.7748e-01, -2.8107e-01,  ..., -1.1258e-01,  1.0714e-01,  3.1865e-01]],

         [[-4.9174e-01, -8.7271e-02,  2.7494e-01,  ..., -1.5068e-01, -3.1654e-01,  4.8799e-01],
          [-2.7733e-01,  9.3752e-02,  2.3056e-01,  ...,  1.4640e-01, -3.1884e-01,  8.0506e-03],
          [-6.2910e-02,  2.7477e-01,  1.8618e-01,  ...,  4.4348e-01, -3.2114e-01, -4.7189e-01],
          ...,
          [ 2.1415e-01, -2.0671e-01, -3.6067e-01,  ..., -1.0654e-01,  4.7310e-02,  3.4913e-01],
          [ 1.9591e-01, -1.1195e-01, -1.5471e-01,  ...,  1.7087e-02,  1.6067e-01, -1.4731e-02],
          [ 1.7767e-01, -1.7190e-02,  5.1255e-02,  ...,  1.4072e-01,  2.7402e-01, -3.7859e-01]],

         [[-2.9239e-01, -8.1575e-02,  3.1172e-02,  ...,  4.4047e-01,  3.8747e-02,  2.4044e-01],
          [-1.8805e-01,  9.8230e-02,  1.4609e-02,  ...,  2.1822e-01,  1.2395e-01, -6.0515e-02],
          [-8.3711e-02,  2.7803e-01, -1.9536e-03,  ..., -4.0279e-03,  2.0915e-01, -3.6146e-01],
          ...,
          [ 3.4308e-01,  2.0892e-01, -1.2133e-01,  ..., -2.3896e-01, -1.3977e-01, -2.4462e-01],
          [ 2.9539e-01,  1.1155e-01, -2.4622e-01,  ..., -2.1007e-01, -1.2699e-02,  4.7357e-02],
          [ 2.4770e-01,  1.4187e-02, -3.7111e-01,  ..., -1.8118e-01,  1.1438e-01,  3.3933e-01]],

         ...,

         [[-4.3400e-01, -2.0937e-01, -3.3237e-01,  ..., -1.7963e-01, -7.8519e-02, -1.1425e-01],
          [-8.8151e-02, -4.5517e-03, -5.4695e-02,  ..., -2.9628e-01, -6.8557e-03,  3.5678e-02],
          [ 2.5770e-01,  2.0027e-01,  2.2298e-01,  ..., -4.1293e-01,  6.4808e-02,  1.8561e-01],
          ...,
          [-3.4927e-01,  2.2763e-02, -4.0659e-01,  ..., -3.0242e-01, -2.1710e-01, -2.6664e-01],
          [-1.8815e-01,  1.3337e-02, -7.6305e-02,  ..., -3.5246e-01, -6.3591e-02, -3.3381e-01],
          [-2.7031e-02,  3.9112e-03,  2.5398e-01,  ..., -4.0251e-01,  8.9915e-02, -4.0098e-01]],

         [[-2.2486e-01,  1.9730e-01,  1.3570e-01,  ...,  1.4924e-01,  1.4944e-01,  1.0446e-01],
          [-2.3736e-01,  2.3165e-01,  1.5710e-01,  ..., -7.0798e-02,  1.2796e-01, -4.0156e-02],
          [-2.4987e-01,  2.6600e-01,  1.7850e-01,  ..., -2.9084e-01,  1.0647e-01, -1.8477e-01],
          ...,
          [ 2.9743e-01, -2.5687e-01, -2.0698e-01,  ...,  4.0258e-01,  9.6709e-02, -7.7568e-02],
          [ 1.8907e-01, -2.0121e-01,  1.0425e-01,  ...,  6.8143e-02,  8.9815e-02, -2.3152e-01],
          [ 8.0701e-02, -1.4556e-01,  4.1547e-01,  ..., -2.6630e-01,  8.2921e-02, -3.8547e-01]],

         [[ 1.3754e-01, -1.2610e-01, -1.9879e-01,  ...,  9.2335e-02, -9.6042e-02, -4.0659e-01],
          [ 2.8784e-01, -1.1214e-01,  6.6985e-02,  ...,  2.7521e-01,  1.5398e-01, -4.1472e-03],
          [ 4.3813e-01, -9.8183e-02,  3.3276e-01,  ...,  4.5808e-01,  4.0399e-01,  3.9829e-01],
          ...,
          [ 4.3419e-01,  6.5138e-02, -5.8770e-02,  ...,  1.8880e-01, -1.9175e-01,  1.6818e-01],
          [ 1.6974e-01,  2.2342e-03, -1.7143e-01,  ...,  1.6576e-02, -2.4589e-01, -7.9779e-02],
          [-9.4715e-02, -6.0670e-02, -2.8409e-01,  ..., -1.5565e-01, -3.0004e-01, -3.2773e-01]]],


        [[[ 4.2019e-02, -8.3970e-02,  3.8153e-02,  ..., -2.7377e-02,  4.9185e-02, -2.0487e-01],
          [-9.0444e-02,  8.1693e-02,  2.7892e-02,  ..., -2.2129e-01,  1.4845e-01, -3.9381e-03],
          [-2.2291e-01,  2.4736e-01,  1.7631e-02,  ..., -4.1521e-01,  2.4771e-01,  1.9700e-01],
          ...,
          [-1.9220e-02, -1.6890e-01,  2.4162e-01,  ...,  3.7965e-01, -6.2957e-02, -3.7879e-01],
          [-2.3106e-01, -4.7212e-02,  2.5024e-01,  ...,  6.1002e-02, -1.9046e-01, -2.3031e-01],
          [-4.4289e-01,  7.4474e-02,  2.5887e-01,  ..., -2.5765e-01, -3.1796e-01, -8.1842e-02]],

         [[-1.6430e-01, -3.4694e-01, -3.0418e-01,  ...,  2.2726e-01, -6.0753e-02, -1.1931e-01],
          [ 1.2992e-01, -2.3961e-01, -4.6095e-02,  ..., -2.5309e-02, -8.5336e-02, -2.8577e-01],
          [ 4.2413e-01, -1.3227e-01,  2.1199e-01,  ..., -2.7788e-01, -1.0992e-01, -4.5224e-01],
          ...,
          [-3.0452e-01, -2.9655e-01,  2.9648e-01,  ..., -1.4915e-01,  6.4377e-02,  3.6790e-01],
          [ 6.2040e-02, -9.3350e-02,  1.3485e-01,  ..., -3.2137e-01,  7.2925e-02,  3.9997e-01],
          [ 4.2860e-01,  1.0985e-01, -2.6774e-02,  ..., -4.9358e-01,  8.1473e-02,  4.3205e-01]],

         [[-4.9106e-01, -1.5155e-01, -1.2259e-01,  ...,  2.0019e-02,  2.0174e-01,  3.3781e-01],
          [-1.7873e-01, -9.5066e-02,  6.8276e-02,  ..., -4.3113e-02,  1.3220e-01,  1.1977e-01],
          [ 1.3359e-01, -3.8586e-02,  2.5915e-01,  ..., -1.0624e-01,  6.2664e-02, -9.8264e-02],
          ...,
          [ 2.0559e-02,  8.0994e-02, -2.1982e-01,  ..., -2.1959e-01, -2.2079e-01,  2.2816e-01],
          [-6.7639e-02,  8.6892e-02, -5.8533e-02,  ...,  4.3843e-02, -2.2863e-01,  2.2069e-01],
          [-1.5584e-01,  9.2791e-02,  1.0276e-01,  ...,  3.0728e-01, -2.3646e-01,  2.1323e-01]],

         ...,

         [[-1.9289e-01,  6.9851e-02, -2.8319e-01,  ...,  2.1199e-01, -4.8958e-01, -4.1823e-01],
          [-1.5700e-01,  1.0867e-01, -6.4419e-02,  ..., -2.3693e-02, -3.4452e-01, -1.6328e-02],
          [-1.2110e-01,  1.4748e-01,  1.5435e-01,  ..., -2.5937e-01, -1.9946e-01,  3.8557e-01],
          ...,
          [ 2.6518e-01, -2.7583e-01,  3.8294e-01,  ..., -5.4248e-02,  1.4640e-01, -3.3223e-02],
          [ 1.7362e-01, -2.9024e-01,  6.8494e-02,  ..., -2.0579e-01, -1.1557e-01,  8.2472e-02],
          [ 8.2066e-02, -3.0466e-01, -2.4595e-01,  ..., -3.5733e-01, -3.7754e-01,  1.9817e-01]],

         [[ 1.3463e-01,  3.6179e-01,  5.1639e-02,  ...,  1.3611e-01, -3.4994e-01, -4.3023e-01],
          [-3.3073e-02,  8.8527e-03, -6.8405e-02,  ...,  6.4694e-02, -1.2471e-01, -4.5286e-01],
          [-2.0078e-01, -3.4409e-01, -1.8845e-01,  ..., -6.7182e-03,  1.0053e-01, -4.7548e-01],
          ...,
          [ 8.5253e-02,  1.8482e-01,  4.0945e-01,  ..., -4.0050e-01, -3.5789e-01,  8.1616e-02],
          [-4.6075e-02,  1.5061e-01,  2.4899e-01,  ...,  1.3254e-02, -3.6518e-01,  2.4003e-01],
          [-1.7740e-01,  1.1639e-01,  8.8522e-02,  ...,  4.2701e-01, -3.7247e-01,  3.9845e-01]],

         [[-1.5385e-01,  2.6695e-02, -8.4735e-02,  ..., -3.5139e-01,  2.5875e-03,  2.0130e-01],
          [ 5.3197e-02,  6.6483e-03, -2.0631e-01,  ...,  6.2085e-02, -1.1191e-01,  1.5795e-02],
          [ 2.6025e-01, -1.3399e-02, -3.2789e-01,  ...,  4.7556e-01, -2.2640e-01, -1.6971e-01],
          ...,
          [ 2.9761e-01,  2.3414e-02, -2.4297e-01,  ...,  3.6979e-01,  1.0614e-01, -3.3614e-01],
          [ 5.8258e-02, -1.0078e-01, -1.0021e-01,  ...,  3.3936e-01,  1.2901e-01, -3.5858e-01],
          [-1.8110e-01, -2.2497e-01,  4.2553e-02,  ...,  3.0893e-01,  1.5188e-01, -3.8102e-01]]],


        [[[-3.4088e-02,  1.4816e-01,  3.8460e-01,  ...,  6.3188e-02,  3.8813e-02, -1.7290e-01],
          [-2.2851e-01,  1.1588e-01,  1.0218e-01,  ..., -4.4333e-03, -4.1310e-02, -2.4145e-01],
          [-4.2294e-01,  8.3611e-02, -1.8024e-01,  ..., -7.2055e-02, -1.2143e-01, -3.1001e-01],
          ...,
          [-3.1641e-01, -2.2501e-02, -3.1520e-01,  ..., -9.2142e-02,  1.0989e-01,  2.4924e-02],
          [ 3.4891e-02, -1.3704e-01, -3.8418e-01,  ...,  7.2082e-02, -2.7948e-02, -2.1588e-01],
          [ 3.8619e-01, -2.5158e-01, -4.5316e-01,  ...,  2.3631e-01, -1.6578e-01, -4.5668e-01]],

         [[-4.3181e-01,  4.1551e-01, -1.4981e-01,  ...,  2.8382e-01, -8.0714e-02,  2.0070e-01],
          [-1.3438e-01,  2.4091e-03, -7.2032e-03,  ..., -4.0400e-04, -1.6715e-01,  3.3929e-01],
          [ 1.6306e-01, -4.1070e-01,  1.3541e-01,  ..., -2.8463e-01, -2.5358e-01,  4.7789e-01],
          ...,
          [-2.1983e-01, -7.6661e-02,  8.3609e-02,  ...,  3.5670e-01, -3.1391e-01, -3.2879e-01],
          [ 1.1471e-01, -1.2734e-02, -1.6705e-02,  ...,  3.9218e-02,  5.8267e-02,  1.5980e-02],
          [ 4.4926e-01,  5.1194e-02, -1.1702e-01,  ..., -2.7827e-01,  4.3045e-01,  3.6075e-01]],

         [[ 5.8252e-03, -1.5394e-01,  2.7757e-01,  ..., -4.0617e-01, -9.4668e-02, -2.3159e-01],
          [-1.6545e-01,  1.8901e-02,  2.7363e-01,  ..., -2.4146e-01, -2.3941e-01, -1.9936e-01],
          [-3.3674e-01,  1.9174e-01,  2.6969e-01,  ..., -7.6746e-02, -3.8415e-01, -1.6713e-01],
          ...,
          [ 1.5261e-01, -1.3225e-01, -3.3728e-01,  ..., -1.7448e-02, -4.2347e-01, -3.8789e-01],
          [ 2.2997e-01,  1.0456e-02, -3.1850e-01,  ...,  8.5199e-03, -2.6317e-01, -3.2350e-01],
          [ 3.0732e-01,  1.5316e-01, -2.9971e-01,  ...,  3.4488e-02, -1.0288e-01, -2.5910e-01]],

         ...,

         [[-1.7680e-01, -2.2808e-01, -3.7153e-01,  ...,  2.9905e-01, -1.9649e-01, -4.2499e-01],
          [-2.9709e-01,  8.0519e-03, -6.9100e-02,  ...,  2.6795e-01, -1.5406e-01, -1.1276e-01],
          [-4.1739e-01,  2.4418e-01,  2.3333e-01,  ...,  2.3686e-01, -1.1163e-01,  1.9948e-01],
          ...,
          [-1.2539e-01, -1.8505e-01,  1.4625e-01,  ...,  3.0954e-01, -2.2567e-01,  2.1078e-01],
          [-5.8437e-02,  5.0494e-02,  2.5740e-01,  ...,  1.2640e-01, -2.1793e-01,  1.6800e-01],
          [ 8.5185e-03,  2.8603e-01,  3.6854e-01,  ..., -5.6749e-02, -2.1019e-01,  1.2523e-01]],

         [[-1.9633e-01, -5.7486e-02, -1.3029e-01,  ...,  9.8963e-02, -1.3151e-01, -4.7623e-01],
          [ 3.6041e-02,  1.1272e-01,  6.2502e-02,  ...,  9.2371e-02, -1.8039e-01, -1.9913e-01],
          [ 2.6841e-01,  2.8293e-01,  2.5529e-01,  ...,  8.5778e-02, -2.2926e-01,  7.7962e-02],
          ...,
          [-1.6031e-01, -9.5132e-02, -2.8638e-01,  ..., -9.3191e-02,  2.3029e-01,  2.4192e-01],
          [-1.1691e-01, -3.5859e-02, -3.3830e-01,  ...,  8.0124e-02,  3.2254e-01, -7.8852e-02],
          [-7.3503e-02,  2.3414e-02, -3.9022e-01,  ...,  2.5344e-01,  4.1479e-01, -3.9962e-01]],

         [[-3.5614e-01, -3.3397e-01, -4.7711e-01,  ..., -2.8272e-01, -2.0449e-01, -4.0199e-01],
          [-2.5058e-01, -2.4460e-02, -3.8063e-01,  ..., -2.5919e-01, -2.0014e-01, -3.8623e-01],
          [-1.4502e-01,  2.8505e-01, -2.8416e-01,  ..., -2.3566e-01, -1.9578e-01, -3.7048e-01],
          ...,
          [-5.1143e-02,  4.0245e-01,  4.3506e-01,  ..., -3.2594e-01,  1.9150e-01, -3.8124e-01],
          [ 1.9004e-01,  9.0755e-03,  3.4609e-01,  ...,  1.1365e-02, -1.2698e-01, -5.8466e-02],
          [ 4.3122e-01, -3.8430e-01,  2.5713e-01,  ...,  3.4867e-01, -4.4546e-01,  2.6430e-01]]]])
DESIRED: (shape=torch.Size([16, 40, 64, 48]), dtype=torch.float32)
tensor([[[[ 2.8440e-01,  3.9283e-01, -5.2521e-02,  ..., -2.8236e-01, -2.7475e-01,  4.3198e-01],
          [ 1.9526e-01,  2.3836e-01, -7.2515e-02,  ..., -2.3913e-01, -2.0138e-01,  3.4614e-01],
          [ 1.6987e-02, -7.0588e-02, -1.1250e-01,  ..., -1.5267e-01, -5.4646e-02,  1.7444e-01],
          ...,
          [-1.2599e-01, -5.7832e-02,  2.2277e-01,  ...,  3.1418e-01,  7.5140e-02,  3.2140e-01],
          [-3.2046e-01,  2.3689e-01,  6.1998e-02,  ...,  2.8155e-01,  3.0292e-03,  3.5461e-01],
          [-4.1769e-01,  3.8425e-01, -1.8391e-02,  ...,  2.6524e-01, -3.3026e-02,  3.7121e-01]],

         [[ 3.4109e-01, -1.7526e-01,  2.8672e-01,  ..., -1.6083e-01, -1.7848e-01, -1.3764e-01],
          [ 2.4681e-01, -1.3323e-01,  1.3752e-01,  ..., -7.9607e-02, -1.8931e-01, -1.2707e-01],
          [ 5.8245e-02, -4.9161e-02, -1.6088e-01,  ...,  8.2845e-02, -2.1096e-01, -1.0592e-01],
          ...,
          [-8.0861e-02,  2.4690e-01,  4.9237e-02,  ..., -1.4058e-01,  6.3895e-02,  7.7711e-02],
          [-1.6226e-01,  1.8150e-01,  9.4111e-02,  ...,  1.1928e-01, -2.6711e-01,  1.2028e-01],
          [-2.0296e-01,  1.4880e-01,  1.1655e-01,  ...,  2.4920e-01, -4.3262e-01,  1.4157e-01]],

         [[ 3.2973e-01, -1.5201e-01, -9.8889e-02,  ...,  3.8859e-01, -3.9438e-01,  1.2609e-01],
          [ 2.8374e-01, -1.6786e-01, -1.0666e-01,  ...,  3.4740e-01, -3.2172e-01,  1.7255e-01],
          [ 1.9177e-01, -1.9958e-01, -1.2220e-01,  ...,  2.6503e-01, -1.7641e-01,  2.6548e-01],
          ...,
          [-4.1457e-01,  3.2596e-01,  1.9987e-01,  ...,  1.1765e-01, -1.4582e-01,  3.0401e-01],
          [-4.0455e-01,  1.7645e-01,  2.4421e-01,  ..., -1.0347e-01,  1.0928e-01,  3.2814e-01],
          [-3.9954e-01,  1.0169e-01,  2.6638e-01,  ..., -2.1402e-01,  2.3683e-01,  3.4020e-01]],

         ...,

         [[-4.7796e-01,  5.7641e-02, -4.1731e-01,  ..., -4.2297e-01,  1.2383e-01, -4.5059e-02],
          [-4.5310e-01,  8.9670e-02, -3.2359e-01,  ..., -2.4702e-01,  3.3576e-02, -3.9194e-02],
          [-4.0340e-01,  1.5373e-01, -1.3615e-01,  ...,  1.0488e-01, -1.4694e-01, -2.7463e-02],
          ...,
          [-1.6184e-01, -9.5595e-02, -2.1391e-01,  ..., -2.2843e-01,  5.4302e-03, -1.7320e-01],
          [-1.2962e-01,  1.2959e-01,  1.1231e-01,  ...,  9.5837e-02, -1.7778e-02, -4.2401e-02],
          [-1.1351e-01,  2.4219e-01,  2.7542e-01,  ...,  2.5797e-01, -2.9381e-02,  2.2998e-02]],

         [[ 3.2852e-01, -3.6590e-01, -1.2102e-01,  ..., -2.2471e-01,  3.1328e-01, -4.2196e-01],
          [ 2.3367e-01, -3.4159e-01, -8.7128e-02,  ..., -2.4417e-01,  1.9001e-01, -4.1009e-01],
          [ 4.3971e-02, -2.9296e-01, -1.9351e-02,  ..., -2.8309e-01, -5.6536e-02, -3.8635e-01],
          ...,
          [ 5.1875e-02, -2.2977e-01,  3.5341e-01,  ..., -9.7708e-02,  5.1039e-02, -1.9965e-01],
          [-8.3802e-02, -3.3216e-02,  2.3724e-01,  ..., -6.9951e-02, -1.9161e-01, -2.1859e-01],
          [-1.5164e-01,  6.5062e-02,  1.7916e-01,  ..., -5.6073e-02, -3.1293e-01, -2.2805e-01]],

         [[ 1.5116e-01,  5.0566e-02, -4.7541e-01,  ...,  3.8318e-01, -3.6290e-01,  3.1814e-01],
          [ 1.0642e-01,  4.0260e-02, -3.2587e-01,  ...,  3.2470e-01, -1.5645e-01,  2.5804e-01],
          [ 1.6932e-02,  1.9647e-02, -2.6792e-02,  ...,  2.0775e-01,  2.5645e-01,  1.3785e-01],
          ...,
          [ 1.2208e-01,  2.3095e-01,  3.0877e-01,  ...,  9.9924e-02,  8.3715e-02,  3.8775e-01],
          [ 1.7352e-02,  3.2752e-01,  1.3356e-01,  ..., -1.9277e-01, -3.6750e-02,  3.8436e-01],
          [-3.5011e-02,  3.7580e-01,  4.5951e-02,  ..., -3.3911e-01, -9.6983e-02,  3.8267e-01]]],


        [[[-4.5766e-01,  3.8692e-01, -3.4134e-01,  ..., -1.2709e-01,  7.5556e-02,  3.8969e-01],
          [-3.2020e-01,  3.1296e-01, -3.4135e-01,  ..., -9.8712e-02, -3.6483e-02,  2.1875e-01],
          [-4.5293e-02,  1.6502e-01, -3.4137e-01,  ..., -4.1963e-02, -2.6056e-01, -1.2312e-01],
          ...,
          [-4.1343e-02, -1.2120e-01,  3.7659e-01,  ...,  7.9779e-02, -3.2363e-01,  2.7510e-02],
          [-3.1602e-01, -2.4246e-01,  2.3046e-01,  ...,  6.2430e-02, -4.0640e-01, -2.6121e-01],
          [-4.5336e-01, -3.0309e-01,  1.5740e-01,  ...,  5.3755e-02, -4.4778e-01, -4.0557e-01]],

         [[ 3.2484e-01, -1.1436e-01,  1.9764e-01,  ...,  9.8611e-03, -7.6034e-02, -2.4793e-01],
          [ 1.8136e-01, -1.0678e-01,  2.7024e-01,  ...,  7.9036e-02, -1.3910e-01, -1.6569e-01],
          [-1.0560e-01, -9.1606e-02,  4.1545e-01,  ...,  2.1739e-01, -2.6524e-01, -1.2189e-03],
          ...,
          [ 2.1607e-01,  1.3250e-01,  2.4089e-01,  ..., -1.5644e-01,  8.3682e-02,  1.5659e-01],
          [-2.1817e-01, -2.8266e-01,  1.3239e-01,  ..., -2.6126e-01, -1.5240e-01,  1.5058e-01],
          [-4.3529e-01, -4.9024e-01,  7.8136e-02,  ..., -3.1367e-01, -2.7044e-01,  1.4758e-01]],

         [[ 2.0600e-01, -1.3696e-01,  4.9244e-02,  ..., -3.2402e-01, -3.2043e-01,  1.1546e-01],
          [ 1.5949e-01, -1.8480e-01,  1.1586e-01,  ..., -1.4288e-01, -3.1475e-01,  6.4476e-02],
          [ 6.6481e-02, -2.8046e-01,  2.4910e-01,  ...,  2.1940e-01, -3.0338e-01, -3.7489e-02],
          ...,
          [-2.6083e-01,  2.9478e-02,  1.0080e-01,  ..., -2.1633e-01,  3.3359e-01,  4.6854e-02],
          [-1.9719e-01,  1.7112e-01,  7.9082e-02,  ..., -2.1696e-01,  1.9447e-01, -2.8135e-01],
          [-1.6536e-01,  2.4193e-01,  6.8222e-02,  ..., -2.1727e-01,  1.2491e-01, -4.4545e-01]],

         ...,

         [[-2.7284e-01, -1.3078e-01,  2.0623e-01,  ..., -2.4905e-01, -4.3526e-01, -3.9228e-02],
          [-3.0236e-01, -2.0151e-01,  2.3388e-01,  ..., -1.2891e-01, -2.7235e-01,  8.7663e-02],
          [-3.6139e-01, -3.4295e-01,  2.8919e-01,  ...,  1.1138e-01,  5.3474e-02,  3.4144e-01],
          ...,
          [ 6.4100e-02, -7.8835e-02, -9.5988e-02,  ..., -2.8797e-01, -1.1280e-01, -7.3258e-02],
          [ 5.7000e-02, -5.8767e-02, -3.4031e-01,  ..., -2.5690e-01,  2.1961e-01, -1.6539e-01],
          [ 5.3450e-02, -4.8734e-02, -4.6247e-01,  ..., -2.4136e-01,  3.8581e-01, -2.1145e-01]],

         [[ 1.9008e-01, -4.7235e-01, -3.7887e-02,  ...,  1.1386e-02,  3.5999e-01,  4.8371e-01],
          [ 1.3262e-01, -3.5759e-01, -1.1793e-01,  ...,  7.3953e-02,  3.5185e-01,  3.4993e-01],
          [ 1.7708e-02, -1.2807e-01, -2.7803e-01,  ...,  1.9909e-01,  3.3558e-01,  8.2353e-02],
          ...,
          [-1.1259e-01,  2.3689e-02, -2.7648e-01,  ...,  2.4519e-01,  4.3559e-03,  1.7554e-01],
          [-1.0998e-01,  2.6252e-01, -2.5147e-01,  ...,  2.7109e-01, -2.1610e-01, -6.3784e-02],
          [-1.0868e-01,  3.8194e-01, -2.3896e-01,  ...,  2.8404e-01, -3.2633e-01, -1.8345e-01]],

         [[ 3.9242e-01, -3.1548e-01,  1.2866e-01,  ...,  2.1384e-03,  2.2059e-01, -2.9867e-01],
          [ 3.7103e-01, -2.4949e-01,  2.1601e-02,  ...,  3.2446e-02,  2.1182e-01, -3.1883e-01],
          [ 3.2824e-01, -1.1752e-01, -1.9252e-01,  ...,  9.3060e-02,  1.9429e-01, -3.5916e-01],
          ...,
          [ 8.0274e-02,  7.3290e-02, -2.5370e-01,  ...,  6.4265e-02, -1.8942e-02, -4.7774e-02],
          [ 6.5592e-02,  1.3953e-01, -1.1556e-01,  ...,  1.0508e-01, -2.0188e-01,  1.6777e-01],
          [ 5.8252e-02,  1.7266e-01, -4.6492e-02,  ...,  1.2548e-01, -2.9335e-01,  2.7555e-01]]],


        [[[-3.9522e-01,  1.7807e-01,  1.7779e-01,  ...,  2.3646e-01,  4.1050e-01, -2.7362e-01],
          [-3.0407e-01,  1.7513e-01,  1.3240e-01,  ...,  2.6342e-01,  2.2500e-01, -1.4043e-01],
          [-1.2178e-01,  1.6923e-01,  4.1629e-02,  ...,  3.1735e-01, -1.4599e-01,  1.2596e-01],
          ...,
          [-3.5995e-01, -2.5224e-01, -3.2718e-02,  ...,  2.0954e-01, -4.4463e-02, -2.6308e-01],
          [-2.4647e-01, -2.5648e-01,  1.4162e-01,  ...,  2.9499e-02, -3.6736e-02, -2.9196e-01],
          [-1.8974e-01, -2.5860e-01,  2.2878e-01,  ..., -6.0521e-02, -3.2872e-02, -3.0640e-01]],

         [[-2.5106e-01, -3.1163e-01,  3.3676e-01,  ..., -2.1129e-01, -2.6768e-01,  4.5771e-01],
          [-2.7487e-01, -2.3030e-01,  1.4966e-01,  ..., -1.5225e-01, -2.0499e-01,  2.6540e-01],
          [-3.2248e-01, -6.7634e-02, -2.2454e-01,  ..., -3.4155e-02, -7.9623e-02, -1.1921e-01],
          ...,
          [ 2.0658e-01, -6.8417e-02,  1.6234e-01,  ...,  3.4757e-01, -3.1673e-01, -1.6440e-01],
          [ 3.2176e-01, -8.1828e-02,  6.6819e-02,  ...,  2.6815e-01, -1.7258e-01,  8.1146e-03],
          [ 3.7935e-01, -8.8534e-02,  1.9062e-02,  ...,  2.2844e-01, -1.0050e-01,  9.4371e-02]],

         [[ 1.5609e-01, -2.1313e-01,  3.1210e-01,  ..., -3.4436e-01, -2.4423e-01,  2.1178e-01],
          [ 5.2386e-02, -1.9803e-01,  1.9393e-01,  ..., -1.7879e-01, -6.4378e-02,  1.3395e-01],
          [-1.5503e-01, -1.6783e-01, -4.2413e-02,  ...,  1.5237e-01,  2.9532e-01, -2.1694e-02],
          ...,
          [-2.2088e-01,  4.9517e-02,  2.4357e-01,  ..., -2.0402e-01,  1.2878e-01,  4.2895e-03],
          [-1.3958e-01,  1.8475e-01,  1.7253e-01,  ..., -1.5513e-01,  2.9531e-01, -1.7885e-01],
          [-9.8921e-02,  2.5237e-01,  1.3701e-01,  ..., -1.3068e-01,  3.7858e-01, -2.7042e-01]],

         ...,

         [[ 9.9840e-02, -8.8611e-02, -1.7345e-01,  ...,  8.3399e-02, -2.1856e-04, -1.7332e-01],
          [ 2.7381e-03, -1.5848e-01, -2.1919e-01,  ..., -1.8914e-03,  5.3203e-02, -4.8662e-02],
          [-1.9147e-01, -2.9823e-01, -3.1066e-01,  ..., -1.7247e-01,  1.6005e-01,  2.0066e-01],
          ...,
          [ 3.2174e-01,  1.7154e-02,  3.5527e-02,  ...,  3.1580e-01, -1.6918e-02, -1.7363e-03],
          [ 2.3449e-01, -2.0678e-01,  7.0511e-02,  ...,  3.5049e-01, -1.9276e-01, -3.5624e-02],
          [ 1.9087e-01, -3.1875e-01,  8.8002e-02,  ...,  3.6783e-01, -2.8068e-01, -5.2568e-02]],

         [[-4.3617e-01, -3.3952e-01, -2.8699e-01,  ..., -3.3300e-01, -2.8933e-01,  3.5484e-01],
          [-3.1924e-01, -3.6094e-01, -2.2749e-01,  ..., -3.2489e-01, -2.1019e-01,  1.4931e-01],
          [-8.5364e-02, -4.0378e-01, -1.0850e-01,  ..., -3.0868e-01, -5.1910e-02, -2.6177e-01],
          ...,
          [ 8.0918e-02, -2.8595e-01,  3.3706e-04,  ...,  1.2276e-01, -2.2389e-01, -1.8339e-01],
          [-1.6411e-01, -1.5101e-02,  1.3328e-01,  ..., -5.4777e-02, -3.3119e-01, -1.1431e-01],
          [-2.8663e-01,  1.2032e-01,  1.9975e-01,  ..., -1.4355e-01, -3.8484e-01, -7.9775e-02]],

         [[-5.1351e-02, -3.9956e-01,  1.5218e-04,  ...,  1.7622e-02, -1.2122e-01,  1.4119e-02],
          [-5.9021e-02, -3.6163e-01, -5.5284e-02,  ..., -5.2405e-02, -5.5435e-02,  9.1652e-02],
          [-7.4361e-02, -2.8577e-01, -1.6616e-01,  ..., -1.9246e-01,  7.6138e-02,  2.4672e-01],
          ...,
          [-8.1931e-02, -1.2898e-02, -7.5671e-02,  ..., -9.5722e-02, -1.6978e-01, -2.0386e-01],
          [-3.4802e-01, -1.4517e-01, -1.8494e-02,  ..., -4.7618e-02,  1.9983e-01, -2.3839e-01],
          [-4.8106e-01, -2.1131e-01,  1.0094e-02,  ..., -2.3566e-02,  3.8463e-01, -2.5565e-01]]],


        ...,


        [[[-7.3572e-02,  1.9349e-01, -2.0685e-01,  ...,  1.3014e-01,  3.2213e-01,  4.4449e-01],
          [-9.0906e-02,  6.0985e-02, -1.5049e-01,  ...,  6.1102e-02,  2.5635e-01,  4.0619e-01],
          [-1.2557e-01, -2.0403e-01, -3.7780e-02,  ..., -7.6979e-02,  1.2480e-01,  3.2960e-01],
          ...,
          [ 2.6881e-01, -9.9036e-03, -4.2839e-01,  ..., -5.0350e-02,  1.5396e-01,  1.4178e-01],
          [-6.2232e-02, -3.7108e-02, -3.5404e-01,  ...,  2.0860e-01, -1.0242e-01,  1.1320e-01],
          [-2.2775e-01, -5.0710e-02, -3.1686e-01,  ...,  3.3808e-01, -2.3062e-01,  9.8910e-02]],

         [[ 4.9098e-01, -3.6456e-02, -8.5110e-02,  ...,  6.0843e-02, -1.6374e-01,  4.9051e-02],
          [ 3.8568e-01, -2.9427e-02, -1.8356e-01,  ...,  1.6467e-01, -6.2220e-02,  9.2234e-02],
          [ 1.7507e-01, -1.5370e-02, -3.8047e-01,  ...,  3.7233e-01,  1.4081e-01,  1.7860e-01],
          ...,
          [-3.7167e-01,  2.3396e-03,  1.0026e-01,  ...,  1.2830e-01,  1.3840e-01, -1.9147e-01],
          [-3.7303e-01,  8.7782e-03,  1.3680e-01,  ..., -6.5965e-02, -1.4584e-01,  9.1849e-02],
          [-3.7371e-01,  1.1998e-02,  1.5507e-01,  ..., -1.6310e-01, -2.8796e-01,  2.3351e-01]],

         [[ 3.0735e-02, -2.8571e-01, -3.6646e-01,  ..., -8.0542e-02,  2.7919e-02,  1.7307e-01],
          [ 7.3163e-02, -1.7005e-01, -2.1110e-01,  ...,  2.8704e-03,  1.1771e-01,  5.5349e-02],
          [ 1.5802e-01,  6.1283e-02,  9.9626e-02,  ...,  1.6970e-01,  2.9729e-01, -1.8009e-01],
          ...,
          [ 1.7422e-01, -1.1284e-01, -1.4839e-01,  ...,  2.4850e-01, -2.3015e-01, -1.1285e-02],
          [-1.3993e-01,  1.4147e-01,  1.5247e-02,  ...,  2.2403e-01, -2.1555e-01, -2.8380e-01],
          [-2.9701e-01,  2.6863e-01,  9.7064e-02,  ...,  2.1180e-01, -2.0825e-01, -4.2006e-01]],

         ...,

         [[ 3.2041e-01,  3.3260e-01,  4.4073e-01,  ...,  1.9653e-03,  2.0751e-01,  1.1533e-01],
          [ 2.8506e-01,  2.7063e-01,  3.1078e-01,  ..., -7.3976e-02,  1.7601e-01,  1.8336e-01],
          [ 2.1435e-01,  1.4667e-01,  5.0882e-02,  ..., -2.2586e-01,  1.1300e-01,  3.1942e-01],
          ...,
          [ 1.3160e-01, -5.4638e-02, -9.0491e-02,  ..., -8.9211e-02, -2.0556e-01, -8.3900e-02],
          [-1.8817e-02,  5.1207e-02, -1.1397e-01,  ..., -1.5365e-01, -9.8682e-02,  8.7339e-02],
          [-9.4025e-02,  1.0413e-01, -1.2571e-01,  ..., -1.8587e-01, -4.5245e-02,  1.7296e-01]],

         [[-7.4380e-02, -2.7629e-01, -6.6116e-02,  ..., -2.6745e-01, -2.9053e-01, -2.4197e-01],
          [-7.8622e-02, -1.8912e-01, -1.3625e-01,  ..., -2.0763e-01, -2.9365e-01, -1.6283e-01],
          [-8.7106e-02, -1.4798e-02, -2.7651e-01,  ..., -8.7984e-02, -2.9989e-01, -4.5576e-03],
          ...,
          [-1.6220e-01,  4.4842e-03,  1.6270e-01,  ...,  1.9982e-01,  3.1884e-01,  3.8447e-01],
          [-3.0946e-01, -1.2801e-01,  1.7853e-01,  ...,  1.4775e-01,  3.3616e-01,  2.9296e-01],
          [-3.8310e-01, -1.9425e-01,  1.8645e-01,  ...,  1.2171e-01,  3.4482e-01,  2.4721e-01]],

         [[ 2.1795e-01, -4.4836e-01, -1.6492e-01,  ..., -3.7741e-01, -5.2544e-02,  2.2576e-01],
          [ 9.2577e-02, -4.2207e-01, -1.6448e-01,  ..., -2.0948e-01,  6.6355e-02,  6.1460e-02],
          [-1.5817e-01, -3.6949e-01, -1.6359e-01,  ...,  1.2640e-01,  3.0415e-01, -2.6715e-01],
          ...,
          [ 1.0966e-01,  2.7023e-02,  1.7383e-01,  ...,  1.1274e-01, -3.7899e-01, -1.8424e-03],
          [ 3.4315e-02,  5.3265e-02, -5.3875e-02,  ...,  7.8251e-02, -3.9502e-01,  2.5193e-01],
          [-3.3587e-03,  6.6386e-02, -1.6773e-01,  ...,  6.1005e-02, -4.0304e-01,  3.7881e-01]]],


        [[[ 3.7251e-01, -1.4789e-01, -9.0121e-02,  ...,  3.0271e-01, -1.4594e-02,  3.6338e-01],
          [ 3.7678e-01, -1.4041e-01, -2.6998e-02,  ...,  1.4042e-01, -1.1388e-01,  1.8150e-01],
          [ 3.8530e-01, -1.2546e-01,  9.9247e-02,  ..., -1.8416e-01, -3.1246e-01, -1.8226e-01],
          ...,
          [-9.0428e-02,  3.2777e-01, -7.6853e-02,  ...,  3.5639e-02,  5.3736e-02,  5.2775e-02],
          [-3.0168e-02,  6.4997e-02, -4.4807e-02,  ...,  8.8596e-02, -2.7830e-01,  9.8247e-02],
          [-3.8079e-05, -6.6390e-02, -2.8784e-02,  ...,  1.1508e-01, -4.4433e-01,  1.2098e-01]],

         [[ 1.6027e-02,  6.5760e-02,  2.5190e-01,  ...,  2.8682e-01,  1.8940e-01, -6.0296e-02],
          [-6.7207e-03, -9.7894e-03,  1.7750e-01,  ...,  3.1413e-01,  5.3596e-02, -6.9797e-02],
          [-5.2217e-02, -1.6089e-01,  2.8689e-02,  ...,  3.6876e-01, -2.1800e-01, -8.8799e-02],
          ...,
          [ 1.2753e-01,  1.5663e-01, -1.2304e-01,  ...,  7.5962e-02, -1.4911e-01, -3.4730e-01],
          [-1.1160e-01,  7.5017e-02, -2.3435e-01,  ..., -8.4204e-02,  1.0818e-01, -2.6848e-01],
          [-2.3117e-01,  3.4212e-02, -2.9000e-01,  ..., -1.6429e-01,  2.3683e-01, -2.2908e-01]],

         [[-2.2515e-01,  1.8442e-01,  1.0835e-01,  ...,  2.8544e-01,  3.1916e-01, -4.0473e-01],
          [-2.8030e-01,  1.4919e-01,  2.8080e-02,  ...,  2.2969e-01,  2.0608e-01, -3.7162e-01],
          [-3.9060e-01,  7.8716e-02, -1.3246e-01,  ...,  1.1819e-01, -2.0079e-02, -3.0541e-01],
          ...,
          [-2.2109e-01, -4.3131e-01, -1.1638e-01,  ...,  4.1653e-02,  3.4072e-01,  1.2206e-01],
          [-1.8582e-01, -3.8098e-01,  1.6588e-01,  ...,  2.9442e-01,  2.1098e-01,  1.0290e-01],
          [-1.6819e-01, -3.5582e-01,  3.0702e-01,  ...,  4.2081e-01,  1.4611e-01,  9.3321e-02]],

         ...,

         [[-1.5147e-01,  1.1840e-01,  6.1511e-02,  ...,  3.2267e-02, -2.9606e-01,  3.8482e-01],
          [-1.4122e-02, -8.1744e-03,  1.1071e-01,  ...,  9.6844e-02, -2.1752e-01,  2.4975e-01],
          [ 2.6058e-01, -2.6133e-01,  2.0910e-01,  ...,  2.2600e-01, -6.0448e-02, -2.0399e-02],
          ...,
          [ 1.7532e-02,  3.8517e-03,  3.0512e-01,  ...,  2.6843e-01,  1.8634e-01, -1.8773e-01],
          [-1.4672e-01,  2.8474e-02,  1.6581e-01,  ...,  3.7697e-01,  1.2964e-01, -3.7108e-01],
          [-2.2884e-01,  4.0785e-02,  9.6160e-02,  ...,  4.3125e-01,  1.0129e-01, -4.6276e-01]],

         [[-5.2358e-02, -4.0460e-01,  1.3622e-01,  ...,  3.0221e-01,  8.2788e-02,  2.8078e-01],
          [-1.4823e-01, -3.7928e-01,  1.6679e-01,  ...,  2.4953e-01, -7.0225e-03,  2.8076e-01],
          [-3.3996e-01, -3.2865e-01,  2.2792e-01,  ...,  1.4417e-01, -1.8664e-01,  2.8071e-01],
          ...,
          [-3.4497e-01,  9.0188e-02, -2.0681e-01,  ..., -6.2235e-02, -1.1228e-01, -1.8998e-01],
          [-3.7827e-01,  3.2966e-01,  8.2767e-02,  ..., -1.9410e-01,  6.6777e-02, -3.4948e-01],
          [-3.9491e-01,  4.4940e-01,  2.2756e-01,  ..., -2.6003e-01,  1.5630e-01, -4.2923e-01]],

         [[-1.5447e-02, -4.1842e-01,  3.6860e-01,  ...,  3.1188e-01,  2.9610e-01, -4.8901e-01],
          [-1.0590e-01, -2.9446e-01,  2.5210e-01,  ...,  2.6781e-01,  2.2951e-01, -3.2691e-01],
          [-2.8682e-01, -4.6529e-02,  1.9086e-02,  ...,  1.7967e-01,  9.6328e-02, -2.7047e-03],
          ...,
          [-2.1667e-01,  1.8906e-01, -2.3619e-01,  ...,  7.5346e-02, -2.6163e-01, -1.3641e-01],
          [ 3.3689e-02,  9.4690e-02, -1.0655e-01,  ..., -2.2892e-01, -2.2270e-01,  1.7979e-01],
          [ 1.5887e-01,  4.7504e-02, -4.1736e-02,  ..., -3.8105e-01, -2.0324e-01,  3.3789e-01]]],


        [[[-4.0633e-01, -3.3861e-05, -3.3632e-03,  ...,  2.3125e-01,  3.8214e-01, -2.5924e-01],
          [-2.3127e-01, -3.5032e-02,  1.5432e-02,  ...,  2.3873e-01,  2.3063e-01, -2.3127e-01],
          [ 1.1884e-01, -1.0503e-01,  5.3022e-02,  ...,  2.5371e-01, -7.2400e-02, -1.7535e-01],
          ...,
          [ 2.6968e-02,  1.4128e-01, -2.6884e-02,  ...,  2.1979e-01,  2.5486e-01,  7.3443e-02],
          [-1.1441e-02,  1.6970e-01, -9.0088e-02,  ...,  1.0231e-02,  3.4189e-02,  3.6553e-02],
          [-3.0646e-02,  1.8392e-01, -1.2169e-01,  ..., -9.4547e-02, -7.6146e-02,  1.8108e-02]],

         [[-8.1494e-02, -2.9381e-01, -1.0612e-01,  ...,  2.5052e-01,  3.0335e-01, -2.0131e-01],
          [-3.8530e-02, -2.5000e-01, -8.5260e-04,  ...,  2.6689e-01,  2.4065e-01, -2.1483e-01],
          [ 4.7396e-02, -1.6238e-01,  2.0968e-01,  ...,  2.9963e-01,  1.1525e-01, -2.4186e-01],
          ...,
          [-3.4152e-01, -4.2467e-01,  4.3422e-01,  ..., -1.0824e-01, -7.9168e-02,  8.0571e-02],
          [-2.3882e-01, -3.2479e-01,  4.1844e-01,  ...,  3.8537e-02, -1.6596e-01, -1.5173e-01],
          [-1.8747e-01, -2.7485e-01,  4.1054e-01,  ...,  1.1192e-01, -2.0935e-01, -2.6789e-01]],

         [[-1.5460e-01, -2.3230e-01,  4.3680e-02,  ...,  4.5007e-02, -9.3851e-02,  4.5968e-01],
          [-1.7830e-01, -8.4987e-02,  9.3736e-02,  ...,  3.2461e-02, -1.5816e-02,  4.1094e-01],
          [-2.2570e-01,  2.0964e-01,  1.9385e-01,  ...,  7.3696e-03,  1.4026e-01,  3.1347e-01],
          ...,
          [-4.1879e-01,  2.9541e-01, -1.9537e-01,  ...,  2.2889e-01,  1.2305e-01,  2.8587e-01],
          [-4.1184e-01,  4.8064e-02, -1.8557e-01,  ..., -1.3185e-01,  9.4518e-02,  2.5544e-01],
          [-4.0836e-01, -7.5607e-02, -1.8066e-01,  ..., -3.1222e-01,  8.0252e-02,  2.4023e-01]],

         ...,

         [[-4.5310e-01,  2.8729e-01,  3.3254e-01,  ...,  3.4378e-01,  2.5345e-01,  1.3803e-01],
          [-3.0975e-01,  2.1838e-01,  2.7213e-01,  ...,  2.7593e-01,  1.1739e-01, -1.3780e-02],
          [-2.3044e-02,  8.0573e-02,  1.5131e-01,  ...,  1.4022e-01, -1.5472e-01, -3.1740e-01],
          ...,
          [-6.8267e-02, -3.4984e-01,  2.7764e-02,  ..., -1.3660e-01, -2.8677e-01, -2.0622e-02],
          [-2.4560e-01, -1.7692e-01, -1.9760e-02,  ..., -3.4727e-01, -5.4393e-02, -5.0229e-02],
          [-3.3426e-01, -9.0463e-02, -4.3523e-02,  ..., -4.5260e-01,  6.1796e-02, -6.5032e-02]],

         [[-4.0940e-01,  2.1912e-01, -3.4557e-02,  ...,  4.5396e-01, -1.9873e-01, -3.9799e-01],
          [-3.1515e-01,  2.2794e-01,  2.7049e-02,  ...,  4.1247e-01, -2.2252e-01, -2.9082e-01],
          [-1.2665e-01,  2.4559e-01,  1.5026e-01,  ...,  3.2951e-01, -2.7008e-01, -7.6481e-02],
          ...,
          [ 9.5990e-02, -5.6728e-02,  1.5075e-01,  ...,  1.7933e-03,  1.1994e-01, -8.6806e-02],
          [ 2.3775e-02, -1.6496e-01,  2.4694e-01,  ..., -2.9369e-01, -8.3194e-02, -2.3136e-02],
          [-1.2333e-02, -2.1907e-01,  2.9503e-01,  ..., -4.4144e-01, -1.8476e-01,  8.6985e-03]],

         [[ 4.5706e-01, -2.9534e-02, -1.6422e-01,  ...,  2.8840e-01,  6.9570e-02, -1.8945e-01],
          [ 2.7559e-01, -6.5584e-03, -1.5643e-01,  ...,  1.5054e-01,  1.5344e-01, -7.2432e-02],
          [-8.7366e-02,  3.9393e-02, -1.4086e-01,  ..., -1.2519e-01,  3.2117e-01,  1.6159e-01],
          ...,
          [-1.8235e-01,  2.9152e-01, -2.0076e-01,  ...,  5.9021e-02,  1.1523e-01, -1.3107e-01],
          [-6.8845e-02,  2.4816e-01, -4.2859e-02,  ...,  2.4835e-01,  1.8345e-01, -4.8850e-02],
          [-1.2094e-02,  2.2648e-01,  3.6091e-02,  ...,  3.4302e-01,  2.1756e-01, -7.7378e-03]]]])

2025-07-09 13:41:02.387313 GPU 6 151383 test begin: paddle.nn.functional.interpolate(Tensor([178258, 32, 20, 20],"float32"), size=list[19,19,], mode="bicubic", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([178258, 32, 20, 20],"float32"), size=list[19,19,], mode="bicubic", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 288942624 / 2059236416 (14.0%)
Greatest absolute difference: 0.04123595356941223 at index (159131, 22, 2, 16) (up to 0.01 allowed)
Greatest relative difference: 67667520.0 at index (58758, 4, 14, 3) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([178258, 32, 19, 19]), dtype=torch.float32)
tensor([[[[ 6.2753e-03,  8.5772e-02,  4.1843e-01,  ..., -2.9942e-03, -4.4649e-01, -3.7285e-02],
          [-2.8810e-01, -4.2402e-01,  2.6762e-01,  ..., -7.0722e-03,  4.9345e-01, -1.4067e-01],
          [-2.2002e-01, -1.9017e-01,  5.7586e-02,  ...,  1.4109e-02, -4.6175e-01,  6.4405e-02],
          ...,
          [ 5.9460e-02,  4.5527e-01, -2.0928e-01,  ..., -3.0856e-01,  1.5131e-01, -6.5259e-02],
          [-3.2970e-01, -4.3604e-01,  3.4705e-01,  ..., -4.4856e-01,  4.5367e-01, -1.7747e-01],
          [-9.3472e-02,  1.4942e-01,  4.0825e-01,  ...,  2.4486e-01,  1.3777e-01, -2.7790e-01]],

         [[-2.0696e-01,  4.4449e-01,  4.2392e-01,  ..., -3.1018e-01, -2.1580e-01, -3.0230e-01],
          [-1.7187e-01,  3.0329e-01,  2.5321e-01,  ...,  4.9896e-02, -2.2913e-01, -1.4440e-01],
          [-3.4138e-01, -2.2154e-01,  2.7151e-01,  ..., -4.3437e-01,  4.6432e-02, -4.4998e-01],
          ...,
          [ 3.4441e-01, -1.8671e-01,  1.3429e-01,  ..., -4.7330e-01,  8.1129e-03, -4.9181e-01],
          [ 1.1799e-01, -2.2766e-01, -2.6301e-01,  ...,  2.6041e-01,  4.9684e-01, -1.1095e-01],
          [ 2.9612e-01,  4.1769e-01,  4.6017e-02,  ..., -4.2435e-02, -5.1606e-02,  2.9190e-01]],

         [[-1.8091e-01,  4.2816e-02,  2.1606e-01,  ..., -3.1991e-01, -3.6128e-01,  2.7611e-01],
          [ 8.7405e-02, -2.1435e-01,  2.5385e-01,  ...,  3.4256e-01, -2.4737e-02, -3.2883e-01],
          [ 4.6714e-01,  3.8230e-01,  3.7407e-01,  ..., -1.5719e-02,  4.0501e-01, -2.9403e-01],
          ...,
          [ 2.0525e-01, -1.8577e-01,  4.0184e-01,  ..., -8.1595e-02, -2.7945e-01, -3.0730e-01],
          [-3.2199e-01,  2.5533e-01,  4.4864e-01,  ..., -1.8018e-01,  1.6529e-01,  3.2985e-01],
          [ 2.1520e-01, -3.8362e-01,  2.5669e-01,  ..., -1.5476e-01,  3.1951e-01, -3.2087e-01]],

         ...,

         [[ 3.6891e-01, -6.0342e-02,  2.2272e-01,  ..., -1.5255e-01, -4.6607e-01, -1.3989e-01],
          [-4.4982e-01,  4.4742e-02, -4.3679e-01,  ..., -5.4623e-02, -1.0015e-01,  4.2567e-01],
          [ 4.2473e-01, -5.0543e-01, -1.9804e-01,  ..., -1.5059e-01, -4.5146e-01, -3.7997e-01],
          ...,
          [ 3.6361e-01, -2.1520e-01,  2.3042e-03,  ..., -2.5941e-01, -8.4830e-02,  1.7790e-01],
          [ 3.3161e-01,  2.2914e-01, -3.8785e-01,  ..., -2.3768e-01, -3.6359e-01,  1.6657e-01],
          [ 4.9014e-01, -4.0586e-01, -2.4136e-01,  ...,  2.1809e-01,  3.8352e-01, -2.2413e-01]],

         [[ 1.3601e-01,  4.5125e-01,  1.5364e-01,  ..., -8.4318e-02,  2.1515e-01, -2.2361e-01],
          [-2.7794e-02,  5.2453e-02, -3.0523e-01,  ...,  1.0972e-01, -4.8040e-01, -2.6546e-01],
          [ 3.5888e-01,  1.1140e-01, -1.5029e-01,  ...,  3.3821e-01, -1.2105e-01,  8.4330e-02],
          ...,
          [-1.5152e-01, -7.2082e-02,  1.6937e-01,  ..., -1.5553e-01, -3.8436e-01, -2.4925e-01],
          [-2.5639e-01, -2.0708e-01, -3.9855e-01,  ..., -4.7253e-01,  1.7251e-01, -3.9652e-01],
          [-2.0642e-01, -2.8082e-01,  1.7102e-02,  ..., -1.7369e-01, -2.2545e-02,  3.8003e-01]],

         [[-2.9171e-01,  2.5897e-01, -4.0253e-01,  ..., -4.8570e-01, -2.0174e-01,  9.6401e-02],
          [-3.5726e-01, -3.6014e-01, -2.3325e-01,  ..., -1.9608e-01, -1.4895e-01,  9.2000e-02],
          [-2.2536e-01, -3.6638e-01,  6.2157e-02,  ..., -2.0896e-01,  1.7825e-01, -1.0058e-03],
          ...,
          [-3.3846e-01,  2.3120e-01,  5.8216e-02,  ..., -3.2936e-01, -4.1950e-01,  4.3519e-01],
          [ 4.3819e-01, -4.2327e-01, -3.3146e-01,  ..., -2.1525e-01,  2.6987e-01, -2.2519e-01],
          [-3.8926e-01, -3.6751e-01,  5.2001e-01,  ..., -3.8794e-01,  3.0164e-01,  4.7769e-01]]],


        [[[ 4.0108e-01, -6.9567e-02,  4.0306e-01,  ..., -1.6316e-01,  4.2532e-01, -1.4112e-01],
          [ 4.4424e-01, -2.9717e-01, -3.6055e-01,  ...,  3.4459e-01, -1.8300e-01, -1.5006e-01],
          [-4.2877e-02,  4.9286e-01, -3.4031e-01,  ..., -3.8536e-01,  1.3422e-01, -1.5078e-01],
          ...,
          [ 1.4608e-01,  1.1783e-01, -3.8850e-01,  ...,  4.2993e-01,  3.7356e-01,  2.0683e-01],
          [ 6.5278e-02, -2.6194e-02, -3.0874e-01,  ...,  1.7184e-01,  2.0130e-01,  2.3745e-01],
          [ 1.4585e-01, -1.3112e-01, -8.0688e-02,  ...,  1.3807e-01,  4.4035e-01, -4.8083e-01]],

         [[ 7.2641e-02,  1.3458e-01, -1.1244e-01,  ...,  4.1428e-02,  2.7752e-01,  1.2284e-01],
          [-1.6636e-01, -1.9989e-02, -4.0471e-01,  ...,  4.3900e-01, -3.2945e-01, -2.4985e-02],
          [-3.1111e-01, -2.2789e-01,  1.4786e-01,  ...,  1.7385e-01, -2.0783e-01, -3.0953e-01],
          ...,
          [ 2.5113e-01,  2.2407e-01,  2.6193e-01,  ..., -2.5669e-01, -2.4239e-01,  2.9341e-01],
          [ 3.8619e-01, -1.6774e-01, -2.1093e-01,  ...,  5.4437e-02,  6.0989e-02,  2.6989e-01],
          [-4.4462e-01, -2.4357e-01, -2.5900e-01,  ..., -8.7403e-02, -1.7066e-01, -4.2687e-01]],

         [[ 4.6842e-01,  1.0205e-01, -4.1046e-01,  ...,  3.6071e-01,  5.0292e-01, -1.2846e-01],
          [ 1.6822e-01,  1.0530e-01,  4.8730e-01,  ...,  1.5815e-01, -3.4405e-01,  8.3285e-02],
          [ 4.3189e-01, -2.8451e-02,  9.4174e-02,  ...,  3.8006e-01, -2.4151e-01, -4.4036e-01],
          ...,
          [-1.8936e-01,  1.6750e-01, -3.9207e-01,  ..., -3.5432e-01,  1.8087e-02, -1.7108e-01],
          [-1.9046e-01,  1.3080e-01,  1.2150e-01,  ..., -2.5065e-02, -6.4099e-02,  3.6808e-02],
          [-3.2019e-01, -3.1698e-01,  3.5669e-02,  ...,  3.6674e-02,  2.7563e-01, -4.3453e-01]],

         ...,

         [[-3.2954e-01,  3.5347e-01,  1.4906e-01,  ..., -5.9264e-02, -2.5712e-01,  1.9624e-01],
          [-3.2380e-01,  2.2536e-02,  2.1138e-01,  ..., -4.3598e-01,  2.2410e-02,  2.4611e-01],
          [ 4.8246e-01, -2.6026e-01, -2.1445e-02,  ..., -2.8852e-01, -1.8650e-01,  9.7352e-02],
          ...,
          [-2.2585e-01,  3.5979e-01, -1.9599e-01,  ...,  8.3169e-02,  9.7958e-02,  4.8316e-01],
          [ 9.9671e-02, -1.0010e-01,  8.8818e-02,  ...,  2.2192e-01,  9.9218e-02,  4.2074e-01],
          [-2.3283e-01,  4.1841e-01,  2.7683e-01,  ..., -3.3769e-01,  4.1667e-01, -1.0876e-02]],

         [[-4.6142e-01, -9.7668e-02, -1.4901e-01,  ...,  3.2747e-01, -1.5398e-01,  4.3052e-01],
          [-3.6014e-01, -1.2281e-01,  2.4961e-04,  ...,  1.9345e-01, -2.7285e-01, -8.4039e-02],
          [-2.9755e-01, -1.6119e-02, -1.2271e-01,  ..., -3.2655e-02, -3.6450e-01, -7.3507e-02],
          ...,
          [ 1.1710e-01,  2.1216e-01, -2.3780e-01,  ...,  3.7388e-01, -2.5951e-01,  1.8434e-01],
          [ 4.5823e-01, -4.0413e-01,  4.0022e-01,  ...,  4.6973e-02,  1.8711e-01, -8.9051e-02],
          [ 2.1633e-01, -3.8383e-01,  4.6727e-01,  ..., -3.1873e-01, -7.9825e-02, -3.5759e-01]],

         [[ 2.0004e-01,  2.0947e-01,  1.5765e-01,  ...,  3.2177e-01,  1.2338e-01, -2.5718e-01],
          [-1.7654e-01,  5.6952e-02,  2.5190e-01,  ..., -3.3925e-01,  3.6456e-01,  3.6647e-01],
          [-2.3652e-01, -1.4354e-01, -2.6490e-01,  ..., -3.2676e-01, -3.7263e-01, -5.4521e-01],
          ...,
          [-2.4496e-01,  1.2054e-01,  4.7103e-02,  ..., -1.1338e-01,  3.8132e-02,  1.5527e-01],
          [-3.5738e-01,  3.8895e-01,  8.1890e-02,  ...,  2.1498e-01,  2.4194e-02, -2.6139e-01],
          [ 1.7752e-01, -3.3735e-01,  1.9907e-02,  ..., -2.8101e-01,  8.9011e-02,  3.4045e-01]]],


        [[[ 3.1457e-01,  2.4558e-01,  3.4346e-01,  ..., -7.8826e-03, -6.4304e-02, -4.2362e-01],
          [ 1.6953e-02, -4.3912e-01,  4.1044e-02,  ...,  3.9599e-01,  6.3967e-02, -1.5766e-01],
          [ 2.0069e-01,  4.7048e-01, -8.2491e-02,  ..., -2.1370e-01,  4.4599e-01, -3.6406e-01],
          ...,
          [ 2.9408e-01,  2.0141e-01,  3.7724e-01,  ...,  3.7030e-01,  2.9963e-01,  1.9747e-02],
          [ 3.7855e-01, -2.7880e-01,  2.3659e-01,  ...,  3.7481e-01, -6.7264e-02, -2.5314e-01],
          [-3.0537e-01,  2.8496e-01, -4.4254e-01,  ..., -7.7109e-02, -2.9878e-01, -4.9022e-01]],

         [[ 3.7830e-01,  3.2041e-01, -7.1942e-02,  ..., -2.6067e-01, -3.3560e-01,  1.5136e-02],
          [-3.6290e-01,  5.2818e-03, -4.6107e-01,  ..., -2.8202e-02,  1.9929e-01, -2.1107e-02],
          [-3.1780e-01,  5.0125e-01,  3.8059e-01,  ..., -2.0901e-01,  2.5220e-01, -1.4237e-01],
          ...,
          [ 1.4650e-01, -3.5563e-01, -1.1826e-01,  ..., -1.3130e-01,  5.2279e-01, -4.3097e-01],
          [-2.5399e-01,  2.0606e-01,  4.0160e-01,  ..., -1.7062e-01, -3.3793e-01, -3.5110e-01],
          [-1.7756e-01,  2.9569e-01, -2.1107e-01,  ..., -1.7626e-01, -2.3440e-01,  1.5095e-01]],

         [[-2.3319e-01, -1.1572e-01,  2.9682e-01,  ...,  4.6129e-01, -4.3419e-01, -4.4700e-01],
          [ 1.6563e-01, -2.3452e-01, -6.1222e-02,  ..., -3.4284e-01,  2.0370e-01,  2.6703e-01],
          [ 3.6866e-01,  4.9676e-01,  1.6230e-01,  ...,  1.0810e-01,  3.7723e-01,  1.5947e-01],
          ...,
          [-4.0674e-01,  2.4514e-02,  3.4193e-01,  ..., -3.4985e-02,  1.8634e-01,  1.9233e-01],
          [-7.0578e-02,  1.3362e-02, -4.0715e-01,  ...,  2.6601e-01,  2.2087e-01, -4.3870e-01],
          [-4.1511e-01, -2.3951e-01,  2.1852e-01,  ...,  5.1378e-01, -1.5230e-01,  3.2452e-01]],

         ...,

         [[-2.5311e-01,  1.2027e-01, -4.4956e-01,  ...,  2.9261e-01, -2.7141e-01, -2.8280e-01],
          [-2.0548e-01, -4.1923e-01, -3.5248e-01,  ...,  3.6387e-01, -7.7348e-02,  1.5094e-01],
          [ 7.0241e-02, -4.0533e-01,  4.0317e-01,  ...,  3.9854e-01, -1.4230e-01, -3.0396e-01],
          ...,
          [ 2.4105e-01, -8.2186e-02, -1.5245e-01,  ...,  2.9364e-01,  2.9788e-01, -9.2543e-02],
          [ 6.4726e-02, -2.2690e-01,  4.9365e-01,  ..., -3.1799e-01, -4.0767e-01, -2.6116e-01],
          [ 3.0588e-02, -5.0898e-02,  1.4409e-02,  ..., -3.3878e-02, -4.1496e-01,  3.8894e-01]],

         [[-4.2617e-01, -3.5360e-01,  1.0913e-03,  ..., -2.5302e-01,  2.9013e-01, -3.7116e-01],
          [-1.8624e-01, -3.1636e-01,  4.9788e-01,  ...,  1.7639e-01, -2.5437e-01, -4.1754e-01],
          [-2.0659e-01,  2.0382e-01, -9.6219e-02,  ..., -4.6667e-02,  1.5872e-01, -1.4507e-01],
          ...,
          [ 1.4774e-01,  1.7826e-02, -6.5499e-02,  ..., -3.5058e-01,  2.4416e-01,  2.6871e-01],
          [-4.3233e-01,  2.7313e-01, -1.6079e-01,  ..., -8.3329e-02,  3.0917e-01, -2.5891e-01],
          [-1.8710e-01,  3.7254e-01, -4.1567e-01,  ...,  2.4650e-01,  3.4766e-01,  2.1522e-01]],

         [[-1.9874e-01, -8.7655e-02,  6.8451e-02,  ..., -3.0640e-01, -1.2636e-01, -1.4916e-01],
          [-2.1035e-02,  2.4941e-01,  2.6931e-01,  ..., -1.0861e-01, -3.8284e-01,  4.9280e-01],
          [-1.7902e-01, -5.1665e-02, -4.1605e-01,  ..., -4.8358e-01,  2.9051e-01, -1.5299e-01],
          ...,
          [-1.8078e-01,  2.8119e-01, -5.2497e-01,  ..., -3.4503e-01, -3.8625e-01,  4.1016e-01],
          [ 1.0490e-01,  2.5938e-03,  1.2873e-01,  ..., -4.7949e-01,  3.3858e-01, -3.3404e-01],
          [-3.5328e-01, -2.0144e-01, -3.9528e-01,  ...,  3.8410e-01,  3.3464e-01, -1.0981e-01]]],


        ...,


        [[[-3.8611e-01,  1.7812e-01, -4.1828e-01,  ..., -4.2109e-01,  8.7098e-02,  1.2522e-01],
          [ 4.5972e-01, -1.3245e-01,  3.6844e-02,  ..., -9.3723e-02,  1.3546e-01,  1.6024e-01],
          [ 4.3489e-01, -1.8951e-01,  1.8595e-01,  ...,  3.6082e-01, -3.9815e-01, -3.4708e-01],
          ...,
          [-3.6926e-01, -3.5780e-01, -3.6475e-01,  ..., -3.4617e-02, -3.3841e-01, -3.5738e-01],
          [-3.2044e-01,  4.4903e-01, -2.4237e-01,  ...,  2.0832e-01, -6.8763e-02,  1.4241e-01],
          [ 1.2504e-01, -3.6738e-01, -3.3867e-01,  ..., -4.6350e-01,  1.9110e-01,  1.4996e-01]],

         [[-3.1798e-01,  3.7591e-01, -1.9421e-01,  ..., -1.6221e-01,  2.7152e-01, -1.0988e-01],
          [-3.0173e-01, -1.4688e-01, -4.5136e-01,  ...,  2.3186e-01, -1.6414e-02,  3.1281e-01],
          [-1.6041e-02,  1.7902e-01, -3.7189e-01,  ...,  1.7015e-01, -2.5490e-01,  1.2910e-01],
          ...,
          [-1.8017e-01, -9.7006e-03,  4.2892e-01,  ...,  3.0982e-01,  4.4795e-01, -2.8200e-01],
          [ 4.5794e-01, -4.8284e-01,  6.6977e-02,  ..., -1.9898e-01,  2.3436e-01,  4.4128e-01],
          [-3.6212e-01, -3.9039e-01, -3.2529e-01,  ..., -2.5773e-01,  3.7577e-01,  4.6963e-01]],

         [[ 2.1549e-02, -2.9034e-01, -4.5758e-02,  ...,  2.2385e-01, -1.5496e-02,  3.2831e-02],
          [ 1.9804e-01,  3.8147e-01, -1.0213e-01,  ..., -9.3121e-02, -2.7814e-01,  2.6279e-01],
          [ 1.8051e-01, -4.8763e-01, -3.8378e-01,  ...,  3.1412e-01, -2.6354e-01,  2.7101e-01],
          ...,
          [ 1.9245e-02, -3.3984e-01,  1.4333e-01,  ...,  3.4713e-01, -1.8247e-01, -2.2481e-01],
          [ 2.0457e-01,  7.7498e-02, -1.2985e-01,  ...,  1.3305e-01,  2.4137e-01, -9.6864e-02],
          [-4.5471e-01,  4.6600e-01,  4.6128e-01,  ..., -1.8859e-01,  3.7223e-01,  4.2411e-01]],

         ...,

         [[ 1.2242e-02,  1.8186e-01,  3.1294e-01,  ..., -2.2615e-02, -2.2725e-01, -4.1854e-01],
          [ 4.7869e-01, -7.1621e-02, -4.3538e-01,  ...,  3.7887e-01,  2.8050e-01,  8.3691e-03],
          [-3.1342e-01, -3.0181e-01, -2.2348e-01,  ..., -4.3694e-01, -1.6776e-01,  1.1899e-01],
          ...,
          [ 1.9360e-02,  3.8215e-01, -3.6294e-03,  ..., -3.0271e-01, -6.0069e-02,  2.3383e-01],
          [-1.8403e-01,  4.1369e-01,  3.8583e-01,  ...,  5.3619e-02, -7.4396e-03, -4.8998e-01],
          [ 2.5417e-01, -3.4095e-01, -2.9701e-01,  ...,  2.5580e-02,  7.0570e-02,  4.8971e-01]],

         [[-2.7781e-01,  3.1580e-01,  2.5787e-01,  ..., -3.0959e-01, -8.0035e-02, -1.3022e-01],
          [ 4.1226e-03,  1.4648e-01, -7.0314e-02,  ..., -1.8814e-02,  8.7805e-02, -8.3074e-02],
          [ 2.0724e-01,  2.1649e-01,  2.8282e-01,  ...,  5.0004e-02, -3.2209e-01,  3.6777e-01],
          ...,
          [-4.2100e-02, -1.9098e-01,  3.6894e-01,  ...,  3.2771e-02, -1.9116e-01, -7.6505e-02],
          [-2.5119e-01,  2.9474e-01, -1.4062e-01,  ...,  3.7113e-01,  4.3983e-01,  2.7589e-01],
          [-2.6192e-01,  1.8643e-01, -3.3130e-01,  ..., -2.0708e-01,  1.4422e-01, -3.9633e-01]],

         [[ 4.8784e-01,  2.7718e-01,  3.3125e-01,  ..., -4.8319e-02, -3.1501e-01,  4.4883e-01],
          [ 3.5505e-01, -2.6240e-02, -4.4213e-03,  ..., -2.2978e-01,  1.4867e-01, -4.7456e-01],
          [-2.7784e-01,  4.2988e-01, -4.4272e-01,  ..., -4.6688e-01,  3.7975e-01, -1.8571e-01],
          ...,
          [-7.8525e-03, -3.6990e-01,  4.5147e-01,  ...,  1.3364e-01,  4.0222e-01, -1.5197e-01],
          [ 2.9617e-01, -3.6474e-02, -3.8781e-01,  ...,  4.7271e-01,  3.1156e-01,  4.6107e-01],
          [-3.3203e-01,  2.6555e-01,  3.6070e-01,  ..., -3.4171e-01,  3.5993e-02, -6.8349e-02]]],


        [[[ 4.6043e-01,  3.9016e-01,  2.1992e-01,  ..., -3.2750e-02, -3.0492e-01, -1.9593e-01],
          [ 3.1143e-02, -4.1665e-01, -4.7534e-02,  ...,  2.4726e-01, -1.7943e-01, -1.4890e-01],
          [-4.2405e-01,  2.7123e-02, -4.2879e-01,  ..., -1.1870e-01,  5.8767e-02,  3.9839e-01],
          ...,
          [ 3.8989e-02, -3.1775e-01,  7.0731e-03,  ..., -1.2455e-01, -3.6867e-01,  3.2058e-01],
          [ 1.8667e-01, -2.8453e-01, -3.7231e-01,  ..., -3.2578e-01, -4.4596e-01,  4.5475e-01],
          [-2.0673e-01, -3.4433e-01,  4.0935e-01,  ..., -4.3104e-01, -1.0245e-01,  4.1778e-01]],

         [[ 6.9296e-02,  2.9583e-02, -8.3796e-02,  ...,  3.0379e-01,  4.1313e-01, -1.6199e-01],
          [-3.5649e-01,  2.6862e-01, -4.5792e-01,  ..., -2.8858e-01, -2.8094e-01,  1.6602e-01],
          [ 3.4757e-01,  1.0389e-01, -2.7540e-01,  ...,  3.5904e-01,  4.5281e-01, -7.2074e-02],
          ...,
          [ 3.2720e-01, -1.7376e-01,  1.2666e-01,  ...,  2.8280e-01,  3.9466e-01, -4.3182e-01],
          [ 7.8109e-02, -2.0743e-01, -1.5886e-01,  ..., -2.3055e-01, -3.9627e-01,  1.4423e-01],
          [ 2.1335e-01, -4.8903e-02, -2.7991e-01,  ...,  4.2350e-01, -2.8302e-02,  2.3936e-01]],

         [[ 5.5481e-05,  3.1158e-01,  2.8709e-02,  ..., -1.7737e-01, -4.7548e-01,  1.8665e-01],
          [ 2.8374e-01,  2.2710e-02,  7.6386e-02,  ...,  2.3616e-01, -3.4784e-01,  1.7388e-01],
          [ 3.6152e-01, -1.9771e-01,  2.2981e-01,  ...,  3.1774e-01,  4.9328e-01, -9.7707e-02],
          ...,
          [ 2.0146e-01, -3.1923e-01,  4.1638e-01,  ..., -1.3795e-01, -4.1639e-01, -3.7531e-01],
          [ 9.6130e-02, -2.2522e-01, -2.3504e-01,  ...,  1.8206e-01, -4.5283e-01, -3.8030e-01],
          [ 2.3740e-01,  1.0636e-01, -1.3547e-04,  ..., -3.9382e-01,  1.1557e-01,  2.9724e-01]],

         ...,

         [[ 4.9055e-01,  1.1344e-01, -1.4242e-02,  ..., -2.6321e-01, -4.1549e-01,  3.2070e-01],
          [-4.7208e-01,  2.0721e-01,  2.2417e-01,  ..., -2.3992e-02, -4.1881e-01, -4.7513e-01],
          [ 4.9451e-01, -3.1167e-01, -3.6707e-01,  ...,  4.1305e-01,  2.3449e-01, -2.7029e-01],
          ...,
          [ 2.4645e-01,  1.3531e-01, -4.4027e-01,  ...,  3.2481e-01, -2.4799e-01, -3.4130e-01],
          [ 5.1210e-02,  9.3544e-02, -1.1139e-01,  ..., -3.3488e-01, -1.5652e-01, -6.6913e-02],
          [-1.4368e-01,  4.5279e-01,  2.2422e-01,  ..., -4.0180e-01, -4.9940e-01, -3.8216e-01]],

         [[ 1.1064e-01,  3.4613e-01, -5.1478e-01,  ..., -7.4597e-02, -5.0258e-01,  4.6364e-01],
          [-4.3070e-01,  1.7322e-01, -6.1460e-02,  ..., -2.5038e-01,  2.1852e-01, -2.7805e-02],
          [-3.1620e-01, -4.7298e-03, -1.8404e-01,  ..., -2.8498e-01,  4.4248e-01,  2.7438e-01],
          ...,
          [ 3.4585e-01,  4.1319e-01,  2.9274e-01,  ...,  1.2491e-02,  6.4185e-03, -3.2702e-01],
          [ 6.2371e-04, -1.6249e-01,  8.0412e-02,  ..., -3.5306e-01, -4.7124e-01,  1.1815e-01],
          [ 3.6807e-01,  4.1937e-01,  3.2276e-01,  ...,  3.5898e-01,  1.7931e-01, -7.1978e-02]],

         [[ 3.6836e-01,  1.5316e-01, -1.7677e-01,  ...,  3.7410e-01, -4.4044e-02,  1.3060e-01],
          [-2.8751e-01, -9.6132e-02, -4.1124e-01,  ...,  4.3378e-01,  5.1004e-01, -3.0591e-01],
          [-5.1148e-02,  2.2022e-01, -5.0228e-01,  ...,  1.1077e-01,  1.4119e-01,  7.9607e-02],
          ...,
          [-4.3225e-01, -3.1623e-01, -3.2155e-01,  ...,  4.6026e-01, -1.8370e-01, -3.4523e-01],
          [ 3.6142e-01,  1.2576e-01, -3.6347e-01,  ..., -9.0796e-02, -2.7146e-02,  2.3172e-01],
          [ 3.1502e-01,  2.5428e-01, -4.5531e-01,  ...,  3.1416e-02,  2.9632e-01, -4.8202e-02]]],


        [[[ 3.7351e-01, -4.5563e-01, -3.5501e-01,  ..., -3.7375e-01, -4.5423e-01, -2.4058e-01],
          [ 2.1074e-01, -9.3883e-02,  1.1958e-01,  ..., -2.6471e-02,  1.8234e-01,  2.9745e-01],
          [-1.2643e-01, -3.7782e-01,  9.4335e-03,  ..., -3.5141e-02, -3.6797e-02,  4.4146e-01],
          ...,
          [-8.4648e-02, -2.8792e-01,  4.5944e-01,  ...,  2.5745e-01, -2.7727e-01,  7.8139e-02],
          [ 2.1977e-01, -4.9172e-02,  6.5982e-02,  ...,  1.7944e-01,  3.4373e-01, -2.5648e-01],
          [ 7.8441e-02,  1.2950e-01, -1.3575e-01,  ..., -1.1437e-01,  1.0844e-01, -1.4097e-01]],

         [[ 3.8457e-01,  2.8915e-01,  3.7130e-01,  ...,  5.0363e-02, -2.3735e-02, -1.2565e-01],
          [-1.2478e-01,  1.1716e-01,  1.7218e-01,  ...,  3.8862e-01,  3.2439e-01,  3.7489e-01],
          [ 2.8176e-01,  1.8173e-01,  1.1879e-01,  ..., -1.8411e-01,  4.1928e-01,  4.1305e-01],
          ...,
          [ 3.1685e-01, -7.3935e-02,  1.9361e-01,  ...,  7.3063e-03, -1.0047e-01,  3.8284e-01],
          [-2.1401e-01,  4.4381e-01, -3.3538e-01,  ..., -1.1973e-01, -4.5677e-01,  1.8228e-01],
          [-4.2711e-01, -3.2186e-01,  3.7439e-01,  ...,  2.5935e-01,  1.1732e-02, -2.6941e-01]],

         [[-4.1856e-01,  3.0287e-01, -1.7628e-01,  ...,  2.5386e-01, -2.4550e-01, -1.4537e-01],
          [-6.4168e-02,  1.4466e-01,  4.1195e-01,  ...,  3.1746e-01,  1.9061e-01, -4.7862e-01],
          [ 3.7990e-01, -2.1814e-01, -1.2034e-01,  ...,  2.4347e-01, -4.5853e-01, -4.5133e-01],
          ...,
          [ 1.1884e-01,  2.3353e-01,  3.1040e-01,  ..., -3.8527e-01, -2.8758e-01, -2.7621e-01],
          [-3.6727e-01, -1.8124e-01, -2.7060e-01,  ...,  2.1037e-01,  3.7332e-01,  3.2470e-01],
          [ 4.0721e-01, -3.6561e-01, -3.5187e-01,  ..., -8.5234e-02, -4.1647e-01,  9.0431e-02]],

         ...,

         [[-4.0086e-01, -3.9471e-01, -2.2278e-01,  ...,  1.4800e-01,  1.8465e-01,  7.0447e-02],
          [-3.9970e-02,  1.2982e-01, -4.4235e-01,  ...,  4.0231e-01, -2.4230e-01,  3.7554e-01],
          [-4.8973e-03, -4.5421e-01, -1.7617e-01,  ...,  1.4170e-01,  4.3987e-02,  3.0309e-01],
          ...,
          [ 1.6861e-01,  1.5243e-01, -2.5627e-01,  ..., -2.1384e-01, -4.0351e-01,  3.1372e-01],
          [ 2.4357e-01, -4.6676e-02, -2.2698e-01,  ..., -2.5655e-02,  7.6775e-02,  2.0231e-01],
          [ 1.2748e-01, -2.4802e-01,  2.0914e-01,  ..., -3.3319e-02,  8.4407e-02,  1.7828e-01]],

         [[ 1.9040e-02, -1.9492e-01,  1.7570e-01,  ..., -4.2290e-01, -1.8428e-01,  5.0764e-02],
          [-5.0165e-01,  4.3014e-01,  3.6683e-01,  ..., -1.4247e-01, -2.0188e-01,  2.5512e-01],
          [-4.1880e-01, -3.4976e-01, -2.1296e-01,  ..., -1.2890e-01,  1.2187e-01,  3.7784e-01],
          ...,
          [-2.4179e-01,  3.0774e-01, -5.2776e-01,  ...,  2.4177e-01, -3.1884e-01, -4.2920e-01],
          [-4.6163e-01, -3.2417e-01,  3.3759e-01,  ..., -3.3128e-01,  4.5452e-03,  3.0653e-01],
          [ 4.9221e-01,  4.5472e-01, -4.0564e-01,  ..., -4.2214e-01, -5.0044e-01,  1.6454e-01]],

         [[ 1.8462e-01,  1.9543e-01, -1.9303e-01,  ...,  3.5307e-02, -1.9826e-01, -1.6042e-01],
          [-3.2697e-01,  3.0429e-01,  8.4730e-02,  ..., -2.9787e-01,  4.5485e-01, -2.1529e-01],
          [ 5.2867e-02, -1.6305e-01, -3.0575e-01,  ...,  7.0949e-02, -2.6414e-01,  1.2550e-01],
          ...,
          [-7.7697e-02,  1.5382e-01, -3.1160e-01,  ...,  2.2128e-01,  2.2215e-01, -3.9766e-02],
          [-2.6191e-01, -3.0270e-01,  9.8943e-02,  ...,  6.6351e-02, -1.3751e-01,  1.2270e-01],
          [-3.3088e-01, -2.4247e-01,  2.4066e-01,  ..., -1.4870e-01, -2.5056e-01,  4.7879e-01]]]])
DESIRED: (shape=torch.Size([178258, 32, 19, 19]), dtype=torch.float32)
tensor([[[[ 0.0016,  0.0831,  0.4184,  ...,  0.0077, -0.4254, -0.0477],
          [-0.2968, -0.4122,  0.2569,  ..., -0.0244,  0.4877, -0.1238],
          [-0.2077, -0.1875,  0.0510,  ...,  0.0266, -0.4596,  0.0461],
          ...,
          [ 0.0769,  0.4382, -0.2039,  ..., -0.2886,  0.1434, -0.0621],
          [-0.3297, -0.4084,  0.3275,  ..., -0.4604,  0.4418, -0.1603],
          [-0.0943,  0.1446,  0.4133,  ...,  0.2372,  0.1507, -0.2680]],

         [[-0.1934,  0.4513,  0.4186,  ..., -0.3010, -0.2175, -0.2969],
          [-0.1666,  0.2996,  0.2431,  ...,  0.0482, -0.2185, -0.1529],
          [-0.3360, -0.2195,  0.2711,  ..., -0.4454,  0.0365, -0.4284],
          ...,
          [ 0.3310, -0.1783,  0.1562,  ..., -0.4628, -0.0063, -0.4787],
          [ 0.1143, -0.2446, -0.2528,  ...,  0.2332,  0.5027, -0.1125],
          [ 0.2951,  0.3983,  0.0453,  ..., -0.0257, -0.0464,  0.2784]],

         [[-0.1723,  0.0425,  0.2227,  ..., -0.2956, -0.3650,  0.2523],
          [ 0.0927, -0.2032,  0.2643,  ...,  0.3415,  0.0033, -0.3305],
          [ 0.4640,  0.3912,  0.3651,  ..., -0.0146,  0.3942, -0.2791],
          ...,
          [ 0.2080, -0.1720,  0.3812,  ..., -0.0710, -0.2748, -0.3173],
          [-0.3096,  0.2653,  0.4551,  ..., -0.1891,  0.1463,  0.3245],
          [ 0.1920, -0.3641,  0.2543,  ..., -0.1604,  0.3167, -0.2946]],

         ...,

         [[ 0.3433, -0.0563,  0.2046,  ..., -0.1395, -0.4590, -0.1344],
          [-0.4322,  0.0295, -0.4373,  ..., -0.0551, -0.1079,  0.4047],
          [ 0.4041, -0.5110, -0.1760,  ..., -0.1648, -0.4475, -0.3735],
          ...,
          [ 0.3398, -0.2168,  0.0040,  ..., -0.2652, -0.0790,  0.1763],
          [ 0.3313,  0.2151, -0.3881,  ..., -0.2361, -0.3766,  0.1607],
          [ 0.4695, -0.4011, -0.2472,  ...,  0.2057,  0.3740, -0.2049]],

         [[ 0.1388,  0.4410,  0.1474,  ..., -0.0810,  0.2008, -0.2162],
          [-0.0204,  0.0394, -0.2983,  ...,  0.1189, -0.4720, -0.2647],
          [ 0.3561,  0.1042, -0.1333,  ...,  0.3185, -0.1164,  0.0834],
          ...,
          [-0.1507, -0.0584,  0.1828,  ..., -0.1403, -0.3786, -0.2533],
          [-0.2537, -0.2081, -0.3956,  ..., -0.4684,  0.1568, -0.3919],
          [-0.2094, -0.2742,  0.0063,  ..., -0.1860, -0.0250,  0.3574]],

         [[-0.2816,  0.2406, -0.3990,  ..., -0.4799, -0.2111,  0.0910],
          [-0.3590, -0.3657, -0.2284,  ..., -0.1996, -0.1433,  0.0870],
          [-0.2125, -0.3579,  0.0698,  ..., -0.2151,  0.1688, -0.0057],
          ...,
          [-0.3292,  0.2309,  0.0604,  ..., -0.3053, -0.4336,  0.4129],
          [ 0.4172, -0.4184, -0.3421,  ..., -0.2196,  0.2528, -0.2099],
          [-0.3732, -0.3524,  0.5076,  ..., -0.3895,  0.2865,  0.4608]]],


        [[[ 0.3927, -0.0724,  0.3916,  ..., -0.1574,  0.4100, -0.1297],
          [ 0.4205, -0.2973, -0.3667,  ...,  0.3263, -0.1746, -0.1538],
          [-0.0282,  0.4851, -0.3247,  ..., -0.3786,  0.1377, -0.1348],
          ...,
          [ 0.1450,  0.0980, -0.3812,  ...,  0.4254,  0.3834,  0.2067],
          [ 0.0650, -0.0257, -0.3220,  ...,  0.1865,  0.1983,  0.2463],
          [ 0.1389, -0.1343, -0.0744,  ...,  0.1235,  0.4445, -0.4490]],

         [[ 0.0697,  0.1277, -0.1203,  ...,  0.0486,  0.2634,  0.1235],
          [-0.1680, -0.0318, -0.3934,  ...,  0.4468, -0.3244, -0.0401],
          [-0.3174, -0.2252,  0.1498,  ...,  0.1715, -0.2014, -0.3060],
          ...,
          [ 0.2444,  0.2284,  0.2502,  ..., -0.2454, -0.2369,  0.2757],
          [ 0.3851, -0.1662, -0.2065,  ...,  0.0454,  0.0526,  0.2767],
          [-0.4246, -0.2428, -0.2466,  ..., -0.0908, -0.1595, -0.4087]],

         [[ 0.4560,  0.0852, -0.3825,  ...,  0.3373,  0.4956, -0.1117],
          [ 0.1675,  0.1117,  0.4895,  ...,  0.1668, -0.3515,  0.0652],
          [ 0.4237, -0.0385,  0.0888,  ...,  0.3763, -0.2172, -0.4326],
          ...,
          [-0.1757,  0.1605, -0.3889,  ..., -0.3490,  0.0205, -0.1754],
          [-0.1835,  0.1442,  0.0971,  ..., -0.0259, -0.0706,  0.0384],
          [-0.3179, -0.3027,  0.0463,  ...,  0.0302,  0.2744, -0.4113]],

         ...,

         [[-0.3172,  0.3536,  0.1439,  ..., -0.0710, -0.2529,  0.1884],
          [-0.2995,  0.0213,  0.2122,  ..., -0.4398,  0.0102,  0.2397],
          [ 0.4603, -0.2729, -0.0271,  ..., -0.2744, -0.1975,  0.0952],
          ...,
          [-0.2034,  0.3563, -0.1903,  ...,  0.0841,  0.0836,  0.4754],
          [ 0.0916, -0.0977,  0.0862,  ...,  0.2272,  0.0951,  0.4214],
          [-0.2134,  0.4159,  0.2619,  ..., -0.3244,  0.4005,  0.0065]],

         [[-0.4524, -0.0959, -0.1389,  ...,  0.3179, -0.1523,  0.4084],
          [-0.3530, -0.1162,  0.0046,  ...,  0.1791, -0.2694, -0.0935],
          [-0.2918, -0.0172, -0.1140,  ..., -0.0327, -0.3542, -0.0870],
          ...,
          [ 0.1201,  0.2108, -0.2420,  ...,  0.3753, -0.2548,  0.1636],
          [ 0.4368, -0.3898,  0.3943,  ...,  0.0579,  0.1831, -0.0724],
          [ 0.2085, -0.3751,  0.4650,  ..., -0.3051, -0.0768, -0.3470]],

         [[ 0.1930,  0.2073,  0.1541,  ...,  0.3158,  0.1367, -0.2368],
          [-0.1777,  0.0608,  0.2371,  ..., -0.3524,  0.3375,  0.3582],
          [-0.2391, -0.1528, -0.2677,  ..., -0.3067, -0.3658, -0.5481],
          ...,
          [-0.2357,  0.1202,  0.0428,  ..., -0.1205,  0.0318,  0.1627],
          [-0.3477,  0.3984,  0.0794,  ...,  0.2204,  0.0309, -0.2572],
          [ 0.1570, -0.3244,  0.0318,  ..., -0.2705,  0.0772,  0.3240]]],


        [[[ 0.3068,  0.2327,  0.3356,  ...,  0.0089, -0.0578, -0.4112],
          [ 0.0073, -0.4270,  0.0336,  ...,  0.3783,  0.0863, -0.1553],
          [ 0.2086,  0.4684, -0.0816,  ..., -0.2174,  0.4403, -0.3434],
          ...,
          [ 0.2856,  0.2017,  0.3865,  ...,  0.3648,  0.3105,  0.0310],
          [ 0.3735, -0.2744,  0.2492,  ...,  0.3656, -0.0429, -0.2428],
          [-0.2800,  0.2658, -0.4274,  ..., -0.0572, -0.2899, -0.4824]],

         [[ 0.3634,  0.3046, -0.0793,  ..., -0.2510, -0.3297,  0.0079],
          [-0.3629,  0.0083, -0.4539,  ..., -0.0275,  0.2088, -0.0176],
          [-0.3070,  0.5055,  0.3894,  ..., -0.2087,  0.2321, -0.1420],
          ...,
          [ 0.1361, -0.3441, -0.1288,  ..., -0.1486,  0.5325, -0.4070],
          [-0.2388,  0.2044,  0.3833,  ..., -0.1734, -0.3185, -0.3600],
          [-0.1698,  0.2927, -0.2076,  ..., -0.1682, -0.2430,  0.1339]],

         [[-0.2243, -0.1103,  0.2947,  ...,  0.4460, -0.4042, -0.4340],
          [ 0.1699, -0.2241, -0.0587,  ..., -0.3344,  0.2024,  0.2748],
          [ 0.3589,  0.4992,  0.1423,  ...,  0.1076,  0.3711,  0.1617],
          ...,
          [-0.3829,  0.0355,  0.3369,  ..., -0.0337,  0.1832,  0.1942],
          [-0.0737,  0.0107, -0.4036,  ...,  0.2503,  0.2357, -0.4235],
          [-0.4051, -0.2216,  0.1967,  ...,  0.5165, -0.1388,  0.2991]],

         ...,

         [[-0.2446,  0.1021, -0.4432,  ...,  0.2861, -0.2543, -0.2744],
          [-0.2061, -0.4264, -0.3383,  ...,  0.3619, -0.0681,  0.1399],
          [ 0.0706, -0.3985,  0.3962,  ...,  0.3904, -0.1329, -0.2894],
          ...,
          [ 0.2418, -0.0912, -0.1518,  ...,  0.2931,  0.2953, -0.0934],
          [ 0.0608, -0.2150,  0.4877,  ..., -0.3099, -0.3918, -0.2675],
          [ 0.0293, -0.0537,  0.0217,  ..., -0.0319, -0.4205,  0.3601]],

         [[-0.4205, -0.3460,  0.0145,  ..., -0.2347,  0.2749, -0.3589],
          [-0.1893, -0.2897,  0.4929,  ...,  0.1693, -0.2418, -0.4110],
          [-0.1861,  0.1998, -0.1024,  ..., -0.0345,  0.1623, -0.1345],
          ...,
          [ 0.1454,  0.0117, -0.0512,  ..., -0.3561,  0.2195,  0.2660],
          [-0.4098,  0.2686, -0.1650,  ..., -0.0882,  0.3085, -0.2424],
          [-0.1807,  0.3641, -0.4186,  ...,  0.2235,  0.3499,  0.2080]],

         [[-0.1931, -0.0765,  0.0769,  ..., -0.2941, -0.1375, -0.1354],
          [-0.0156,  0.2522,  0.2497,  ..., -0.1085, -0.3780,  0.4706],
          [-0.1827, -0.0546, -0.3999,  ..., -0.4843,  0.2710, -0.1411],
          ...,
          [-0.1726,  0.2663, -0.5291,  ..., -0.3278, -0.3956,  0.3852],
          [ 0.1032,  0.0157,  0.1101,  ..., -0.4925,  0.3157, -0.3063],
          [-0.3408, -0.2022, -0.3732,  ...,  0.3582,  0.3445, -0.1067]]],


        ...,


        [[[-0.3581,  0.1647, -0.3960,  ..., -0.4028,  0.0753,  0.1266],
          [ 0.4580, -0.1406,  0.0399,  ..., -0.0880,  0.1227,  0.1517],
          [ 0.4255, -0.1919,  0.1884,  ...,  0.3464, -0.3838, -0.3581],
          ...,
          [-0.3576, -0.3692, -0.3593,  ..., -0.0223, -0.3262, -0.3612],
          [-0.3139,  0.4438, -0.2539,  ...,  0.2112, -0.0759,  0.1281],
          [ 0.1075, -0.3576, -0.3318,  ..., -0.4427,  0.1723,  0.1522]],

         [[-0.3041,  0.3644, -0.2076,  ..., -0.1471,  0.2613, -0.0936],
          [-0.2911, -0.1522, -0.4550,  ...,  0.2232, -0.0241,  0.3071],
          [-0.0187,  0.1730, -0.3601,  ...,  0.1746, -0.2411,  0.1232],
          ...,
          [-0.1815,  0.0048,  0.4289,  ...,  0.3147,  0.4503, -0.2595],
          [ 0.4376, -0.4768,  0.0841,  ..., -0.1897,  0.2270,  0.4213],
          [-0.3463, -0.3924, -0.3160,  ..., -0.2611,  0.3591,  0.4691]],

         [[ 0.0189, -0.2768, -0.0388,  ...,  0.2173, -0.0166,  0.0358],
          [ 0.2028,  0.3646, -0.1110,  ..., -0.0885, -0.2855,  0.2563],
          [ 0.1734, -0.4923, -0.3760,  ...,  0.3151, -0.2560,  0.2543],
          ...,
          [ 0.0065, -0.3219,  0.1439,  ...,  0.3344, -0.1751, -0.2216],
          [ 0.2079,  0.0558, -0.1340,  ...,  0.1440,  0.2345, -0.1004],
          [-0.4241,  0.4708,  0.4512,  ..., -0.1860,  0.3591,  0.4141]],

         ...,

         [[ 0.0252,  0.1838,  0.2886,  ..., -0.0020, -0.2125, -0.4068],
          [ 0.4580, -0.0955, -0.4376,  ...,  0.3654,  0.2847,  0.0216],
          [-0.3112, -0.2996, -0.2194,  ..., -0.4284, -0.1826,  0.1135],
          ...,
          [ 0.0176,  0.3717,  0.0057,  ..., -0.2971, -0.0577,  0.2289],
          [-0.1720,  0.4330,  0.3823,  ...,  0.0407, -0.0033, -0.4791],
          [ 0.2341, -0.3333, -0.2823,  ...,  0.0274,  0.0627,  0.4619]],

         [[-0.2612,  0.3217,  0.2373,  ..., -0.2947, -0.0819, -0.1285],
          [ 0.0135,  0.1431, -0.0575,  ..., -0.0045,  0.0790, -0.0702],
          [ 0.2156,  0.2092,  0.2859,  ...,  0.0355, -0.3155,  0.3560],
          ...,
          [-0.0345, -0.1833,  0.3660,  ...,  0.0396, -0.1824, -0.0776],
          [-0.2382,  0.2865, -0.1332,  ...,  0.3698,  0.4309,  0.2804],
          [-0.2523,  0.1824, -0.3180,  ..., -0.2040,  0.1525, -0.3719]],

         [[ 0.4819,  0.2701,  0.3183,  ..., -0.0602, -0.3095,  0.4157],
          [ 0.3315, -0.0243, -0.0087,  ..., -0.2456,  0.1630, -0.4680],
          [-0.2535,  0.4062, -0.4467,  ..., -0.4580,  0.3627, -0.1746],
          ...,
          [-0.0086, -0.3611,  0.4422,  ...,  0.1386,  0.3939, -0.1347],
          [ 0.2905, -0.0584, -0.3799,  ...,  0.4710,  0.3220,  0.4516],
          [-0.3082,  0.2713,  0.3366,  ..., -0.3275,  0.0353, -0.0547]]],


        [[[ 0.4514,  0.3711,  0.2073,  ..., -0.0303, -0.2978, -0.1983],
          [ 0.0051, -0.4168, -0.0551,  ...,  0.2392, -0.1644, -0.1374],
          [-0.4055,  0.0183, -0.4197,  ..., -0.1212,  0.0515,  0.3885],
          ...,
          [ 0.0178, -0.3169,  0.0124,  ..., -0.1361, -0.3612,  0.3010],
          [ 0.1824, -0.2934, -0.3717,  ..., -0.3085, -0.4627,  0.4360],
          [-0.2028, -0.3302,  0.3944,  ..., -0.4163, -0.1254,  0.4089]],

         [[ 0.0597,  0.0299, -0.0823,  ...,  0.2814,  0.4059, -0.1445],
          [-0.3342,  0.2599, -0.4489,  ..., -0.2788, -0.2809,  0.1577],
          [ 0.3403,  0.0931, -0.2780,  ...,  0.3531,  0.4521, -0.0663],
          ...,
          [ 0.3024, -0.1620,  0.1481,  ...,  0.2823,  0.3971, -0.4161],
          [ 0.0784, -0.2137, -0.1563,  ..., -0.2243, -0.3881,  0.1208],
          [ 0.2056, -0.0628, -0.2647,  ...,  0.4127, -0.0314,  0.2324]],

         [[ 0.0115,  0.3067,  0.0182,  ..., -0.1529, -0.4801,  0.1736],
          [ 0.2857,  0.0093,  0.0885,  ...,  0.2438, -0.3234,  0.1571],
          [ 0.3404, -0.1895,  0.2208,  ...,  0.3109,  0.4990, -0.0843],
          ...,
          [ 0.1809, -0.3067,  0.3942,  ..., -0.1324, -0.4121, -0.3576],
          [ 0.0917, -0.2357, -0.2261,  ...,  0.1831, -0.4477, -0.3952],
          [ 0.2320,  0.0960, -0.0056,  ..., -0.3758,  0.0908,  0.2815]],

         ...,

         [[ 0.4636,  0.1100, -0.0157,  ..., -0.2486, -0.4251,  0.2907],
          [-0.4516,  0.2070,  0.2041,  ..., -0.0080, -0.3970, -0.4806],
          [ 0.4762, -0.3146, -0.3477,  ...,  0.4023,  0.2464, -0.2615],
          ...,
          [ 0.2413,  0.1238, -0.4336,  ...,  0.3217, -0.2263, -0.3463],
          [ 0.0595,  0.0844, -0.1201,  ..., -0.3239, -0.1595, -0.0687],
          [-0.1285,  0.4494,  0.2172,  ..., -0.3850, -0.4954, -0.3780]],

         [[ 0.1059,  0.3289, -0.5068,  ..., -0.0775, -0.4932,  0.4346],
          [-0.4249,  0.1699, -0.0644,  ..., -0.2645,  0.2282, -0.0206],
          [-0.3026,  0.0025, -0.1676,  ..., -0.2769,  0.4294,  0.2684],
          ...,
          [ 0.3521,  0.4129,  0.2727,  ...,  0.0182,  0.0134, -0.3254],
          [-0.0017, -0.1531,  0.0736,  ..., -0.3388, -0.4795,  0.1009],
          [ 0.3614,  0.4093,  0.3014,  ...,  0.3490,  0.1722, -0.0634]],

         [[ 0.3516,  0.1396, -0.1869,  ...,  0.3723, -0.0255,  0.1180],
          [-0.2872, -0.0973, -0.4140,  ...,  0.4290,  0.5197, -0.2896],
          [-0.0438,  0.2073, -0.5028,  ...,  0.0978,  0.1420,  0.0896],
          ...,
          [-0.4407, -0.3178, -0.3208,  ...,  0.4494, -0.1689, -0.3388],
          [ 0.3439,  0.1011, -0.3528,  ..., -0.0799, -0.0387,  0.2179],
          [ 0.3167,  0.2378, -0.4578,  ...,  0.0361,  0.2880, -0.0350]]],


        [[[ 0.3549, -0.4594, -0.3336,  ..., -0.3551, -0.4451, -0.2345],
          [ 0.1941, -0.0935,  0.1132,  ..., -0.0192,  0.1779,  0.3072],
          [-0.1242, -0.3691,  0.0177,  ..., -0.0362, -0.0328,  0.4235],
          ...,
          [-0.0875, -0.2835,  0.4696,  ...,  0.2471, -0.2772,  0.0759],
          [ 0.2095, -0.0540,  0.0647,  ...,  0.1893,  0.3396, -0.2405],
          [ 0.0828,  0.1227, -0.1372,  ..., -0.1065,  0.1123, -0.1383]],

         [[ 0.3721,  0.2873,  0.3624,  ...,  0.0544, -0.0137, -0.1143],
          [-0.1174,  0.1219,  0.1548,  ...,  0.3733,  0.3338,  0.3836],
          [ 0.2728,  0.1852,  0.1161,  ..., -0.1712,  0.4015,  0.4034],
          ...,
          [ 0.3018, -0.0797,  0.1849,  ...,  0.0223, -0.1080,  0.3569],
          [-0.1852,  0.4390, -0.3401,  ..., -0.1135, -0.4595,  0.1826],
          [-0.4224, -0.2906,  0.3566,  ...,  0.2594,  0.0092, -0.2562]],

         [[-0.3979,  0.3015, -0.1696,  ...,  0.2509, -0.2262, -0.1542],
          [-0.0453,  0.1468,  0.3943,  ...,  0.3048,  0.1963, -0.4706],
          [ 0.3581, -0.2284, -0.1080,  ...,  0.2476, -0.4396, -0.4507],
          ...,
          [ 0.1203,  0.2446,  0.3065,  ..., -0.3694, -0.2894, -0.2755],
          [-0.3633, -0.1719, -0.2526,  ...,  0.1914,  0.3692,  0.3167],
          [ 0.3769, -0.3751, -0.3384,  ..., -0.0823, -0.3993,  0.0857]],

         ...,

         [[-0.3938, -0.3829, -0.2150,  ...,  0.1492,  0.1777,  0.0783],
          [-0.0305,  0.1136, -0.4320,  ...,  0.4035, -0.2397,  0.3680],
          [-0.0129, -0.4577, -0.1560,  ...,  0.1341,  0.0554,  0.2833],
          ...,
          [ 0.1736,  0.1296, -0.2304,  ..., -0.2160, -0.4133,  0.2916],
          [ 0.2376, -0.0453, -0.2366,  ..., -0.0269,  0.0630,  0.2039],
          [ 0.1221, -0.2424,  0.2113,  ..., -0.0282,  0.0804,  0.1769]],

         [[ 0.0047, -0.1771,  0.1826,  ..., -0.4170, -0.1931,  0.0502],
          [-0.4913,  0.4351,  0.3449,  ..., -0.1336, -0.2007,  0.2533],
          [-0.4066, -0.3428, -0.2008,  ..., -0.1251,  0.1092,  0.3648],
          ...,
          [-0.2149,  0.2999, -0.5331,  ...,  0.2384, -0.3032, -0.4249],
          [-0.4712, -0.3052,  0.3234,  ..., -0.3078, -0.0081,  0.2875],
          [ 0.4739,  0.4219, -0.3936,  ..., -0.4061, -0.5008,  0.1556]],

         [[ 0.1751,  0.1904, -0.1868,  ...,  0.0280, -0.1803, -0.1628],
          [-0.3134,  0.3008,  0.0835,  ..., -0.3029,  0.4424, -0.1933],
          [ 0.0484, -0.1793, -0.2910,  ...,  0.0679, -0.2546,  0.1067],
          ...,
          [-0.0735,  0.1419, -0.3177,  ...,  0.2078,  0.2256, -0.0415],
          [-0.2585, -0.2851,  0.0864,  ...,  0.0677, -0.1264,  0.1109],
          [-0.3289, -0.2314,  0.2284,  ..., -0.1334, -0.2587,  0.4579]]]])

2025-07-09 13:41:05.085866 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([197516, 32, 19, 19],"float32"), size=tuple(20,20,), mode="bicubic", align_corners=True, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([197516, 32, 19, 19],"float32"), size=tuple(20,20,), mode="bicubic", align_corners=True, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 256955700 / 2528204800 (10.2%)
Greatest absolute difference: 0.03945586085319519 at index (83977, 19, 2, 17) (up to 0.01 allowed)
Greatest relative difference: 13314988.0 at index (186889, 9, 11, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([197516, 32, 20, 20]), dtype=torch.float32)
tensor([[[[-2.2491e-01, -3.4750e-01, -3.9049e-01,  ...,  4.8219e-01,  1.8574e-01, -1.4761e-01],
          [-4.0140e-01,  3.3488e-02,  3.4204e-01,  ...,  2.6806e-01,  3.3301e-01,  3.3342e-01],
          [-5.4879e-02,  4.1118e-01,  3.3272e-01,  ...,  3.3491e-02,  3.4282e-01,  3.5031e-01],
          ...,
          [ 4.8947e-02,  4.0584e-01,  3.9509e-01,  ..., -3.9568e-01, -5.3240e-02, -4.7749e-01],
          [ 2.1234e-01,  3.5911e-01, -4.0049e-01,  ...,  1.3081e-01, -4.5937e-01, -3.3959e-01],
          [ 2.6162e-01, -4.1250e-01,  3.2468e-01,  ...,  1.8357e-01, -4.4034e-01, -1.8972e-01]],

         [[ 1.0487e-01, -3.6047e-01,  9.2181e-02,  ..., -1.3745e-02, -3.3191e-01, -3.3348e-01],
          [ 4.0389e-01, -2.3125e-01, -5.9082e-03,  ..., -5.7125e-02, -1.0466e-01,  1.9620e-02],
          [ 5.0816e-01,  3.1753e-01, -2.0223e-01,  ...,  1.0617e-01, -3.4414e-02,  3.3769e-01],
          ...,
          [ 2.5535e-01,  3.2748e-01,  4.3729e-01,  ..., -1.4010e-01, -4.6499e-01, -4.1958e-01],
          [-1.0584e-01, -1.8048e-01, -1.4462e-01,  ..., -4.5844e-01, -7.6759e-03,  3.6690e-01],
          [-1.2592e-01, -1.6315e-02, -1.7153e-01,  ...,  3.9797e-01, -3.1870e-01,  1.4845e-01]],

         [[-3.2224e-01, -1.5810e-01,  4.9172e-02,  ..., -3.3821e-01, -4.6561e-01,  6.3520e-02],
          [ 2.0242e-01,  9.7433e-02, -4.4030e-02,  ...,  3.8620e-01,  6.9099e-02, -2.8054e-02],
          [ 4.2974e-01, -3.0614e-01,  2.6195e-01,  ..., -1.2950e-01, -3.9976e-01,  9.0851e-03],
          ...,
          [ 3.0367e-01,  1.8483e-01,  3.8544e-01,  ..., -1.1078e-01, -1.8872e-01, -2.3118e-01],
          [-1.9974e-01,  3.5663e-01, -2.0939e-01,  ...,  3.6989e-01, -3.4526e-01, -4.4325e-01],
          [-3.1958e-01, -1.9075e-01,  4.6257e-01,  ..., -4.0609e-01,  2.5286e-01,  1.6679e-01]],

         ...,

         [[ 1.0855e-02,  2.4115e-01, -1.4666e-03,  ...,  4.6796e-01,  4.3681e-01,  6.5732e-02],
          [-2.0326e-01, -3.9051e-01, -3.5207e-01,  ..., -1.7678e-01,  4.7199e-01, -5.4168e-02],
          [ 1.7389e-01,  2.5953e-01, -8.7221e-02,  ..., -4.5942e-01,  1.1851e-01,  2.9272e-01],
          ...,
          [-3.8744e-01,  3.9577e-01,  2.1217e-01,  ..., -2.8018e-01,  4.7660e-01,  2.4909e-01],
          [-1.2571e-01,  4.5441e-01, -4.0820e-01,  ...,  6.3427e-02,  4.2467e-01,  4.7022e-01],
          [-3.6573e-01,  3.2279e-01, -1.8261e-01,  ..., -4.5242e-01,  4.8858e-01,  3.5082e-01]],

         [[-1.1570e-01,  2.8721e-01, -3.3574e-01,  ...,  4.6234e-01,  2.8954e-01,  3.0107e-01],
          [-4.1932e-01,  2.3680e-01, -1.4911e-02,  ...,  2.6576e-01,  3.9341e-01,  3.4466e-01],
          [-2.9265e-01,  2.1009e-01,  6.2231e-02,  ..., -4.2299e-01, -3.5927e-01,  4.1574e-01],
          ...,
          [-3.6526e-01, -3.0299e-02,  1.4549e-01,  ..., -2.5808e-01, -4.4853e-01,  1.1422e-01],
          [-3.8004e-01, -1.4239e-01, -3.0110e-01,  ..., -4.8409e-01, -4.4686e-01, -2.4950e-01],
          [ 7.5165e-02, -3.2993e-01,  2.3262e-01,  ..., -2.1537e-01, -3.4153e-01,  2.7089e-01]],

         [[ 2.3290e-01,  3.0268e-01,  4.7432e-01,  ..., -5.9200e-02, -8.2950e-02, -3.5631e-01],
          [ 2.3825e-01, -2.7421e-02, -3.5472e-02,  ..., -2.0037e-01,  2.5906e-02, -3.3451e-01],
          [ 4.4878e-01, -1.9117e-01, -3.7344e-01,  ..., -6.9776e-02, -1.9067e-01,  1.8992e-01],
          ...,
          [-3.1100e-01, -4.2152e-01,  2.6257e-01,  ..., -9.2688e-02, -4.0469e-01,  5.8968e-02],
          [-4.0708e-03, -5.9685e-02, -1.4278e-01,  ...,  3.9099e-01, -1.1391e-03,  2.7099e-01],
          [ 2.9312e-02, -3.6956e-01,  6.1676e-02,  ...,  7.4265e-02, -4.9653e-01, -1.6942e-01]]],


        [[[ 3.3765e-01,  1.3051e-01,  4.2243e-01,  ..., -9.0718e-02, -1.0438e-01, -4.2185e-01],
          [-4.4660e-01, -3.0636e-01,  1.5814e-02,  ...,  2.1809e-01, -4.8702e-01,  1.8462e-02],
          [-5.0551e-01, -3.9657e-01, -2.4660e-01,  ..., -4.1678e-01, -4.9336e-02,  4.6347e-01],
          ...,
          [ 4.4875e-01,  5.5597e-01, -2.0351e-01,  ..., -1.4851e-02,  3.7999e-01,  8.3952e-02],
          [ 4.6510e-01,  3.3283e-01,  2.9260e-01,  ...,  2.8685e-01, -2.0828e-01, -4.0239e-01],
          [ 4.5949e-02,  4.2771e-02, -2.0760e-01,  ...,  1.1494e-01, -2.9636e-01,  1.3544e-01]],

         [[-2.8067e-01,  4.2503e-01, -6.7660e-02,  ..., -3.7592e-01,  4.3068e-01, -1.7898e-01],
          [ 1.6502e-01, -4.9853e-02, -4.1356e-01,  ...,  7.6578e-02, -2.2899e-01,  4.8620e-01],
          [ 3.6403e-01,  4.1601e-01, -3.9329e-02,  ..., -3.8893e-01, -3.9549e-01,  1.1841e-01],
          ...,
          [ 3.2308e-01, -1.7976e-01, -3.8032e-01,  ..., -3.7294e-01, -3.6356e-01, -3.9018e-01],
          [-1.6319e-01,  1.6251e-01,  4.6808e-01,  ..., -4.6409e-02, -1.2977e-01,  3.5989e-01],
          [ 3.1011e-01, -2.7908e-01, -3.5919e-01,  ...,  1.1126e-02, -3.6733e-01, -4.5299e-01]],

         [[-2.7897e-01,  4.4833e-01, -4.5195e-01,  ..., -3.5505e-01,  1.9058e-03,  5.5419e-02],
          [ 1.5066e-01,  3.8404e-01,  1.6807e-01,  ..., -4.2596e-01,  1.4758e-01, -7.3924e-02],
          [ 3.9041e-01,  4.2655e-01,  4.7440e-01,  ..., -9.4242e-03,  2.7402e-01, -2.9236e-01],
          ...,
          [-4.4697e-01,  1.8844e-01,  2.9629e-01,  ..., -3.9833e-01, -4.6361e-01,  1.9167e-01],
          [-2.1997e-01, -3.1032e-01, -2.8733e-01,  ...,  1.6498e-01, -4.5271e-01, -2.6856e-01],
          [ 3.5866e-02, -4.0233e-01, -5.2927e-01,  ...,  2.1341e-01,  4.3636e-01,  3.2435e-01]],

         ...,

         [[-2.8851e-01, -4.9561e-01,  1.6516e-01,  ..., -8.3614e-02,  1.9872e-01,  3.7462e-01],
          [ 3.3148e-01,  4.4687e-01,  1.9326e-01,  ..., -4.0196e-01,  1.6469e-01, -9.2282e-02],
          [ 2.2413e-01, -3.0748e-01, -4.3035e-01,  ...,  2.7001e-01, -3.6117e-01, -4.0211e-01],
          ...,
          [ 1.3384e-01, -4.7148e-01,  3.7504e-02,  ...,  7.2494e-02,  3.6628e-01, -1.5879e-01],
          [-1.2156e-01,  5.4647e-02,  4.4599e-01,  ..., -2.6870e-01,  7.5933e-02,  3.4941e-01],
          [-1.2424e-01, -1.1053e-01, -3.8853e-01,  ..., -5.1048e-01, -3.9877e-01,  1.0007e-01]],

         [[ 1.4405e-01,  5.9662e-03, -3.8264e-01,  ..., -8.6601e-02,  4.5109e-01, -3.0037e-01],
          [ 3.1342e-01,  4.5676e-02, -3.3987e-01,  ..., -2.7825e-01,  4.9193e-02,  4.7173e-01],
          [-1.4481e-01, -2.7356e-01, -2.8759e-01,  ..., -3.0556e-01,  1.4960e-01,  2.0375e-02],
          ...,
          [-2.8998e-01, -3.3367e-01,  2.3702e-01,  ...,  2.6593e-01,  3.9042e-01, -3.1684e-01],
          [ 5.0499e-01,  4.8398e-01, -2.7001e-01,  ...,  4.2009e-01,  1.8100e-01, -1.9581e-01],
          [ 3.1762e-02, -2.5793e-01,  7.4645e-02,  ..., -3.7107e-01,  5.5102e-02, -4.1963e-01]],

         [[ 3.8045e-01,  4.7932e-02,  3.1285e-01,  ..., -3.3711e-01,  3.6487e-01,  1.9039e-02],
          [ 2.4836e-01, -4.2253e-01, -2.1057e-02,  ..., -4.7638e-01, -3.7750e-01,  3.2986e-01],
          [-2.0724e-02,  2.8223e-01, -3.3227e-01,  ...,  1.7862e-01,  2.0815e-01,  2.5778e-01],
          ...,
          [ 3.9392e-01, -4.4744e-01, -3.5347e-01,  ...,  4.0704e-01, -4.3657e-02,  7.4754e-02],
          [-1.5065e-01, -2.7711e-01,  3.0683e-01,  ...,  6.4686e-02,  2.2684e-02,  2.8157e-01],
          [-3.8417e-01, -5.4839e-02, -1.9088e-01,  ...,  2.9961e-01,  2.4528e-01,  1.5327e-01]]],


        [[[ 4.4820e-01,  2.8655e-01, -4.1773e-01,  ..., -2.8267e-01,  4.2666e-01, -3.7764e-01],
          [-1.6130e-01,  7.3797e-02,  1.5183e-01,  ..., -1.3827e-01,  1.5121e-01,  1.8229e-01],
          [-2.3269e-03, -3.6637e-01,  4.7442e-02,  ..., -6.9613e-02, -3.6272e-02, -5.3456e-02],
          ...,
          [ 2.5259e-01, -1.6054e-01,  6.8006e-02,  ...,  4.3873e-01,  3.4267e-01, -1.0818e-01],
          [ 1.2854e-01, -1.2863e-01, -9.3068e-02,  ..., -6.3483e-02, -4.6308e-01,  2.1258e-01],
          [-3.7906e-01,  3.5244e-01,  1.2764e-01,  ..., -1.4651e-01, -4.5598e-01,  4.3859e-01]],

         [[ 4.7142e-01,  2.8209e-01, -4.0183e-01,  ..., -4.1672e-01, -1.4492e-01,  1.9351e-01],
          [ 2.7186e-01, -1.2734e-01,  2.1566e-01,  ..., -7.7052e-02,  4.4550e-01,  4.4499e-01],
          [-4.7622e-01,  2.3887e-04,  3.1305e-01,  ..., -1.6779e-01, -9.4519e-02,  4.3816e-01],
          ...,
          [-2.5226e-01, -7.3814e-02,  3.6293e-01,  ..., -3.4083e-01, -2.5804e-02, -2.7011e-01],
          [-1.5564e-01,  3.4209e-01, -4.1916e-01,  ..., -1.7801e-01,  4.6643e-01, -4.3178e-01],
          [ 3.1710e-02,  7.2007e-02, -1.3103e-01,  ...,  4.0764e-01,  4.7668e-01, -1.5838e-01]],

         [[ 2.6924e-01, -3.0950e-01,  3.9604e-01,  ...,  4.3247e-01, -1.0896e-01, -1.3273e-01],
          [ 4.4952e-01, -4.5078e-01, -2.7470e-01,  ..., -3.3892e-02,  4.4487e-02, -2.8187e-02],
          [-2.3022e-02, -1.5678e-01, -1.0336e-02,  ..., -3.6969e-01, -1.4665e-01,  1.7553e-01],
          ...,
          [ 3.6907e-01, -1.8631e-01, -2.3415e-01,  ...,  3.8913e-01, -2.3091e-01,  2.6978e-01],
          [-4.9162e-01, -3.0804e-01, -1.9489e-01,  ...,  4.3413e-01, -4.3768e-01, -4.6242e-01],
          [-2.1323e-01,  1.2404e-01,  2.5603e-02,  ..., -2.2154e-01,  1.2915e-01, -2.1161e-01]],

         ...,

         [[-9.0989e-02, -2.5151e-01,  2.4939e-01,  ...,  3.2745e-01, -1.9037e-01,  7.2938e-02],
          [-3.7988e-01,  3.1971e-01, -3.5883e-01,  ..., -4.2706e-01,  3.0939e-01,  3.3085e-01],
          [-4.0896e-01, -3.4070e-01,  8.1652e-02,  ..., -1.0099e-01, -3.7269e-01, -2.6066e-01],
          ...,
          [ 4.0191e-01,  3.2798e-01, -2.1411e-01,  ..., -1.9293e-01, -1.1722e-01, -9.1151e-02],
          [ 6.7229e-02, -2.1020e-01, -6.7937e-03,  ..., -1.6276e-01,  4.8626e-01, -3.5941e-01],
          [ 1.2507e-01, -5.3805e-02, -3.1774e-01,  ...,  3.5263e-02, -1.5135e-01, -2.6480e-01]],

         [[ 2.0086e-01,  3.0786e-01,  3.4385e-01,  ...,  5.1761e-02,  2.3235e-03,  2.2360e-01],
          [-2.9621e-01, -1.1710e-01,  2.1103e-01,  ..., -3.2913e-01, -4.6271e-01, -2.4849e-01],
          [-3.2938e-01, -4.9429e-01,  2.9081e-01,  ..., -4.6132e-01,  4.4027e-01,  4.1861e-01],
          ...,
          [ 1.1571e-01,  4.1100e-01,  1.3216e-01,  ..., -1.4969e-01, -2.0284e-01,  3.6014e-01],
          [-7.0080e-02, -4.6202e-01, -4.6171e-01,  ..., -1.3575e-01, -9.6835e-02,  1.8333e-01],
          [ 2.3707e-01, -1.7775e-01,  2.0758e-01,  ...,  3.6909e-01,  3.3927e-01,  2.1945e-01]],

         [[ 9.3514e-03,  1.0734e-01,  3.4877e-01,  ..., -6.0955e-02, -1.8523e-02,  1.1588e-01],
          [ 4.5523e-01,  1.8265e-01, -2.3792e-01,  ...,  4.4875e-01,  4.0738e-01, -2.6753e-01],
          [-1.6435e-01,  3.9038e-01, -3.1851e-01,  ...,  4.0915e-01, -2.6618e-01, -8.5467e-02],
          ...,
          [-2.0013e-01, -3.4789e-01,  4.1713e-01,  ...,  3.7398e-01,  2.0815e-01, -8.3282e-02],
          [ 1.0136e-01, -3.7255e-01,  2.7206e-02,  ..., -4.2315e-02, -4.9908e-01, -3.5200e-01],
          [ 1.6315e-01, -1.1347e-01,  1.0007e-01,  ..., -3.7101e-01,  7.0609e-02, -4.8408e-01]]],


        ...,


        [[[-4.5168e-01,  3.2560e-02,  6.0349e-02,  ..., -4.0116e-01,  4.2570e-01, -9.3180e-02],
          [-4.9187e-01,  2.3597e-01, -4.1872e-01,  ..., -5.4963e-01, -4.1629e-01, -2.9569e-01],
          [ 6.4214e-02,  1.3553e-01,  2.8980e-01,  ..., -3.5562e-02,  6.6386e-02, -4.6889e-02],
          ...,
          [ 4.9406e-01, -2.5841e-01, -3.2771e-01,  ..., -1.4172e-01, -1.3600e-01, -4.4898e-01],
          [ 2.3647e-01, -2.6232e-01, -1.1249e-01,  ..., -1.2547e-01,  8.0217e-02, -3.6998e-01],
          [ 9.3454e-02,  4.9903e-02, -1.8382e-01,  ...,  1.2077e-01,  4.4981e-01, -2.9995e-01]],

         [[ 3.0639e-01,  1.8699e-01,  3.0294e-01,  ...,  3.6214e-01, -2.5628e-01, -2.5928e-01],
          [-7.5846e-03, -6.3733e-02,  2.9680e-01,  ..., -3.9020e-01,  2.6850e-01,  3.1004e-01],
          [-1.2953e-01,  4.5272e-01, -5.7116e-02,  ...,  2.3376e-01, -4.1519e-01,  4.0952e-01],
          ...,
          [-4.7993e-01,  1.2295e-01, -1.7795e-01,  ...,  3.6342e-01, -1.2992e-01, -4.5996e-02],
          [-4.3753e-01, -3.5148e-01,  2.8075e-01,  ..., -4.6430e-01,  1.3086e-01,  8.9776e-02],
          [-1.8986e-01,  2.3771e-01,  4.8326e-01,  ..., -3.9433e-01, -2.7133e-01,  3.7326e-03]],

         [[-2.9213e-02,  6.4674e-02, -2.3805e-01,  ...,  4.1668e-01,  3.9783e-01, -3.9355e-01],
          [ 1.7693e-02, -2.6758e-01,  3.3583e-02,  ...,  1.1198e-01,  2.4505e-01,  3.9880e-02],
          [-4.6875e-02, -4.6463e-01, -2.0363e-01,  ..., -7.4396e-02,  2.0050e-02,  1.3158e-01],
          ...,
          [ 2.9574e-01, -4.6773e-01,  4.3487e-01,  ...,  4.6905e-01,  2.5984e-01,  5.0581e-01],
          [-2.7221e-01, -2.6732e-01,  5.6046e-02,  ...,  1.4386e-01, -6.5658e-02,  4.5955e-01],
          [ 1.5083e-01, -3.7212e-02, -1.8648e-01,  ...,  3.6466e-01,  2.5270e-01,  4.4292e-01]],

         ...,

         [[-4.5283e-01, -4.0826e-01, -2.3717e-01,  ...,  4.6322e-01, -1.7012e-01, -2.2172e-01],
          [-1.0988e-01, -2.7733e-01, -4.8822e-01,  ...,  3.5868e-01,  1.2941e-03,  3.9420e-02],
          [ 3.0673e-01,  1.3588e-01, -4.1133e-01,  ...,  4.2109e-01, -3.0085e-01, -3.0136e-01],
          ...,
          [-2.0702e-01, -1.3851e-01, -3.7693e-01,  ...,  3.8794e-01,  7.7080e-02,  2.4581e-01],
          [-4.1784e-01,  2.8423e-01,  6.2724e-02,  ...,  3.7350e-01, -3.7744e-01, -3.8693e-01],
          [-3.7226e-01, -2.0770e-01,  3.9334e-02,  ..., -2.0561e-01, -2.7891e-01,  1.9399e-02]],

         [[-4.7113e-03,  3.8111e-01,  1.1010e-01,  ...,  3.3327e-01, -2.3786e-01, -1.7775e-01],
          [ 2.9019e-01,  3.6202e-02, -4.8804e-01,  ...,  2.9734e-01,  3.0091e-01,  6.4345e-02],
          [ 2.4476e-01, -1.0816e-01,  1.6213e-02,  ...,  5.8567e-02,  2.9855e-01, -1.8372e-01],
          ...,
          [ 2.0682e-01,  1.3406e-02, -4.7450e-01,  ...,  1.2035e-01, -6.1960e-03,  3.8356e-01],
          [ 2.9804e-01,  2.3245e-01, -9.5344e-02,  ...,  4.3218e-01, -2.5123e-01,  4.6309e-01],
          [ 6.0476e-02,  2.5313e-01, -2.7990e-01,  ...,  4.1554e-01,  4.7903e-02,  9.1410e-03]],

         [[ 5.0474e-02,  4.2080e-01,  2.5868e-01,  ..., -6.1868e-02, -1.5654e-01, -4.1735e-01],
          [-2.7958e-01,  2.4521e-02, -3.8251e-02,  ..., -2.0640e-01, -6.0369e-03, -5.1291e-02],
          [-5.2973e-02, -1.4322e-01, -2.1109e-01,  ..., -1.6429e-01,  1.1219e-01, -3.9034e-01],
          ...,
          [ 1.3255e-01, -2.5184e-01, -1.1089e-03,  ..., -2.5466e-01, -2.4835e-01,  4.0278e-01],
          [ 4.7696e-01, -3.2813e-02,  2.0537e-01,  ...,  1.3032e-01,  4.2519e-01,  1.6894e-01],
          [ 2.1772e-02,  1.3586e-01, -1.5807e-01,  ..., -8.1206e-02,  5.0701e-01,  1.1818e-01]]],


        [[[ 4.5583e-01,  1.0591e-01, -2.8144e-01,  ...,  3.8623e-01,  3.5219e-01, -1.6771e-01],
          [ 3.5091e-01, -1.0376e-01, -3.5599e-01,  ..., -3.3648e-01, -2.4160e-02, -9.6304e-02],
          [ 2.7517e-01,  4.5650e-01,  1.9758e-02,  ...,  9.6134e-02,  1.9116e-01, -3.4363e-01],
          ...,
          [-3.0680e-01, -5.5112e-02,  2.8709e-02,  ..., -4.5304e-01,  8.3232e-02,  1.1122e-01],
          [-3.6256e-01,  1.2252e-01, -1.4886e-01,  ...,  1.9624e-01, -4.1763e-01, -3.0592e-01],
          [ 8.1749e-02, -2.1252e-01, -2.8442e-02,  ...,  4.2870e-01, -3.6340e-01,  2.4716e-01]],

         [[-1.3215e-01, -1.3383e-01,  1.0312e-01,  ...,  1.7681e-01,  4.8068e-01,  1.1927e-01],
          [ 9.5818e-02,  4.3454e-01,  5.2227e-01,  ...,  3.6870e-01, -3.1615e-01,  2.0840e-01],
          [ 1.9106e-01,  2.1976e-01, -9.6277e-02,  ...,  4.6415e-01, -3.5801e-01,  4.2312e-01],
          ...,
          [-5.1295e-01, -4.5577e-01,  1.7120e-01,  ..., -1.7170e-02, -2.5423e-01,  7.5384e-02],
          [-2.9834e-01, -3.1562e-01,  1.6009e-01,  ..., -1.4440e-01, -2.5786e-01, -4.9674e-01],
          [-3.4273e-01,  3.5860e-01,  3.9998e-01,  ...,  6.5033e-02, -2.2848e-01, -3.8749e-01]],

         [[-4.3924e-02, -2.5391e-01, -4.5643e-01,  ..., -2.5476e-01, -4.2985e-02, -3.6941e-01],
          [-3.3923e-01, -1.2370e-01, -4.1416e-02,  ...,  1.8969e-01,  1.2948e-01,  3.8687e-02],
          [-3.9552e-01,  8.4612e-02, -1.1336e-02,  ...,  4.6579e-01,  5.7297e-02, -2.1845e-01],
          ...,
          [ 2.7958e-01, -6.5848e-02, -1.6049e-02,  ...,  2.3174e-01, -4.1523e-01, -4.1405e-01],
          [-6.9006e-03, -2.8717e-01,  6.8842e-03,  ..., -4.5998e-01, -3.9562e-01, -4.9338e-01],
          [ 4.7653e-01, -2.4703e-01,  1.1396e-01,  ..., -1.2116e-01, -4.0636e-01, -4.8791e-01]],

         ...,

         [[-1.1140e-01,  3.4991e-01, -2.1692e-01,  ..., -2.6582e-01,  3.0260e-01, -1.6502e-01],
          [ 8.8584e-02, -4.5866e-01, -2.8606e-01,  ...,  6.0808e-02,  3.9080e-01,  1.2462e-01],
          [-1.1292e-01, -1.1655e-01,  2.7252e-04,  ..., -4.3490e-01,  1.2590e-01,  1.7836e-02],
          ...,
          [ 7.3900e-02,  6.0184e-02,  4.2120e-01,  ..., -1.7059e-01, -3.2745e-01,  2.4911e-01],
          [-4.1594e-01,  3.3626e-01,  6.9418e-02,  ..., -1.0397e-02,  1.4586e-01,  3.7705e-01],
          [-2.9898e-01,  3.7319e-01,  3.1447e-01,  ..., -4.2287e-01,  1.5290e-01,  8.4918e-02]],

         [[ 3.1641e-01, -1.4195e-01, -4.2960e-01,  ...,  2.7870e-02, -4.5641e-01,  2.4850e-01],
          [-2.3883e-01, -1.3222e-01,  3.4503e-02,  ..., -1.2189e-01, -6.7569e-03, -2.2441e-02],
          [ 6.1059e-02, -1.4424e-01,  3.6862e-01,  ...,  5.8773e-02,  4.0189e-01, -3.5934e-01],
          ...,
          [ 3.4422e-01, -6.2910e-02, -9.3266e-02,  ..., -4.4519e-01, -3.0238e-01,  6.6704e-02],
          [ 4.0304e-01,  1.8404e-01, -2.7144e-01,  ...,  1.9597e-01,  1.7607e-01,  1.2079e-01],
          [-7.4889e-03, -2.8459e-01, -3.4572e-01,  ..., -7.5637e-02, -1.7063e-01, -3.1346e-01]],

         [[ 1.4116e-01,  1.3577e-01, -1.5186e-01,  ..., -7.0480e-02, -2.9271e-01,  4.7846e-01],
          [ 6.1941e-02,  4.8892e-01, -2.8112e-01,  ...,  1.4756e-01, -3.7994e-02, -3.3977e-02],
          [-1.8563e-01, -1.9523e-01, -1.3797e-01,  ..., -2.8328e-01, -1.4661e-01, -1.5843e-02],
          ...,
          [ 3.2937e-01, -5.8577e-02, -1.5982e-01,  ...,  3.4934e-01,  1.4767e-01,  2.3950e-01],
          [-1.8459e-01,  3.7539e-01, -2.8866e-01,  ...,  1.1698e-01, -2.7948e-02,  2.5617e-01],
          [-1.2612e-01, -3.2436e-01,  2.6465e-01,  ...,  4.1058e-01,  2.0203e-02,  3.8105e-01]]],


        [[[ 4.3054e-01,  4.0297e-01,  4.0979e-01,  ..., -3.3388e-01, -1.2237e-01, -3.1204e-02],
          [ 9.1699e-02, -2.4906e-01,  1.9203e-01,  ..., -3.5733e-01,  4.4852e-01, -4.3419e-01],
          [ 1.1024e-01,  2.6190e-01, -3.9120e-01,  ..., -2.3751e-02,  1.9970e-01, -4.5182e-01],
          ...,
          [-2.6250e-01, -2.0051e-01, -2.9335e-01,  ...,  5.6739e-02, -1.5875e-02, -3.8289e-01],
          [ 4.1586e-01, -2.4788e-01, -4.0541e-01,  ...,  1.9733e-01,  4.2950e-01,  2.9399e-01],
          [ 3.8440e-01,  3.6882e-01,  2.9602e-01,  ..., -2.2287e-01,  1.2690e-01,  2.6010e-01]],

         [[-3.7944e-01, -2.7008e-01,  3.5195e-01,  ...,  1.7554e-01, -4.9512e-01,  4.9391e-02],
          [-4.1324e-01, -3.6702e-01,  1.4855e-01,  ...,  3.1035e-01, -2.3069e-01,  3.5421e-01],
          [-4.1811e-01,  3.1387e-01,  3.2386e-01,  ..., -5.5240e-02,  3.1818e-01, -4.1356e-01],
          ...,
          [ 2.9490e-01, -2.5520e-01, -2.4097e-01,  ..., -8.5572e-02, -3.7733e-01,  1.2254e-01],
          [-3.0535e-01, -2.8872e-01, -4.1043e-01,  ..., -3.4247e-01,  9.6697e-02,  1.8184e-01],
          [ 2.8709e-01,  4.1312e-01,  2.3145e-01,  ..., -1.6771e-01, -3.5079e-02, -4.4028e-01]],

         [[-1.2787e-01,  3.6233e-02,  2.1792e-01,  ...,  1.1717e-01,  2.8725e-01, -1.3083e-01],
          [ 3.5541e-01,  2.2529e-01, -4.6835e-01,  ..., -5.0256e-01, -1.0113e-01,  1.0925e-01],
          [ 6.1192e-02, -4.8647e-02, -1.4557e-01,  ..., -1.7441e-01, -1.8546e-01,  1.4074e-02],
          ...,
          [-8.3133e-02,  2.7327e-01, -6.6474e-02,  ..., -5.3116e-02,  4.8258e-01, -1.1072e-01],
          [-4.0447e-01,  4.2673e-01,  3.3430e-02,  ...,  4.1625e-01,  2.2442e-01,  9.8301e-02],
          [-3.3193e-01, -4.4817e-01,  4.1373e-01,  ...,  4.7977e-01, -5.0718e-02,  8.3416e-02]],

         ...,

         [[ 4.9037e-01, -3.9395e-01, -2.5513e-01,  ..., -6.2861e-02, -3.1061e-01, -2.8175e-01],
          [ 4.9178e-01, -2.9115e-01,  7.6040e-02,  ...,  3.8576e-01, -1.8056e-01, -4.7140e-01],
          [ 2.0102e-01,  2.6355e-01, -3.4979e-01,  ..., -3.8584e-01, -3.0959e-01, -4.8536e-01],
          ...,
          [ 4.0272e-01, -4.5539e-01, -3.5140e-01,  ...,  4.4758e-01,  2.9243e-01,  4.8592e-01],
          [ 4.5005e-01, -3.2364e-01, -3.2619e-02,  ...,  7.8264e-02,  2.9589e-01,  4.9178e-01],
          [ 3.5355e-01,  4.6931e-01,  3.8445e-01,  ...,  2.4925e-01, -1.7729e-01,  3.7708e-01]],

         [[ 1.6767e-01, -2.7511e-01, -4.3120e-01,  ..., -2.0413e-01,  4.1865e-02, -4.8794e-02],
          [ 6.5913e-02, -4.8052e-01, -2.6540e-01,  ..., -4.0135e-01,  1.2770e-01, -3.1847e-01],
          [ 3.1036e-01,  7.9941e-03,  2.5557e-01,  ...,  2.6057e-01,  8.3182e-02,  1.8998e-02],
          ...,
          [ 4.6285e-01,  1.9563e-01, -3.6281e-01,  ...,  5.1670e-02, -4.0790e-01,  1.0541e-01],
          [ 3.5734e-01, -4.6558e-02, -4.3975e-01,  ...,  3.0714e-01,  3.6496e-01,  3.0148e-01],
          [ 4.7881e-01,  2.5422e-01,  1.9740e-01,  ..., -2.4364e-01, -6.6248e-02,  4.1049e-01]],

         [[ 1.5530e-01, -3.9674e-01,  4.1708e-01,  ..., -1.5375e-01, -1.4884e-01, -2.2394e-01],
          [-2.5175e-01, -1.3986e-02, -2.4443e-01,  ..., -1.2100e-01, -4.2391e-01, -2.4007e-01],
          [-1.8665e-01,  1.6736e-02, -4.8397e-01,  ..., -3.7133e-01, -2.5593e-02, -3.7227e-01],
          ...,
          [ 1.9239e-02, -1.6608e-01,  2.0708e-03,  ..., -5.9703e-02, -7.1783e-02,  1.0332e-01],
          [ 3.1393e-01,  1.1352e-01, -2.2241e-01,  ...,  3.5893e-01, -2.2297e-01,  3.2434e-01],
          [-3.0886e-01,  3.0765e-01,  4.9565e-01,  ..., -3.6302e-01,  4.2016e-01,  4.9819e-01]]]])
DESIRED: (shape=torch.Size([197516, 32, 20, 20]), dtype=torch.float32)
tensor([[[[-2.1937e-01, -3.5177e-01, -4.0541e-01,  ...,  4.8867e-01,  1.7295e-01, -1.6303e-01],
          [-4.1086e-01,  9.0001e-03,  3.2348e-01,  ...,  2.7543e-01,  3.3122e-01,  3.2404e-01],
          [-6.8760e-02,  4.0219e-01,  3.3566e-01,  ...,  4.5234e-02,  3.5407e-01,  3.5232e-01],
          ...,
          [ 4.2140e-02,  4.0783e-01,  3.9061e-01,  ..., -3.8683e-01, -6.6543e-02, -4.8479e-01],
          [ 2.1341e-01,  3.5227e-01, -3.8496e-01,  ...,  1.2662e-01, -4.7102e-01, -3.3299e-01],
          [ 2.7494e-01, -4.2389e-01,  3.2829e-01,  ...,  1.7881e-01, -4.4371e-01, -1.8255e-01]],

         [[ 1.0756e-01, -3.6057e-01,  8.7975e-02,  ..., -1.9836e-02, -3.4090e-01, -3.4053e-01],
          [ 4.0905e-01, -2.3266e-01, -9.8012e-03,  ..., -6.3903e-02, -1.0918e-01,  1.1097e-02],
          [ 5.1806e-01,  3.2078e-01, -1.9678e-01,  ...,  9.9740e-02, -3.4125e-02,  3.4510e-01],
          ...,
          [ 2.5060e-01,  3.1947e-01,  4.3567e-01,  ..., -1.5424e-01, -4.6314e-01, -4.0608e-01],
          [-1.0968e-01, -1.8331e-01, -1.5497e-01,  ..., -4.4665e-01,  5.2588e-03,  3.8095e-01],
          [-1.2851e-01, -1.3329e-02, -1.6721e-01,  ...,  4.0251e-01, -3.2532e-01,  1.5315e-01]],

         [[-3.3542e-01, -1.6892e-01,  4.6025e-02,  ..., -3.5813e-01, -4.6700e-01,  7.5129e-02],
          [ 1.9134e-01,  1.0135e-01, -4.6836e-02,  ...,  3.7670e-01,  5.9240e-02, -2.8727e-02],
          [ 4.4343e-01, -2.9587e-01,  2.5392e-01,  ..., -1.2603e-01, -3.9011e-01,  1.5701e-02],
          ...,
          [ 3.0095e-01,  1.8948e-01,  3.7291e-01,  ..., -1.0212e-01, -1.9393e-01, -2.3913e-01],
          [-2.1869e-01,  3.4550e-01, -1.9742e-01,  ...,  3.5109e-01, -3.4751e-01, -4.3710e-01],
          [-3.2483e-01, -2.1226e-01,  4.6616e-01,  ..., -4.1210e-01,  2.7142e-01,  1.7682e-01]],

         ...,

         [[ 1.0332e-02,  2.5154e-01,  1.1414e-02,  ...,  4.8509e-01,  4.2922e-01,  6.0888e-02],
          [-2.0109e-01, -3.8422e-01, -3.4924e-01,  ..., -1.4603e-01,  4.7523e-01, -6.5750e-02],
          [ 1.6548e-01,  2.5346e-01, -8.8497e-02,  ..., -4.5346e-01,  1.3532e-01,  2.9454e-01],
          ...,
          [-4.0102e-01,  3.8572e-01,  2.1491e-01,  ..., -2.6832e-01,  4.8470e-01,  2.4915e-01],
          [-1.3721e-01,  4.5317e-01, -4.0304e-01,  ...,  6.8920e-02,  4.3188e-01,  4.7213e-01],
          [-3.8251e-01,  3.1403e-01, -1.7079e-01,  ..., -4.5231e-01,  4.9969e-01,  3.4676e-01]],

         [[-1.1716e-01,  2.8902e-01, -3.3406e-01,  ...,  4.6810e-01,  2.8630e-01,  3.0034e-01],
          [-4.2777e-01,  2.2909e-01, -1.9682e-02,  ...,  2.8227e-01,  4.0255e-01,  3.4183e-01],
          [-3.1019e-01,  2.0337e-01,  6.0581e-02,  ..., -4.2164e-01, -3.3221e-01,  4.2868e-01],
          ...,
          [-3.7344e-01, -4.4027e-02,  1.3548e-01,  ..., -2.7440e-01, -4.4399e-01,  1.1797e-01],
          [-3.7571e-01, -1.5026e-01, -2.9663e-01,  ..., -4.9039e-01, -4.4151e-01, -2.4089e-01],
          [ 9.0973e-02, -3.3322e-01,  2.3569e-01,  ..., -2.2124e-01, -3.3024e-01,  2.9169e-01]],

         [[ 2.3115e-01,  3.0529e-01,  4.8409e-01,  ..., -5.7522e-02, -9.0479e-02, -3.6212e-01],
          [ 2.4017e-01, -1.3891e-02, -1.9967e-02,  ..., -1.9617e-01,  2.2983e-02, -3.4840e-01],
          [ 4.5960e-01, -1.7596e-01, -3.7716e-01,  ..., -7.0443e-02, -1.7965e-01,  1.9373e-01],
          ...,
          [-3.0693e-01, -4.2048e-01,  2.4447e-01,  ..., -9.6636e-02, -3.9668e-01,  7.6501e-02],
          [ 1.8045e-03, -5.7968e-02, -1.4213e-01,  ...,  3.8477e-01, -5.6273e-03,  2.7074e-01],
          [ 3.7357e-02, -3.7340e-01,  5.7717e-02,  ...,  6.0873e-02, -5.0700e-01, -1.7172e-01]]],


        [[[ 3.5603e-01,  1.3905e-01,  4.2879e-01,  ..., -9.6820e-02, -1.0408e-01, -4.3649e-01],
          [-4.3386e-01, -3.0419e-01,  2.0371e-02,  ...,  2.0707e-01, -4.8684e-01,  1.3112e-02],
          [-5.1146e-01, -4.0209e-01, -2.4389e-01,  ..., -4.0566e-01, -4.7600e-02,  4.7352e-01],
          ...,
          [ 4.5780e-01,  5.6916e-01, -1.8229e-01,  ..., -5.4993e-04,  3.6816e-01,  6.8648e-02],
          [ 4.6070e-01,  3.2821e-01,  2.9323e-01,  ...,  2.8639e-01, -2.2820e-01, -4.0267e-01],
          [ 3.8411e-02,  4.0833e-02, -2.1214e-01,  ...,  9.9665e-02, -2.9609e-01,  1.5302e-01]],

         [[-3.0215e-01,  4.2676e-01, -5.0830e-02,  ..., -3.6694e-01,  4.4263e-01, -2.0225e-01],
          [ 1.5788e-01, -3.8505e-02, -4.1287e-01,  ...,  6.7324e-02, -2.0545e-01,  4.9176e-01],
          [ 3.6282e-01,  4.1765e-01, -3.9969e-02,  ..., -3.9174e-01, -3.8998e-01,  1.4042e-01],
          ...,
          [ 3.2808e-01, -1.6599e-01, -3.6576e-01,  ..., -3.7555e-01, -3.6651e-01, -3.7724e-01],
          [-1.6662e-01,  1.4864e-01,  4.6539e-01,  ..., -4.4719e-02, -1.2376e-01,  3.6372e-01],
          [ 3.2988e-01, -2.7460e-01, -3.7675e-01,  ...,  4.3390e-03, -3.7895e-01, -4.6963e-01]],

         [[-3.0019e-01,  4.4763e-01, -4.5325e-01,  ..., -3.5487e-01,  4.5509e-03,  5.9272e-02],
          [ 1.3473e-01,  3.8438e-01,  1.6292e-01,  ..., -4.2469e-01,  1.4675e-01, -7.2054e-02],
          [ 3.8606e-01,  4.3073e-01,  4.7327e-01,  ..., -8.3253e-03,  2.6731e-01, -3.0005e-01],
          ...,
          [-4.5875e-01,  1.6761e-01,  2.9036e-01,  ..., -4.0454e-01, -4.6294e-01,  1.9069e-01],
          [-2.1029e-01, -3.1754e-01, -3.0420e-01,  ...,  1.6801e-01, -4.4110e-01, -2.6124e-01],
          [ 4.9068e-02, -3.9497e-01, -5.4256e-01,  ...,  2.2362e-01,  4.5434e-01,  3.3303e-01]],

         ...,

         [[-2.9648e-01, -5.1831e-01,  1.4954e-01,  ..., -7.6671e-02,  2.0682e-01,  3.8705e-01],
          [ 3.1915e-01,  4.4086e-01,  2.0693e-01,  ..., -3.9798e-01,  1.7546e-01, -8.4169e-02],
          [ 2.3710e-01, -2.7906e-01, -4.2527e-01,  ...,  2.5394e-01, -3.6317e-01, -4.0785e-01],
          ...,
          [ 1.4468e-01, -4.6591e-01,  3.5559e-02,  ...,  7.4207e-02,  3.5842e-01, -1.6125e-01],
          [-1.2842e-01,  4.9728e-02,  4.3376e-01,  ..., -2.7371e-01,  7.3172e-02,  3.5713e-01],
          [-1.2446e-01, -1.0956e-01, -4.0015e-01,  ..., -5.2380e-01, -3.9755e-01,  1.0516e-01]],

         [[ 1.4399e-01,  1.3029e-02, -3.8155e-01,  ..., -7.4838e-02,  4.5110e-01, -3.2836e-01],
          [ 3.2179e-01,  5.9171e-02, -3.3810e-01,  ..., -2.7145e-01,  6.7930e-02,  4.7155e-01],
          [-1.3438e-01, -2.7110e-01, -2.9422e-01,  ..., -3.0148e-01,  1.5413e-01,  3.2625e-02],
          ...,
          [-2.7601e-01, -3.2196e-01,  2.2286e-01,  ...,  2.7406e-01,  3.7621e-01, -3.3414e-01],
          [ 5.0795e-01,  4.9198e-01, -2.6325e-01,  ...,  4.0838e-01,  1.6580e-01, -2.0610e-01],
          [ 2.8750e-02, -2.7033e-01,  7.5116e-02,  ..., -3.8230e-01,  4.9202e-02, -4.3230e-01]],

         [[ 3.8898e-01,  5.8640e-02,  3.1286e-01,  ..., -3.2298e-01,  3.8119e-01,  7.0506e-03],
          [ 2.6707e-01, -4.1623e-01, -2.1696e-02,  ..., -4.8477e-01, -3.5667e-01,  3.3845e-01],
          [-1.9774e-02,  2.7449e-01, -3.2552e-01,  ...,  1.7111e-01,  2.0155e-01,  2.6630e-01],
          ...,
          [ 4.0070e-01, -4.3819e-01, -3.4607e-01,  ...,  3.9825e-01, -4.5366e-02,  8.3757e-02],
          [-1.6104e-01, -2.7674e-01,  3.0133e-01,  ...,  6.6895e-02,  3.2603e-02,  2.8701e-01],
          [-3.9506e-01, -5.5062e-02, -1.9721e-01,  ...,  3.0584e-01,  2.4709e-01,  1.4919e-01]]],


        [[[ 4.6298e-01,  3.0329e-01, -4.2448e-01,  ..., -2.6902e-01,  4.2651e-01, -4.0257e-01],
          [-1.5677e-01,  7.8940e-02,  1.4443e-01,  ..., -1.3842e-01,  1.6307e-01,  1.7575e-01],
          [-4.9457e-03, -3.5767e-01,  4.5584e-02,  ..., -7.3148e-02, -3.7606e-02, -4.4741e-02],
          ...,
          [ 2.5971e-01, -1.5677e-01,  6.0051e-02,  ...,  4.3635e-01,  3.2123e-01, -1.1594e-01],
          [ 1.2156e-01, -1.1516e-01, -9.4249e-02,  ..., -7.8522e-02, -4.6626e-01,  2.3358e-01],
          [-4.0232e-01,  3.5082e-01,  1.4253e-01,  ..., -1.5899e-01, -4.4389e-01,  4.5946e-01]],

         [[ 4.7952e-01,  3.0253e-01, -4.0640e-01,  ..., -4.2094e-01, -1.4538e-01,  1.9545e-01],
          [ 2.9278e-01, -1.1759e-01,  2.0030e-01,  ..., -7.1811e-02,  4.4942e-01,  4.4142e-01],
          [-4.8007e-01, -1.3702e-02,  3.1817e-01,  ..., -1.6833e-01, -7.0997e-02,  4.5715e-01],
          ...,
          [-2.5798e-01, -7.1447e-02,  3.5120e-01,  ..., -3.4318e-01, -2.2061e-02, -2.7866e-01],
          [-1.5969e-01,  3.4397e-01, -4.1461e-01,  ..., -1.4756e-01,  4.6530e-01, -4.4519e-01],
          [ 3.4758e-02,  6.8971e-02, -1.2733e-01,  ...,  4.2351e-01,  4.6624e-01, -1.6507e-01]],

         [[ 2.7636e-01, -3.0603e-01,  3.9624e-01,  ...,  4.3943e-01, -1.1913e-01, -1.3557e-01],
          [ 4.6907e-01, -4.3849e-01, -2.7926e-01,  ..., -2.3493e-02,  4.3610e-02, -3.4155e-02],
          [-1.4515e-02, -1.6295e-01, -1.9848e-02,  ..., -3.6942e-01, -1.3080e-01,  1.8125e-01],
          ...,
          [ 3.6424e-01, -1.7977e-01, -2.4204e-01,  ...,  3.9005e-01, -2.3515e-01,  2.7156e-01],
          [-5.0202e-01, -3.0692e-01, -1.9214e-01,  ...,  4.1546e-01, -4.4179e-01, -4.6857e-01],
          [-2.1490e-01,  1.2697e-01,  3.2782e-02,  ..., -2.3202e-01,  1.3763e-01, -2.1364e-01]],

         ...,

         [[-8.2702e-02, -2.6571e-01,  2.4968e-01,  ...,  3.2910e-01, -2.0187e-01,  7.3156e-02],
          [-3.8640e-01,  3.1422e-01, -3.4334e-01,  ..., -4.0687e-01,  3.1929e-01,  3.3536e-01],
          [-4.1108e-01, -3.3264e-01,  6.7043e-02,  ..., -1.1686e-01, -3.6755e-01, -2.4573e-01],
          ...,
          [ 4.0168e-01,  3.3229e-01, -2.0540e-01,  ..., -1.8922e-01, -1.0110e-01, -9.5864e-02],
          [ 6.9036e-02, -2.1187e-01, -1.4685e-02,  ..., -1.4386e-01,  4.7586e-01, -3.7659e-01],
          [ 1.2940e-01, -4.4313e-02, -3.2168e-01,  ...,  3.3018e-02, -1.6773e-01, -2.6524e-01]],

         [[ 2.0812e-01,  3.1356e-01,  3.4604e-01,  ...,  5.7121e-02,  1.3832e-02,  2.3599e-01],
          [-2.9000e-01, -1.1172e-01,  2.1074e-01,  ..., -3.2317e-01, -4.6438e-01, -2.4488e-01],
          [-3.2871e-01, -5.0023e-01,  2.8197e-01,  ..., -4.5527e-01,  4.3482e-01,  4.0634e-01],
          ...,
          [ 1.0989e-01,  3.9719e-01,  1.2644e-01,  ..., -1.5137e-01, -1.8987e-01,  3.7360e-01],
          [-5.9128e-02, -4.6181e-01, -4.6708e-01,  ..., -1.2370e-01, -8.1017e-02,  1.8714e-01],
          [ 2.5012e-01, -1.7054e-01,  2.1289e-01,  ...,  3.8377e-01,  3.4510e-01,  2.1767e-01]],

         [[-5.7572e-04,  1.0085e-01,  3.6174e-01,  ..., -6.6755e-02, -2.2234e-02,  1.2568e-01],
          [ 4.6101e-01,  1.8945e-01, -2.2114e-01,  ...,  4.4387e-01,  3.9558e-01, -2.7537e-01],
          [-1.6006e-01,  3.9055e-01, -3.0783e-01,  ...,  4.0937e-01, -2.6106e-01, -8.8121e-02],
          ...,
          [-1.9791e-01, -3.6244e-01,  4.0116e-01,  ...,  3.7345e-01,  1.8538e-01, -9.3604e-02],
          [ 1.1481e-01, -3.6516e-01,  1.3153e-02,  ..., -6.2816e-02, -5.0190e-01, -3.5604e-01],
          [ 1.6942e-01, -1.0628e-01,  9.8598e-02,  ..., -3.7016e-01,  7.6118e-02, -4.9695e-01]]],


        ...,


        [[[-4.6026e-01,  1.9225e-02,  6.9655e-02,  ..., -3.8104e-01,  4.4284e-01, -9.8845e-02],
          [-5.1207e-01,  2.2863e-01, -4.1121e-01,  ..., -5.6003e-01, -4.0342e-01, -2.9295e-01],
          [ 5.2662e-02,  1.3916e-01,  2.8491e-01,  ..., -4.0267e-02,  5.7682e-02, -5.1023e-02],
          ...,
          [ 5.1129e-01, -2.4764e-01, -3.3503e-01,  ..., -1.4877e-01, -1.3812e-01, -4.5913e-01],
          [ 2.3975e-01, -2.4985e-01, -1.1875e-01,  ..., -1.1252e-01,  8.4951e-02, -3.7638e-01],
          [ 9.1481e-02,  5.9971e-02, -1.7807e-01,  ...,  1.3078e-01,  4.4691e-01, -3.1243e-01]],

         [[ 3.1447e-01,  1.9212e-01,  3.0212e-01,  ...,  3.6659e-01, -2.7442e-01, -2.7044e-01],
          [ 8.3536e-04, -6.9693e-02,  2.9865e-01,  ..., -3.7710e-01,  2.7723e-01,  2.9883e-01],
          [-1.4111e-01,  4.4014e-01, -3.9239e-02,  ...,  2.0777e-01, -3.9753e-01,  4.2394e-01],
          ...,
          [-4.9355e-01,  1.0513e-01, -1.6711e-01,  ...,  3.4685e-01, -1.3292e-01, -3.9083e-02],
          [-4.3451e-01, -3.5725e-01,  2.8294e-01,  ..., -4.7207e-01,  1.3347e-01,  9.0018e-02],
          [-1.9343e-01,  2.3749e-01,  4.9241e-01,  ..., -3.9995e-01, -2.7240e-01,  7.5015e-03]],

         [[-3.1719e-02,  7.3433e-02, -2.3696e-01,  ...,  4.2527e-01,  3.8536e-01, -4.1658e-01],
          [ 2.2200e-02, -2.5839e-01,  2.2726e-02,  ...,  1.2479e-01,  2.4846e-01,  2.6219e-02],
          [-4.2690e-02, -4.6675e-01, -2.1033e-01,  ..., -6.7584e-02,  2.2706e-02,  1.3158e-01],
          ...,
          [ 2.9769e-01, -4.7171e-01,  4.2197e-01,  ...,  4.6870e-01,  2.5536e-01,  5.1709e-01],
          [-2.7257e-01, -2.6512e-01,  4.5316e-02,  ...,  1.4372e-01, -5.6735e-02,  4.6885e-01],
          [ 1.6196e-01, -2.6785e-02, -1.8662e-01,  ...,  3.7257e-01,  2.6093e-01,  4.4599e-01]],

         ...,

         [[-4.6051e-01, -4.1439e-01, -2.3582e-01,  ...,  4.5848e-01, -1.8265e-01, -2.2774e-01],
          [-1.1901e-01, -2.7959e-01, -4.8551e-01,  ...,  3.5308e-01, -2.2857e-03,  3.9543e-02],
          [ 3.0457e-01,  1.4500e-01, -4.0657e-01,  ...,  4.1815e-01, -3.0651e-01, -2.9650e-01],
          ...,
          [-2.1289e-01, -1.2481e-01, -3.7095e-01,  ...,  3.9476e-01,  6.5469e-02,  2.3983e-01],
          [-4.3286e-01,  2.7065e-01,  7.2236e-02,  ...,  3.5271e-01, -3.9243e-01, -3.8837e-01],
          [-3.7460e-01, -2.2271e-01,  3.8869e-02,  ..., -2.1896e-01, -2.7256e-01,  3.2119e-02]],

         [[-1.7334e-02,  3.8400e-01,  1.2895e-01,  ...,  3.2795e-01, -2.5436e-01, -1.8126e-01],
          [ 2.9027e-01,  5.5945e-02, -4.8164e-01,  ...,  3.0359e-01,  2.8660e-01,  5.8532e-02],
          [ 2.5548e-01, -1.0418e-01, -4.6821e-03,  ...,  6.9980e-02,  2.9650e-01, -1.9253e-01],
          ...,
          [ 2.1977e-01,  2.4618e-02, -4.6858e-01,  ...,  1.2830e-01, -4.7135e-03,  4.0164e-01],
          [ 2.9675e-01,  2.4128e-01, -8.9663e-02,  ...,  4.2351e-01, -2.4504e-01,  4.6895e-01],
          [ 5.2870e-02,  2.5760e-01, -2.7114e-01,  ...,  4.1658e-01,  4.7897e-02, -3.8665e-04]],

         [[ 4.9650e-02,  4.2373e-01,  2.7084e-01,  ..., -5.9758e-02, -1.6580e-01, -4.2892e-01],
          [-2.8258e-01,  2.8989e-02, -2.9447e-02,  ..., -2.0187e-01, -8.8566e-03, -5.4711e-02],
          [-6.0173e-02, -1.4229e-01, -2.1104e-01,  ..., -1.6406e-01,  1.0348e-01, -3.9840e-01],
          ...,
          [ 1.4748e-01, -2.5055e-01, -4.6532e-03,  ..., -2.5436e-01, -2.2467e-01,  4.1711e-01],
          [ 4.8233e-01, -2.0634e-02,  1.9752e-01,  ...,  1.3929e-01,  4.3544e-01,  1.6048e-01],
          [ 1.1413e-02,  1.4044e-01, -1.6488e-01,  ..., -6.9963e-02,  5.1018e-01,  1.1025e-01]]],


        [[[ 4.6458e-01,  1.2163e-01, -2.7300e-01,  ...,  4.0302e-01,  3.4870e-01, -1.7870e-01],
          [ 3.6289e-01, -9.5717e-02, -3.6442e-01,  ..., -3.2285e-01, -1.7332e-02, -9.5782e-02],
          [ 2.7579e-01,  4.5273e-01,  2.4306e-02,  ...,  9.0144e-02,  1.7511e-01, -3.5339e-01],
          ...,
          [-3.1530e-01, -5.3972e-02,  2.5034e-02,  ..., -4.4797e-01,  7.8400e-02,  1.0822e-01],
          [-3.6363e-01,  1.1396e-01, -1.3997e-01,  ...,  1.9646e-01, -4.3053e-01, -2.9898e-01],
          [ 9.5558e-02, -2.1526e-01, -2.8546e-02,  ...,  4.1960e-01, -3.6196e-01,  2.6813e-01]],

         [[-1.3646e-01, -1.4768e-01,  8.8905e-02,  ...,  1.7832e-01,  4.9268e-01,  1.1067e-01],
          [ 8.4314e-02,  4.1978e-01,  5.3104e-01,  ...,  3.5497e-01, -3.0011e-01,  2.1275e-01],
          [ 1.9429e-01,  2.2804e-01, -7.9806e-02,  ...,  4.5887e-01, -3.5866e-01,  4.3494e-01],
          ...,
          [-5.1593e-01, -4.6804e-01,  1.6476e-01,  ..., -3.3412e-02, -2.5690e-01,  6.7647e-02],
          [-2.9691e-01, -3.0761e-01,  1.5627e-01,  ..., -1.4493e-01, -2.6416e-01, -5.0760e-01],
          [-3.5670e-01,  3.5769e-01,  4.1151e-01,  ...,  6.2490e-02, -2.3525e-01, -3.8901e-01]],

         [[-3.4229e-02, -2.4943e-01, -4.6111e-01,  ..., -2.5762e-01, -4.9294e-02, -3.8282e-01],
          [-3.3706e-01, -1.3431e-01, -5.3385e-02,  ...,  1.7943e-01,  1.2505e-01,  3.2277e-02],
          [-4.0822e-01,  7.4109e-02, -3.7504e-03,  ...,  4.6471e-01,  4.9589e-02, -2.2380e-01],
          ...,
          [ 2.8140e-01, -6.0993e-02, -1.3835e-02,  ...,  2.1715e-01, -4.2735e-01, -4.1629e-01],
          [ 3.5758e-03, -2.8782e-01,  4.8652e-03,  ..., -4.6862e-01, -3.9751e-01, -4.9624e-01],
          [ 4.9865e-01, -2.3764e-01,  1.0720e-01,  ..., -1.2137e-01, -4.1229e-01, -4.8958e-01]],

         ...,

         [[-1.2352e-01,  3.6318e-01, -2.0984e-01,  ..., -2.5760e-01,  3.0047e-01, -1.7869e-01],
          [ 9.7247e-02, -4.4097e-01, -2.9643e-01,  ...,  7.1202e-02,  3.9301e-01,  1.1544e-01],
          [-1.1061e-01, -1.3150e-01, -6.2434e-03,  ..., -4.2424e-01,  1.4023e-01,  1.4782e-02],
          ...,
          [ 6.4888e-02,  6.2591e-02,  4.1545e-01,  ..., -1.6916e-01, -3.1077e-01,  2.6730e-01],
          [-4.3452e-01,  3.3021e-01,  7.4514e-02,  ..., -1.6901e-02,  1.5882e-01,  3.7792e-01],
          [-3.0954e-01,  3.6268e-01,  3.2779e-01,  ..., -4.1959e-01,  1.6016e-01,  7.8638e-02]],

         [[ 3.3540e-01, -1.2911e-01, -4.3478e-01,  ...,  1.8117e-02, -4.5847e-01,  2.6673e-01],
          [-2.3483e-01, -1.3639e-01,  1.9498e-02,  ..., -1.2288e-01, -1.9445e-02, -1.2971e-02],
          [ 5.3527e-02, -1.4847e-01,  3.6484e-01,  ...,  6.8711e-02,  3.9001e-01, -3.7438e-01],
          ...,
          [ 3.5655e-01, -4.7662e-02, -9.8471e-02,  ..., -4.4428e-01, -2.8367e-01,  7.5526e-02],
          [ 4.0066e-01,  1.8920e-01, -2.6835e-01,  ...,  2.0292e-01,  1.7513e-01,  1.1213e-01],
          [-9.8636e-03, -2.8719e-01, -3.4817e-01,  ..., -8.6570e-02, -1.8128e-01, -3.2426e-01]],

         [[ 1.4323e-01,  1.3358e-01, -1.4777e-01,  ..., -8.5817e-02, -2.8608e-01,  5.0234e-01],
          [ 5.9804e-02,  4.9492e-01, -2.6530e-01,  ...,  1.4754e-01, -4.3389e-02, -2.4647e-02],
          [-1.7884e-01, -1.8012e-01, -1.5142e-01,  ..., -2.8567e-01, -1.3944e-01, -1.8786e-02],
          ...,
          [ 3.2764e-01, -3.7146e-02, -1.6508e-01,  ...,  3.5090e-01,  1.4267e-01,  2.4259e-01],
          [-2.0055e-01,  3.6704e-01, -2.6626e-01,  ...,  1.1502e-01, -2.6341e-02,  2.6415e-01],
          [-1.2187e-01, -3.4139e-01,  2.6565e-01,  ...,  4.1119e-01,  2.2618e-02,  3.8985e-01]]],


        [[[ 4.3724e-01,  4.1523e-01,  4.1484e-01,  ..., -3.3264e-01, -1.2808e-01, -2.1578e-02],
          [ 1.0409e-01, -2.4312e-01,  1.9596e-01,  ..., -3.5135e-01,  4.3603e-01, -4.4231e-01],
          [ 1.0870e-01,  2.6214e-01, -3.7334e-01,  ..., -3.0320e-02,  2.0312e-01, -4.6950e-01],
          ...,
          [-2.5677e-01, -2.0354e-01, -3.0026e-01,  ...,  6.2008e-02, -1.9241e-02, -3.7716e-01],
          [ 4.3666e-01, -2.2210e-01, -3.9243e-01,  ...,  1.9614e-01,  4.3054e-01,  3.0067e-01],
          [ 3.8439e-01,  3.8160e-01,  3.1340e-01,  ..., -2.2947e-01,  1.2869e-01,  2.6272e-01]],

         [[-3.8131e-01, -2.7924e-01,  3.4852e-01,  ...,  1.6136e-01, -4.9938e-01,  5.3909e-02],
          [-4.1433e-01, -3.8229e-01,  1.4359e-01,  ...,  3.0785e-01, -2.3929e-01,  3.6990e-01],
          [-4.3973e-01,  2.9035e-01,  3.2349e-01,  ..., -3.7885e-02,  2.9957e-01, -4.1334e-01],
          ...,
          [ 2.9429e-01, -2.4917e-01, -2.5108e-01,  ..., -1.0270e-01, -3.6603e-01,  1.3897e-01],
          [-3.0281e-01, -2.7537e-01, -4.0526e-01,  ..., -3.3691e-01,  1.0799e-01,  1.7289e-01],
          [ 2.9544e-01,  4.2654e-01,  2.5004e-01,  ..., -1.6658e-01, -4.3779e-02, -4.5941e-01]],

         [[-1.3992e-01,  2.7211e-02,  2.3037e-01,  ...,  1.3681e-01,  2.8908e-01, -1.4305e-01],
          [ 3.5304e-01,  2.3715e-01, -4.5505e-01,  ..., -4.9836e-01, -8.3946e-02,  1.1032e-01],
          [ 6.8944e-02, -3.6921e-02, -1.5577e-01,  ..., -1.8772e-01, -1.8068e-01,  2.4209e-02],
          ...,
          [-9.5646e-02,  2.8042e-01, -6.1446e-02,  ..., -3.2000e-02,  4.7863e-01, -1.1860e-01],
          [-4.2257e-01,  4.0202e-01,  4.7663e-02,  ...,  4.2893e-01,  2.1156e-01,  9.8609e-02],
          [-3.2891e-01, -4.7356e-01,  4.1135e-01,  ...,  4.7660e-01, -6.0394e-02,  8.5518e-02]],

         ...,

         [[ 5.0688e-01, -3.8199e-01, -2.7335e-01,  ..., -7.3022e-02, -3.1551e-01, -2.7777e-01],
          [ 5.1041e-01, -2.9075e-01,  6.7894e-02,  ...,  3.8491e-01, -1.9451e-01, -4.7407e-01],
          [ 2.0625e-01,  2.6295e-01, -3.3567e-01,  ..., -3.7286e-01, -3.1461e-01, -4.9751e-01],
          ...,
          [ 4.2184e-01, -4.4536e-01, -3.5430e-01,  ...,  4.4146e-01,  3.0308e-01,  4.9268e-01],
          [ 4.6291e-01, -2.9683e-01, -2.7212e-02,  ...,  8.0273e-02,  2.9450e-01,  4.9385e-01],
          [ 3.4945e-01,  4.8363e-01,  4.0263e-01,  ...,  2.4443e-01, -1.8139e-01,  3.8509e-01]],

         [[ 1.7769e-01, -2.6130e-01, -4.3633e-01,  ..., -1.9792e-01,  4.1960e-02, -4.5411e-02],
          [ 7.4568e-02, -4.7657e-01, -2.8547e-01,  ..., -3.9853e-01,  1.2585e-01, -3.2569e-01],
          [ 3.1280e-01, -9.4153e-04,  2.4121e-01,  ...,  2.5164e-01,  8.4696e-02,  1.4078e-02],
          ...,
          [ 4.7024e-01,  2.0742e-01, -3.6369e-01,  ...,  4.9335e-02, -3.9186e-01,  1.1694e-01],
          [ 3.6626e-01, -3.1440e-02, -4.3005e-01,  ...,  3.0004e-01,  3.6704e-01,  3.0514e-01],
          [ 4.8518e-01,  2.6504e-01,  2.1461e-01,  ..., -2.5855e-01, -6.2592e-02,  4.2177e-01]],

         [[ 1.7276e-01, -4.0435e-01,  4.1792e-01,  ..., -1.5139e-01, -1.4515e-01, -2.2503e-01],
          [-2.4925e-01, -2.3364e-02, -2.2868e-01,  ..., -1.2718e-01, -4.2533e-01, -2.3518e-01],
          [-1.9565e-01,  2.0274e-02, -4.7990e-01,  ..., -3.7371e-01, -3.6621e-02, -3.8156e-01],
          ...,
          [ 3.2752e-02, -1.6354e-01, -1.0451e-02,  ..., -5.6579e-02, -7.1498e-02,  1.0829e-01],
          [ 3.1019e-01,  1.2888e-01, -2.0821e-01,  ...,  3.4457e-01, -2.0982e-01,  3.4029e-01],
          [-3.3203e-01,  2.9750e-01,  5.1596e-01,  ..., -3.6668e-01,  4.4422e-01,  5.0339e-01]]]])

2025-07-09 13:41:10.131514 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 10, 10, 2147484],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=False, align_mode=1, data_format="NDHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 10, 10, 2147484],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=False, align_mode=1, data_format="NDHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 96047900 / 103079232 (93.2%)
Greatest absolute difference: 1.0 at index (1, 0, 1, 0, 1915844) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 0, 1, 5963) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 4, 2, 3, 2147484]), dtype=torch.float16)
tensor([[[[[-0.2435, -0.4436, -0.4343,  ..., -0.4324,  0.3066, -0.0177],
           [-0.0588,  0.1705, -0.2107,  ...,  0.0620,  0.2664,  0.0160],
           [ 0.3005, -0.0438,  0.2405,  ...,  0.3906, -0.3076, -0.1338]],

          [[ 0.4260,  0.4673,  0.4153,  ...,  0.0770,  0.4897,  0.0153],
           [-0.0093,  0.3298, -0.0795,  ...,  0.1761,  0.2668,  0.1930],
           [ 0.1204, -0.1512, -0.4680,  ..., -0.1923,  0.1613, -0.0796]]],


         [[[-0.0708, -0.2340, -0.0479,  ...,  0.0693, -0.0022, -0.0083],
           [ 0.1066, -0.1238,  0.0247,  ..., -0.4026,  0.1353,  0.0037],
           [-0.2661,  0.0287, -0.0597,  ..., -0.1349, -0.3447,  0.1919]],

          [[-0.1670,  0.3325, -0.1307,  ..., -0.1007, -0.2404,  0.0410],
           [ 0.0239,  0.2996,  0.1147,  ..., -0.0274, -0.1459,  0.0830],
           [ 0.0743,  0.2815, -0.1111,  ...,  0.0920,  0.0827, -0.3076]]],


         [[[ 0.1456, -0.2800, -0.4922,  ...,  0.2820,  0.0205,  0.0022],
           [ 0.1755, -0.0127, -0.2412,  ...,  0.0047, -0.1204,  0.2202],
           [-0.2957, -0.2006, -0.3235,  ..., -0.2161, -0.0801,  0.3586]],

          [[ 0.1388,  0.2003, -0.3503,  ...,  0.3296,  0.0923, -0.2117],
           [ 0.1622, -0.1050,  0.2117,  ...,  0.2291,  0.3728, -0.0968],
           [ 0.1343,  0.1071, -0.1722,  ...,  0.2698,  0.4509, -0.1757]]],


         [[[-0.1035, -0.0226, -0.1752,  ...,  0.2603,  0.1959, -0.0995],
           [-0.2539,  0.2949,  0.2058,  ..., -0.1914, -0.0161,  0.1704],
           [ 0.0547,  0.1287,  0.0731,  ..., -0.0434, -0.1927, -0.1038]],

          [[ 0.2346,  0.0963,  0.0417,  ..., -0.0022, -0.1163,  0.1733],
           [ 0.0507,  0.1527,  0.2424,  ...,  0.1993, -0.0078, -0.2449],
           [ 0.0063, -0.2959,  0.0150,  ...,  0.1418,  0.1475, -0.2101]]]],



        [[[[-0.3274,  0.2058,  0.4890,  ..., -0.1742,  0.2974,  0.3596],
           [-0.0451,  0.1401, -0.2124,  ..., -0.2556,  0.3708, -0.0858],
           [-0.1031, -0.1838, -0.3088,  ..., -0.0451,  0.0437, -0.2500]],

          [[-0.4395,  0.1785,  0.2144,  ...,  0.4590,  0.3882,  0.3677],
           [ 0.0803,  0.3167,  0.2473,  ..., -0.2201, -0.0569,  0.1672],
           [ 0.1473, -0.2693,  0.0226,  ..., -0.2512,  0.2715,  0.1101]]],


         [[[-0.3223,  0.0718, -0.1334,  ...,  0.2151, -0.0289,  0.0965],
           [ 0.0192, -0.2529,  0.0515,  ..., -0.0221,  0.0818,  0.2430],
           [-0.0293, -0.0079, -0.1748,  ..., -0.1144, -0.1746, -0.1381]],

          [[ 0.2495,  0.1011, -0.2830,  ..., -0.1506,  0.0997, -0.1655],
           [ 0.1331,  0.2036, -0.1960,  ..., -0.1471,  0.0805, -0.0463],
           [ 0.1713, -0.0606, -0.1331,  ...,  0.0585, -0.1732, -0.0166]]],


         [[[ 0.4119, -0.4531, -0.0455,  ..., -0.0514,  0.4736, -0.4282],
           [-0.4050, -0.1637,  0.4133,  ...,  0.1976, -0.1914,  0.1995],
           [ 0.2546,  0.0481,  0.1059,  ...,  0.1390, -0.1464, -0.1824]],

          [[-0.3291,  0.2749, -0.3472,  ..., -0.3118, -0.0188, -0.1151],
           [ 0.2781, -0.2374,  0.0531,  ..., -0.3147, -0.2590, -0.0845],
           [ 0.0349, -0.0986, -0.4363,  ...,  0.2852,  0.0646,  0.4304]]],


         [[[-0.0847, -0.2400, -0.0604,  ..., -0.1255, -0.2021, -0.0364],
           [ 0.0983,  0.0461,  0.0379,  ...,  0.0136, -0.1194,  0.0230],
           [ 0.0748, -0.0238, -0.0358,  ..., -0.0593, -0.0531, -0.1980]],

          [[-0.0737, -0.2932,  0.1138,  ..., -0.0561, -0.1382,  0.4524],
           [-0.0577, -0.0771,  0.1345,  ..., -0.0165, -0.1073,  0.0822],
           [ 0.0652,  0.1184, -0.0609,  ..., -0.0140, -0.0500, -0.2800]]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 4, 2, 3, 2147484]), dtype=torch.float16)
tensor([[[[[-2.4353e-01, -4.4360e-01, -4.3433e-01,  ..., -4.3237e-01,  3.0664e-01, -1.7700e-02],
           [ 9.5215e-02,  3.2178e-01, -3.1860e-02,  ..., -1.0229e-01,  1.0394e-01, -1.9043e-01],
           [ 4.1602e-01,  1.8408e-01,  1.4258e-01,  ..., -2.0065e-02,  3.5229e-01,  6.9153e-02]],

          [[ 8.4595e-02,  3.2568e-01,  4.3488e-02,  ...,  2.4792e-01,  3.7378e-01, -3.4595e-01],
           [-3.3838e-01,  2.0728e-01, -1.0730e-01,  ..., -5.1758e-02, -8.0490e-03, -2.4695e-01],
           [-1.3977e-01,  4.0234e-01, -4.4580e-01,  ..., -2.4261e-03, -5.9174e-02,  1.6113e-01]]],


         [[[ 4.5471e-02, -1.4160e-01, -2.9004e-01,  ...,  8.9844e-02,  2.9556e-02, -3.6353e-01],
           [ 1.3855e-02,  5.4688e-02,  1.7590e-01,  ..., -3.7524e-01,  2.2681e-01, -2.3645e-01],
           [-4.3677e-01, -1.4099e-01, -2.1521e-01,  ...,  1.9360e-01,  2.7222e-01,  4.6240e-01]],

          [[ 3.3627e-03,  1.8445e-01,  1.2842e-01,  ..., -4.0747e-01, -4.5654e-01,  7.4951e-02],
           [ 6.2439e-02, -2.3071e-01,  2.6514e-01,  ...,  2.5171e-01,  1.9678e-01,  3.9502e-01],
           [-4.6387e-01,  1.6602e-01,  2.0447e-01,  ..., -3.0688e-01,  3.4766e-01, -1.5771e-01]]],


         [[[ 3.4570e-01,  3.2080e-01,  5.5725e-02,  ..., -1.2695e-01, -3.3154e-01,  2.5024e-02],
           [ 4.2212e-01,  5.2551e-02, -2.5098e-01,  ..., -3.0884e-01, -4.0283e-03, -2.1838e-01],
           [-4.7974e-01, -4.7925e-01,  4.3481e-01,  ..., -2.8223e-01, -1.7810e-01,  1.6992e-01]],

          [[-1.5063e-01, -2.2522e-01, -2.9248e-01,  ...,  1.1066e-01,  2.9517e-01,  2.3267e-01],
           [-3.2959e-01,  2.0020e-02, -8.2520e-02,  ..., -1.5369e-01, -2.7435e-02,  2.7539e-01],
           [-3.7183e-01, -7.3181e-02, -3.9966e-01,  ...,  3.4351e-01, -4.4775e-01,  1.3940e-01]]],


         [[[-3.9258e-01,  1.8347e-01,  2.4927e-01,  ..., -1.6577e-01, -4.1309e-01, -4.4165e-01],
           [-3.6816e-01,  1.9458e-01,  8.7646e-02,  ..., -1.3208e-01, -3.2910e-01, -2.0874e-02],
           [-3.6963e-01, -3.4424e-01,  1.5747e-02,  ..., -1.6556e-02, -1.6541e-01,  1.7407e-01]],

          [[ 3.8403e-01, -9.5398e-02,  1.0419e-01,  ...,  3.1128e-01, -4.5679e-01,  4.8950e-01],
           [ 1.9238e-01, -2.6343e-01,  1.4246e-01,  ...,  4.4946e-01, -1.9116e-01, -1.5869e-01],
           [-4.8389e-01,  1.0522e-01,  2.9785e-01,  ..., -7.1907e-03,  1.5405e-01,  2.7686e-01]]]],



        [[[[-3.2739e-01,  2.0581e-01,  4.8901e-01,  ..., -1.7419e-01,  2.9736e-01,  3.5962e-01],
           [ 2.4536e-02,  3.6407e-02, -3.1433e-02,  ...,  1.7041e-01,  3.0029e-02,  1.4636e-01],
           [-4.6191e-01,  3.0054e-01, -3.3618e-01,  ...,  4.6173e-02, -2.7246e-01, -3.5034e-01]],

          [[-1.0431e-01, -2.0959e-01,  9.9243e-02,  ..., -3.3838e-01,  3.9453e-01,  2.9443e-01],
           [-3.5938e-01, -3.6157e-01, -6.3965e-02,  ...,  2.9468e-01, -3.2251e-01,  1.1774e-01],
           [ 1.4685e-01,  2.1924e-01,  2.1997e-01,  ..., -7.1777e-02, -1.8387e-02,  1.7014e-02]]],


         [[[-4.3213e-01,  2.6343e-01, -4.6606e-01,  ...,  1.6626e-01,  1.0498e-01, -2.3865e-01],
           [-1.8555e-01, -2.8809e-02, -5.2002e-02,  ..., -3.4424e-01, -3.0127e-01,  2.5146e-01],
           [ 4.1602e-01,  1.8372e-01,  5.9448e-02,  ..., -5.1086e-02,  2.8979e-01,  8.1604e-02]],

          [[ 2.8638e-01,  5.9265e-02,  1.0632e-01,  ..., -2.7515e-01,  2.8442e-01,  1.1237e-01],
           [ 1.9775e-02, -3.3936e-01, -8.3008e-03,  ..., -1.3086e-01, -6.4697e-03, -4.2480e-02],
           [-1.8774e-01,  4.3549e-02,  3.1348e-01,  ...,  3.2910e-01, -3.4668e-01,  1.4258e-01]]],


         [[[-7.0251e-02, -5.3406e-02, -4.2090e-01,  ..., -6.5063e-02, -4.6167e-01, -4.9609e-01],
           [-4.0186e-01, -2.2620e-01,  1.0278e-01,  ..., -2.8076e-02, -3.8696e-01, -2.1265e-01],
           [ 1.2177e-01, -1.4624e-01,  2.6514e-01,  ..., -4.4653e-01,  5.3711e-02,  2.7686e-01]],

          [[ 3.6865e-01, -3.1714e-01, -4.1455e-01,  ..., -4.7485e-01, -1.1743e-01,  3.1470e-01],
           [-1.8726e-01,  3.0371e-01,  2.7637e-01,  ..., -4.4775e-01,  2.4634e-01,  0.0000e+00],
           [-8.1116e-02,  2.4121e-01,  4.5166e-01,  ..., -2.3462e-01,  2.4815e-03,  3.8989e-01]]],


         [[[ 2.0862e-01, -4.0430e-01, -2.9053e-01,  ..., -1.5234e-01, -2.5368e-04, -1.4160e-01],
           [-3.2642e-01,  5.2002e-02,  2.7002e-01,  ...,  1.8262e-01, -4.0796e-01,  3.7988e-01],
           [ 4.4287e-01,  3.5718e-01, -8.7585e-02,  ..., -1.1255e-01,  4.6875e-01, -2.4060e-01]],

          [[-4.6558e-01, -1.3496e-02,  4.9487e-01,  ..., -3.1812e-01, -7.5562e-02, -3.2886e-01],
           [ 3.0664e-01,  3.9673e-01,  2.0911e-01,  ...,  1.5027e-01,  4.8096e-02, -2.0935e-01],
           [ 2.0142e-02,  2.3792e-01, -1.3049e-01,  ...,  7.1899e-02,  6.6345e-02,  7.6050e-02]]]]],
       dtype=torch.float16)

2025-07-09 13:41:12.900088 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 10, 10, 2147484],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=True, align_mode=0, data_format="NDHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 10, 10, 2147484],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=True, align_mode=0, data_format="NDHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 100357570 / 103079232 (97.4%)
Greatest absolute difference: 0.9892578125 at index (1, 1, 1, 2, 912925) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 0, 0, 118679) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 4, 2, 3, 2147484]), dtype=torch.float16)
tensor([[[[[-0.1057, -0.4094, -0.2228,  ...,  0.2264,  0.0063, -0.3774],
           [-0.1591, -0.0573, -0.1296,  ..., -0.1697, -0.0962, -0.0804],
           [ 0.4214, -0.3306, -0.4009,  ...,  0.0974, -0.2429, -0.3887]],

          [[-0.0853,  0.3792, -0.1743,  ..., -0.4363, -0.2389,  0.2318],
           [-0.2209, -0.0582,  0.4021,  ..., -0.1495, -0.1982, -0.0819],
           [ 0.4692, -0.4546, -0.0307,  ...,  0.2480, -0.4871, -0.2432]]],


         [[[-0.0925,  0.4685, -0.2473,  ..., -0.1935, -0.3320, -0.1431],
           [ 0.1562, -0.3052,  0.0569,  ...,  0.2284,  0.0541, -0.0324],
           [-0.2461, -0.3452,  0.1281,  ..., -0.2542, -0.3528,  0.3796]],

          [[-0.2430, -0.2239,  0.2815,  ...,  0.2230,  0.0504, -0.2452],
           [-0.2175, -0.3445,  0.1703,  ..., -0.4102,  0.2490,  0.0289],
           [-0.3523, -0.1895,  0.3972,  ..., -0.3660,  0.0154,  0.2045]]],


         [[[-0.4316, -0.3696,  0.0819,  ...,  0.0190,  0.0211, -0.0400],
           [-0.2196, -0.0464, -0.1133,  ..., -0.1758, -0.0270,  0.0886],
           [-0.2710,  0.3147,  0.0798,  ..., -0.2292,  0.3489,  0.1620]],

          [[-0.4343,  0.1353, -0.1002,  ...,  0.4402,  0.2457, -0.0836],
           [-0.0642, -0.0226, -0.1191,  ...,  0.0700, -0.0478, -0.2174],
           [-0.0386,  0.2827,  0.3650,  ...,  0.1534,  0.3054,  0.0902]]],


         [[[-0.4968,  0.0082, -0.1666,  ...,  0.2908,  0.4270, -0.2013],
           [-0.3530, -0.1004, -0.1204,  ..., -0.0542,  0.0636,  0.3618],
           [-0.2864, -0.1932, -0.4346,  ..., -0.2817, -0.3877, -0.0338]],

          [[ 0.0302,  0.1685,  0.2922,  ..., -0.2937, -0.3552,  0.0321],
           [-0.0156, -0.1724,  0.3203,  ...,  0.4211,  0.1942,  0.0638],
           [-0.1365, -0.0865, -0.2369,  ..., -0.3777,  0.2148,  0.4543]]]],



        [[[[ 0.4207,  0.0753,  0.1772,  ..., -0.1829, -0.3164, -0.2622],
           [-0.2976,  0.0734,  0.3384,  ...,  0.0223, -0.2786, -0.0143],
           [-0.0330,  0.0496, -0.4211,  ..., -0.0629,  0.4346, -0.4739]],

          [[-0.1262,  0.3779, -0.2742,  ...,  0.0082, -0.1550, -0.4500],
           [ 0.2112, -0.0544, -0.4253,  ..., -0.3594, -0.2218,  0.2791],
           [-0.1176,  0.0220, -0.2922,  ...,  0.3928, -0.1512,  0.3059]]],


         [[[ 0.1292,  0.0521, -0.2937,  ..., -0.3003,  0.3145,  0.2878],
           [ 0.0350,  0.0961, -0.3389,  ..., -0.0163,  0.2365, -0.4692],
           [ 0.0380, -0.4976, -0.0142,  ..., -0.2209,  0.4531,  0.2045]],

          [[ 0.1141, -0.0903, -0.0207,  ..., -0.4333, -0.4297,  0.2162],
           [-0.3696, -0.2964, -0.1213,  ..., -0.1541, -0.0226,  0.2032],
           [ 0.4585, -0.2456,  0.2700,  ...,  0.2502, -0.2140,  0.1768]]],


         [[[ 0.3574,  0.4548,  0.1027,  ...,  0.3787, -0.0197, -0.4626],
           [ 0.2064,  0.4297,  0.1666,  ..., -0.1047,  0.0472,  0.0469],
           [-0.3696, -0.1903,  0.2703,  ..., -0.1342, -0.4556, -0.1221]],

          [[ 0.2124, -0.3435, -0.0747,  ...,  0.3474, -0.0064,  0.2118],
           [ 0.1095, -0.0380, -0.1498,  ...,  0.1053,  0.1490,  0.1069],
           [-0.1276,  0.4377,  0.3801,  ...,  0.2267,  0.2156,  0.0859]]],


         [[[ 0.0408,  0.2267, -0.1636,  ...,  0.1096,  0.4636,  0.4102],
           [ 0.0197,  0.1804, -0.2184,  ...,  0.0768, -0.3789,  0.1062],
           [-0.3188,  0.2220, -0.4121,  ..., -0.2786,  0.2944,  0.0644]],

          [[-0.0257,  0.0946,  0.0515,  ..., -0.2607,  0.3831,  0.0812],
           [ 0.3213, -0.0500, -0.1576,  ..., -0.2202, -0.0552, -0.1487],
           [ 0.2722,  0.0093,  0.3315,  ..., -0.3379,  0.4255, -0.1799]]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 4, 2, 3, 2147484]), dtype=torch.float16)
tensor([[[[[-0.2515, -0.2061, -0.2291,  ..., -0.3765,  0.1362, -0.1499],
           [ 0.2776,  0.1292, -0.2396,  ...,  0.2167, -0.0042, -0.3081],
           [ 0.1801, -0.0123,  0.2725,  ..., -0.0306, -0.3147,  0.2771]],

          [[-0.3567, -0.0754, -0.3079,  ...,  0.0126,  0.2390, -0.1921],
           [ 0.0490,  0.0170, -0.0385,  ...,  0.0182, -0.1588,  0.2231],
           [ 0.0257,  0.0371, -0.1912,  ..., -0.2532, -0.1332,  0.2212]]],


         [[[ 0.1703,  0.0902, -0.0436,  ..., -0.0465, -0.1775,  0.1279],
           [-0.1313, -0.1032,  0.1442,  ..., -0.0610, -0.0222, -0.0883],
           [ 0.2238, -0.1514, -0.2761,  ..., -0.0440,  0.0745, -0.2301]],

          [[ 0.0098, -0.3232, -0.2291,  ...,  0.2695,  0.1127,  0.1152],
           [ 0.0174, -0.0225,  0.2788,  ..., -0.1115, -0.0138, -0.0630],
           [-0.3875,  0.2600, -0.2551,  ..., -0.0865,  0.0557, -0.1587]]],


         [[[-0.0664, -0.0631, -0.0536,  ...,  0.0225, -0.2168, -0.1626],
           [-0.0956, -0.0782,  0.1777,  ..., -0.2668,  0.1656,  0.1406],
           [-0.2146,  0.2109, -0.0815,  ..., -0.2451,  0.0117,  0.0277]],

          [[ 0.3604,  0.3169, -0.3115,  ...,  0.1281,  0.1479,  0.3074],
           [ 0.0519,  0.3030, -0.0607,  ...,  0.0685, -0.0428, -0.1008],
           [ 0.0503, -0.2859, -0.0986,  ..., -0.3411, -0.0778, -0.1180]]],


         [[[-0.1311,  0.0829,  0.2405,  ..., -0.4148, -0.1059, -0.1312],
           [ 0.2908, -0.0269,  0.2319,  ..., -0.0508, -0.1129, -0.0824],
           [-0.3838, -0.0246, -0.1398,  ..., -0.0755, -0.2172,  0.1204]],

          [[-0.0867, -0.0767, -0.0317,  ..., -0.0844, -0.0170, -0.3740],
           [-0.0446,  0.1171,  0.0494,  ...,  0.0615,  0.2020,  0.0649],
           [-0.3281,  0.1680, -0.0875,  ..., -0.0975,  0.1577,  0.1648]]]],



        [[[[ 0.1498,  0.1121, -0.1666,  ...,  0.1924,  0.1506,  0.0348],
           [-0.2137, -0.0667,  0.0369,  ...,  0.0612, -0.2791, -0.1934],
           [-0.1523,  0.0308,  0.0868,  ..., -0.3457, -0.0770, -0.1442]],

          [[-0.0448,  0.1165,  0.3477,  ...,  0.1569,  0.1580,  0.1934],
           [ 0.0203, -0.3362, -0.2256,  ..., -0.0926, -0.0969, -0.0195],
           [-0.1700,  0.2054, -0.0571,  ...,  0.0338,  0.0724,  0.0635]]],


         [[[-0.0732, -0.1393,  0.1853,  ..., -0.3281,  0.0910,  0.0006],
           [ 0.1942,  0.0010,  0.2585,  ..., -0.2205, -0.1356, -0.0258],
           [ 0.0436,  0.1006,  0.3088,  ..., -0.0251,  0.2612,  0.1793]],

          [[ 0.1226, -0.2893,  0.1954,  ...,  0.0544, -0.0803, -0.3308],
           [-0.2030, -0.2615,  0.1729,  ..., -0.0737, -0.2969, -0.2163],
           [-0.2488, -0.1942,  0.0925,  ...,  0.0828,  0.0334, -0.0106]]],


         [[[-0.3030, -0.0517, -0.0480,  ...,  0.2271,  0.0624,  0.1266],
           [-0.0304,  0.1724, -0.3069,  ...,  0.1379, -0.1251, -0.2222],
           [-0.1681, -0.0560,  0.1530,  ..., -0.3037, -0.3928,  0.0763]],

          [[ 0.2302, -0.0087,  0.0041,  ..., -0.1128,  0.1493,  0.1744],
           [ 0.0061,  0.1748,  0.1576,  ..., -0.0466,  0.3159,  0.1238],
           [-0.2338, -0.1619,  0.0714,  ...,  0.0403,  0.1842, -0.2404]]],


         [[[-0.3113,  0.2544,  0.0336,  ...,  0.0254, -0.1646,  0.0931],
           [-0.0853, -0.1532,  0.0768,  ..., -0.0718,  0.0212, -0.0045],
           [ 0.1204,  0.0614,  0.2507,  ...,  0.0765,  0.0896, -0.0748]],

          [[-0.2278, -0.1498, -0.2146,  ...,  0.1793, -0.0193,  0.1945],
           [ 0.1371,  0.0378,  0.2581,  ..., -0.2168, -0.0289,  0.3108],
           [ 0.3877, -0.0117, -0.0480,  ...,  0.1461, -0.3640,  0.0659]]]]], dtype=torch.float16)

2025-07-09 13:41:54.337107 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bicubic", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bicubic", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1395294869 / 1546188264 (90.2%)
Greatest absolute difference: 0.61328125 at index (1, 0, 0, 18924215) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 0, 117042) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 6, 6, 21474837]), dtype=torch.float16)
tensor([[[[ 3.7036e-01,  7.7087e-02,  8.9661e-02,  ...,  2.7588e-01,  4.3243e-02, -1.0052e-01],
          [-7.2365e-03,  1.2341e-01,  4.1840e-02,  ...,  2.4365e-01, -1.5356e-01, -1.0541e-01],
          [-1.3196e-01, -1.1871e-01, -2.9175e-01,  ...,  6.4011e-03, -2.7124e-01,  2.6276e-02],
          [ 2.5439e-01, -1.1774e-01,  1.2402e-01,  ..., -2.9354e-03, -2.1069e-01, -1.9263e-01],
          [ 1.1421e-02, -3.8623e-01, -4.6692e-02,  ...,  3.8239e-02,  2.3254e-01,  1.6345e-01],
          [-2.5610e-01,  1.0138e-03, -1.1145e-01,  ...,  1.3786e-02, -3.0566e-01,  1.5332e-01]],

         [[ 3.0322e-01, -1.7285e-01,  3.2544e-01,  ..., -4.8657e-01, -1.8665e-01,  2.8760e-01],
          [ 2.9712e-01, -2.4023e-01,  1.7004e-01,  ..., -4.7070e-01,  4.7900e-01,  1.5723e-01],
          [ 4.0723e-01,  3.1403e-02, -3.5083e-01,  ..., -2.3669e-01,  4.2554e-01,  2.4158e-01],
          [-4.3066e-01, -1.8152e-01,  1.4514e-01,  ...,  8.9294e-02, -3.2324e-01, -3.1934e-01],
          [-4.2920e-01,  4.7211e-02,  1.1627e-01,  ..., -4.0112e-01, -3.6499e-01, -1.6443e-01],
          [ 4.1211e-01,  1.3928e-01,  1.1426e-01,  ..., -3.7842e-01,  5.6335e-02, -2.7344e-01]],

         [[-6.0059e-02,  1.9775e-01, -6.4331e-02,  ...,  1.4709e-01,  8.9783e-02,  1.3840e-02],
          [ 1.2854e-01,  4.2798e-01, -3.1885e-01,  ..., -3.9258e-01,  1.7285e-01, -1.0065e-01],
          [-1.6138e-01, -1.6370e-01,  3.9502e-01,  ..., -3.8818e-01,  3.3984e-01,  2.5903e-01],
          [ 1.4368e-01, -2.1545e-01, -1.7456e-01,  ...,  5.5664e-02, -4.4702e-01,  2.4036e-01],
          [ 1.9556e-01,  4.3823e-01, -1.5747e-01,  ..., -2.3758e-02,  2.7124e-01,  3.9233e-01],
          [ 1.8433e-01, -5.2588e-01, -1.3672e-01,  ...,  2.7539e-01, -6.8054e-02, -1.3379e-01]],

         [[-4.7510e-01, -3.3398e-01,  3.1013e-03,  ..., -3.0899e-02,  5.4474e-02, -6.1127e-02],
          [ 4.5044e-02, -4.7217e-01,  3.7329e-01,  ..., -2.8418e-01, -2.0752e-01,  3.9307e-01],
          [-1.1621e-01,  1.4244e-02, -8.2581e-02,  ..., -2.2125e-02, -2.2473e-01,  1.5369e-01],
          [ 2.2803e-01, -8.1604e-02, -3.5629e-03,  ...,  1.1255e-01, -1.0718e-01, -4.3359e-01],
          [-1.9104e-01, -1.9507e-01,  1.7517e-01,  ..., -2.9712e-01,  4.0942e-01, -1.3855e-02],
          [-5.8655e-02,  3.7891e-01, -2.9565e-01,  ...,  1.3928e-01,  2.6831e-01,  4.3579e-01]],

         [[ 2.3291e-01,  3.4619e-01,  2.5269e-01,  ..., -2.8638e-01,  1.5247e-01,  3.7842e-01],
          [-1.9556e-01,  4.8950e-01,  2.8662e-01,  ..., -1.5369e-01,  3.6523e-01,  5.1758e-02],
          [-5.8167e-02,  3.7842e-01,  3.3661e-02,  ..., -1.7798e-01, -4.6655e-01, -1.4856e-01],
          [ 2.8857e-01, -8.3862e-02, -3.5376e-01,  ..., -3.6572e-01,  5.1231e-03, -6.8176e-02],
          [ 3.1665e-01, -4.2749e-01, -3.9893e-01,  ..., -9.0820e-02, -1.9119e-02, -2.9712e-01],
          [ 4.0479e-01,  3.9917e-01, -9.2834e-02,  ..., -4.3701e-01, -3.0762e-01, -5.6763e-02]],

         [[ 3.0923e-04,  2.5314e-02,  6.6345e-02,  ...,  2.2510e-01,  1.3965e-01, -2.9736e-01],
          [-5.5428e-03,  1.0907e-01,  1.4880e-01,  ..., -3.0493e-01,  7.3669e-02, -2.4329e-01],
          [-7.1228e-02,  7.3242e-02,  8.5022e-02,  ...,  2.9694e-02, -8.8989e-02,  3.0884e-02],
          [ 1.8274e-01,  2.8589e-01,  4.2664e-02,  ...,  2.0972e-01,  3.0908e-01,  9.6069e-02],
          [-3.3984e-01, -2.8564e-01,  7.7148e-02,  ...,  1.6150e-01,  6.9519e-02, -1.0071e-01],
          [ 7.1907e-03, -2.2559e-01, -1.8176e-01,  ...,  1.8372e-01, -1.8579e-01,  9.7412e-02]]],


        [[[ 9.0210e-02,  1.2598e-01,  1.4795e-01,  ...,  3.8062e-01,  2.0218e-02, -2.5342e-01],
          [ 1.0590e-01,  1.9169e-03, -1.9019e-01,  ...,  3.0176e-01,  4.2627e-01, -3.4473e-01],
          [-2.2351e-01, -3.9124e-02, -2.6538e-01,  ..., -6.6895e-02,  1.7456e-01, -7.3669e-02],
          [-1.7236e-01,  2.1497e-01, -2.5464e-01,  ..., -3.2129e-01, -1.9995e-01, -2.1362e-01],
          [ 3.6694e-01,  2.2156e-01, -2.0630e-01,  ...,  7.3853e-02, -3.3813e-01,  2.4933e-02],
          [ 3.0542e-01, -9.9487e-02, -1.6882e-01,  ..., -1.1560e-01,  4.4525e-02,  1.9775e-01]],

         [[-3.2446e-01, -5.1514e-01, -3.0103e-01,  ...,  2.6764e-02, -6.0516e-02, -1.1707e-01],
          [-1.9421e-01,  6.7749e-02,  3.8281e-01,  ..., -4.4824e-01,  1.1639e-01,  3.4082e-01],
          [ 5.7275e-01,  2.2546e-01,  5.9814e-02,  ...,  2.9517e-01, -6.5613e-02, -3.7671e-01],
          [-3.9722e-01, -6.2134e-02,  3.0542e-01,  ..., -2.8101e-01,  2.3730e-01, -3.7158e-01],
          [-2.8101e-01, -2.5482e-02, -4.8047e-01,  ..., -4.1846e-01, -2.8027e-01, -3.2739e-01],
          [-4.0894e-01, -2.8931e-01, -4.3121e-02,  ...,  3.7793e-01,  4.1504e-01,  3.1543e-01]],

         [[-1.7834e-01, -2.2156e-01,  3.7817e-01,  ...,  2.4609e-01, -3.2104e-01, -6.2675e-03],
          [ 8.5388e-02,  2.8442e-01, -1.3196e-01,  ...,  4.4263e-01,  2.6392e-01, -5.5878e-02],
          [-3.7207e-01,  3.5547e-01, -8.3740e-02,  ...,  4.0869e-01, -3.9032e-02,  1.2561e-01],
          [ 3.5181e-01,  3.0225e-01, -2.7222e-01,  ..., -5.5029e-01,  2.5220e-01, -1.3229e-02],
          [-1.8723e-02, -1.8579e-01, -2.9956e-01,  ...,  2.3193e-01,  1.8188e-01, -2.9126e-01],
          [ 1.7822e-02,  7.4036e-02,  2.5562e-01,  ..., -9.3628e-02,  2.8296e-01,  1.1108e-01]],

         [[ 8.3191e-02, -3.3630e-02, -1.6150e-01,  ...,  2.0233e-02, -2.4634e-01,  5.5817e-02],
          [ 4.5264e-01, -2.9248e-01,  3.5840e-01,  ..., -4.0894e-01, -1.0292e-02,  3.4253e-01],
          [ 2.2278e-01, -2.4817e-01, -3.2666e-01,  ..., -1.8982e-01,  2.9761e-01, -7.2876e-02],
          [-1.7358e-01,  7.2575e-04,  3.9337e-02,  ...,  6.6605e-03, -2.2937e-01,  4.8267e-01],
          [ 8.3984e-02,  2.0496e-01,  2.4597e-01,  ...,  1.5283e-01, -4.2847e-01,  2.2415e-02],
          [-2.5122e-01, -3.9453e-01,  2.5000e-01,  ...,  1.8396e-01, -1.9043e-02, -1.5540e-01]],

         [[ 2.3853e-01,  1.8661e-02, -2.3718e-01,  ..., -3.2373e-01,  3.9600e-01, -3.5962e-01],
          [ 1.5808e-01,  4.3799e-01, -1.4946e-02,  ...,  4.8755e-01,  1.7627e-01, -2.9150e-01],
          [-3.3472e-01, -1.1650e-02, -2.6367e-01,  ...,  5.0391e-01,  2.8516e-01,  6.1798e-03],
          [ 2.2424e-01,  3.6816e-01,  2.7856e-01,  ..., -1.1194e-04,  2.3401e-01, -1.1612e-02],
          [ 4.4775e-01,  4.8901e-01,  2.5732e-01,  ...,  3.6108e-01, -3.5400e-01,  3.1647e-02],
          [ 2.4011e-01, -1.3245e-01,  4.9347e-02,  ...,  2.1228e-01,  1.9852e-02,  4.0817e-03]],

         [[-1.2585e-01, -6.1920e-02, -2.2852e-01,  ...,  1.2512e-01,  1.6711e-01, -1.0681e-01],
          [-1.7297e-01, -3.3398e-01,  3.0786e-01,  ..., -5.5078e-01, -2.8442e-01,  1.7505e-01],
          [-1.9482e-01, -1.3489e-01,  1.3831e-01,  ..., -2.0764e-01, -3.5229e-01, -3.5059e-01],
          [ 8.3435e-02,  2.2693e-01, -2.2913e-01,  ...,  3.1372e-01,  7.4097e-02,  3.3154e-01],
          [-2.4207e-01, -3.5187e-02, -2.2180e-01,  ...,  1.9885e-01, -1.2256e-01,  4.0680e-02],
          [ 1.3794e-02, -3.4888e-01, -1.4539e-01,  ..., -7.6416e-02, -9.5764e-02,  1.9214e-01]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 6, 6, 21474837]), dtype=torch.float16)
tensor([[[[ 0.4048, -0.0823,  0.1416,  ...,  0.2952, -0.1797, -0.2612],
          [ 0.0922,  0.2391, -0.2031,  ...,  0.4382, -0.2249, -0.1353],
          [-0.3086, -0.1376, -0.1968,  ..., -0.0177, -0.1246,  0.1234],
          [ 0.0611, -0.3999,  0.2737,  ..., -0.0892, -0.3655, -0.4097],
          [-0.1395, -0.2328,  0.1302,  ...,  0.0290,  0.0256,  0.1312],
          [-0.1759, -0.3042, -0.1183,  ...,  0.3162, -0.4956, -0.1049]],

         [[ 0.4451, -0.2250,  0.3494,  ..., -0.4148,  0.0069,  0.2703],
          [ 0.2593, -0.2341,  0.3369,  ..., -0.4509,  0.3271,  0.1780],
          [ 0.3962,  0.0509, -0.4316,  ..., -0.1917,  0.2710,  0.1508],
          [-0.3428, -0.1182,  0.1174,  ...,  0.1212, -0.2407, -0.1812],
          [-0.2065, -0.0184, -0.0373,  ..., -0.3755, -0.1656, -0.1218],
          [ 0.2390,  0.2461,  0.1685,  ..., -0.4722,  0.0553, -0.2888]],

         [[-0.1992,  0.2847,  0.0172,  ...,  0.1735,  0.3555, -0.0029],
          [ 0.1411,  0.4226, -0.3848,  ..., -0.3457,  0.1355, -0.0814],
          [-0.1306, -0.2114,  0.4146,  ..., -0.3813,  0.2593,  0.2546],
          [ 0.1415, -0.1443, -0.1821,  ...,  0.0724, -0.4045,  0.1692],
          [ 0.1261,  0.3718, -0.0893,  ..., -0.1478,  0.1893,  0.3792],
          [ 0.3408, -0.6265, -0.1711,  ...,  0.5371, -0.0707, -0.1442]],

         [[-0.4675, -0.2156, -0.0753,  ..., -0.0043, -0.1262,  0.0341],
          [-0.0183, -0.5288,  0.3164,  ..., -0.2485, -0.0643,  0.3264],
          [-0.0526,  0.0137, -0.0473,  ...,  0.0156, -0.2208,  0.1747],
          [ 0.2832, -0.1307,  0.0240,  ...,  0.1121, -0.1509, -0.4082],
          [-0.1949, -0.0570,  0.1237,  ..., -0.2153,  0.4624,  0.0706],
          [-0.2778,  0.2676, -0.3882,  ..., -0.0931,  0.3218,  0.3967]],

         [[ 0.0738,  0.1565,  0.4663,  ..., -0.2678,  0.1155,  0.2556],
          [-0.1465,  0.4802,  0.1235,  ..., -0.0600,  0.4536,  0.1664],
          [-0.1151,  0.2764,  0.0554,  ..., -0.2449, -0.4182, -0.2024],
          [ 0.2556, -0.0410, -0.4167,  ..., -0.2140,  0.0059,  0.0205],
          [ 0.3672, -0.3247, -0.3569,  ..., -0.1678, -0.1208, -0.2629],
          [ 0.4966,  0.4492, -0.1192,  ..., -0.3049, -0.1721,  0.0605]],

         [[ 0.0335,  0.3748,  0.2443,  ...,  0.3462,  0.0779, -0.4424],
          [ 0.0975,  0.1544,  0.3701,  ..., -0.5649, -0.0300, -0.4475],
          [ 0.0674,  0.3262,  0.0125,  ...,  0.1070, -0.1583,  0.0612],
          [ 0.0153,  0.3374,  0.2078,  ..., -0.0395,  0.4690, -0.0363],
          [-0.2573, -0.4175,  0.1082,  ...,  0.3552, -0.0517, -0.2766],
          [-0.4285, -0.4595,  0.0599,  ...,  0.2646, -0.3611,  0.2208]]],


        [[[ 0.4578,  0.0391, -0.1205,  ...,  0.2627, -0.1648, -0.4246],
          [-0.0428,  0.2146, -0.0609,  ...,  0.4939,  0.5181, -0.3240],
          [-0.1981, -0.3245, -0.2125,  ..., -0.1459, -0.0062, -0.2966],
          [-0.0510,  0.3333, -0.3013,  ..., -0.5464, -0.1708, -0.3699],
          [ 0.2559,  0.1832, -0.2676,  ...,  0.2296, -0.3774,  0.3567],
          [ 0.3950,  0.1859, -0.4341,  ..., -0.3596,  0.2642,  0.2520]],

         [[-0.4934, -0.3962, -0.1602,  ..., -0.1299, -0.1223, -0.1705],
          [-0.1473, -0.1470,  0.2161,  ..., -0.3533,  0.2244,  0.3420],
          [ 0.5103,  0.3176,  0.0487,  ...,  0.2321, -0.0403, -0.2649],
          [-0.4519, -0.1400,  0.2778,  ..., -0.1581,  0.1628, -0.3369],
          [-0.2094, -0.0325, -0.4126,  ..., -0.4143, -0.1711, -0.2700],
          [-0.3123, -0.3103, -0.0463,  ...,  0.4734,  0.3015,  0.2529]],

         [[-0.2915, -0.2361,  0.2915,  ...,  0.2072, -0.3247, -0.1248],
          [ 0.0858,  0.2291, -0.0323,  ...,  0.4812,  0.1904, -0.0725],
          [-0.3074,  0.3447, -0.1046,  ...,  0.4038, -0.0464,  0.0818],
          [ 0.3044,  0.2969, -0.2460,  ..., -0.5146,  0.2339,  0.0458],
          [ 0.0554, -0.1943, -0.2732,  ...,  0.3716,  0.1365, -0.2360],
          [-0.0733,  0.1306,  0.3799,  ..., -0.4241,  0.2551, -0.0467]],

         [[ 0.1558, -0.0721, -0.2607,  ...,  0.1534, -0.2292,  0.1182],
          [ 0.3994, -0.2595,  0.3682,  ..., -0.4458, -0.1118,  0.2974],
          [ 0.1989, -0.2100, -0.2815,  ..., -0.1923,  0.2439, -0.0065],
          [-0.0853, -0.0092, -0.0118,  ...,  0.0067, -0.1449,  0.4539],
          [ 0.0145,  0.0937,  0.2395,  ...,  0.0793, -0.4458, -0.0360],
          [-0.1931, -0.3721,  0.3350,  ...,  0.4917,  0.0217, -0.0452]],

         [[ 0.2322,  0.2493, -0.2959,  ..., -0.2620,  0.4866, -0.3562],
          [ 0.0660,  0.3225,  0.0014,  ...,  0.2729,  0.1366, -0.4624],
          [-0.2861, -0.0358, -0.2294,  ...,  0.5098,  0.2465,  0.0588],
          [ 0.1171,  0.3071,  0.2036,  ...,  0.0449,  0.1472, -0.0489],
          [ 0.3474,  0.3591,  0.1260,  ...,  0.3081, -0.3191,  0.1311],
          [ 0.2585, -0.1866, -0.0087,  ...,  0.0348, -0.0932, -0.2268]],

         [[-0.0869, -0.0072, -0.2313,  ...,  0.2056,  0.2421, -0.1942],
          [-0.0842, -0.4028,  0.2991,  ..., -0.4277, -0.2046,  0.4053],
          [-0.2817, -0.2256,  0.0706,  ..., -0.3291, -0.3606, -0.4438],
          [ 0.2079,  0.3752, -0.1231,  ...,  0.3267,  0.1647,  0.5698],
          [-0.1926,  0.0435, -0.0370,  ...,  0.2837,  0.0711, -0.1337],
          [ 0.1218, -0.3691, -0.2603,  ...,  0.0548, -0.1477,  0.4272]]]], dtype=torch.float16)

2025-07-09 13:42:01.463700 GPU 4 151556 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bicubic", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bicubic", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1395201686 / 1546188264 (90.2%)
Greatest absolute difference: 0.619140625 at index (0, 5, 5, 6588085) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 0, 8346) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 6, 6, 21474837]), dtype=torch.float16)
tensor([[[[ 0.1646,  0.2314, -0.4983,  ..., -0.2903, -0.2732,  0.0370],
          [ 0.0473, -0.0097, -0.2791,  ...,  0.1300,  0.0934,  0.0476],
          [ 0.4858, -0.1206, -0.2250,  ...,  0.2189, -0.2852, -0.2817],
          [-0.3611, -0.0936,  0.0424,  ..., -0.0936,  0.1890,  0.0036],
          [-0.2290, -0.1877, -0.3262,  ..., -0.1346, -0.3181, -0.4539],
          [-0.1747, -0.0061,  0.0306,  ..., -0.0959, -0.0926, -0.3247]],

         [[-0.3743, -0.4407,  0.1687,  ...,  0.1807,  0.1520,  0.4604],
          [-0.3501, -0.0300, -0.0826,  ...,  0.0852, -0.4333, -0.1907],
          [ 0.2411,  0.0547, -0.4690,  ..., -0.3640, -0.4182, -0.0806],
          [-0.2058, -0.3699,  0.3386,  ..., -0.0820,  0.2590, -0.0306],
          [ 0.2654,  0.1735, -0.1146,  ...,  0.0731,  0.0615,  0.1890],
          [-0.3110,  0.4802, -0.2996,  ..., -0.1364, -0.3848,  0.0762]],

         [[ 0.2322,  0.2610,  0.0142,  ...,  0.1775, -0.2917,  0.2249],
          [ 0.2703,  0.1315,  0.1092,  ...,  0.1293,  0.3340,  0.0695],
          [-0.3713,  0.3318, -0.0417,  ..., -0.1838, -0.1674,  0.2483],
          [-0.0626,  0.2522,  0.4194,  ...,  0.2739, -0.2510, -0.2834],
          [ 0.0918,  0.2010, -0.3337,  ...,  0.1659, -0.2556,  0.0085],
          [-0.4785, -0.4397, -0.0950,  ..., -0.1370, -0.3191,  0.2131]],

         [[ 0.3042, -0.0389, -0.0182,  ...,  0.3669, -0.3035, -0.0059],
          [-0.3015, -0.4636, -0.1636,  ...,  0.2140, -0.1819,  0.0223],
          [ 0.0737, -0.2532, -0.2377,  ...,  0.0282, -0.0588,  0.4299],
          [-0.1521,  0.3113,  0.0216,  ..., -0.1387,  0.3955,  0.0579],
          [-0.0370, -0.0605,  0.1501,  ...,  0.1509,  0.2917,  0.0813],
          [-0.0517,  0.1913,  0.4392,  ...,  0.1747,  0.1643, -0.1196]],

         [[ 0.2808,  0.1793, -0.3784,  ..., -0.1829, -0.2627, -0.2108],
          [ 0.2427,  0.3992,  0.3992,  ..., -0.3955, -0.3000, -0.3376],
          [-0.0363, -0.2183,  0.4058,  ...,  0.1765, -0.0881, -0.0356],
          [ 0.0232, -0.3352,  0.4412,  ...,  0.0103,  0.0018,  0.3037],
          [-0.1331,  0.0056, -0.1810,  ...,  0.4133,  0.2181, -0.0435],
          [-0.0605,  0.0985, -0.0337,  ...,  0.1245,  0.0200,  0.4729]],

         [[-0.2542,  0.3081,  0.2260,  ..., -0.4067, -0.3726,  0.0208],
          [ 0.1006, -0.0732, -0.4260,  ...,  0.2834, -0.3733, -0.1411],
          [-0.0363,  0.0706,  0.3071,  ...,  0.5039,  0.2551, -0.2803],
          [ 0.1172,  0.3452, -0.1204,  ...,  0.0340,  0.2382,  0.5903],
          [-0.1658, -0.3645, -0.1986,  ...,  0.2454,  0.3599, -0.1210],
          [ 0.2167, -0.0364,  0.1364,  ..., -0.3975,  0.1886, -0.3118]]],


        [[[-0.1683,  0.2595,  0.4824,  ..., -0.1913, -0.4556, -0.2783],
          [-0.0130, -0.3953,  0.4436,  ..., -0.4136,  0.1523,  0.1033],
          [-0.2037,  0.3386, -0.0246,  ..., -0.1412, -0.1614, -0.3477],
          [ 0.1648,  0.0485,  0.1960,  ...,  0.5303,  0.1094, -0.2964],
          [ 0.4465,  0.2150, -0.1533,  ..., -0.4438, -0.0246,  0.1801],
          [ 0.1951, -0.3777,  0.3228,  ...,  0.0531, -0.2808,  0.2124]],

         [[ 0.2947, -0.0807,  0.4314,  ...,  0.1630,  0.3501,  0.3770],
          [ 0.0768,  0.3354,  0.3430,  ...,  0.1126,  0.1187, -0.4492],
          [ 0.1926, -0.3801,  0.0589,  ..., -0.2230,  0.5312,  0.3677],
          [-0.3452, -0.2443,  0.2223,  ..., -0.3230, -0.4656,  0.0809],
          [ 0.2832, -0.3787, -0.0160,  ..., -0.3206,  0.3181,  0.0367],
          [-0.2651, -0.2059,  0.2937,  ...,  0.4648,  0.2988,  0.0110]],

         [[-0.0727, -0.4771,  0.1099,  ...,  0.0771,  0.1020,  0.0638],
          [-0.1492,  0.4019,  0.2759,  ...,  0.1993, -0.3113,  0.1276],
          [ 0.0365,  0.0781, -0.1175,  ..., -0.2134,  0.1364, -0.0026],
          [ 0.0621,  0.3059, -0.1136,  ...,  0.2103, -0.0464, -0.3877],
          [ 0.3967,  0.1649, -0.3442,  ...,  0.0134,  0.2786,  0.0045],
          [-0.4929,  0.3008, -0.4993,  ..., -0.2001,  0.0604,  0.2018]],

         [[ 0.3081,  0.1278,  0.2288,  ...,  0.4648, -0.0217, -0.2180],
          [-0.0205, -0.2974, -0.0601,  ..., -0.2971, -0.1560, -0.3035],
          [ 0.1670, -0.1614, -0.0519,  ...,  0.4348, -0.1906,  0.2522],
          [-0.0453, -0.1681, -0.2976,  ..., -0.0341,  0.0259, -0.1176],
          [-0.1236, -0.1658,  0.0729,  ..., -0.2646,  0.3540, -0.1637],
          [ 0.4668, -0.4575,  0.4773,  ..., -0.0048,  0.0240, -0.2651]],

         [[-0.2722, -0.2778,  0.4495,  ..., -0.4819,  0.1048, -0.2136],
          [ 0.2825,  0.0631, -0.4663,  ..., -0.3831,  0.3081,  0.1926],
          [-0.0680, -0.0344, -0.1066,  ...,  0.2306,  0.1132, -0.3213],
          [-0.0768, -0.0094, -0.3586,  ..., -0.1489,  0.0605, -0.1472],
          [ 0.5015, -0.2310,  0.4631,  ..., -0.0881, -0.0367, -0.4270],
          [ 0.1555,  0.2415, -0.0962,  ..., -0.3311, -0.4907,  0.1466]],

         [[ 0.0330,  0.4214,  0.4331,  ..., -0.0756, -0.2600, -0.4587],
          [ 0.1426, -0.4514, -0.1746,  ...,  0.0393, -0.0456,  0.4385],
          [-0.2991, -0.0471, -0.0745,  ...,  0.4680,  0.3564,  0.0194],
          [ 0.1171, -0.2195,  0.4788,  ..., -0.0728, -0.1416,  0.3735],
          [-0.3586, -0.2412,  0.1158,  ...,  0.2805,  0.3298, -0.0442],
          [ 0.4031, -0.2520,  0.0177,  ...,  0.1376, -0.1586, -0.2484]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 6, 6, 21474837]), dtype=torch.float16)
tensor([[[[-6.3843e-02,  2.3401e-01, -2.2595e-01,  ..., -2.2546e-01, -3.1274e-01,  2.1802e-01],
          [ 1.8311e-01,  4.4159e-02, -2.9443e-01,  ...,  1.0248e-01,  1.1902e-01,  8.5449e-02],
          [ 2.7710e-01, -2.3181e-01, -2.6709e-01,  ...,  3.1177e-01, -3.4546e-01, -2.6172e-01],
          [-4.4995e-01, -1.8933e-01,  8.0139e-02,  ...,  5.2765e-02,  1.2793e-01, -7.6233e-02],
          [ 2.7267e-02, -9.7412e-02, -1.0040e-01,  ..., -1.3708e-01, -3.6938e-01, -2.5244e-01],
          [-2.3767e-01,  4.3365e-02, -3.4332e-02,  ..., -2.1570e-01, -1.8814e-02, -2.5464e-01]],

         [[-3.1958e-01, -4.6289e-01,  1.2695e-01,  ...,  2.9395e-01,  2.4452e-03,  4.4360e-01],
          [-2.8687e-01, -7.4524e-02, -6.5979e-02,  ...,  4.9438e-02, -3.9233e-01, -3.2959e-01],
          [ 2.4353e-01,  1.9238e-01, -4.5605e-01,  ..., -4.6655e-01, -4.1431e-01, -4.6356e-02],
          [-2.0483e-01, -3.5083e-01,  3.7305e-01,  ..., -2.1155e-01,  2.6978e-01, -3.8635e-02],
          [ 1.8237e-01,  2.5757e-01, -1.5588e-01,  ...,  6.9580e-02,  5.3101e-02,  1.4429e-01],
          [-1.5063e-01,  1.7004e-01, -2.6074e-01,  ...,  1.4725e-02, -2.1448e-01,  2.1899e-01]],

         [[ 2.0190e-01,  3.0249e-01,  8.6670e-02,  ...,  2.1167e-01, -1.3806e-01,  2.4695e-01],
          [ 1.8726e-01,  1.2329e-01,  1.9669e-02,  ...,  7.0679e-02,  2.9028e-01,  3.1006e-02],
          [-3.8574e-01,  3.7500e-01, -1.3489e-02,  ..., -2.5317e-01, -1.7493e-01,  2.1948e-01],
          [ 6.9214e-02,  2.8052e-01,  4.5483e-01,  ...,  3.2666e-01, -2.5610e-01, -3.2349e-01],
          [ 1.0742e-01,  2.2070e-01, -3.3984e-01,  ...,  1.9971e-01, -3.4863e-01, -1.0907e-01],
          [-4.9268e-01, -2.7393e-01, -1.7664e-01,  ..., -3.0003e-03, -9.3079e-02,  3.0762e-01]],

         [[ 2.3779e-01, -1.3721e-01, -1.5918e-01,  ...,  3.0469e-01, -2.8491e-01, -3.0167e-02],
          [-3.8916e-01, -4.4019e-01, -4.6906e-02,  ...,  1.6919e-01, -1.1633e-01,  1.0368e-02],
          [ 1.7737e-01, -2.9517e-01, -2.4890e-01,  ...,  8.0078e-02, -8.0444e-02,  4.4458e-01],
          [-2.2083e-01,  3.7427e-01,  4.1901e-02,  ..., -2.2864e-01,  3.9722e-01,  8.3862e-02],
          [-1.0788e-02, -2.3003e-03,  1.0175e-01,  ...,  1.7334e-01,  4.3774e-01,  1.4331e-01],
          [-1.2683e-01,  6.4087e-02,  3.2715e-01,  ...,  1.3733e-01, -2.3041e-02, -2.5659e-01]],

         [[ 3.0273e-01,  2.6831e-01, -3.4302e-01,  ...,  4.0985e-02, -1.2976e-01, -2.8992e-02],
          [ 2.1790e-01,  4.1333e-01,  4.7217e-01,  ..., -3.7769e-01, -3.3887e-01, -3.9673e-01],
          [ 1.8692e-02, -2.4524e-01,  4.1284e-01,  ...,  2.0337e-01, -1.8018e-01,  1.6815e-02],
          [ 4.5929e-02, -3.5278e-01,  3.8379e-01,  ...,  3.5553e-02, -2.1347e-02,  3.1714e-01],
          [-7.9224e-02, -1.5466e-01, -1.9873e-01,  ...,  4.7632e-01,  2.8687e-01,  1.2428e-02],
          [ 1.4783e-01,  2.3254e-01,  1.4600e-01,  ...,  1.5320e-01,  1.7136e-02,  3.0762e-01]],

         [[-5.7220e-02,  3.2764e-01,  3.5767e-02,  ..., -2.1423e-01, -2.0471e-01, -4.5319e-02],
          [ 1.0719e-02, -2.7075e-01, -2.4365e-01,  ...,  9.6130e-02, -3.9258e-01, -7.6355e-02],
          [-1.3513e-01,  3.3386e-02,  1.8494e-01,  ...,  2.3608e-01,  2.4805e-01, -3.1909e-01],
          [-2.6245e-02,  2.1545e-01,  6.3171e-02,  ...,  1.5137e-01,  2.9297e-01,  5.1367e-01],
          [-3.0615e-01, -1.7554e-01, -2.3840e-01,  ...,  1.3416e-01,  2.3486e-01, -8.3679e-02],
          [ 7.5806e-02, -4.2755e-02,  1.7212e-01,  ..., -2.9126e-01,  1.1804e-01, -1.4160e-01]]],


        [[[-9.4482e-02,  9.2773e-03,  2.6904e-01,  ..., -2.9663e-01, -2.0422e-01, -4.3610e-02],
          [ 1.6028e-01, -1.6016e-01,  1.4392e-01,  ..., -2.3584e-01,  4.7016e-04, -1.2708e-01],
          [-2.5610e-01,  9.0820e-02,  1.5564e-01,  ..., -2.5098e-01,  3.1738e-02, -1.7065e-01],
          [ 2.0557e-01,  2.1103e-02,  2.4060e-01,  ...,  5.6738e-01,  1.6492e-01, -1.5662e-01],
          [ 4.0405e-01,  3.1152e-01, -2.2522e-01,  ..., -4.2505e-01,  1.2878e-02,  1.7426e-02],
          [ 2.5684e-01, -2.4219e-01,  1.4722e-01,  ...,  7.8613e-02, -1.8103e-01,  1.4514e-01]],

         [[ 1.9360e-01, -5.3467e-02,  4.2139e-01,  ...,  6.1188e-02,  3.2422e-01,  1.8494e-01],
          [-2.7496e-02,  3.6450e-01,  4.6875e-01,  ...,  1.4502e-01,  1.6968e-01, -3.2007e-01],
          [ 3.4814e-01, -3.6353e-01,  3.0167e-02,  ..., -1.8811e-01,  4.5923e-01,  3.2007e-01],
          [-3.8135e-01, -1.9983e-01,  1.8652e-01,  ..., -4.2432e-01, -4.6729e-01,  7.4280e-02],
          [ 3.4717e-01, -4.1284e-01, -6.0692e-03,  ..., -3.4790e-01,  3.9893e-01,  1.9507e-01],
          [-4.2603e-01, -2.2449e-01,  2.6294e-01,  ...,  3.6548e-01,  8.8440e-02, -1.7603e-01]],

         [[ 1.4465e-01, -4.6289e-01,  2.0239e-01,  ...,  1.5793e-02,  6.4453e-02,  1.2091e-01],
          [-2.7173e-01,  5.0684e-01,  2.1338e-01,  ...,  2.6538e-01, -2.9272e-01,  8.2092e-02],
          [-2.9087e-03,  8.5327e-02, -1.7371e-01,  ..., -1.9031e-01,  1.6003e-01,  1.5945e-02],
          [ 5.0507e-02,  3.2812e-01, -8.4778e-02,  ...,  2.4023e-01, -1.0913e-01, -4.3091e-01],
          [ 3.7598e-01,  1.2433e-01, -4.6191e-01,  ..., -6.8481e-02,  3.4912e-01, -7.5293e-04],
          [-2.6489e-01,  1.4819e-01, -2.4182e-01,  ..., -5.4962e-02, -2.9465e-02,  1.0052e-01]],

         [[ 1.6687e-01,  7.8979e-02,  8.1238e-02,  ...,  1.7749e-01,  1.4124e-01, -1.1432e-01],
          [ 1.5671e-02, -3.4839e-01,  4.8859e-02,  ..., -2.2400e-01, -2.9297e-01, -3.5913e-01],
          [ 1.6602e-01, -2.1741e-01, -7.0618e-02,  ...,  4.1748e-01, -2.4683e-01,  2.8516e-01],
          [-1.5343e-02, -2.2583e-01, -3.3374e-01,  ..., -3.2898e-02,  3.3325e-02, -1.1298e-01],
          [-2.4857e-02, -1.5491e-01,  2.4490e-02,  ..., -2.7124e-01,  3.4497e-01, -2.5146e-01],
          [ 2.0386e-01, -3.6353e-01,  3.9136e-01,  ..., -1.5869e-02,  7.6782e-02, -1.4844e-01]],

         [[-2.1387e-01, -1.4832e-01,  2.2339e-01,  ..., -1.1658e-01, -6.2195e-02, -2.8149e-01],
          [ 4.2725e-01, -2.7496e-02, -4.3213e-01,  ..., -4.6704e-01,  3.1958e-01,  1.8323e-01],
          [-9.9792e-02,  4.6463e-03, -7.2693e-02,  ...,  3.0908e-01,  1.0681e-01, -2.3047e-01],
          [-1.1542e-01, -3.1189e-02, -3.6475e-01,  ..., -1.7297e-01, -5.1605e-02, -2.5220e-01],
          [ 4.9512e-01, -4.2163e-01,  4.3945e-01,  ..., -1.1340e-01, -1.0925e-01, -4.1260e-01],
          [ 3.5205e-01,  3.3569e-01,  1.0162e-01,  ..., -3.1030e-01, -1.9727e-01, -1.0155e-02]],

         [[-2.0126e-02,  1.3525e-01,  1.6174e-01,  ..., -3.6011e-02,  4.8035e-02, -2.1521e-01],
          [-2.4033e-02, -2.8052e-01, -2.1619e-01,  ..., -6.4758e-02,  1.1688e-01,  3.2520e-01],
          [-1.8079e-01,  3.2104e-02, -1.5857e-01,  ...,  2.7856e-01,  3.0176e-01, -3.7018e-02],
          [ 1.8509e-02, -2.2705e-01,  3.7158e-01,  ..., -8.7524e-02,  4.4464e-02,  2.9834e-01],
          [-3.0005e-01,  3.7933e-02,  8.9722e-02,  ...,  1.3269e-01,  2.6636e-01,  5.2948e-02],
          [ 1.2952e-01, -2.6978e-01, -7.1106e-02,  ...,  2.9492e-01, -5.8716e-02, -1.3281e-01]]]], dtype=torch.float16)

2025-07-09 13:43:11.289233 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=list[13,2,], scale_factor=None, mode="bicubic", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=list[13,2,], scale_factor=None, mode="bicubic", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1089208585 / 1116691524 (97.5%)
Greatest absolute difference: 1.29296875 at index (1, 10, 1, 7269644) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 0, 127348) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 13, 2, 21474837]), dtype=torch.float16)
tensor([[[[ 0.2046,  0.1281,  0.0919,  ..., -0.0881,  0.3574, -0.1351],
          [ 0.3232,  0.0782, -0.0872,  ..., -0.3962, -0.0010,  0.4915]],

         [[-0.1285,  0.0231, -0.2213,  ...,  0.2329,  0.0387, -0.1271],
          [ 0.0195,  0.0072,  0.3682,  ..., -0.3228, -0.4177,  0.1720]],

         [[-0.0868,  0.1993, -0.1780,  ...,  0.4448,  0.1990,  0.0838],
          [-0.1198, -0.0101,  0.1211,  ..., -0.0263, -0.1052, -0.2629]],

         ...,

         [[-0.1108,  0.3762, -0.4011,  ..., -0.2859, -0.0067,  0.2778],
          [-0.2974,  0.1440, -0.1572,  ..., -0.3660,  0.0512,  0.4482]],

         [[-0.1898,  0.0367, -0.0697,  ..., -0.1705, -0.1248,  0.2510],
          [-0.2832,  0.0528, -0.1126,  ..., -0.1364,  0.0189,  0.1703]],

         [[ 0.1088, -0.3164,  0.1793,  ..., -0.3516,  0.1753,  0.1591],
          [ 0.3118,  0.0952, -0.1488,  ..., -0.0944, -0.4324, -0.0313]]],


        [[[-0.1935, -0.3286, -0.2004,  ...,  0.1315, -0.1094,  0.4268],
          [ 0.2169,  0.2091,  0.4421,  ...,  0.3054, -0.3137, -0.5151]],

         [[ 0.1920, -0.1947, -0.3284,  ...,  0.2184,  0.0972,  0.4695],
          [ 0.0352,  0.1146,  0.3999,  ...,  0.1014, -0.0559, -0.3840]],

         [[ 0.3916, -0.2905,  0.0240,  ...,  0.1891,  0.0331,  0.2131],
          [-0.0276,  0.2969,  0.1722,  ..., -0.2471, -0.0526,  0.1327]],

         ...,

         [[-0.1110, -0.0737,  0.0698,  ...,  0.5166,  0.3584, -0.2166],
          [ 0.4536, -0.0628, -0.3171,  ...,  0.1659,  0.1458, -0.2063]],

         [[-0.5122,  0.3535,  0.2173,  ...,  0.3013,  0.4321,  0.1058],
          [ 0.3074,  0.2971, -0.0204,  ...,  0.3467,  0.2487, -0.4329]],

         [[-0.4097,  0.4861,  0.1218,  ..., -0.1139,  0.1699,  0.2046],
          [-0.0121,  0.0593,  0.2357,  ..., -0.0050, -0.3269, -0.1064]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 13, 2, 21474837]), dtype=torch.float16)
tensor([[[[-2.4353e-01, -4.4360e-01, -4.3433e-01,  ..., -1.2610e-01, -3.3447e-02,  4.9976e-01],
          [-3.3496e-01, -2.0361e-01, -4.1724e-01,  ..., -3.6816e-01, -3.4790e-01,  2.7539e-01]],

         [[-5.0684e-01, -2.7026e-01,  1.0559e-01,  ..., -2.2839e-01,  1.4465e-01, -2.3682e-01],
          [ 7.1777e-02, -3.9209e-01, -5.6366e-02,  ..., -1.4307e-01, -1.6760e-01,  4.2090e-01]],

         [[-1.9446e-01,  1.4343e-01, -5.5573e-02,  ..., -3.5181e-01, -1.3806e-01, -5.5322e-01],
          [-1.9141e-01, -1.8921e-02,  2.9688e-01,  ...,  2.3718e-01,  3.7476e-02,  2.6465e-01]],

         ...,

         [[-4.5435e-01,  3.0615e-01, -1.8945e-01,  ...,  1.7371e-01,  1.1578e-01,  5.5225e-01],
          [ 2.1362e-01,  1.9824e-01,  8.3374e-02,  ...,  4.2267e-02, -1.3550e-01, -2.7734e-01]],

         [[-4.0796e-01,  2.7930e-01, -2.3291e-01,  ...,  5.1807e-01, -1.5283e-01,  2.6489e-01],
          [ 3.3032e-01,  1.0883e-01, -1.8945e-01,  ..., -1.5674e-01, -8.1787e-02, -4.8242e-01]],

         [[-1.0073e-05,  3.7744e-01,  3.1372e-01,  ...,  3.8110e-01,  1.3745e-01, -4.0796e-01],
          [-1.2964e-01,  3.7866e-01, -1.3269e-01,  ...,  3.7549e-01, -3.7500e-01, -2.7661e-01]]],


        [[[-3.3057e-01, -3.3325e-01,  2.8760e-01,  ...,  2.1179e-01,  3.5352e-01, -2.1655e-01],
          [ 1.4734e-01, -3.6914e-01,  1.5881e-01,  ...,  4.7852e-01,  7.9773e-02, -4.1553e-01]],

         [[ 5.2979e-02,  2.7563e-01,  2.2742e-01,  ...,  3.5156e-01, -2.0984e-01, -4.4360e-01],
          [-2.2424e-01,  9.3811e-02,  3.6572e-01,  ...,  4.2578e-01,  3.1909e-01, -1.8188e-01]],

         [[ 2.5342e-01,  1.4673e-01,  3.8940e-01,  ...,  4.4873e-01, -5.6885e-01, -3.2568e-01],
          [-2.1838e-01,  2.5732e-01, -3.4302e-02,  ..., -3.2902e-03,  1.0809e-01, -1.8079e-01]],

         ...,

         [[ 4.6558e-01, -3.1082e-02,  2.5049e-01,  ...,  2.4182e-01, -2.8589e-01, -4.0186e-01],
          [ 2.0667e-01, -5.2051e-01, -4.7241e-02,  ...,  4.4678e-01, -5.3024e-03,  1.4954e-01]],

         [[ 3.1274e-01,  5.5127e-01,  1.2732e-01,  ..., -2.7417e-01,  1.2140e-01, -1.2561e-01],
          [ 1.7102e-01, -2.8101e-01,  5.0781e-01,  ...,  4.0137e-01, -2.8271e-01, -2.2827e-01]],

         [[-3.8232e-01,  2.8345e-01,  2.4292e-01,  ..., -3.3105e-01,  4.3286e-01,  1.8661e-02],
          [-3.2910e-01, -1.1523e-01,  3.8013e-01,  ...,  3.1665e-01, -1.1322e-01, -1.3965e-01]]]], dtype=torch.float16)

2025-07-09 13:43:22.287421 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=list[13,2,], scale_factor=None, mode="bicubic", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=list[13,2,], scale_factor=None, mode="bicubic", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1089203575 / 1116691524 (97.5%)
Greatest absolute difference: 1.3203125 at index (1, 6, 1, 3436295) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 0, 159814) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 13, 2, 21474837]), dtype=torch.float16)
tensor([[[[-0.1057, -0.4094, -0.2228,  ...,  0.1967, -0.0262,  0.4387],
          [-0.1718, -0.3586,  0.2688,  ..., -0.1210, -0.4180,  0.4529]],

         [[ 0.2930, -0.1060, -0.2354,  ...,  0.0805, -0.0473,  0.5142],
          [ 0.1874,  0.3586,  0.4958,  ...,  0.3274, -0.4575, -0.0259]],

         [[ 0.0637,  0.1152,  0.1326,  ...,  0.1493,  0.2397,  0.1166],
          [-0.0833,  0.3293,  0.2798,  ...,  0.3940, -0.0673, -0.0540]],

         ...,

         [[-0.1946, -0.0608, -0.3115,  ..., -0.0270,  0.0778,  0.4973],
          [-0.4260,  0.1201,  0.3689,  ..., -0.2109,  0.3711, -0.2068]],

         [[ 0.1772,  0.0425,  0.0937,  ...,  0.1863,  0.4175,  0.2720],
          [-0.1881,  0.3816,  0.4727,  ..., -0.4307,  0.2277,  0.0953]],

         [[ 0.0075, -0.4958,  0.3657,  ..., -0.4661,  0.4153,  0.0019],
          [ 0.1464,  0.3203,  0.2454,  ..., -0.1729,  0.0844,  0.2556]]],


        [[[-0.1482, -0.2495, -0.3572,  ..., -0.0635,  0.1235, -0.0336],
          [-0.2452, -0.1549, -0.1614,  ..., -0.1860,  0.0420, -0.0792]],

         [[-0.1917,  0.1516,  0.3264,  ..., -0.0811,  0.2483,  0.4043],
          [ 0.0525, -0.4785, -0.0632,  ...,  0.0728,  0.0481, -0.3745]],

         [[ 0.0684,  0.2517,  0.0380,  ..., -0.3948, -0.1519,  0.4143],
          [ 0.1307, -0.1190, -0.1033,  ...,  0.1685, -0.1786, -0.3955]],

         ...,

         [[ 0.1707, -0.0672, -0.0099,  ...,  0.0021, -0.1198,  0.2086],
          [-0.0528,  0.2018,  0.2305,  ...,  0.0380, -0.2084,  0.3982]],

         [[-0.0133,  0.3738,  0.0619,  ...,  0.2245,  0.3188,  0.3987],
          [-0.3210,  0.2878,  0.0817,  ..., -0.3635, -0.0504,  0.2455]],

         [[ 0.2495,  0.3157,  0.0372,  ...,  0.3943,  0.2532, -0.0490],
          [-0.0678, -0.4958,  0.1096,  ..., -0.2112, -0.3235,  0.4177]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 13, 2, 21474837]), dtype=torch.float16)
tensor([[[[-0.5034, -0.2688,  0.3811,  ...,  0.5244,  0.1764, -0.2153],
          [-0.1669, -0.5615,  0.3252,  ..., -0.2296,  0.4888, -0.1887]],

         [[-0.3853, -0.3499,  0.1710,  ...,  0.0681, -0.1798, -0.3091],
          [-0.4768,  0.2141,  0.3552,  ..., -0.2345,  0.2559, -0.2693]],

         [[-0.4136, -0.1567, -0.0699,  ..., -0.0093, -0.3425, -0.3562],
          [-0.0335,  0.2571,  0.3630,  ..., -0.3008, -0.1565, -0.3013]],

         ...,

         [[-0.1597, -0.0183,  0.0735,  ..., -0.0857, -0.3254,  0.1094],
          [-0.2690, -0.1488,  0.3669,  ...,  0.2764,  0.0726,  0.3662]],

         [[ 0.0647, -0.1215,  0.0985,  ..., -0.2310, -0.1760, -0.0440],
          [ 0.1539, -0.3867,  0.1598,  ..., -0.0202, -0.2507,  0.1610]],

         [[ 0.5361,  0.0074,  0.3711,  ...,  0.0242,  0.4209,  0.2079],
          [ 0.4661,  0.1847,  0.0401,  ..., -0.2671, -0.4902,  0.0027]]],


        [[[-0.4331, -0.1633, -0.1995,  ..., -0.1692,  0.4248,  0.0072],
          [-0.4109, -0.1385,  0.0905,  ..., -0.1473, -0.0694,  0.4773]],

         [[ 0.0019, -0.2781,  0.3069,  ..., -0.3225,  0.3884, -0.2966],
          [ 0.2274,  0.3730, -0.1827,  ..., -0.0483,  0.0286,  0.4824]],

         [[ 0.1681, -0.0321,  0.2761,  ..., -0.1038, -0.0297, -0.3396],
          [ 0.1997,  0.1180, -0.0026,  ..., -0.1935,  0.2413,  0.0790]],

         ...,

         [[ 0.4543,  0.2373, -0.0520,  ..., -0.3989, -0.0234,  0.3130],
          [ 0.0336, -0.2993, -0.4253,  ..., -0.3044, -0.2029, -0.2854]],

         [[ 0.1941,  0.1442, -0.0903,  ..., -0.3745, -0.0662,  0.1819],
          [-0.1114, -0.1497, -0.1008,  ..., -0.2401, -0.1672, -0.3516]],

         [[-0.4233, -0.0425, -0.5366,  ..., -0.3496,  0.0526, -0.4714],
          [-0.1003, -0.1554,  0.1393,  ...,  0.3464,  0.5498, -0.2576]]]], dtype=torch.float16)

2025-07-09 13:43:50.394603 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=list[2,13,], scale_factor=None, mode="bicubic", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=list[2,13,], scale_factor=None, mode="bicubic", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1089209683 / 1116691524 (97.5%)
Greatest absolute difference: 1.3076171875 at index (0, 0, 6, 15661865) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 0, 33299) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 2, 13, 21474837]), dtype=torch.float16)
tensor([[[[ 0.3713,  0.0342,  0.1154,  ...,  0.2754,  0.4556, -0.4624],
          [ 0.3323, -0.0261, -0.2273,  ...,  0.4502,  0.1443, -0.3965],
          [-0.0556,  0.1095, -0.0709,  ...,  0.3838, -0.2472,  0.0014],
          ...,
          [-0.2089,  0.2250,  0.4509,  ..., -0.0565, -0.0890, -0.2333],
          [-0.1621,  0.3318,  0.3450,  ..., -0.1643, -0.2830, -0.1444],
          [-0.5107, -0.1478, -0.0081,  ..., -0.3843,  0.2781, -0.2007]],

         [[-0.2788, -0.4751,  0.4067,  ..., -0.5098, -0.0279, -0.3875],
          [ 0.1622, -0.1388,  0.4341,  ...,  0.0399, -0.1043, -0.0757],
          [ 0.4741,  0.0383,  0.3760,  ...,  0.0357, -0.2330,  0.2617],
          ...,
          [ 0.0189,  0.0771,  0.0897,  ...,  0.3816, -0.1812,  0.0884],
          [ 0.2537,  0.0280,  0.0507,  ...,  0.2776, -0.4290, -0.3445],
          [-0.1451, -0.0512, -0.2218,  ...,  0.2910, -0.1740, -0.3491]]],


        [[[-0.4124, -0.0290, -0.1599,  ...,  0.4756,  0.4629, -0.2053],
          [ 0.1847, -0.2964,  0.0025,  ...,  0.5181, -0.2052, -0.1356],
          [ 0.4233, -0.3340,  0.2423,  ...,  0.0889, -0.1048,  0.1256],
          ...,
          [ 0.0792, -0.1278,  0.4189,  ...,  0.5132, -0.2668,  0.1934],
          [-0.3257,  0.0759,  0.2539,  ...,  0.2065,  0.1512, -0.1232],
          [-0.2559,  0.2795, -0.2395,  ..., -0.4924,  0.4355, -0.1024]],

         [[-0.0474,  0.2871, -0.1816,  ...,  0.4163, -0.5044, -0.4958],
          [-0.4490,  0.1456, -0.0833,  ...,  0.3694, -0.3931,  0.0613],
          [-0.2094,  0.0632, -0.0172,  ...,  0.2039, -0.0525,  0.2839],
          ...,
          [ 0.1030,  0.1427, -0.4312,  ..., -0.4417,  0.0786,  0.0698],
          [ 0.0222,  0.3584, -0.2256,  ..., -0.3909, -0.0229,  0.1808],
          [-0.3079,  0.2069,  0.4167,  ..., -0.3501,  0.4678,  0.5210]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 2, 13, 21474837]), dtype=torch.float16)
tensor([[[[ 0.2754,  0.0271,  0.3081,  ...,  0.0479,  0.1917,  0.2327],
          [ 0.1843,  0.1412, -0.0287,  ...,  0.3994, -0.1782, -0.0567],
          [ 0.0862,  0.2031, -0.2092,  ...,  0.2178,  0.1290, -0.1154],
          ...,
          [ 0.0051,  0.3682, -0.1685,  ..., -0.1428, -0.1595,  0.1357],
          [-0.1909,  0.1418, -0.0418,  ..., -0.3296,  0.3271,  0.1000],
          [ 0.3254, -0.3447,  0.1244,  ..., -0.1125,  0.4929, -0.3333]],

         [[-0.0375,  0.3721,  0.2068,  ...,  0.3604,  0.4736,  0.4644],
          [-0.4038,  0.4048,  0.1595,  ..., -0.3430, -0.0936,  0.4993],
          [-0.2118,  0.1974,  0.1838,  ..., -0.0047, -0.4863,  0.1770],
          ...,
          [ 0.4055, -0.1267,  0.0413,  ..., -0.3069,  0.1571,  0.1126],
          [ 0.3674, -0.2795, -0.1167,  ..., -0.4824, -0.1910,  0.0503],
          [ 0.2854, -0.1748,  0.4187,  ..., -0.4927, -0.3511,  0.1708]]],


        [[[ 0.2366,  0.4819,  0.4128,  ...,  0.4756,  0.3965, -0.3701],
          [ 0.4658, -0.0528,  0.0239,  ...,  0.2228,  0.4185,  0.2542],
          [ 0.3691,  0.0314,  0.0490,  ..., -0.1414, -0.1635,  0.0175],
          ...,
          [-0.2686, -0.1154,  0.2340,  ...,  0.0154,  0.0456,  0.1582],
          [-0.0458,  0.2186,  0.3760,  ..., -0.0192,  0.1115,  0.2788],
          [-0.0141, -0.2952,  0.4431,  ..., -0.2861,  0.4390,  0.1097]],

         [[-0.4221, -0.3672, -0.1046,  ..., -0.1998,  0.1921, -0.2949],
          [-0.1069,  0.3228, -0.4080,  ...,  0.2317, -0.1268, -0.2434],
          [ 0.2434,  0.2947, -0.2935,  ...,  0.4312, -0.3789, -0.3931],
          ...,
          [ 0.4165,  0.2781,  0.1903,  ...,  0.0492, -0.2362, -0.2119],
          [ 0.0152,  0.3779,  0.0064,  ...,  0.0870,  0.1366,  0.0013],
          [-0.3843,  0.2561,  0.4451,  ..., -0.4058,  0.0547, -0.2205]]]], dtype=torch.float16)

2025-07-09 13:43:52.018077 GPU 6 151383 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=list[2,13,], scale_factor=None, mode="bicubic", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 10, 21474837],"float16"), size=list[2,13,], scale_factor=None, mode="bicubic", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1089192826 / 1116691524 (97.5%)
Greatest absolute difference: 1.3134765625 at index (0, 1, 2, 14516848) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 0, 119153) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 2, 13, 21474837]), dtype=torch.float16)
tensor([[[[-2.4121e-01,  2.0630e-01, -2.5659e-01,  ..., -3.2349e-01, -1.5747e-01,  1.2306e-02],
          [ 3.2300e-01,  1.2866e-01, -2.7563e-01,  ..., -4.3237e-01,  1.6882e-01,  1.8140e-01],
          [ 8.9600e-02, -1.2189e-01, -3.3569e-02,  ..., -7.1533e-02,  1.2891e-01, -8.6975e-02],
          ...,
          [ 2.6392e-01, -2.4854e-01,  4.4922e-01,  ..., -3.4497e-01, -8.2397e-02, -3.9136e-01],
          [ 2.2839e-01, -2.1643e-01,  3.6743e-01,  ..., -1.1212e-01,  3.5919e-02, -1.8262e-01],
          [ 5.1575e-02,  4.5459e-01,  9.2041e-02,  ..., -2.7832e-01,  4.0503e-01,  2.2876e-01]],

         [[ 3.8940e-01,  5.7648e-02, -4.2041e-01,  ..., -1.1603e-01, -2.2595e-01, -4.7612e-04],
          [ 4.9609e-01, -9.7595e-02, -1.8103e-01,  ..., -2.2900e-01, -1.4722e-01,  1.5541e-02],
          [ 1.3074e-01, -3.6719e-01, -5.5725e-02,  ..., -4.3042e-01, -2.9590e-01, -2.7634e-02],
          ...,
          [-1.6821e-01,  3.9600e-01, -1.7505e-01,  ..., -3.1323e-01, -2.5488e-01, -2.6660e-01],
          [-2.6562e-01,  4.1528e-01, -1.1322e-02,  ..., -3.5492e-02, -3.0640e-01, -3.5742e-01],
          [ 7.9346e-02,  3.3081e-01, -3.3032e-01,  ...,  3.0347e-01,  2.6367e-01, -2.2314e-01]]],


        [[[-4.6021e-01,  3.5583e-02, -4.3481e-01,  ..., -2.9224e-01,  4.2145e-02,  3.1567e-01],
          [-4.8309e-02,  1.5576e-01,  1.6919e-01,  ..., -5.3131e-02,  4.8370e-02,  4.8047e-01],
          [ 5.3528e-02,  1.0492e-01,  1.9507e-01,  ..., -4.6387e-02,  2.3767e-01, -2.4841e-02],
          ...,
          [-1.1658e-01,  9.7595e-02,  8.2642e-02,  ..., -1.2283e-02,  1.4648e-01,  2.9858e-01],
          [-1.2927e-01,  4.1479e-01, -2.8906e-01,  ..., -4.2749e-01,  3.3496e-01,  5.7587e-02],
          [ 1.4661e-01, -3.1958e-01, -1.0858e-01,  ...,  5.9967e-03, -3.9703e-02, -4.9487e-01]],

         [[-4.8633e-01, -9.4849e-02,  3.9453e-01,  ...,  2.3914e-01,  4.6533e-01, -3.1104e-01],
          [ 7.6965e-02, -2.1692e-01, -3.4863e-01,  ...,  1.7993e-01, -7.9956e-02, -3.9154e-02],
          [-7.4341e-02,  1.0651e-01, -2.2778e-01,  ...,  3.5669e-01, -3.8071e-03,  2.0288e-01],
          ...,
          [-3.6224e-02, -1.1798e-01, -2.1704e-01,  ..., -3.5400e-01, -2.2974e-01, -2.7197e-01],
          [-3.6621e-01, -3.0176e-01,  4.8859e-02,  ..., -2.2546e-01, -2.2278e-01, -4.9585e-01],
          [-4.7485e-01, -6.7932e-02,  1.8250e-01,  ...,  3.7402e-01,  2.2900e-01, -4.2090e-01]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 2, 13, 21474837]), dtype=torch.float16)
tensor([[[[-0.4209,  0.0409, -0.4182,  ..., -0.0361, -0.1083,  0.0473],
          [-0.1144, -0.1109, -0.3101,  ...,  0.2944, -0.4160,  0.3694],
          [-0.1168, -0.1520, -0.1096,  ...,  0.0148, -0.1404,  0.4290],
          ...,
          [ 0.0325,  0.4722, -0.1174,  ..., -0.1193,  0.2722, -0.1960],
          [-0.4534,  0.0111, -0.4038,  ...,  0.2491,  0.0591, -0.3000],
          [-0.3206, -0.4739, -0.4524,  ...,  0.2998, -0.0641,  0.2487]],

         [[ 0.0348, -0.1682, -0.5283,  ...,  0.3188, -0.1544, -0.4912],
          [ 0.2910,  0.0739,  0.0081,  ..., -0.2184, -0.0063,  0.1902],
          [ 0.4802,  0.4114,  0.2659,  ..., -0.3467,  0.2050,  0.2842],
          ...,
          [-0.2377, -0.3865,  0.0133,  ..., -0.3357, -0.3855,  0.1893],
          [-0.1259, -0.3848, -0.3533,  ..., -0.1160, -0.4243,  0.2479],
          [ 0.3357, -0.3628, -0.4451,  ...,  0.1312, -0.4910, -0.0170]]],


        [[[-0.1047,  0.4915,  0.0433,  ..., -0.3618, -0.4609,  0.4563],
          [-0.0801, -0.0739, -0.0244,  ..., -0.2878, -0.2312,  0.2368],
          [-0.3259, -0.5156, -0.2808,  ...,  0.1127,  0.1614,  0.0630],
          ...,
          [-0.0468, -0.2830, -0.0274,  ..., -0.2627,  0.0668, -0.0293],
          [-0.3743, -0.1570,  0.0739,  ..., -0.1648,  0.3191,  0.0955],
          [ 0.0363,  0.0532, -0.0540,  ...,  0.4424,  0.3340, -0.2035]],

         [[ 0.4966,  0.4043,  0.1847,  ..., -0.3445,  0.2175,  0.3323],
          [ 0.3081, -0.0591, -0.2625,  ..., -0.0591,  0.2883,  0.5234],
          [-0.1003, -0.2812, -0.1193,  ..., -0.1519,  0.2231,  0.0979],
          ...,
          [ 0.4077, -0.3379,  0.1930,  ..., -0.0281,  0.2749, -0.4102],
          [ 0.2947, -0.1600, -0.0398,  ..., -0.2532,  0.1014, -0.3491],
          [-0.2384, -0.1310, -0.4670,  ...,  0.0160,  0.0636, -0.4993]]]], dtype=torch.float16)

2025-07-09 13:51:00.374982 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 10, 5368710, 4],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=False, align_mode=1, data_format="NDHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 10, 5368710, 4],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=False, align_mode=1, data_format="NDHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 181 / 192 (94.3%)
Greatest absolute difference: 0.80859375 at index (0, 0, 0, 2, 0) (up to 0.01 allowed)
Greatest relative difference: 168.5 at index (0, 0, 1, 2, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 4, 2, 3, 4]), dtype=torch.float16)
tensor([[[[[-0.2435, -0.4436, -0.4343,  0.4321],
           [-0.2224,  0.1431,  0.4497,  0.2024],
           [-0.3086,  0.4980, -0.2524,  0.4412]],

          [[ 0.4260,  0.4673,  0.4153, -0.0545],
           [ 0.1959,  0.3857, -0.2512, -0.3335],
           [ 0.1196, -0.4114, -0.1759,  0.1248]]],


         [[[-0.0708, -0.2340, -0.0479, -0.2673],
           [-0.0391,  0.0533, -0.0647,  0.1123],
           [ 0.0818,  0.2412,  0.0110,  0.2317]],

          [[-0.1670,  0.3325, -0.1307, -0.4329],
           [ 0.2139,  0.0591,  0.0193, -0.0654],
           [ 0.1409,  0.1265,  0.1487,  0.0778]]],


         [[[ 0.1456, -0.2800, -0.4922, -0.3494],
           [-0.4966,  0.2195,  0.0858, -0.2275],
           [ 0.3718, -0.0703, -0.1348,  0.0416]],

          [[ 0.1388,  0.2003, -0.3503,  0.1212],
           [ 0.4834, -0.4326, -0.1197, -0.1182],
           [ 0.1224,  0.2693, -0.1046, -0.1184]]],


         [[[-0.1035, -0.0226, -0.1752, -0.0103],
           [-0.2852,  0.0358, -0.2338, -0.1571],
           [ 0.2524,  0.2524, -0.0481, -0.1746]],

          [[ 0.2346,  0.0963,  0.0417,  0.1456],
           [-0.3047,  0.2136, -0.1316, -0.1962],
           [-0.0256, -0.0913, -0.0925,  0.2554]]]],



        [[[[-0.3274,  0.2058,  0.4890,  0.2128],
           [ 0.0160,  0.3040,  0.4482,  0.2632],
           [ 0.3201,  0.2766,  0.2812, -0.1921]],

          [[-0.4395,  0.1785,  0.2144, -0.2512],
           [-0.0409, -0.3281,  0.4360, -0.4275],
           [-0.4316, -0.0500, -0.4866, -0.1044]]],


         [[[-0.3223,  0.0718, -0.1334,  0.3936],
           [-0.0852,  0.4016,  0.1298, -0.2559],
           [ 0.0859,  0.0624,  0.0544,  0.1824]],

          [[ 0.2495,  0.1011, -0.2830,  0.1710],
           [ 0.4180, -0.0928, -0.0914,  0.0251],
           [ 0.0723, -0.0736,  0.2051, -0.2402]]],


         [[[ 0.4119, -0.4531, -0.0455,  0.3550],
           [-0.4104,  0.2520,  0.2111, -0.0820],
           [-0.3069, -0.1753,  0.2056, -0.3032]],

          [[-0.3291,  0.2749, -0.3472, -0.0887],
           [-0.2362,  0.1791, -0.3828, -0.3735],
           [-0.3784, -0.0326, -0.0210,  0.2537]]],


         [[[-0.0847, -0.2400, -0.0604, -0.0427],
           [-0.1128, -0.2861,  0.2832, -0.0293],
           [-0.1191, -0.3230,  0.1293, -0.0981]],

          [[-0.0737, -0.2932,  0.1138, -0.1068],
           [-0.0138,  0.1024, -0.3562, -0.0052],
           [-0.3071, -0.3081, -0.2710, -0.4268]]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 4, 2, 3, 4]), dtype=torch.float16)
tensor([[[[[-0.2435, -0.4436, -0.4343,  0.4321],
           [ 0.1526, -0.0072, -0.2080, -0.2783],
           [ 0.4998, -0.0201,  0.3523,  0.0692]],

          [[ 0.0846,  0.3257,  0.0435, -0.2959],
           [-0.1819, -0.0629, -0.0352,  0.0350],
           [ 0.4004, -0.0024, -0.0592,  0.1611]]],


         [[[ 0.0455, -0.1416, -0.2900, -0.4153],
           [-0.1482, -0.0838,  0.3306, -0.0967],
           [-0.1036,  0.1936,  0.2722,  0.4624]],

          [[ 0.0034,  0.1844,  0.1284, -0.1248],
           [ 0.2483,  0.0438,  0.4116,  0.1941],
           [ 0.1450, -0.3069,  0.3477, -0.1577]]],


         [[[ 0.3457,  0.3208,  0.0557,  0.2357],
           [ 0.0592, -0.2407,  0.1226,  0.0083],
           [ 0.0377, -0.2822, -0.1781,  0.1699]],

          [[-0.1506, -0.2252, -0.2925, -0.3516],
           [-0.0168, -0.3506, -0.0446,  0.1726],
           [ 0.0349,  0.3435, -0.4478,  0.1394]]],


         [[[-0.3926,  0.1835,  0.2493, -0.2759],
           [-0.2014, -0.0304, -0.1272, -0.1167],
           [-0.3752, -0.0166, -0.1654,  0.1741]],

          [[ 0.3840, -0.0954,  0.1042, -0.0637],
           [ 0.0391,  0.0656, -0.0333,  0.1814],
           [-0.3291, -0.0072,  0.1541,  0.2769]]]],



        [[[[-0.3274,  0.2058,  0.4890,  0.2128],
           [ 0.0098,  0.1406,  0.0688,  0.0938],
           [ 0.1674,  0.0462, -0.2725, -0.3503]],

          [[-0.1043, -0.2096,  0.0992,  0.4890],
           [-0.0671,  0.0402,  0.0231, -0.3008],
           [-0.3220, -0.0718, -0.0184,  0.0170]]],


         [[[-0.4321,  0.2634, -0.4661,  0.4583],
           [-0.2114, -0.1113,  0.1265,  0.0829],
           [-0.2018, -0.0511,  0.2898,  0.0816]],

          [[ 0.2864,  0.0593,  0.1063, -0.4583],
           [ 0.3486, -0.0599,  0.0389,  0.3418],
           [ 0.1833,  0.3291, -0.3467,  0.1426]]],


         [[[-0.0703, -0.0534, -0.4209,  0.3315],
           [-0.2922, -0.0226, -0.2605,  0.1787],
           [ 0.0577, -0.4465,  0.0537,  0.2769]],

          [[ 0.3687, -0.3171, -0.4146,  0.1306],
           [-0.2949, -0.0267,  0.2744,  0.0612],
           [-0.1396, -0.2346,  0.0025,  0.3899]]],


         [[[ 0.2086, -0.4043, -0.2905,  0.2184],
           [-0.3398,  0.0691, -0.1143,  0.2292],
           [ 0.1122, -0.1125,  0.4688, -0.2406]],

          [[-0.4656, -0.0135,  0.4949,  0.4937],
           [ 0.1948,  0.2157,  0.0410, -0.3179],
           [-0.0211,  0.0719,  0.0663,  0.0760]]]]], dtype=torch.float16)

2025-07-09 13:51:11.302536 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 10, 5368710, 4],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=True, align_mode=0, data_format="NDHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 10, 5368710, 4],"float16"), size=list[4,2,3,], scale_factor=None, mode="trilinear", align_corners=True, align_mode=0, data_format="NDHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 186 / 192 (96.9%)
Greatest absolute difference: 0.76171875 at index (1, 3, 1, 0, 0) (up to 0.01 allowed)
Greatest relative difference: 191.375 at index (1, 2, 1, 0, 2) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 4, 2, 3, 4]), dtype=torch.float16)
tensor([[[[[-0.2435, -0.4436, -0.4343,  0.4321],
           [ 0.1526, -0.0072, -0.2080, -0.2783],
           [ 0.4998, -0.0201,  0.3523,  0.0692]],

          [[ 0.0846,  0.3257,  0.0435, -0.2959],
           [-0.1819, -0.0629, -0.0352,  0.0350],
           [ 0.4004, -0.0024, -0.0592,  0.1611]]],


         [[[ 0.0455, -0.1416, -0.2900, -0.4153],
           [-0.1482, -0.0838,  0.3306, -0.0967],
           [-0.1036,  0.1936,  0.2722,  0.4624]],

          [[ 0.0034,  0.1844,  0.1284, -0.1248],
           [ 0.2483,  0.0438,  0.4116,  0.1941],
           [ 0.1450, -0.3069,  0.3477, -0.1577]]],


         [[[ 0.3457,  0.3208,  0.0557,  0.2357],
           [ 0.0592, -0.2407,  0.1226,  0.0083],
           [ 0.0377, -0.2822, -0.1781,  0.1699]],

          [[-0.1506, -0.2252, -0.2925, -0.3516],
           [-0.0168, -0.3506, -0.0446,  0.1726],
           [ 0.0349,  0.3435, -0.4478,  0.1394]]],


         [[[-0.3926,  0.1835,  0.2493, -0.2759],
           [-0.2014, -0.0304, -0.1272, -0.1167],
           [-0.3752, -0.0166, -0.1654,  0.1741]],

          [[ 0.3840, -0.0954,  0.1042, -0.0637],
           [ 0.0391,  0.0656, -0.0333,  0.1814],
           [-0.3291, -0.0072,  0.1541,  0.2769]]]],



        [[[[-0.3274,  0.2058,  0.4890,  0.2128],
           [ 0.0098,  0.1406,  0.0688,  0.0938],
           [ 0.1674,  0.0462, -0.2725, -0.3503]],

          [[-0.1043, -0.2096,  0.0992,  0.4890],
           [-0.0671,  0.0402,  0.0231, -0.3008],
           [-0.3220, -0.0718, -0.0184,  0.0170]]],


         [[[-0.4321,  0.2634, -0.4661,  0.4583],
           [-0.2114, -0.1113,  0.1265,  0.0829],
           [-0.2018, -0.0511,  0.2898,  0.0816]],

          [[ 0.2864,  0.0593,  0.1063, -0.4583],
           [ 0.3486, -0.0599,  0.0389,  0.3418],
           [ 0.1833,  0.3291, -0.3467,  0.1426]]],


         [[[-0.0703, -0.0534, -0.4209,  0.3315],
           [-0.2922, -0.0226, -0.2605,  0.1787],
           [ 0.0577, -0.4465,  0.0537,  0.2769]],

          [[ 0.3687, -0.3171, -0.4146,  0.1306],
           [-0.2949, -0.0267,  0.2744,  0.0612],
           [-0.1396, -0.2346,  0.0025,  0.3899]]],


         [[[ 0.2086, -0.4043, -0.2905,  0.2184],
           [-0.3398,  0.0691, -0.1143,  0.2292],
           [ 0.1122, -0.1125,  0.4688, -0.2406]],

          [[-0.4656, -0.0135,  0.4949,  0.4937],
           [ 0.1948,  0.2157,  0.0410, -0.3179],
           [-0.0211,  0.0719,  0.0663,  0.0760]]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 4, 2, 3, 4]), dtype=torch.float16)
tensor([[[[[-0.2603, -0.0594, -0.0441, -0.1635],
           [-0.1334, -0.0237,  0.0668, -0.0808],
           [ 0.1218, -0.0697,  0.0611, -0.0794]],

          [[-0.0958,  0.2615, -0.1938, -0.1769],
           [-0.0439,  0.2198, -0.2261,  0.3147],
           [-0.1824, -0.1962,  0.1968,  0.0419]]],


         [[[ 0.0327,  0.0623, -0.0651,  0.2593],
           [-0.1720, -0.2908, -0.1200,  0.0698],
           [ 0.1188,  0.1139, -0.1721, -0.0915]],

          [[ 0.0770, -0.1875, -0.1282,  0.3440],
           [-0.0843, -0.1310, -0.0289,  0.0664],
           [-0.0346,  0.0288, -0.0286,  0.2471]]],


         [[[-0.0635, -0.0248,  0.0622,  0.0188],
           [ 0.0809,  0.2710,  0.0193, -0.0919],
           [ 0.0765,  0.0240, -0.0818, -0.0590]],

          [[ 0.2251,  0.0211,  0.1591,  0.2086],
           [ 0.0890, -0.1110, -0.1143, -0.2014],
           [ 0.1223, -0.2423,  0.1373, -0.1165]]],


         [[[ 0.0590,  0.1218, -0.2202, -0.1849],
           [ 0.2358, -0.0845, -0.1769, -0.1232],
           [ 0.1719, -0.0461, -0.1064, -0.3381]],

          [[-0.0403, -0.0951,  0.1335, -0.0972],
           [ 0.0796, -0.2122, -0.0865, -0.0063],
           [-0.0588,  0.0790, -0.0561, -0.1593]]]],



        [[[[ 0.0779, -0.0829, -0.1866, -0.1108],
           [-0.0356, -0.1566, -0.0005, -0.0675],
           [ 0.0398, -0.2932,  0.0178,  0.2854]],

          [[-0.1698, -0.0165,  0.0486,  0.0429],
           [-0.0564,  0.1147,  0.1015, -0.0925],
           [-0.2913, -0.1119,  0.1315, -0.1886]]],


         [[[ 0.0426, -0.0357,  0.1825, -0.1947],
           [-0.1381,  0.1268, -0.3181,  0.1172],
           [ 0.1304,  0.2020,  0.1482,  0.1963]],

          [[-0.0564,  0.0356,  0.0109,  0.3003],
           [ 0.2396,  0.3123, -0.1174,  0.0241],
           [-0.2327,  0.1364,  0.3284,  0.0816]]],


         [[[ 0.0470,  0.2290, -0.0248,  0.0612],
           [ 0.1222, -0.1731,  0.2262, -0.3901],
           [ 0.0223,  0.0682, -0.0214, -0.0606]],

          [[-0.0199, -0.0689,  0.0022, -0.3103],
           [ 0.0099, -0.1895, -0.0402,  0.1180],
           [ 0.4150, -0.1752,  0.1885,  0.2050]]],


         [[[ 0.1868,  0.0645,  0.1769,  0.3267],
           [ 0.0424,  0.0266, -0.0664, -0.0169],
           [ 0.1224,  0.1333, -0.1741,  0.0385]],

          [[ 0.2961,  0.0046,  0.1865,  0.1221],
           [-0.1262,  0.0665, -0.0536, -0.3096],
           [-0.1224, -0.0864,  0.0682,  0.0318]]]]], dtype=torch.float16)

2025-07-09 13:51:19.018768 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bicubic", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bicubic", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 6, 64424512, 2]) != torch.Size([2, 6, 64424509, 2]).
ACTUAL: (shape=torch.Size([2, 6, 64424512, 2]), dtype=torch.float16)
tensor([[[[-0.2068, -0.0338],
          [ 0.2744,  0.1780],
          [ 0.3057,  0.1509],
          ...,
          [ 0.2859,  0.3123],
          [-0.3096, -0.3301],
          [-0.3096, -0.3301]],

         [[-0.0373, -0.3838],
          [ 0.2424,  0.4263],
          [-0.4292,  0.0229],
          ...,
          [-0.4153,  0.0274],
          [-0.1542, -0.2935],
          [-0.1542, -0.2935]],

         [[-0.1086, -0.1251],
          [ 0.1581,  0.2573],
          [ 0.5122,  0.3022],
          ...,
          [-0.4031, -0.1248],
          [-0.3940,  0.2808],
          [-0.3940,  0.2808]],

         [[-0.2136,  0.2273],
          [-0.0998, -0.1735],
          [ 0.1070, -0.1428],
          ...,
          [ 0.1097,  0.1913],
          [-0.3459, -0.2861],
          [-0.3459, -0.2861]],

         [[-0.1030,  0.2076],
          [ 0.3970, -0.1506],
          [ 0.0797,  0.1805],
          ...,
          [-0.2620,  0.2305],
          [-0.1082,  0.3655],
          [-0.1082,  0.3655]],

         [[ 0.1060, -0.1283],
          [-0.2468,  0.0875],
          [-0.0168,  0.2382],
          ...,
          [ 0.0368, -0.0563],
          [-0.2106, -0.4119],
          [-0.2106, -0.4119]]],


        [[[-0.1176, -0.1938],
          [-0.2056, -0.4341],
          [-0.0698,  0.1469],
          ...,
          [-0.2163, -0.2842],
          [-0.1289,  0.1691],
          [-0.1289,  0.1691]],

         [[ 0.1749,  0.1976],
          [-0.1854, -0.3618],
          [ 0.3533, -0.1891],
          ...,
          [ 0.3362, -0.1898],
          [ 0.4104, -0.1262],
          [ 0.4104, -0.1262]],

         [[ 0.1476,  0.2744],
          [ 0.3130,  0.1065],
          [-0.1610,  0.1326],
          ...,
          [ 0.0344,  0.2839],
          [ 0.3201, -0.3240],
          [ 0.3201, -0.3240]],

         [[-0.0868, -0.1971],
          [-0.4580,  0.1268],
          [-0.0341, -0.2230],
          ...,
          [ 0.3784,  0.0428],
          [-0.4634, -0.0383],
          [-0.4634, -0.0383]],

         [[ 0.0585, -0.4150],
          [ 0.0505,  0.4500],
          [ 0.1257,  0.0088],
          ...,
          [ 0.3579, -0.4192],
          [ 0.1072, -0.0344],
          [ 0.1072, -0.0344]],

         [[-0.0352,  0.1528],
          [ 0.2129, -0.4258],
          [ 0.3433, -0.1859],
          ...,
          [ 0.0040,  0.4143],
          [ 0.0192,  0.2908],
          [ 0.0192,  0.2908]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 6, 64424509, 2]), dtype=torch.float16)
tensor([[[[-0.1057, -0.4094],
          [ 0.0671,  0.4617],
          [ 0.3970, -0.1481],
          ...,
          [ 0.4365,  0.2367],
          [-0.2389, -0.2825],
          [-0.2389, -0.2825]],

         [[ 0.0327, -0.3423],
          [ 0.1487,  0.2642],
          [-0.4578, -0.1199],
          ...,
          [-0.3740,  0.1057],
          [-0.1798, -0.3870],
          [-0.1798, -0.3870]],

         [[-0.2412, -0.0615],
          [ 0.1992,  0.1305],
          [ 0.4685,  0.1306],
          ...,
          [-0.4060, -0.1201],
          [-0.4014,  0.3162],
          [-0.4014,  0.3162]],

         [[-0.3357,  0.3713],
          [-0.0282, -0.2163],
          [-0.0074, -0.0438],
          ...,
          [ 0.0944,  0.2000],
          [-0.3164, -0.2725],
          [-0.3164, -0.2725]],

         [[ 0.0182,  0.1154],
          [ 0.0593, -0.0115],
          [ 0.2319,  0.0348],
          ...,
          [-0.3328,  0.1709],
          [-0.1129,  0.2573],
          [-0.1129,  0.2573]],

         [[ 0.2703,  0.0039],
          [ 0.0036, -0.0682],
          [-0.2120,  0.3662],
          ...,
          [ 0.2888, -0.0159],
          [-0.2454, -0.3264],
          [-0.2454, -0.3264]]],


        [[[-0.0324, -0.2262],
          [-0.3376, -0.4590],
          [ 0.0782,  0.3611],
          ...,
          [-0.2781, -0.1654],
          [-0.1580,  0.2603],
          [-0.1580,  0.2603]],

         [[ 0.4333,  0.1808],
          [-0.3635, -0.3452],
          [ 0.3730, -0.1776],
          ...,
          [ 0.3054, -0.2563],
          [ 0.4097, -0.1385],
          [ 0.4097, -0.1385]],

         [[ 0.1597,  0.5278],
          [ 0.2491,  0.0382],
          [-0.0396, -0.0830],
          ...,
          [ 0.0298,  0.2432],
          [ 0.2487, -0.2803],
          [ 0.2487, -0.2803]],

         [[ 0.1650, -0.2847],
          [-0.5796,  0.1185],
          [ 0.0330,  0.0038],
          ...,
          [ 0.3704,  0.0055],
          [-0.4285, -0.0417],
          [-0.4285, -0.0417]],

         [[-0.1384, -0.5005],
          [ 0.0973,  0.2498],
          [ 0.1521, -0.1593],
          ...,
          [ 0.3164, -0.3152],
          [ 0.0145, -0.0212],
          [ 0.0145, -0.0212]],

         [[ 0.1156,  0.2832],
          [ 0.3875, -0.1974],
          [ 0.0509, -0.2156],
          ...,
          [-0.0060,  0.4331],
          [ 0.2546,  0.3916],
          [ 0.2546,  0.3916]]]], dtype=torch.float16)

2025-07-09 13:51:20.696563 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bicubic", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bicubic", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 6, 64424512, 2]) != torch.Size([2, 6, 64424509, 2]).
ACTUAL: (shape=torch.Size([2, 6, 64424512, 2]), dtype=torch.float16)
tensor([[[[-2.4353e-01, -4.4360e-01],
          [ 6.4392e-02, -1.2549e-01],
          [ 1.1237e-01, -9.0881e-02],
          ...,
          [ 2.2156e-01, -3.6060e-01],
          [ 3.0225e-01,  3.5278e-01],
          [ 3.0225e-01,  3.5278e-01]],

         [[ 1.5759e-01, -4.9658e-01],
          [ 4.4775e-04, -2.3407e-02],
          [ 3.5187e-02,  3.2056e-01],
          ...,
          [ 4.0234e-01,  3.9459e-02],
          [-1.2793e-01, -3.3032e-01],
          [-1.2793e-01, -3.3032e-01]],

         [[ 3.5864e-01,  2.1887e-01],
          [-2.6782e-01,  1.5784e-01],
          [ 1.0034e-01,  8.9233e-02],
          ...,
          [ 1.8701e-01, -2.8271e-01],
          [-1.0040e-01,  5.2094e-02],
          [-1.0040e-01,  5.2094e-02]],

         [[-4.0576e-01,  7.4959e-03],
          [ 3.9368e-02,  5.7617e-02],
          [-8.2703e-02, -1.8082e-02],
          ...,
          [-2.4768e-01,  4.2773e-01],
          [-1.4783e-01, -3.1403e-02],
          [-1.4783e-01, -3.1403e-02]],

         [[-2.2412e-01,  9.0149e-02],
          [-4.2480e-02,  9.2224e-02],
          [ 2.5439e-01, -3.0396e-01],
          ...,
          [ 1.5381e-01, -2.0911e-01],
          [ 3.9941e-01, -3.9771e-01],
          [ 3.9941e-01, -3.9771e-01]],

         [[-4.8950e-01, -2.2546e-01],
          [ 1.7126e-01, -4.8279e-02],
          [ 4.8633e-01,  1.8030e-01],
          ...,
          [-4.3106e-03, -4.8047e-01],
          [ 2.1460e-01,  4.0210e-01],
          [ 2.1460e-01,  4.0210e-01]]],


        [[[ 4.0112e-01, -1.4282e-01],
          [-1.0461e-01, -4.9976e-01],
          [ 4.4022e-03, -2.1164e-02],
          ...,
          [-4.1138e-01, -3.9209e-01],
          [ 1.3196e-01, -2.7832e-01],
          [ 1.3196e-01, -2.7832e-01]],

         [[-3.0347e-01,  1.9722e-03],
          [-2.6050e-01,  3.3618e-01],
          [ 5.0079e-02, -4.1479e-01],
          ...,
          [-3.3984e-01,  1.8509e-02],
          [ 3.8745e-01,  4.0112e-01],
          [ 3.8745e-01,  4.0112e-01]],

         [[ 5.7190e-02, -3.6896e-02],
          [ 2.0923e-01, -4.4360e-01],
          [-2.5562e-01,  2.0947e-01],
          ...,
          [ 2.4451e-01, -2.1167e-01],
          [-3.9697e-01, -3.6835e-02],
          [-3.9697e-01, -3.6835e-02]],

         [[-9.4666e-02, -1.4636e-01],
          [-4.8657e-01,  3.6182e-01],
          [-7.5455e-03, -3.4937e-01],
          ...,
          [ 1.8506e-01,  3.2520e-01],
          [-3.2788e-01,  1.3708e-01],
          [-3.2788e-01,  1.3708e-01]],

         [[-3.2288e-02,  4.1199e-02],
          [ 3.9966e-01, -2.7515e-01],
          [ 2.9248e-01, -1.8738e-01],
          ...,
          [-1.1053e-03, -3.9209e-01],
          [ 3.2812e-01,  2.3120e-01],
          [ 3.2812e-01,  2.3120e-01]],

         [[-3.5107e-01,  7.4097e-02],
          [-7.7698e-02, -2.5665e-02],
          [ 7.7934e-03, -2.1350e-01],
          ...,
          [ 1.6187e-01,  3.9648e-01],
          [-2.4353e-01,  3.0811e-01],
          [-2.4353e-01,  3.0811e-01]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 6, 64424509, 2]), dtype=torch.float16)
tensor([[[[-0.1796, -0.1186],
          [ 0.0480, -0.3237],
          [-0.2072, -0.2444],
          ...,
          [ 0.0693,  0.1216],
          [ 0.0709, -0.2496],
          [ 0.0709, -0.2496]],

         [[ 0.2314, -0.3176],
          [-0.1327, -0.0199],
          [ 0.1672,  0.3987],
          ...,
          [-0.2418, -0.2052],
          [ 0.4507,  0.0379],
          [ 0.4507,  0.0379]],

         [[ 0.4124,  0.1333],
          [-0.4219,  0.2421],
          [ 0.2137,  0.1326],
          ...,
          [ 0.3994, -0.2722],
          [ 0.2479, -0.3142],
          [ 0.2479, -0.3142]],

         [[-0.3953, -0.0552],
          [ 0.0654,  0.1554],
          [-0.1160, -0.2024],
          ...,
          [ 0.1052,  0.4590],
          [-0.2925,  0.4312],
          [-0.2925,  0.4312]],

         [[-0.0823,  0.0876],
          [-0.1234,  0.0616],
          [-0.0714, -0.2389],
          ...,
          [-0.1925, -0.1707],
          [ 0.1108, -0.3250],
          [ 0.1108, -0.3250]],

         [[-0.2664, -0.1681],
          [ 0.2903,  0.0469],
          [ 0.4380, -0.0209],
          ...,
          [ 0.0071,  0.0806],
          [ 0.1186, -0.1262],
          [ 0.1186, -0.1262]]],


        [[[ 0.2932, -0.2316],
          [ 0.0360, -0.2440],
          [ 0.0517,  0.0776],
          ...,
          [ 0.0854, -0.2627],
          [-0.4177, -0.3474],
          [-0.4177, -0.3474]],

         [[-0.3008, -0.0613],
          [-0.3604,  0.4441],
          [-0.1606, -0.2593],
          ...,
          [ 0.4512, -0.3750],
          [-0.2603,  0.0291],
          [-0.2603,  0.0291]],

         [[ 0.1294, -0.1705],
          [ 0.1411, -0.4165],
          [-0.3345,  0.2391],
          ...,
          [-0.3083, -0.4446],
          [ 0.2220, -0.1963],
          [ 0.2220, -0.1963]],

         [[-0.1688,  0.0928],
          [-0.4634,  0.1173],
          [ 0.1791, -0.1665],
          ...,
          [ 0.1814, -0.0690],
          [ 0.1460,  0.3279],
          [ 0.1460,  0.3279]],

         [[ 0.0719, -0.0894],
          [ 0.3804, -0.1740],
          [ 0.1113, -0.0913],
          ...,
          [ 0.4683,  0.2502],
          [ 0.1504, -0.3831],
          [ 0.1504, -0.3831]],

         [[-0.0779, -0.1918],
          [-0.2153,  0.1542],
          [ 0.2373, -0.0458],
          ...,
          [ 0.1738, -0.1004],
          [-0.0672,  0.2649],
          [-0.0672,  0.2649]]]], dtype=torch.float16)

2025-07-09 13:51:38.963961 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 6, 64424512, 2]) != torch.Size([2, 6, 64424509, 2]).
ACTUAL: (shape=torch.Size([2, 6, 64424512, 2]), dtype=torch.float16)
tensor([[[[-2.4353e-01, -4.4360e-01],
          [ 7.1716e-02, -1.1407e-01],
          [ 1.2622e-01, -1.1224e-01],
          ...,
          [ 2.2156e-01, -3.6060e-01],
          [ 3.0225e-01,  3.5278e-01],
          [ 3.0225e-01,  3.5278e-01]],

         [[ 1.8677e-01, -4.0894e-01],
          [-1.0010e-01, -2.8641e-02],
          [ 1.8219e-02,  1.8335e-01],
          ...,
          [ 2.5024e-01,  2.4857e-02],
          [ 2.5234e-03, -2.3145e-01],
          [ 2.5234e-03, -2.3145e-01]],

         [[ 1.2720e-01,  2.7026e-01],
          [-2.6440e-01, -3.4657e-03],
          [ 1.0236e-01,  2.6392e-01],
          ...,
          [-2.8152e-02, -9.3384e-02],
          [ 1.6565e-01, -1.3562e-01],
          [ 1.6565e-01, -1.3562e-01]],

         [[-4.0576e-01, -4.5276e-04],
          [-2.4963e-01,  6.9580e-02],
          [ 1.0864e-01,  2.3718e-01],
          ...,
          [-3.8623e-01,  3.6450e-01],
          [-1.7175e-01, -2.1448e-01],
          [-1.7175e-01, -2.1448e-01]],

         [[-2.2937e-01,  1.2427e-01],
          [ 1.2549e-01,  4.9042e-02],
          [-2.2781e-02, -3.1958e-01],
          ...,
          [ 1.3318e-01, -1.4209e-01],
          [ 2.3413e-01, -2.1606e-01],
          [ 2.3413e-01, -2.1606e-01]],

         [[-2.2046e-01, -2.8394e-01],
          [ 1.8896e-01,  1.1426e-01],
          [ 4.1870e-01, -8.4717e-02],
          ...,
          [ 2.2839e-01,  1.7273e-01],
          [ 1.4612e-01,  2.8345e-01],
          [ 1.4612e-01,  2.8345e-01]]],


        [[[ 4.0112e-01, -1.4282e-01],
          [-7.3181e-02, -4.5776e-01],
          [ 3.0563e-02, -3.5980e-02],
          ...,
          [-4.1138e-01, -3.9209e-01],
          [ 1.3196e-01, -2.7832e-01],
          [ 1.3196e-01, -2.7832e-01]],

         [[-1.3074e-01,  1.0620e-02],
          [-1.4514e-01,  1.7358e-01],
          [ 8.7967e-03, -2.2583e-01],
          ...,
          [-3.0859e-01, -5.0018e-02],
          [ 3.1421e-01,  2.7197e-01],
          [ 3.1421e-01,  2.7197e-01]],

         [[ 5.0476e-02,  1.5906e-01],
          [ 1.8021e-02, -1.9727e-01],
          [-2.9126e-01,  1.6199e-01],
          ...,
          [ 2.4255e-01, -1.9543e-01],
          [-3.7622e-01,  1.3596e-02],
          [-3.7622e-01,  1.3596e-02]],

         [[ 1.0963e-02,  6.1951e-02],
          [-3.8330e-01,  1.1396e-03],
          [ 5.1666e-02, -2.7734e-01],
          ...,
          [-2.5921e-03,  2.9980e-01],
          [-2.0288e-01,  1.0236e-01],
          [-2.0288e-01,  1.0236e-01]],

         [[-1.3660e-01, -8.7891e-02],
          [ 1.7969e-01, -3.1952e-02],
          [ 1.3049e-01, -2.4304e-01],
          ...,
          [ 2.5293e-01, -2.0288e-01],
          [ 1.0895e-01,  1.6785e-01],
          [ 1.0895e-01,  1.6785e-01]],

         [[-6.7322e-02, -2.8906e-01],
          [-5.0903e-02,  5.0049e-02],
          [ 4.6631e-02, -5.7587e-02],
          ...,
          [-2.5146e-01,  8.1848e-02],
          [-2.5635e-01,  4.2822e-01],
          [-2.5635e-01,  4.2822e-01]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 6, 64424509, 2]), dtype=torch.float16)
tensor([[[[-0.2435, -0.4436],
          [ 0.0717, -0.1141],
          [ 0.1262, -0.1122],
          ...,
          [ 0.2216, -0.3606],
          [-0.3479,  0.2754],
          [-0.3479,  0.2754]],

         [[ 0.1392, -0.4170],
          [-0.0315, -0.0313],
          [ 0.0131,  0.2576],
          ...,
          [ 0.3303,  0.0301],
          [-0.0299, -0.1053],
          [-0.0299, -0.1053]],

         [[ 0.2656,  0.1635],
          [-0.1951,  0.1240],
          [ 0.0626,  0.1233],
          ...,
          [ 0.1439, -0.1998],
          [-0.0795, -0.0158],
          [-0.0795, -0.0158]],

         [[-0.3140,  0.0171],
          [ 0.0044,  0.0834],
          [-0.0580, -0.0392],
          ...,
          [-0.1605,  0.3081],
          [ 0.3030, -0.0898],
          [ 0.3030, -0.0898]],

         [[-0.2220,  0.0692],
          [ 0.0369,  0.0530],
          [ 0.1753, -0.2419],
          ...,
          [ 0.1576, -0.1602],
          [ 0.1510, -0.0338],
          [ 0.1510, -0.0338]],

         [[-0.4895, -0.2255],
          [ 0.1616, -0.0153],
          [ 0.4614,  0.1460],
          ...,
          [-0.0043, -0.4805],
          [-0.3030, -0.3755],
          [-0.3030, -0.3755]]],


        [[[ 0.4011, -0.1428],
          [-0.0732, -0.4578],
          [ 0.0306, -0.0360],
          ...,
          [-0.4114, -0.3921],
          [ 0.4563, -0.4753],
          [ 0.4563, -0.4753]],

         [[-0.2383,  0.0282],
          [-0.2001,  0.1959],
          [-0.0370, -0.2622],
          ...,
          [-0.2893, -0.0184],
          [ 0.1160,  0.0148],
          [ 0.1160,  0.0148]],

         [[ 0.0260, -0.0155],
          [ 0.0914, -0.2974],
          [-0.1855,  0.0835],
          ...,
          [ 0.1930, -0.1519],
          [-0.1511,  0.0026],
          [-0.1511,  0.0026]],

         [[-0.0890, -0.1487],
          [-0.3264,  0.1760],
          [-0.0308, -0.2700],
          ...,
          [ 0.1816,  0.2429],
          [ 0.1245,  0.0325],
          [ 0.1245,  0.0325]],

         [[-0.0534, -0.0137],
          [ 0.3059, -0.1936],
          [ 0.2322, -0.1882],
          ...,
          [ 0.0287, -0.3215],
          [ 0.3037, -0.0191],
          [ 0.3037, -0.0191]],

         [[-0.3511,  0.0741],
          [-0.0923, -0.0504],
          [ 0.0012, -0.1699],
          ...,
          [ 0.1619,  0.3965],
          [ 0.0000,  0.0000],
          [ 0.0000,  0.0000]]]], dtype=torch.float16)

2025-07-09 13:51:40.496412 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=None, scale_factor=list[0.6,0.6,], mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 6, 64424512, 2]) != torch.Size([2, 6, 64424509, 2]).
ACTUAL: (shape=torch.Size([2, 6, 64424512, 2]), dtype=torch.float16)
tensor([[[[-0.1057, -0.4094],
          [ 0.0888,  0.3499],
          [ 0.3542, -0.0858],
          ...,
          [ 0.4365,  0.2367],
          [-0.2389, -0.2825],
          [-0.2389, -0.2825]],

         [[ 0.0055, -0.2898],
          [ 0.1099,  0.1620],
          [-0.2825, -0.0714],
          ...,
          [-0.3518,  0.1038],
          [-0.2067, -0.3174],
          [-0.2067, -0.3174]],

         [[-0.2258, -0.0234],
          [ 0.1654,  0.0832],
          [ 0.3584,  0.1176],
          ...,
          [-0.3589, -0.0880],
          [-0.3921,  0.2258],
          [-0.3921,  0.2258]],

         [[-0.3064,  0.3052],
          [-0.0237, -0.1244],
          [ 0.0439, -0.0083],
          ...,
          [ 0.0311,  0.1742],
          [-0.3054, -0.1976],
          [-0.3054, -0.1976]],

         [[-0.0132,  0.0928],
          [ 0.0673, -0.0065],
          [ 0.1572,  0.0187],
          ...,
          [-0.3044,  0.1667],
          [-0.1127,  0.2018],
          [-0.1127,  0.2018]],

         [[ 0.2703,  0.0039],
          [-0.0153, -0.0395],
          [-0.2069,  0.2969],
          ...,
          [ 0.2888, -0.0159],
          [-0.2454, -0.3264],
          [-0.2454, -0.3264]]],


        [[[-0.0324, -0.2262],
          [-0.2805, -0.3645],
          [ 0.0624,  0.2742],
          ...,
          [-0.2781, -0.1654],
          [-0.1580,  0.2603],
          [-0.1580,  0.2603]],

         [[ 0.3625,  0.1738],
          [-0.2288, -0.2505],
          [ 0.2444, -0.1555],
          ...,
          [ 0.2651, -0.2499],
          [ 0.3296, -0.1098],
          [ 0.3296, -0.1098]],

         [[ 0.1560,  0.4263],
          [ 0.1417,  0.0289],
          [ 0.0147, -0.0494],
          ...,
          [ 0.0800,  0.2098],
          [ 0.1827, -0.2463],
          [ 0.1827, -0.2463]],

         [[ 0.1721, -0.2205],
          [-0.3904,  0.0863],
          [ 0.0118, -0.0206],
          ...,
          [ 0.3428,  0.0135],
          [-0.3066, -0.0800],
          [-0.3066, -0.0800]],

         [[-0.0792, -0.4326],
          [ 0.0398,  0.1326],
          [ 0.1212, -0.1033],
          ...,
          [ 0.3015, -0.2844],
          [ 0.0052, -0.0164],
          [ 0.0052, -0.0164]],

         [[ 0.1156,  0.2832],
          [ 0.3291, -0.1707],
          [ 0.1050, -0.2253],
          ...,
          [-0.0060,  0.4331],
          [ 0.2546,  0.3916],
          [ 0.2546,  0.3916]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 6, 64424509, 2]), dtype=torch.float16)
tensor([[[[-0.1687, -0.0561],
          [ 0.2715,  0.1963],
          [ 0.2360,  0.1155],
          ...,
          [-0.2510,  0.1921],
          [ 0.2583,  0.2942],
          [ 0.2583,  0.2942]],

         [[-0.0197, -0.3408],
          [ 0.2424,  0.4263],
          [-0.3274,  0.0375],
          ...,
          [ 0.4180, -0.1788],
          [-0.4153,  0.0274],
          [-0.4153,  0.0274]],

         [[-0.1014, -0.0770],
          [ 0.1322,  0.2207],
          [ 0.4053,  0.2445],
          ...,
          [-0.0546, -0.0321],
          [-0.3547, -0.0927],
          [-0.3547, -0.0927]],

         [[-0.2013,  0.1763],
          [-0.0609, -0.1221],
          [ 0.1131, -0.0469],
          ...,
          [-0.2798, -0.0607],
          [ 0.0461,  0.1658],
          [ 0.0461,  0.1658]],

         [[-0.0721,  0.1902],
          [ 0.3970, -0.1506],
          [ 0.0561,  0.1027],
          ...,
          [ 0.4180, -0.0204],
          [-0.2620,  0.2305],
          [-0.2620,  0.2305]],

         [[ 0.0896, -0.1003],
          [-0.2083,  0.0726],
          [-0.0459,  0.1990],
          ...,
          [-0.0068, -0.2693],
          [ 0.0346, -0.0399],
          [ 0.0346, -0.0399]]],


        [[[-0.1067, -0.1930],
          [-0.2062, -0.4309],
          [-0.0321,  0.1159],
          ...,
          [ 0.0983, -0.1757],
          [-0.1918, -0.2737],
          [-0.1918, -0.2737]],

         [[ 0.1718,  0.1755],
          [-0.1854, -0.3618],
          [ 0.3152, -0.2086],
          ...,
          [ 0.3457, -0.0116],
          [ 0.3362, -0.1898],
          [ 0.3362, -0.1898]],

         [[ 0.1252,  0.2296],
          [ 0.2217,  0.0901],
          [-0.1005,  0.0771],
          ...,
          [ 0.1957,  0.1412],
          [ 0.0811,  0.2474],
          [ 0.0811,  0.2474]],

         [[-0.0548, -0.1584],
          [-0.3611,  0.1285],
          [-0.0196, -0.1490],
          ...,
          [-0.0685,  0.2024],
          [ 0.3484,  0.0496],
          [ 0.3484,  0.0496]],

         [[ 0.0510, -0.3728],
          [ 0.0505,  0.4500],
          [ 0.1228, -0.0186],
          ...,
          [-0.0921,  0.3345],
          [ 0.3579, -0.4192],
          [ 0.3579, -0.4192]],

         [[-0.0096,  0.1053],
          [ 0.2186, -0.3838],
          [ 0.2974, -0.1587],
          ...,
          [-0.1810,  0.1482],
          [ 0.0212,  0.3735],
          [ 0.0212,  0.3735]]]], dtype=torch.float16)

2025-07-09 13:51:57.320152 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,12,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,12,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 638 / 672 (94.9%)
Greatest absolute difference: 0.978515625 at index (1, 13, 5, 1) (up to 0.01 allowed)
Greatest relative difference: 270.0 at index (0, 5, 4, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 14, 12, 2]), dtype=torch.float16)
tensor([[[[-0.2435, -0.4436],
          [-0.2275, -0.2423],
          [-0.1770,  0.1558],
          ...,
          [ 0.4346,  0.2661],
          [-0.3167, -0.4148],
          [ 0.1777,  0.0558]],

         [[ 0.2339, -0.3896],
          [-0.1359, -0.3716],
          [-0.1991,  0.1642],
          ...,
          [ 0.0640, -0.0627],
          [ 0.0442, -0.3914],
          [ 0.2676, -0.0058]],

         [[ 0.2717, -0.3943],
          [ 0.0357, -0.3760],
          [ 0.0203,  0.0973],
          ...,
          [-0.0166,  0.0824],
          [ 0.0203, -0.0774],
          [ 0.2507, -0.2073]],

         ...,

         [[-0.1102, -0.2448],
          [ 0.2050, -0.2174],
          [-0.0752,  0.0285],
          ...,
          [-0.0110,  0.3201],
          [-0.2910, -0.3669],
          [-0.3005,  0.0408]],

         [[-0.3164, -0.2629],
          [ 0.1648, -0.1078],
          [ 0.1152, -0.0318],
          ...,
          [-0.1987,  0.0958],
          [-0.3081, -0.2206],
          [-0.1097, -0.0352]],

         [[-0.4895, -0.2255],
          [ 0.0850,  0.0135],
          [ 0.2627, -0.1431],
          ...,
          [-0.3523, -0.1534],
          [-0.3142, -0.0953],
          [ 0.0850, -0.1582]]],


        [[[ 0.4011, -0.1428],
          [-0.4883,  0.4402],
          [-0.1251, -0.1415],
          ...,
          [-0.2776,  0.0992],
          [-0.3977, -0.0469],
          [ 0.0653,  0.2646]],

         [[ 0.4053, -0.0961],
          [-0.1696,  0.4316],
          [-0.1650, -0.0778],
          ...,
          [ 0.2283,  0.3345],
          [-0.0463, -0.3572],
          [ 0.3347, -0.1792]],

         [[ 0.0613, -0.0208],
          [-0.2290,  0.1442],
          [ 0.0214, -0.1197],
          ...,
          [ 0.0439,  0.2681],
          [ 0.0491, -0.0908],
          [ 0.3103, -0.0245]],

         ...,

         [[ 0.0517, -0.3889],
          [ 0.3564,  0.4426],
          [ 0.1722, -0.0928],
          ...,
          [-0.1748, -0.3728],
          [ 0.1801, -0.0699],
          [ 0.1624,  0.0140]],

         [[-0.1687, -0.1593],
          [ 0.2114,  0.1127],
          [ 0.2698,  0.1912],
          ...,
          [-0.1814, -0.1499],
          [ 0.1376,  0.0749],
          [ 0.0889,  0.2133]],

         [[-0.3511,  0.0741],
          [ 0.0580, -0.1719],
          [ 0.2627,  0.3958],
          ...,
          [-0.1760,  0.0402],
          [ 0.0731,  0.2300],
          [ 0.0238,  0.3188]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 14, 12, 2]), dtype=torch.float16)
tensor([[[[-2.4353e-01, -4.4360e-01],
          [ 5.1788e-02,  1.8225e-01],
          [-4.5557e-01, -7.8613e-02],
          ...,
          [ 4.8145e-01, -3.9575e-01],
          [-1.8506e-01,  4.7876e-01],
          [ 2.2156e-01, -3.6060e-01]],

         [[ 2.1912e-01, -3.9136e-01],
          [ 1.3867e-01,  2.1887e-01],
          [-2.0935e-01, -1.5710e-01],
          ...,
          [-1.2683e-01, -4.4751e-01],
          [-1.9568e-01,  3.2690e-01],
          [-3.6285e-02, -1.1182e-01]],

         [[ 2.8760e-01, -3.9160e-01],
          [ 1.8750e-01,  1.1530e-01],
          [ 7.2021e-03, -2.4609e-01],
          ...,
          [-1.2219e-01, -3.0249e-01],
          [-6.7139e-02,  2.9175e-01],
          [ 8.0505e-02,  1.3809e-02]],

         ...,

         [[-1.5137e-01, -1.2927e-01],
          [ 1.6101e-01,  1.6748e-01],
          [-3.8867e-01,  1.7297e-01],
          ...,
          [ 2.2537e-02,  1.4612e-01],
          [ 1.0309e-01,  2.4792e-01],
          [ 2.5464e-01,  1.8225e-01]],

         [[-2.1008e-01, -2.8613e-01],
          [ 4.6045e-01,  1.1584e-01],
          [-3.9624e-01,  2.3181e-01],
          ...,
          [-2.3499e-01,  1.1902e-01],
          [ 3.0908e-01,  5.4962e-02],
          [ 2.3730e-01,  1.9775e-01]],

         [[-4.8950e-01, -2.2546e-01],
          [ 4.8853e-01,  2.1863e-01],
          [-2.0911e-01,  4.8438e-01],
          ...,
          [-2.9248e-01,  4.8486e-01],
          [ 1.9385e-01, -1.5125e-01],
          [-4.3106e-03, -4.8047e-01]]],


        [[[ 4.0112e-01, -1.4282e-01],
          [-3.8525e-01,  3.2764e-01],
          [ 1.8213e-01, -4.4312e-01],
          ...,
          [-3.2568e-01,  4.1870e-01],
          [ 3.8013e-01,  2.6099e-01],
          [-4.1138e-01, -3.9209e-01]],

         [[ 4.0527e-01, -9.7595e-02],
          [-1.9897e-01,  1.0175e-01],
          [ 3.4326e-01, -2.3975e-01],
          ...,
          [ 1.4490e-01,  1.5588e-01],
          [-1.6553e-01,  1.5210e-01],
          [-4.0723e-01, -2.6489e-01]],

         [[ 9.6741e-02, -2.6642e-02],
          [-1.8372e-01,  9.2346e-02],
          [ 1.8750e-01,  8.5815e-02],
          ...,
          [ 3.7402e-01, -1.0687e-01],
          [-4.3604e-01, -8.1055e-02],
          [-3.4961e-01, -1.1700e-01]],

         ...,

         [[ 1.3054e-02, -2.5098e-01],
          [-4.2700e-01,  3.3057e-01],
          [-2.4182e-01,  3.4302e-01],
          ...,
          [ 2.8458e-02,  3.1143e-02],
          [-1.3391e-01,  1.0822e-01],
          [-2.2412e-01, -1.9373e-01]],

         [[-5.6396e-02, -3.0298e-01],
          [-3.5547e-01,  1.3855e-01],
          [-2.6440e-01,  4.0161e-01],
          ...,
          [-1.6736e-01,  3.6682e-02],
          [-2.0859e-02,  1.1719e-01],
          [-2.6733e-01,  6.9763e-02]],

         [[-3.5107e-01,  7.4097e-02],
          [-2.1399e-01, -1.6919e-01],
          [-3.3035e-03,  4.3628e-01],
          ...,
          [-2.6221e-01, -2.3234e-04],
          [-1.6394e-01,  1.0077e-01],
          [ 1.6187e-01,  3.9648e-01]]]], dtype=torch.float16)

2025-07-09 13:52:00.582045 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,12,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,12,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 654 / 672 (97.3%)
Greatest absolute difference: 0.9580078125 at index (1, 3, 2, 0) (up to 0.01 allowed)
Greatest relative difference: 161.5 at index (0, 11, 9, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 14, 12, 2]), dtype=torch.float16)
tensor([[[[-0.1057, -0.4094],
          [ 0.2338,  0.4050],
          [ 0.2352, -0.1118],
          ...,
          [ 0.1193,  0.3950],
          [-0.1191, -0.2632],
          [ 0.4365,  0.2367]],

         [[-0.1127, -0.2097],
          [ 0.1433, -0.0190],
          [ 0.0091,  0.2480],
          ...,
          [ 0.1967,  0.1539],
          [-0.2866, -0.3635],
          [ 0.0663,  0.3562]],

         [[-0.0575, -0.2021],
          [ 0.1040, -0.0282],
          [-0.0599,  0.1257],
          ...,
          [ 0.2109,  0.1144],
          [-0.1392, -0.1626],
          [-0.2202,  0.2625]],

         ...,

         [[-0.0725, -0.1631],
          [ 0.3782, -0.0721],
          [ 0.3037,  0.1978],
          ...,
          [-0.3276,  0.1742],
          [-0.2140, -0.0092],
          [-0.3923,  0.0345]],

         [[-0.0051, -0.2756],
          [ 0.2544, -0.0750],
          [ 0.1897,  0.2288],
          ...,
          [-0.1296,  0.1202],
          [-0.0816, -0.0473],
          [-0.2393, -0.0659]],

         [[ 0.2703,  0.0039],
          [-0.0188, -0.1996],
          [-0.2744, -0.1776],
          ...,
          [ 0.2131,  0.1936],
          [ 0.2759,  0.4226],
          [ 0.2888, -0.0159]]],


        [[[-0.0324, -0.2262],
          [ 0.2106, -0.1111],
          [-0.0071,  0.3545],
          ...,
          [ 0.4412, -0.0484],
          [-0.2837,  0.3977],
          [-0.2781, -0.1654]],

         [[-0.0676, -0.3418],
          [-0.2101, -0.3508],
          [-0.1718,  0.3916],
          ...,
          [ 0.4690,  0.1660],
          [-0.1987, -0.0575],
          [-0.0989, -0.3901]],

         [[ 0.1311, -0.1206],
          [-0.1339, -0.4587],
          [ 0.0363,  0.1544],
          ...,
          [ 0.1231,  0.2180],
          [ 0.0144, -0.1063],
          [ 0.1174, -0.3745]],

         ...,

         [[-0.0697, -0.4285],
          [-0.2023, -0.1305],
          [-0.1323, -0.2634],
          ...,
          [ 0.0063, -0.1398],
          [ 0.0296,  0.0539],
          [ 0.1842, -0.0047]],

         [[-0.0066, -0.2068],
          [-0.0819, -0.0328],
          [ 0.0271, -0.1450],
          ...,
          [-0.0246, -0.0090],
          [-0.1132,  0.0507],
          [ 0.0505,  0.3093]],

         [[ 0.1156,  0.2832],
          [ 0.0854,  0.1527],
          [-0.0659,  0.3196],
          ...,
          [-0.1561, -0.0029],
          [-0.4968, -0.3250],
          [-0.0060,  0.4331]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 14, 12, 2]), dtype=torch.float16)
tensor([[[[ 0.0475,  0.4524],
          [ 0.4658,  0.2520],
          [-0.4282,  0.3843],
          ...,
          [ 0.1611, -0.1135],
          [ 0.2788, -0.3164],
          [-0.1199, -0.3406]],

         [[ 0.1048,  0.0429],
          [ 0.3933,  0.3440],
          [-0.1523,  0.3201],
          ...,
          [-0.0649,  0.1742],
          [ 0.2788, -0.0682],
          [-0.0819,  0.0312]],

         [[ 0.0147, -0.2996],
          [ 0.3560,  0.3181],
          [ 0.0873,  0.3025],
          ...,
          [-0.0410,  0.1431],
          [ 0.1105, -0.0081],
          [-0.0974,  0.3506]],

         ...,

         [[ 0.3655, -0.2793],
          [-0.3096, -0.1512],
          [ 0.1255,  0.3215],
          ...,
          [ 0.0020,  0.1885],
          [-0.1459, -0.0958],
          [-0.1488, -0.2898]],

         [[ 0.3389, -0.3435],
          [-0.0428, -0.1794],
          [ 0.2035,  0.1716],
          ...,
          [-0.0314,  0.3787],
          [-0.0305, -0.0392],
          [-0.2018, -0.2147]],

         [[ 0.2639, -0.4248],
          [ 0.4324,  0.0098],
          [ 0.3467,  0.0366],
          ...,
          [ 0.0308,  0.3374],
          [ 0.0298,  0.3379],
          [-0.4187, -0.2249]]],


        [[[-0.1298, -0.4761],
          [-0.1763, -0.2072],
          [-0.2529,  0.4868],
          ...,
          [ 0.4214, -0.3875],
          [ 0.4177,  0.1000],
          [ 0.0712, -0.4402]],

         [[ 0.2164,  0.0606],
          [-0.3447, -0.1375],
          [-0.1641,  0.2815],
          ...,
          [ 0.0296, -0.4270],
          [ 0.3767,  0.1299],
          [ 0.2288,  0.0464]],

         [[ 0.2268,  0.3630],
          [-0.3413, -0.0448],
          [-0.2092,  0.2183],
          ...,
          [-0.2981, -0.1982],
          [ 0.3894,  0.1083],
          [ 0.1919,  0.3853]],

         ...,

         [[-0.1136, -0.3315],
          [-0.1831,  0.3809],
          [-0.2673, -0.1061],
          ...,
          [ 0.1133, -0.2218],
          [ 0.2063,  0.2808],
          [ 0.0779,  0.0773]],

         [[-0.0721, -0.0805],
          [-0.0724,  0.1050],
          [-0.0872, -0.1294],
          ...,
          [ 0.1288, -0.0661],
          [ 0.0731,  0.2032],
          [-0.0140,  0.3240]],

         [[-0.1956,  0.1732],
          [ 0.0396, -0.4050],
          [ 0.3562, -0.1567],
          ...,
          [-0.0629,  0.2031],
          [-0.1682,  0.1328],
          [ 0.0108,  0.3857]]]], dtype=torch.float16)

2025-07-09 13:52:06.538890 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 88 / 112 (78.6%)
Greatest absolute difference: 0.685546875 at index (1, 0, 1, 1) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 13, 1, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 14, 2, 2]), dtype=torch.float16)
tensor([[[[-0.2435, -0.4436],
          [-0.3542,  0.3794]],

         [[ 0.2339, -0.3896],
          [-0.3682,  0.0268]],

         [[ 0.2717, -0.3943],
          [-0.3237, -0.1674]],

         ...,

         [[-0.1102, -0.2448],
          [ 0.2152,  0.3608]],

         [[-0.3164, -0.2629],
          [ 0.1146,  0.0282]],

         [[-0.4895, -0.2255],
          [-0.0453, -0.2576]]],


        [[[ 0.4011, -0.1428],
          [-0.1664,  0.2101]],

         [[ 0.4053, -0.0961],
          [-0.2852,  0.3835]],

         [[ 0.0613, -0.0208],
          [-0.0125,  0.4460]],

         ...,

         [[ 0.0517, -0.3889],
          [-0.2334,  0.0734]],

         [[-0.1687, -0.1593],
          [ 0.0430,  0.1675]],

         [[-0.3511,  0.0741],
          [ 0.2649,  0.2285]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 14, 2, 2]), dtype=torch.float16)
tensor([[[[-0.2435, -0.4436],
          [-0.3479,  0.2754]],

         [[ 0.2191, -0.3914],
          [ 0.2251,  0.0388]],

         [[ 0.2876, -0.3916],
          [ 0.2347, -0.0851]],

         ...,

         [[-0.1514, -0.1293],
          [ 0.0030,  0.0562]],

         [[-0.2101, -0.2861],
          [-0.1859, -0.0190]],

         [[-0.4895, -0.2255],
          [-0.3030, -0.3755]]],


        [[[ 0.4011, -0.1428],
          [ 0.4563, -0.4753]],

         [[ 0.4053, -0.0976],
          [ 0.0529, -0.2847]],

         [[ 0.0967, -0.0266],
          [-0.0099, -0.0967]],

         ...,

         [[ 0.0131, -0.2510],
          [ 0.1675, -0.2478]],

         [[-0.0564, -0.3030],
          [ 0.0287, -0.3181]],

         [[-0.3511,  0.0741],
          [ 0.0000,  0.0000]]]], dtype=torch.float16)

2025-07-09 13:52:11.452934 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 108 / 112 (96.4%)
Greatest absolute difference: 0.78515625 at index (0, 2, 1, 1) (up to 0.01 allowed)
Greatest relative difference: 245.0 at index (0, 1, 0, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 14, 2, 2]), dtype=torch.float16)
tensor([[[[-0.1057, -0.4094],
          [-0.2389, -0.2825]],

         [[-0.1127, -0.2097],
          [-0.3621, -0.3728]],

         [[-0.0575, -0.2021],
          [-0.3157, -0.3669]],

         ...,

         [[-0.0725, -0.1631],
          [-0.1221, -0.1383]],

         [[-0.0051, -0.2756],
          [-0.1660, -0.4141]],

         [[ 0.2703,  0.0039],
          [-0.2454, -0.3264]]],


        [[[-0.0324, -0.2262],
          [-0.1580,  0.2603]],

         [[-0.0676, -0.3418],
          [-0.0446,  0.0496]],

         [[ 0.1311, -0.1206],
          [ 0.1614, -0.0757]],

         ...,

         [[-0.0697, -0.4285],
          [-0.2067,  0.0210]],

         [[-0.0066, -0.2068],
          [-0.2006,  0.1589]],

         [[ 0.1156,  0.2832],
          [ 0.2546,  0.3916]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 14, 2, 2]), dtype=torch.float16)
tensor([[[[-1.1603e-01,  2.8979e-01],
          [-2.0569e-01, -4.5337e-01]],

         [[ 4.6206e-04,  1.8542e-01],
          [-8.4595e-02,  7.3669e-02]],

         [[ 1.1908e-01,  1.3306e-02],
          [-1.3721e-01,  4.1797e-01]],

         ...,

         [[ 3.0981e-01, -1.4832e-02],
          [ 2.2629e-02,  1.9983e-01]],

         [[ 4.0796e-01, -6.9702e-02],
          [-1.8713e-01,  2.4902e-02]],

         [[ 4.4312e-01, -2.1240e-01],
          [-2.4561e-01, -3.3740e-01]]],


        [[[-1.4258e-01, -1.5088e-01],
          [-2.2400e-01,  2.3328e-01]],

         [[ 1.6394e-01, -8.2321e-03],
          [-2.4695e-01,  2.5049e-01]],

         [[ 2.9443e-01,  7.5150e-03],
          [-2.4927e-01,  2.0422e-01]],

         ...,

         [[ 2.3694e-01, -2.4927e-01],
          [ 4.0601e-01, -3.4009e-01]],

         [[ 2.5073e-01, -4.7900e-01],
          [ 2.6831e-01, -2.9565e-01]],

         [[ 3.8232e-01, -4.8901e-01],
          [-2.3499e-02, -2.8442e-01]]]], dtype=torch.float16)

2025-07-09 13:52:17.123350 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,22,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,22,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1177 / 1232 (95.5%)
Greatest absolute difference: 0.9599609375 at index (0, 0, 15, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 13, 21, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 14, 22, 2]), dtype=torch.float16)
tensor([[[[-0.2435, -0.4436],
          [-0.0781, -0.0872],
          [ 0.0518,  0.1823],
          ...,
          [ 0.0815, -0.0230],
          [-0.1851,  0.4788],
          [ 0.3274,  0.3552]],

         [[ 0.2339, -0.3896],
          [ 0.2230, -0.0457],
          [ 0.1415,  0.2200],
          ...,
          [-0.2539,  0.1334],
          [-0.1959,  0.3223],
          [ 0.3352, -0.0886]],

         [[ 0.2717, -0.3943],
          [ 0.1583,  0.0990],
          [ 0.1886,  0.1016],
          ...,
          [-0.4080,  0.2888],
          [-0.0519,  0.2954],
          [ 0.2450, -0.0494]],

         ...,

         [[-0.1102, -0.2448],
          [-0.0969,  0.1968],
          [ 0.3413,  0.1063],
          ...,
          [ 0.1042, -0.1301],
          [ 0.2649,  0.1842],
          [-0.1113,  0.2344]],

         [[-0.3164, -0.2629],
          [ 0.0035,  0.2642],
          [ 0.4712,  0.1550],
          ...,
          [ 0.1716,  0.0388],
          [ 0.2651, -0.0236],
          [ 0.0230,  0.2542]],

         [[-0.4895, -0.2255],
          [ 0.0990,  0.2864],
          [ 0.4885,  0.2186],
          ...,
          [ 0.1570,  0.1398],
          [ 0.1938, -0.1512],
          [ 0.1881,  0.2988]]],


        [[[ 0.4011, -0.1428],
          [ 0.2335,  0.1785],
          [-0.3853,  0.3276],
          ...,
          [-0.4824, -0.0620],
          [ 0.3801,  0.2610],
          [ 0.2275,  0.4258]],

         [[ 0.4053, -0.0961],
          [-0.0035, -0.0287],
          [-0.1931,  0.0945],
          ...,
          [-0.1324,  0.0031],
          [-0.1827,  0.1487],
          [ 0.0563,  0.1683]],

         [[ 0.0613, -0.0208],
          [-0.1321,  0.0222],
          [-0.1915,  0.1028],
          ...,
          [ 0.1644, -0.1133],
          [-0.4395, -0.1022],
          [ 0.0605,  0.0358]],

         ...,

         [[ 0.0517, -0.3889],
          [-0.1595,  0.1238],
          [-0.4214,  0.2959],
          ...,
          [ 0.0036, -0.3508],
          [-0.0229,  0.1185],
          [ 0.2961, -0.2410]],

         [[-0.1687, -0.1593],
          [ 0.1345,  0.2566],
          [-0.3015,  0.0213],
          ...,
          [-0.2025, -0.3679],
          [-0.0754,  0.1110],
          [ 0.3186, -0.0339]],

         [[-0.3511,  0.0741],
          [ 0.3374,  0.3616],
          [-0.2140, -0.1692],
          ...,
          [-0.3335, -0.3174],
          [-0.1639,  0.1008],
          [ 0.3584,  0.2050]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 14, 22, 2]), dtype=torch.float16)
tensor([[[[-0.2435, -0.4436],
          [ 0.1169,  0.1058],
          [-0.1904,  0.1842],
          ...,
          [ 0.1338,  0.4023],
          [ 0.4111, -0.2014],
          [-0.3479,  0.2754]],

         [[ 0.2191, -0.3914],
          [ 0.0150, -0.1459],
          [ 0.2476, -0.2217],
          ...,
          [ 0.0867,  0.1752],
          [-0.1782,  0.0415],
          [ 0.2251,  0.0388]],

         [[ 0.2876, -0.3916],
          [-0.0823, -0.1361],
          [ 0.1277, -0.3149],
          ...,
          [-0.0856,  0.0189],
          [-0.4373,  0.0620],
          [ 0.2347, -0.0851]],

         ...,

         [[-0.1514, -0.1293],
          [ 0.0311,  0.1343],
          [-0.1067, -0.0278],
          ...,
          [ 0.0912,  0.4304],
          [ 0.2632,  0.4050],
          [ 0.0030,  0.0562]],

         [[-0.2101, -0.2861],
          [-0.1185,  0.3032],
          [-0.2323, -0.0358],
          ...,
          [-0.1449,  0.3523],
          [ 0.3857,  0.2957],
          [-0.1859, -0.0190]],

         [[-0.4895, -0.2255],
          [-0.2368,  0.2607],
          [-0.3840, -0.2361],
          ...,
          [-0.3430,  0.1846],
          [ 0.3167, -0.1582],
          [-0.3030, -0.3755]]],


        [[[ 0.4011, -0.1428],
          [-0.4185,  0.0690],
          [ 0.4539,  0.2153],
          ...,
          [ 0.4116, -0.1776],
          [-0.1746,  0.1232],
          [ 0.4563, -0.4753]],

         [[ 0.4053, -0.0976],
          [ 0.0170, -0.0937],
          [ 0.4087,  0.0283],
          ...,
          [ 0.1661, -0.2947],
          [-0.1305,  0.0773],
          [ 0.0529, -0.2847]],

         [[ 0.0967, -0.0266],
          [ 0.2230, -0.0475],
          [ 0.2408, -0.1827],
          ...,
          [ 0.0455, -0.0649],
          [-0.0316, -0.0928],
          [-0.0099, -0.0967]],

         ...,

         [[ 0.0131, -0.2510],
          [ 0.0691,  0.0048],
          [-0.2180, -0.1503],
          ...,
          [ 0.3274, -0.0534],
          [ 0.1085,  0.3354],
          [ 0.1675, -0.2478]],

         [[-0.0564, -0.3030],
          [-0.0120,  0.1384],
          [ 0.0506, -0.1840],
          ...,
          [ 0.2412, -0.3054],
          [-0.0056,  0.3030],
          [ 0.0287, -0.3181]],

         [[-0.3511,  0.0741],
          [ 0.0716,  0.0967],
          [ 0.3894, -0.2839],
          ...,
          [-0.2064, -0.2869],
          [-0.2776,  0.1187],
          [ 0.0000,  0.0000]]]], dtype=torch.float16)

2025-07-09 13:52:23.690047 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,22,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[14,22,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1195 / 1232 (97.0%)
Greatest absolute difference: 0.9111328125 at index (1, 10, 3, 1) (up to 0.01 allowed)
Greatest relative difference: 151.875 at index (1, 4, 19, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 14, 22, 2]), dtype=torch.float16)
tensor([[[[-0.1057, -0.4094],
          [ 0.0151, -0.1070],
          [ 0.1151, -0.1924],
          ...,
          [-0.0746, -0.4790],
          [ 0.1759,  0.3135],
          [-0.2389, -0.2825]],

         [[-0.1127, -0.2097],
          [ 0.1875, -0.0922],
          [ 0.3037,  0.1345],
          ...,
          [-0.2908,  0.0270],
          [-0.1478,  0.1063],
          [-0.3621, -0.3728]],

         [[-0.0575, -0.2021],
          [ 0.0726,  0.0556],
          [ 0.3357,  0.2507],
          ...,
          [-0.1617,  0.0753],
          [-0.2700,  0.0868],
          [-0.3157, -0.3669]],

         ...,

         [[-0.0725, -0.1631],
          [-0.0316, -0.2651],
          [ 0.0602,  0.0462],
          ...,
          [-0.1384,  0.3159],
          [-0.0545,  0.1754],
          [-0.1221, -0.1383]],

         [[-0.0051, -0.2756],
          [ 0.0400, -0.2203],
          [-0.0131,  0.1993],
          ...,
          [-0.3142,  0.2744],
          [-0.3188,  0.0898],
          [-0.1660, -0.4141]],

         [[ 0.2703,  0.0039],
          [ 0.0140,  0.0126],
          [ 0.0449, -0.0647],
          ...,
          [-0.3999,  0.3708],
          [-0.2224, -0.4053],
          [-0.2454, -0.3264]]],


        [[[-0.0324, -0.2262],
          [ 0.1764, -0.1675],
          [ 0.2020,  0.1749],
          ...,
          [-0.0311,  0.3733],
          [-0.3452,  0.4092],
          [-0.1580,  0.2603]],

         [[-0.0676, -0.3418],
          [ 0.2223,  0.0729],
          [-0.0839,  0.1298],
          ...,
          [-0.3259,  0.2183],
          [-0.0008,  0.0491],
          [-0.0446,  0.0496]],

         [[ 0.1311, -0.1206],
          [ 0.1033,  0.0126],
          [-0.2952,  0.1019],
          ...,
          [-0.3652,  0.1857],
          [ 0.1812,  0.0997],
          [ 0.1614, -0.0757]],

         ...,

         [[-0.0697, -0.4285],
          [-0.1229, -0.0160],
          [-0.0438, -0.4312],
          ...,
          [ 0.3562, -0.0903],
          [ 0.3020,  0.0580],
          [-0.2067,  0.0210]],

         [[-0.0066, -0.2068],
          [ 0.1201,  0.1018],
          [-0.0774, -0.3611],
          ...,
          [ 0.3999, -0.0737],
          [ 0.1305, -0.1095],
          [-0.2006,  0.1589]],

         [[ 0.1156,  0.2832],
          [ 0.2109,  0.4761],
          [-0.2046, -0.2861],
          ...,
          [ 0.3501, -0.2140],
          [-0.4453,  0.0975],
          [ 0.2546,  0.3916]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 14, 22, 2]), dtype=torch.float16)
tensor([[[[-0.0540, -0.3284],
          [ 0.3877,  0.3555],
          [ 0.4075,  0.4053],
          ...,
          [ 0.1187,  0.2678],
          [-0.0396,  0.2461],
          [-0.1962, -0.4197]],

         [[ 0.0376, -0.3333],
          [ 0.0936,  0.4348],
          [-0.0578,  0.0032],
          ...,
          [ 0.1528,  0.0196],
          [-0.0281,  0.3630],
          [ 0.1924, -0.1572]],

         [[ 0.0463, -0.2118],
          [ 0.0207,  0.2559],
          [-0.3826, -0.1516],
          ...,
          [ 0.1819, -0.0414],
          [ 0.0580,  0.3928],
          [ 0.2383, -0.0711]],

         ...,

         [[-0.0362,  0.2722],
          [-0.0343,  0.1932],
          [-0.1583,  0.4321],
          ...,
          [ 0.0692,  0.1006],
          [-0.1105,  0.0168],
          [-0.2251,  0.0516]],

         [[-0.0035,  0.1212],
          [-0.0144,  0.2705],
          [-0.1191,  0.1851],
          ...,
          [ 0.0628, -0.1644],
          [-0.0322, -0.1105],
          [-0.0147, -0.1621]],

         [[-0.1306, -0.2686],
          [-0.2047,  0.3821],
          [ 0.0289, -0.1174],
          ...,
          [-0.1039, -0.3506],
          [-0.1026, -0.2712],
          [ 0.2981, -0.3682]]],


        [[[-0.0962, -0.2957],
          [-0.1750, -0.3530],
          [ 0.1038, -0.1186],
          ...,
          [-0.4033,  0.3616],
          [-0.3293, -0.3362],
          [-0.4878,  0.4717]],

         [[-0.0538, -0.0849],
          [-0.1901, -0.0166],
          [ 0.2427, -0.1202],
          ...,
          [-0.0049,  0.3005],
          [-0.0076, -0.0115],
          [-0.4309,  0.2856]],

         [[-0.0092,  0.1083],
          [-0.1760,  0.1593],
          [ 0.2047, -0.1293],
          ...,
          [ 0.0839,  0.1094],
          [ 0.1539,  0.0501],
          [-0.2070,  0.1091]],

         ...,

         [[-0.0704, -0.1523],
          [ 0.0447, -0.3457],
          [-0.2854,  0.0781],
          ...,
          [-0.2260,  0.0455],
          [ 0.2930,  0.2822],
          [-0.2483,  0.2050]],

         [[-0.1655, -0.2717],
          [ 0.0547, -0.0788],
          [-0.0987, -0.1725],
          ...,
          [-0.2996,  0.0550],
          [ 0.0232,  0.2854],
          [-0.1406,  0.0782]],

         [[-0.0349, -0.1760],
          [ 0.2737,  0.4099],
          [ 0.0595, -0.3647],
          ...,
          [-0.4575,  0.2432],
          [-0.2347,  0.1001],
          [ 0.1405, -0.1227]]]], dtype=torch.float16)

2025-07-09 13:52:26.694761 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,12,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,12,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 91 / 96 (94.8%)
Greatest absolute difference: 0.9619140625 at index (0, 1, 8, 1) (up to 0.01 allowed)
Greatest relative difference: 1697.0 at index (1, 1, 9, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 2, 12, 2]), dtype=torch.float16)
tensor([[[[-2.4353e-01, -4.4360e-01],
          [-2.2754e-01, -2.4231e-01],
          [-1.7700e-01,  1.5576e-01],
          [ 1.9861e-01, -5.3940e-03],
          [ 2.5711e-02,  1.8417e-02],
          [-4.6924e-01, -3.0664e-01],
          [ 1.9348e-01,  8.5938e-02],
          [-2.8833e-01, -3.4570e-01],
          [-2.6489e-01,  1.5210e-01],
          [ 4.3457e-01,  2.6611e-01],
          [-3.1665e-01, -4.1479e-01],
          [ 1.7773e-01,  5.5756e-02]],

         [[-4.0576e-01, -4.5276e-04],
          [ 2.9810e-01,  4.8071e-01],
          [-3.7183e-01, -1.4221e-01],
          [ 3.1738e-01, -1.6357e-01],
          [-1.8530e-01, -4.6802e-01],
          [ 1.7929e-02,  1.6687e-01],
          [-1.0132e-01, -4.1284e-01],
          [ 2.8271e-01, -2.7075e-01],
          [-8.9722e-02,  4.8096e-01],
          [-2.2461e-01,  2.1045e-01],
          [-1.8372e-01,  1.2061e-01],
          [ 1.7261e-01,  2.7246e-01]]],


        [[[ 4.0112e-01, -1.4282e-01],
          [-4.8828e-01,  4.4019e-01],
          [-1.2512e-01, -1.4148e-01],
          [ 1.9482e-01,  4.1211e-01],
          [-2.8638e-01, -9.2407e-02],
          [-5.7640e-03, -4.9164e-02],
          [ 4.2188e-01,  2.6074e-01],
          [ 3.5156e-01,  1.7480e-01],
          [ 1.6809e-01, -3.5547e-01],
          [-2.7759e-01,  9.9182e-02],
          [-3.9771e-01, -4.6936e-02],
          [ 6.5308e-02,  2.6465e-01]],

         [[ 1.0963e-02,  6.1951e-02],
          [ 4.5020e-01,  9.5764e-02],
          [ 1.0925e-01, -7.4280e-02],
          [ 1.5747e-01,  1.8896e-01],
          [ 2.1130e-01, -2.0935e-01],
          [ 2.4426e-01,  2.1729e-01],
          [ 1.5808e-01,  2.4768e-01],
          [ 3.9307e-01,  3.6560e-02],
          [ 6.7688e-02, -1.8774e-01],
          [-3.5034e-01,  3.9404e-01],
          [ 1.3733e-01,  1.4465e-01],
          [-2.7271e-01,  3.2959e-01]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 2, 12, 2]), dtype=torch.float16)
tensor([[[[-2.4353e-01, -4.4360e-01],
          [ 5.1788e-02,  1.8225e-01],
          [-4.5557e-01, -7.8613e-02],
          [ 2.7417e-01,  2.2754e-01],
          [ 1.3123e-01, -2.0447e-01],
          [-2.7979e-01, -2.5366e-01],
          [ 2.6947e-02, -3.4302e-01],
          [-4.8071e-01,  2.3315e-01],
          [-1.0431e-01,  4.5020e-01],
          [ 4.8145e-01, -3.9575e-01],
          [-1.8506e-01,  4.7876e-01],
          [ 2.2156e-01, -3.6060e-01]],

         [[-4.8950e-01, -2.2546e-01],
          [ 4.8853e-01,  2.1863e-01],
          [-2.0911e-01,  4.8438e-01],
          [-3.0441e-02,  1.8274e-01],
          [-2.6514e-01, -4.5483e-01],
          [ 3.5645e-01,  2.0325e-02],
          [ 4.5581e-01, -4.3774e-01],
          [-4.3115e-01,  3.0469e-01],
          [-2.7686e-01, -4.8096e-01],
          [-2.9248e-01,  4.8486e-01],
          [ 1.9385e-01, -1.5125e-01],
          [-4.3106e-03, -4.8047e-01]]],


        [[[ 4.0112e-01, -1.4282e-01],
          [-3.8525e-01,  3.2764e-01],
          [ 1.8213e-01, -4.4312e-01],
          [-5.3833e-02, -1.9348e-01],
          [ 3.6060e-01, -2.0483e-01],
          [-1.3464e-01,  1.6064e-01],
          [-2.8275e-02,  5.6793e-02],
          [-4.9048e-01, -1.5405e-01],
          [-2.7783e-01, -4.7168e-01],
          [-3.2568e-01,  4.1870e-01],
          [ 3.8013e-01,  2.6099e-01],
          [-4.1138e-01, -3.9209e-01]],

         [[-3.5107e-01,  7.4097e-02],
          [-2.1399e-01, -1.6919e-01],
          [-3.3035e-03,  4.3628e-01],
          [ 2.3669e-01, -1.0400e-01],
          [-2.7466e-01,  3.4058e-01],
          [-4.0259e-01,  4.9146e-01],
          [-4.3701e-01, -1.3074e-01],
          [ 4.1187e-01,  4.2822e-01],
          [-2.3462e-01, -1.7090e-01],
          [-2.6221e-01, -2.3234e-04],
          [-1.6394e-01,  1.0077e-01],
          [ 1.6187e-01,  3.9648e-01]]]], dtype=torch.float16)

2025-07-09 13:52:27.380084 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,12,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,12,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 93 / 96 (96.9%)
Greatest absolute difference: 0.8359375 at index (1, 1, 9, 1) (up to 0.01 allowed)
Greatest relative difference: 171.875 at index (1, 1, 3, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 2, 12, 2]), dtype=torch.float16)
tensor([[[[ 0.2754,  0.0271],
          [-0.0606,  0.2705],
          [ 0.1631,  0.0696],
          [-0.2683,  0.0014],
          [ 0.2045, -0.2281],
          [-0.2737, -0.3091],
          [ 0.3716, -0.2725],
          [-0.4617,  0.0833],
          [ 0.3940,  0.4829],
          [ 0.1786, -0.1589],
          [-0.2094, -0.2837],
          [-0.2717,  0.0435]],

         [[ 0.4543, -0.4351],
          [ 0.1242,  0.2585],
          [ 0.2356, -0.1094],
          [ 0.0594,  0.0756],
          [ 0.4304,  0.4165],
          [ 0.3096,  0.1076],
          [ 0.1312,  0.2832],
          [ 0.0384, -0.0403],
          [-0.2634,  0.3318],
          [ 0.4707,  0.4849],
          [-0.3086,  0.0279],
          [ 0.0454,  0.1215]]],


        [[[-0.3533,  0.3630],
          [-0.2385, -0.4575],
          [ 0.1497,  0.3081],
          [-0.0104, -0.1919],
          [ 0.2517,  0.0840],
          [ 0.2961, -0.2300],
          [-0.1417, -0.3831],
          [ 0.3989, -0.0720],
          [-0.0919, -0.2347],
          [ 0.2446,  0.2891],
          [-0.0518, -0.4658],
          [-0.0626, -0.2871]],

         [[-0.2976, -0.4846],
          [-0.4148,  0.1206],
          [ 0.3821, -0.4900],
          [ 0.0916, -0.1630],
          [ 0.3035, -0.1228],
          [-0.0334, -0.0111],
          [-0.4854, -0.2607],
          [-0.2505,  0.0663],
          [ 0.1748, -0.1209],
          [ 0.2362, -0.4910],
          [-0.4763, -0.2329],
          [-0.4749, -0.0420]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 2, 12, 2]), dtype=torch.float16)
tensor([[[[-0.2235,  0.0322],
          [ 0.3640,  0.0394],
          [-0.3748,  0.2440],
          [-0.2703,  0.0410],
          [-0.4585,  0.2566],
          [-0.3584,  0.1357],
          [ 0.1296,  0.3264],
          [-0.0839,  0.1334],
          [ 0.1211,  0.3252],
          [ 0.0206, -0.0818],
          [-0.1334,  0.0276],
          [-0.2006, -0.1603]],

         [[-0.0476,  0.0291],
          [-0.4541,  0.0571],
          [-0.1315,  0.2573],
          [-0.2505,  0.2328],
          [-0.3953, -0.1689],
          [ 0.3345,  0.2246],
          [-0.4375,  0.0957],
          [ 0.4561, -0.1010],
          [-0.3025,  0.0489],
          [ 0.3611,  0.0533],
          [ 0.3047,  0.4651],
          [-0.0232, -0.3743]]],


        [[[ 0.3630,  0.1443],
          [ 0.1119, -0.2617],
          [ 0.3572, -0.0220],
          [ 0.0283,  0.3130],
          [ 0.1437, -0.2357],
          [-0.3059, -0.4067],
          [ 0.1083,  0.1947],
          [ 0.4146, -0.1156],
          [-0.0220,  0.2224],
          [ 0.1466,  0.4187],
          [-0.1669,  0.1298],
          [-0.3013, -0.3022]],

         [[-0.4675, -0.4573],
          [-0.4661,  0.1595],
          [ 0.0256, -0.0096],
          [-0.4531, -0.0009],
          [ 0.1221,  0.4380],
          [-0.2083, -0.0500],
          [ 0.1884, -0.0635],
          [ 0.3311,  0.1344],
          [ 0.0223, -0.3064],
          [ 0.3877,  0.3447],
          [-0.1086,  0.4692],
          [-0.0511, -0.0511]]]], dtype=torch.float16)

2025-07-09 13:52:30.576985 GPU 4 151556 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 12 / 16 (75.0%)
Greatest absolute difference: 0.53173828125 at index (1, 0, 1, 1) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 1, 1, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 2, 2, 2]), dtype=torch.float16)
tensor([[[[ 0.1646,  0.2314],
          [-0.4053, -0.4700]],

         [[ 0.3298,  0.2130],
          [ 0.1247,  0.0992]]],


        [[[-0.1354, -0.1116],
          [-0.4583,  0.1781]],

         [[ 0.3335,  0.2076],
          [-0.3716, -0.0694]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 2, 2, 2]), dtype=torch.float16)
tensor([[[[ 0.1646,  0.2314],
          [-0.0926, -0.3247]],

         [[ 0.1902, -0.0969],
          [ 0.2949,  0.2659]]],


        [[[-0.1354, -0.1116],
          [-0.2886, -0.3535]],

         [[ 0.3594,  0.1880],
          [ 0.0000,  0.0000]]]], dtype=torch.float16)

2025-07-09 13:52:35.390076 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 15 / 16 (93.8%)
Greatest absolute difference: 0.822265625 at index (1, 1, 1, 1) (up to 0.01 allowed)
Greatest relative difference: 6.7421875 at index (0, 1, 1, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 2, 2, 2]), dtype=torch.float16)
tensor([[[[-0.1057, -0.4094],
          [-0.2389, -0.2825]],

         [[ 0.2703,  0.0039],
          [-0.2454, -0.3264]]],


        [[[-0.0324, -0.2262],
          [-0.1580,  0.2603]],

         [[ 0.1156,  0.2832],
          [ 0.2546,  0.3916]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 2, 2, 2]), dtype=torch.float16)
tensor([[[[ 0.1973, -0.2211],
          [-0.4958,  0.2905]],

         [[ 0.1305, -0.1451],
          [ 0.4375, -0.0421]]],


        [[[ 0.0464, -0.2206],
          [-0.2119,  0.0563]],

         [[ 0.4490,  0.3069],
          [ 0.2026, -0.4304]]]], dtype=torch.float16)

2025-07-09 13:52:35.910896 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,22,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,22,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 169 / 176 (96.0%)
Greatest absolute difference: 0.9599609375 at index (0, 0, 15, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 1, 21, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 2, 22, 2]), dtype=torch.float16)
tensor([[[[-2.4353e-01, -4.4360e-01],
          [-7.8125e-02, -8.7158e-02],
          [ 5.1788e-02,  1.8225e-01],
          ...,
          [ 8.1482e-02, -2.3010e-02],
          [-1.8506e-01,  4.7876e-01],
          [ 3.2739e-01,  3.5522e-01]],

         [[-4.0576e-01, -4.5276e-04],
          [-3.0054e-01, -1.9775e-02],
          [ 2.4500e-01,  4.0088e-01],
          ...,
          [ 4.4092e-01, -4.7821e-02],
          [ 4.1919e-01, -1.2878e-01],
          [-5.1910e-02,  8.4473e-02]]],


        [[[ 4.0112e-01, -1.4282e-01],
          [ 2.3352e-01,  1.7847e-01],
          [-3.8525e-01,  3.2764e-01],
          ...,
          [-4.8242e-01, -6.2012e-02],
          [ 3.8013e-01,  2.6099e-01],
          [ 2.2754e-01,  4.2578e-01]],

         [[ 1.0963e-02,  6.1951e-02],
          [ 3.4448e-01, -5.3711e-03],
          [-2.3694e-01, -3.5669e-01],
          ...,
          [-2.9810e-01,  1.4351e-02],
          [-4.2065e-01,  2.2632e-01],
          [ 1.5759e-01, -2.1094e-01]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 2, 22, 2]), dtype=torch.float16)
tensor([[[[-0.2435, -0.4436],
          [ 0.1169,  0.1058],
          [-0.1904,  0.1842],
          ...,
          [ 0.1338,  0.4023],
          [ 0.4111, -0.2014],
          [-0.3479,  0.2754]],

         [[-0.4895, -0.2255],
          [-0.2368,  0.2607],
          [-0.3840, -0.2361],
          ...,
          [-0.3430,  0.1846],
          [ 0.3167, -0.1582],
          [-0.3030, -0.3755]]],


        [[[ 0.4011, -0.1428],
          [-0.4185,  0.0690],
          [ 0.4539,  0.2153],
          ...,
          [ 0.4116, -0.1776],
          [-0.1746,  0.1232],
          [ 0.4563, -0.4753]],

         [[-0.3511,  0.0741],
          [ 0.0716,  0.0967],
          [ 0.3894, -0.2839],
          ...,
          [-0.2064, -0.2869],
          [-0.2776,  0.1187],
          [ 0.0000,  0.0000]]]], dtype=torch.float16)

2025-07-09 13:52:38.513365 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,22,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[2,22,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 172 / 176 (97.7%)
Greatest absolute difference: 0.947265625 at index (0, 1, 13, 0) (up to 0.01 allowed)
Greatest relative difference: 25.484375 at index (1, 0, 20, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 2, 22, 2]), dtype=torch.float16)
tensor([[[[ 0.2754,  0.0271],
          [ 0.0878, -0.1208],
          [-0.4949, -0.3696],
          ...,
          [ 0.3821, -0.4946],
          [ 0.2239, -0.4800],
          [ 0.1017, -0.3933]],

         [[ 0.4543, -0.4351],
          [-0.0411, -0.2546],
          [ 0.1998, -0.2808],
          ...,
          [ 0.1725,  0.4697],
          [-0.1220,  0.1015],
          [ 0.2097, -0.2871]]],


        [[[-0.3533,  0.3630],
          [-0.4082,  0.0347],
          [ 0.2269,  0.3240],
          ...,
          [ 0.2886, -0.0366],
          [-0.1584,  0.2727],
          [ 0.0258, -0.2047]],

         [[-0.2976, -0.4846],
          [-0.1571,  0.0262],
          [-0.2603,  0.4526],
          ...,
          [ 0.3838,  0.3291],
          [ 0.1680, -0.1401],
          [-0.4434, -0.4661]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 2, 22, 2]), dtype=torch.float16)
tensor([[[[ 0.1829, -0.1422],
          [-0.3640, -0.0149],
          [ 0.2734, -0.0875],
          ...,
          [ 0.1127,  0.1440],
          [-0.3079,  0.2347],
          [-0.2336, -0.4280]],

         [[ 0.0434, -0.3364],
          [ 0.4531, -0.0486],
          [-0.4592, -0.1774],
          ...,
          [-0.4849, -0.2939],
          [ 0.4001,  0.1489],
          [-0.1367,  0.3569]]],


        [[[-0.3237, -0.0629],
          [-0.4194,  0.0703],
          [-0.3071,  0.3696],
          ...,
          [-0.2505, -0.3342],
          [-0.2203, -0.0111],
          [-0.3997,  0.2537]],

         [[ 0.2825, -0.2196],
          [ 0.2979,  0.4028],
          [-0.3535,  0.4392],
          ...,
          [-0.3398, -0.1152],
          [ 0.2283,  0.0891],
          [ 0.4141, -0.1932]]]], dtype=torch.float16)

2025-07-09 13:52:43.573994 GPU 4 151556 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,12,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,12,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1202 / 1248 (96.3%)
Greatest absolute difference: 0.9228515625 at index (1, 0, 8, 0) (up to 0.01 allowed)
Greatest relative difference: 2164.0 at index (1, 5, 2, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 26, 12, 2]), dtype=torch.float16)
tensor([[[[ 0.1646,  0.2314],
          [-0.3413,  0.1436],
          [-0.0517, -0.2225],
          ...,
          [ 0.4016, -0.0130],
          [-0.1876,  0.3801],
          [-0.1592, -0.4211]],

         [[ 0.2333,  0.1055],
          [-0.3982,  0.2703],
          [ 0.0062,  0.0259],
          ...,
          [ 0.2494,  0.1479],
          [ 0.0454,  0.1436],
          [ 0.0639, -0.1320]],

         [[ 0.3020, -0.0204],
          [-0.4553,  0.3970],
          [ 0.0641,  0.2744],
          ...,
          [ 0.0972,  0.3088],
          [ 0.2783, -0.0930],
          [ 0.2871,  0.1572]],

         ...,

         [[ 0.1191, -0.0662],
          [ 0.2581, -0.1077],
          [ 0.0723,  0.1105],
          ...,
          [ 0.3496, -0.2974],
          [-0.1992,  0.4307],
          [-0.1749, -0.0939]],

         [[ 0.1902, -0.0969],
          [ 0.2229, -0.1678],
          [ 0.0709,  0.1643],
          ...,
          [ 0.4141, -0.3398],
          [-0.3105,  0.4531],
          [-0.1531, -0.1042]],

         [[ 0.1902, -0.0969],
          [ 0.2229, -0.1678],
          [ 0.0709,  0.1643],
          ...,
          [ 0.4141, -0.3398],
          [-0.3105,  0.4531],
          [-0.1531, -0.1042]]],


        [[[-0.1354, -0.1116],
          [-0.0596,  0.0948],
          [ 0.2600, -0.1984],
          ...,
          [ 0.2563, -0.4448],
          [ 0.1268,  0.4072],
          [-0.0865, -0.1013]],

         [[ 0.0560, -0.0049],
          [-0.0376,  0.1051],
          [ 0.3396, -0.1678],
          ...,
          [ 0.3418, -0.2917],
          [ 0.1808,  0.1224],
          [-0.1615, -0.0800]],

         [[ 0.2474,  0.1017],
          [-0.0156,  0.1154],
          [ 0.4192, -0.1373],
          ...,
          [ 0.4270, -0.1388],
          [ 0.2347, -0.1624],
          [-0.2365, -0.0588]],

         ...,

         [[ 0.3352,  0.1168],
          [ 0.0647,  0.0583],
          [-0.1265,  0.0955],
          ...,
          [-0.2505,  0.3694],
          [ 0.2196, -0.1562],
          [-0.0051, -0.3269]],

         [[ 0.3594,  0.1880],
          [ 0.0414,  0.0786],
          [-0.2268,  0.1163],
          ...,
          [-0.3738,  0.4272],
          [ 0.2788, -0.2156],
          [-0.0839, -0.4238]],

         [[ 0.3594,  0.1880],
          [ 0.0414,  0.0786],
          [-0.2268,  0.1163],
          ...,
          [-0.3738,  0.4272],
          [ 0.2788, -0.2156],
          [-0.0839, -0.4238]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 26, 12, 2]), dtype=torch.float16)
tensor([[[[ 0.1646,  0.2314],
          [-0.2365, -0.3840],
          [-0.2859, -0.1024],
          ...,
          [ 0.4136, -0.1395],
          [ 0.1571,  0.0419],
          [-0.0173,  0.4863]],

         [[ 0.2289,  0.1136],
          [-0.0497, -0.1489],
          [-0.0497,  0.0693],
          ...,
          [ 0.1105, -0.1405],
          [ 0.0010,  0.1635],
          [ 0.1627,  0.3318]],

         [[ 0.2932, -0.0042],
          [ 0.1370,  0.0862],
          [ 0.1864,  0.2410],
          ...,
          [-0.1925, -0.1415],
          [-0.1552,  0.2852],
          [ 0.3428,  0.1774]],

         ...,

         [[-0.1423,  0.0470],
          [ 0.1750,  0.3335],
          [ 0.0091,  0.3025],
          ...,
          [-0.2620,  0.3408],
          [-0.0339,  0.2073],
          [ 0.0992, -0.2306]],

         [[ 0.0239, -0.0250],
          [ 0.3281,  0.1926],
          [ 0.0236,  0.0569],
          ...,
          [-0.1245,  0.1993],
          [-0.1170, -0.0532],
          [ 0.1696,  0.0840]],

         [[ 0.1902, -0.0969],
          [ 0.4812,  0.0518],
          [ 0.0381, -0.1886],
          ...,
          [ 0.0130,  0.0579],
          [-0.2001, -0.3137],
          [ 0.2399,  0.3987]]],


        [[[-0.1354, -0.1116],
          [-0.4104, -0.1924],
          [-0.1021,  0.3376],
          ...,
          [-0.3425, -0.3950],
          [-0.3091, -0.4070],
          [ 0.1891, -0.3589]],

         [[ 0.0438, -0.0118],
          [-0.1268, -0.1069],
          [ 0.0934,  0.0718],
          ...,
          [-0.0948, -0.1508],
          [-0.3367, -0.2778],
          [ 0.1219, -0.3479]],

         [[ 0.2229,  0.0880],
          [ 0.1566, -0.0214],
          [ 0.2888, -0.1941],
          ...,
          [ 0.1528,  0.0935],
          [-0.3643, -0.1484],
          [ 0.0547, -0.3372]],

         ...,

         [[ 0.2463, -0.1455],
          [-0.3291, -0.2411],
          [ 0.2781,  0.3064],
          ...,
          [ 0.4146, -0.2340],
          [ 0.2690, -0.2136],
          [ 0.1643, -0.2152]],

         [[ 0.3030,  0.0213],
          [-0.3064, -0.2089],
          [ 0.0133,  0.1013],
          ...,
          [ 0.3584, -0.0271],
          [ 0.0167,  0.0878],
          [-0.0492, -0.1707]],

         [[ 0.3594,  0.1880],
          [-0.2834, -0.1766],
          [-0.2515, -0.1037],
          ...,
          [ 0.3025,  0.1798],
          [-0.2355,  0.3892],
          [-0.2627, -0.1261]]]], dtype=torch.float16)

2025-07-09 13:52:46.473052 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,12,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,12,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1204 / 1248 (96.5%)
Greatest absolute difference: 0.939453125 at index (1, 6, 2, 0) (up to 0.01 allowed)
Greatest relative difference: 846.0 at index (0, 23, 6, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 26, 12, 2]), dtype=torch.float16)
tensor([[[[-1.0565e-01, -4.0942e-01],
          [ 2.3376e-01,  4.0503e-01],
          [ 2.3523e-01, -1.1182e-01],
          ...,
          [ 1.1926e-01,  3.9502e-01],
          [-1.1908e-01, -2.6318e-01],
          [ 4.3652e-01,  2.3669e-01]],

         [[-1.0931e-01, -3.0566e-01],
          [ 1.8677e-01,  1.8457e-01],
          [ 1.1768e-01,  7.5317e-02],
          ...,
          [ 1.5955e-01,  2.6953e-01],
          [-2.0618e-01, -3.1519e-01],
          [ 2.4402e-01,  2.9883e-01]],

         [[-1.1298e-01, -2.0178e-01],
          [ 1.3965e-01, -3.5919e-02],
          [ 7.8142e-05,  2.6245e-01],
          ...,
          [ 1.9983e-01,  1.4429e-01],
          [-2.9321e-01, -3.6743e-01],
          [ 5.1514e-02,  3.6108e-01]],

         ...,

         [[-1.6083e-02, -2.8687e-01],
          [ 2.6538e-01, -6.9946e-02],
          [ 2.0825e-01,  2.4500e-01],
          ...,
          [-1.4331e-01,  1.1725e-01],
          [-9.5886e-02, -6.6040e-02],
          [-2.6025e-01, -6.7871e-02]],

         [[ 1.2708e-01, -1.4148e-01],
          [ 1.2329e-01, -1.3477e-01],
          [-3.3081e-02,  3.3722e-02],
          ...,
          [ 3.4882e-02,  1.5540e-01],
          [ 8.9966e-02,  1.7822e-01],
          [ 1.4252e-02, -4.1870e-02]],

         [[ 2.7026e-01,  3.9291e-03],
          [-1.8845e-02, -1.9958e-01],
          [-2.7441e-01, -1.7761e-01],
          ...,
          [ 2.1313e-01,  1.9360e-01],
          [ 2.7588e-01,  4.2261e-01],
          [ 2.8882e-01, -1.5869e-02]]],


        [[[-3.2410e-02, -2.2620e-01],
          [ 2.1057e-01, -1.1115e-01],
          [-7.0763e-03,  3.5449e-01],
          ...,
          [ 4.4116e-01, -4.8370e-02],
          [-2.8369e-01,  3.9771e-01],
          [-2.7808e-01, -1.6541e-01]],

         [[-5.0720e-02, -2.8638e-01],
          [-8.1482e-03, -2.3584e-01],
          [-9.2712e-02,  3.7378e-01],
          ...,
          [ 4.5557e-01,  6.3110e-02],
          [-2.3950e-01,  1.6101e-01],
          [-1.8494e-01, -2.8223e-01]],

         [[-6.9031e-02, -3.4644e-01],
          [-2.2681e-01, -3.6060e-01],
          [-1.7834e-01,  3.9307e-01],
          ...,
          [ 4.7021e-01,  1.7456e-01],
          [-1.9531e-01, -7.5684e-02],
          [-9.1736e-02, -3.9917e-01]],

         ...,

         [[-1.1467e-02, -2.2644e-01],
          [-8.8562e-02, -4.0222e-02],
          [ 3.0823e-02, -1.6370e-01],
          ...,
          [-1.9348e-02, -9.2163e-03],
          [-9.7778e-02,  6.5735e-02],
          [ 5.2734e-02,  3.0444e-01]],

         [[ 5.2063e-02,  2.8412e-02],
          [-1.6012e-03,  5.6244e-02],
          [-1.7517e-02,  7.7942e-02],
          ...,
          [-8.7769e-02, -6.0692e-03],
          [-2.9736e-01, -1.2964e-01],
          [ 2.3392e-02,  3.6865e-01]],

         [[ 1.1560e-01,  2.8320e-01],
          [ 8.5388e-02,  1.5271e-01],
          [-6.5857e-02,  3.1958e-01],
          ...,
          [-1.5613e-01, -2.9259e-03],
          [-4.9683e-01, -3.2495e-01],
          [-5.9509e-03,  4.3311e-01]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 26, 12, 2]), dtype=torch.float16)
tensor([[[[ 0.0475,  0.4524],
          [ 0.4658,  0.2520],
          [-0.4282,  0.3843],
          ...,
          [ 0.1611, -0.1135],
          [ 0.2788, -0.3164],
          [-0.1199, -0.3406]],

         [[ 0.0552,  0.3972],
          [ 0.4561,  0.2644],
          [-0.3911,  0.3757],
          ...,
          [ 0.1307, -0.0748],
          [ 0.2788, -0.2830],
          [-0.1148, -0.2905]],

         [[ 0.0938,  0.1217],
          [ 0.4072,  0.3262],
          [-0.2053,  0.3325],
          ...,
          [-0.0214,  0.1188],
          [ 0.2788, -0.1160],
          [-0.0892, -0.0403]],

         ...,

         [[ 0.3245, -0.3591],
          [ 0.0486, -0.1429],
          [ 0.2310,  0.1456],
          ...,
          [-0.0194,  0.3708],
          [-0.0189,  0.0333],
          [-0.2435, -0.2167]],

         [[ 0.2739, -0.4138],
          [ 0.3684, -0.0157],
          [ 0.3274,  0.0547],
          ...,
          [ 0.0224,  0.3430],
          [ 0.0216,  0.2871],
          [-0.3894, -0.2235]],

         [[ 0.2639, -0.4248],
          [ 0.4324,  0.0098],
          [ 0.3467,  0.0366],
          ...,
          [ 0.0308,  0.3374],
          [ 0.0298,  0.3379],
          [-0.4187, -0.2249]]],


        [[[-0.1298, -0.4761],
          [-0.1763, -0.2072],
          [-0.2529,  0.4868],
          ...,
          [ 0.4214, -0.3875],
          [ 0.4177,  0.1000],
          [ 0.0712, -0.4402]],

         [[-0.0831, -0.4038],
          [-0.1990, -0.1978],
          [-0.2410,  0.4592],
          ...,
          [ 0.3687, -0.3928],
          [ 0.4121,  0.1040],
          [ 0.0924, -0.3748]],

         [[ 0.1499, -0.0426],
          [-0.3123, -0.1509],
          [-0.1812,  0.3208],
          ...,
          [ 0.1050, -0.4194],
          [ 0.3845,  0.1241],
          [ 0.1985, -0.0471]],

         ...,

         [[-0.0959, -0.0317],
          [-0.0509,  0.0069],
          [-0.0019, -0.1346],
          ...,
          [ 0.0919, -0.0144],
          [ 0.0267,  0.1897],
          [-0.0092,  0.3359]],

         [[-0.1790,  0.1390],
          [ 0.0245, -0.3364],
          [ 0.2966, -0.1531],
          ...,
          [-0.0371,  0.1669],
          [-0.1357,  0.1423],
          [ 0.0075,  0.3774]],

         [[-0.1956,  0.1732],
          [ 0.0396, -0.4050],
          [ 0.3562, -0.1567],
          ...,
          [-0.0629,  0.2031],
          [-0.1682,  0.1328],
          [ 0.0108,  0.3857]]]], dtype=torch.float16)

2025-07-09 13:52:46.653696 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,2,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 181 / 208 (87.0%)
Greatest absolute difference: 0.7841796875 at index (0, 3, 1, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 25, 1, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 26, 2, 2]), dtype=torch.float16)
tensor([[[[-2.4353e-01, -4.4360e-01],
          [-3.5425e-01,  3.7939e-01]],

         [[ 1.3519e-02, -4.1455e-01],
          [-3.6182e-01,  1.8958e-01]],

         [[ 2.7051e-01, -3.8550e-01],
          [-3.6938e-01, -2.9111e-04]],

         ...,

         [[-4.2749e-01, -2.3889e-01],
          [ 1.2108e-02, -1.5503e-01]],

         [[-4.8950e-01, -2.2546e-01],
          [-4.5258e-02, -2.5757e-01]],

         [[-4.8950e-01, -2.2546e-01],
          [-4.5258e-02, -2.5757e-01]]],


        [[[ 4.0112e-01, -1.4282e-01],
          [-1.6638e-01,  2.1008e-01]],

         [[ 4.0332e-01, -1.1768e-01],
          [-2.3022e-01,  3.0347e-01]],

         [[ 4.0552e-01, -9.2529e-02],
          [-2.9419e-01,  3.9697e-01]],

         ...,

         [[-2.8564e-01, -9.6817e-03],
          [ 1.8518e-01,  2.0667e-01]],

         [[-3.5107e-01,  7.4097e-02],
          [ 2.6489e-01,  2.2852e-01]],

         [[-3.5107e-01,  7.4097e-02],
          [ 2.6489e-01,  2.2852e-01]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 26, 2, 2]), dtype=torch.float16)
tensor([[[[-0.2435, -0.4436],
          [-0.3479,  0.2754]],

         [[-0.0029, -0.4165],
          [-0.0500,  0.1523]],

         [[ 0.2377, -0.3894],
          [ 0.2480,  0.0293]],

         ...,

         [[-0.1989, -0.2886],
          [-0.1813, -0.0048]],

         [[-0.3442, -0.2571],
          [-0.2421, -0.1902]],

         [[-0.4895, -0.2255],
          [-0.3030, -0.3755]]],


        [[[ 0.4011, -0.1428],
          [ 0.4563, -0.4753]],

         [[ 0.4033, -0.1193],
          [ 0.2465, -0.3762]],

         [[ 0.4053, -0.0958],
          [ 0.0367, -0.2771]],

         ...,

         [[-0.0446, -0.3179],
          [ 0.0298, -0.3308]],

         [[-0.1979, -0.1219],
          [ 0.0149, -0.1654]],

         [[-0.3511,  0.0741],
          [ 0.0000,  0.0000]]]], dtype=torch.float16)

2025-07-09 13:52:50.622594 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,2,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 199 / 208 (95.7%)
Greatest absolute difference: 0.7900390625 at index (0, 20, 1, 1) (up to 0.01 allowed)
Greatest relative difference: 228.5 at index (0, 7, 0, 1) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 26, 2, 2]), dtype=torch.float16)
tensor([[[[ 0.2754,  0.0271],
          [ 0.1017, -0.3933]],

         [[ 0.3167, -0.0232],
          [-0.0911, -0.1770]],

         [[ 0.3579, -0.0734],
          [-0.2839,  0.0392]],

         ...,

         [[-0.1305,  0.0729],
          [ 0.1846,  0.0279]],

         [[ 0.1620, -0.1810],
          [ 0.1971, -0.1296]],

         [[ 0.4543, -0.4351],
          [ 0.2097, -0.2871]]],


        [[[-0.3533,  0.3630],
          [ 0.0258, -0.2047]],

         [[-0.0740,  0.0674],
          [ 0.0963,  0.0256]],

         [[ 0.2053, -0.2281],
          [ 0.1666,  0.2559]],

         ...,

         [[ 0.0163,  0.1531],
          [ 0.1251,  0.0461]],

         [[-0.1406, -0.1658],
          [-0.1592, -0.2100]],

         [[-0.2976, -0.4846],
          [-0.4434, -0.4661]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 26, 2, 2]), dtype=torch.float16)
tensor([[[[-0.0381,  0.0194],
          [ 0.0756,  0.2771]],

         [[-0.0610,  0.0036],
          [ 0.1031,  0.2837]],

         [[-0.1757, -0.0755],
          [ 0.2402,  0.3169]],

         ...,

         [[-0.1124, -0.0373],
          [ 0.0603,  0.0538]],

         [[ 0.0665,  0.0142],
          [-0.2119,  0.3901]],

         [[ 0.1023,  0.0245],
          [-0.2664,  0.4573]]],


        [[[ 0.1565,  0.3838],
          [ 0.0255, -0.2380]],

         [[ 0.1392,  0.3188],
          [ 0.0401, -0.2478]],

         [[ 0.0523, -0.0053],
          [ 0.1130, -0.2964]],

         ...,

         [[-0.1738, -0.1129],
          [ 0.1715,  0.1168]],

         [[-0.4070,  0.0926],
          [-0.0201,  0.2445]],

         [[-0.4536,  0.1337],
          [-0.0585,  0.2700]]]], dtype=torch.float16)

2025-07-09 13:52:56.248758 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,22,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,22,], scale_factor=None, mode="bilinear", align_corners=False, align_mode=1, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2206 / 2288 (96.4%)
Greatest absolute difference: 0.9599609375 at index (0, 0, 15, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 25, 21, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 26, 22, 2]), dtype=torch.float16)
tensor([[[[-0.2435, -0.4436],
          [-0.0781, -0.0872],
          [ 0.0518,  0.1823],
          ...,
          [ 0.0815, -0.0230],
          [-0.1851,  0.4788],
          [ 0.3274,  0.3552]],

         [[ 0.0135, -0.4146],
          [ 0.0840, -0.0648],
          [ 0.1001,  0.2026],
          ...,
          [-0.0991,  0.0612],
          [-0.1909,  0.3945],
          [ 0.3315,  0.1163]],

         [[ 0.2705, -0.3855],
          [ 0.2462, -0.0425],
          [ 0.1484,  0.2229],
          ...,
          [-0.2795,  0.1454],
          [-0.1968,  0.3101],
          [ 0.3359, -0.1227]],

         ...,

         [[-0.4275, -0.2389],
          [ 0.0648,  0.2786],
          [ 0.4822,  0.1958],
          ...,
          [ 0.1622,  0.1035],
          [ 0.2195, -0.1054],
          [ 0.1288,  0.2827]],

         [[-0.4895, -0.2255],
          [ 0.0990,  0.2864],
          [ 0.4885,  0.2186],
          ...,
          [ 0.1570,  0.1398],
          [ 0.1938, -0.1512],
          [ 0.1881,  0.2988]],

         [[-0.4895, -0.2255],
          [ 0.0990,  0.2864],
          [ 0.4885,  0.2186],
          ...,
          [ 0.1570,  0.1398],
          [ 0.1938, -0.1512],
          [ 0.1881,  0.2988]]],


        [[[ 0.4011, -0.1428],
          [ 0.2335,  0.1785],
          [-0.3853,  0.3276],
          ...,
          [-0.4824, -0.0620],
          [ 0.3801,  0.2610],
          [ 0.2275,  0.4258]],

         [[ 0.4033, -0.1177],
          [ 0.1059,  0.0669],
          [-0.2817,  0.2021],
          ...,
          [-0.2939, -0.0269],
          [ 0.0770,  0.2004],
          [ 0.1354,  0.2871]],

         [[ 0.4055, -0.0925],
          [-0.0217, -0.0447],
          [-0.1783,  0.0766],
          ...,
          [-0.1055,  0.0081],
          [-0.2261,  0.1400],
          [ 0.0432,  0.1484]],

         ...,

         [[-0.2856, -0.0097],
          [ 0.2644,  0.3240],
          [-0.2454, -0.1008],
          ...,
          [-0.2864, -0.3354],
          [-0.1322,  0.1044],
          [ 0.3442,  0.1192]],

         [[-0.3511,  0.0741],
          [ 0.3374,  0.3616],
          [-0.2140, -0.1692],
          ...,
          [-0.3335, -0.3174],
          [-0.1639,  0.1008],
          [ 0.3584,  0.2050]],

         [[-0.3511,  0.0741],
          [ 0.3374,  0.3616],
          [-0.2140, -0.1692],
          ...,
          [-0.3335, -0.3174],
          [-0.1639,  0.1008],
          [ 0.3584,  0.2050]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 26, 22, 2]), dtype=torch.float16)
tensor([[[[-0.2435, -0.4436],
          [ 0.1169,  0.1058],
          [-0.1904,  0.1842],
          ...,
          [ 0.1338,  0.4023],
          [ 0.4111, -0.2014],
          [-0.3479,  0.2754]],

         [[-0.0029, -0.4165],
          [ 0.0639, -0.0251],
          [ 0.0373, -0.0269],
          ...,
          [ 0.1093,  0.2842],
          [ 0.1047, -0.0751],
          [-0.0500,  0.1523]],

         [[ 0.2377, -0.3894],
          [ 0.0109, -0.1559],
          [ 0.2651, -0.2379],
          ...,
          [ 0.0848,  0.1661],
          [-0.2018,  0.0512],
          [ 0.2480,  0.0293]],

         ...,

         [[-0.1989, -0.2886],
          [-0.1138,  0.3049],
          [-0.2263, -0.0278],
          ...,
          [-0.1370,  0.3589],
          [ 0.3884,  0.3137],
          [-0.1813, -0.0048]],

         [[-0.3442, -0.2571],
          [-0.1753,  0.2827],
          [-0.3052, -0.1320],
          ...,
          [-0.2400,  0.2717],
          [ 0.3525,  0.0778],
          [-0.2421, -0.1902]],

         [[-0.4895, -0.2255],
          [-0.2368,  0.2607],
          [-0.3840, -0.2361],
          ...,
          [-0.3430,  0.1846],
          [ 0.3167, -0.1582],
          [-0.3030, -0.3755]]],


        [[[ 0.4011, -0.1428],
          [-0.4185,  0.0690],
          [ 0.4539,  0.2153],
          ...,
          [ 0.4116, -0.1776],
          [-0.1746,  0.1232],
          [ 0.4563, -0.4753]],

         [[ 0.4033, -0.1193],
          [-0.1920, -0.0156],
          [ 0.4304,  0.1181],
          ...,
          [ 0.2839, -0.2385],
          [-0.1516,  0.0993],
          [ 0.2465, -0.3762]],

         [[ 0.4053, -0.0958],
          [ 0.0345, -0.1002],
          [ 0.4070,  0.0208],
          ...,
          [ 0.1562, -0.2993],
          [-0.1288,  0.0754],
          [ 0.0367, -0.2771]],

         ...,

         [[-0.0446, -0.3179],
          [-0.0153,  0.1400],
          [ 0.0370, -0.1799],
          ...,
          [ 0.2590, -0.3062],
          [ 0.0053,  0.3103],
          [ 0.0298, -0.3308]],

         [[-0.1979, -0.1219],
          [ 0.0281,  0.1183],
          [ 0.2133, -0.2319],
          ...,
          [ 0.0264, -0.2966],
          [-0.1361,  0.2145],
          [ 0.0149, -0.1654]],

         [[-0.3511,  0.0741],
          [ 0.0716,  0.0967],
          [ 0.3894, -0.2839],
          ...,
          [-0.2064, -0.2869],
          [-0.2776,  0.1187],
          [ 0.0000,  0.0000]]]], dtype=torch.float16)

2025-07-09 13:52:56.509275 GPU 4 151556 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,22,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 107374183, 2],"float16"), size=list[26,22,], scale_factor=None, mode="bilinear", align_corners=True, align_mode=0, data_format="NHWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2228 / 2288 (97.4%)
Greatest absolute difference: 0.919921875 at index (0, 17, 12, 0) (up to 0.01 allowed)
Greatest relative difference: 506.0 at index (1, 6, 20, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 26, 22, 2]), dtype=torch.float16)
tensor([[[[ 0.1646,  0.2314],
          [ 0.0211, -0.1246],
          [-0.4023, -0.0725],
          ...,
          [ 0.4290,  0.3262],
          [-0.2576, -0.3440],
          [ 0.1936, -0.1022]],

         [[ 0.2289,  0.1136],
          [ 0.0079, -0.0905],
          [-0.3438, -0.1575],
          ...,
          [ 0.4495,  0.0867],
          [-0.2052, -0.1417],
          [ 0.1227, -0.1036]],

         [[ 0.2932, -0.0042],
          [-0.0052, -0.0564],
          [-0.2852, -0.2423],
          ...,
          [ 0.4700, -0.1528],
          [-0.1530,  0.0605],
          [ 0.0518, -0.1050]],

         ...,

         [[-0.1423,  0.0470],
          [ 0.0493, -0.2144],
          [-0.0475,  0.0462],
          ...,
          [-0.0070, -0.0827],
          [-0.3423,  0.1390],
          [ 0.2888, -0.3723]],

         [[ 0.0239, -0.0250],
          [ 0.2590, -0.0296],
          [ 0.0242,  0.0864],
          ...,
          [ 0.1853, -0.1772],
          [-0.3342,  0.3047],
          [ 0.1710, -0.2925]],

         [[ 0.1902, -0.0969],
          [ 0.4688,  0.1553],
          [ 0.0959,  0.1267],
          ...,
          [ 0.3777, -0.2717],
          [-0.3264,  0.4705],
          [ 0.0533, -0.2125]]],


        [[[-0.1354, -0.1116],
          [ 0.0652,  0.2273],
          [-0.0964, -0.0906],
          ...,
          [ 0.1853,  0.3013],
          [-0.3401, -0.1632],
          [ 0.3818, -0.2822]],

         [[ 0.0438, -0.0118],
          [ 0.0358,  0.2140],
          [-0.0667, -0.0585],
          ...,
          [ 0.2286,  0.3511],
          [-0.3303, -0.2605],
          [ 0.3723, -0.1925]],

         [[ 0.2229,  0.0880],
          [ 0.0064,  0.2008],
          [-0.0371, -0.0265],
          ...,
          [ 0.2720,  0.4009],
          [-0.3208, -0.3579],
          [ 0.3628, -0.1028]],

         ...,

         [[ 0.2463, -0.1455],
          [-0.0349, -0.2474],
          [ 0.1664, -0.1432],
          ...,
          [-0.3984, -0.0835],
          [ 0.2244, -0.0029],
          [ 0.1532,  0.1641]],

         [[ 0.3030,  0.0213],
          [ 0.0365, -0.0653],
          [ 0.0870, -0.2401],
          ...,
          [-0.4446, -0.0047],
          [-0.0313, -0.1923],
          [ 0.0186,  0.0697]],

         [[ 0.3594,  0.1880],
          [ 0.1079,  0.1168],
          [ 0.0076, -0.3372],
          ...,
          [-0.4910,  0.0740],
          [-0.2871, -0.3816],
          [-0.1160, -0.0247]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 26, 22, 2]), dtype=torch.float16)
tensor([[[[-6.8970e-02,  4.0131e-02],
          [-9.0393e-02,  2.8735e-01],
          [-3.3301e-01,  1.4990e-01],
          ...,
          [-1.9226e-01,  1.8115e-01],
          [ 2.7246e-01,  9.5520e-02],
          [ 2.9492e-01,  1.2817e-01]],

         [[-8.0383e-02,  6.8115e-02],
          [-8.4351e-02,  2.9224e-01],
          [-3.0444e-01,  1.0101e-01],
          ...,
          [-1.5271e-01,  1.4075e-01],
          [ 2.2876e-01,  1.1499e-01],
          [ 2.3401e-01,  1.1072e-01]],

         [[-1.3770e-01,  2.0813e-01],
          [-5.4047e-02,  3.1665e-01],
          [-1.6223e-01, -1.4331e-01],
          ...,
          [ 4.5105e-02, -6.1096e-02],
          [ 1.0254e-02,  2.1228e-01],
          [-7.0618e-02,  2.3529e-02]],

         ...,

         [[-3.2739e-01, -1.9434e-01],
          [ 4.3042e-01,  1.0657e-01],
          [ 2.2614e-02, -4.1534e-02],
          ...,
          [ 2.2180e-01,  3.6224e-02],
          [-1.2878e-01,  4.6069e-01],
          [-9.3628e-02,  1.1884e-01]],

         [[-2.7808e-01, -2.2070e-01],
          [ 4.6753e-01,  1.1162e-02],
          [-1.7419e-01, -4.2261e-01],
          ...,
          [ 2.5732e-01, -1.7529e-01],
          [-8.1238e-02,  4.6826e-01],
          [-4.0137e-01,  6.6833e-02]],

         [[-2.6807e-01, -2.2595e-01],
          [ 4.7510e-01, -7.9193e-03],
          [-2.1350e-01, -4.9878e-01],
          ...,
          [ 2.6440e-01, -2.1753e-01],
          [-7.1716e-02,  4.6973e-01],
          [-4.6289e-01,  5.6458e-02]]],


        [[[-3.8849e-02, -6.2744e-02],
          [ 4.1821e-01, -2.8296e-01],
          [ 2.1704e-01,  9.5215e-02],
          ...,
          [ 7.8491e-02,  3.7988e-01],
          [-4.5441e-02, -7.3242e-02],
          [ 2.4609e-01,  3.9209e-01]],

         [[-5.8350e-02, -5.0598e-02],
          [ 3.5913e-01, -2.6196e-01],
          [ 2.1350e-01,  7.9285e-02],
          ...,
          [ 5.7861e-02,  3.3276e-01],
          [-1.0979e-02, -9.8633e-02],
          [ 1.9373e-01,  3.9990e-01]],

         [[-1.5588e-01,  1.0277e-02],
          [ 6.4270e-02, -1.5710e-01],
          [ 1.9592e-01, -4.2248e-04],
          ...,
          [-4.5288e-02,  9.6680e-02],
          [ 1.6138e-01, -2.2571e-01],
          [-6.7810e-02,  4.3848e-01]],

         ...,

         [[-1.7395e-01,  1.3794e-01],
          [-2.9785e-01, -1.3586e-01],
          [-3.2812e-01, -6.4270e-02],
          ...,
          [-7.6782e-02,  1.1467e-02],
          [ 3.1812e-01,  9.5947e-02],
          [ 1.4603e-02,  8.3252e-02]],

         [[-2.2559e-01,  1.3672e-01],
          [-1.9434e-01, -1.0083e-01],
          [-3.6987e-01,  1.5979e-01],
          ...,
          [-9.2041e-02, -5.7556e-02],
          [ 4.6338e-01,  2.8247e-01],
          [ 1.5454e-01, -1.6663e-02]],

         [[-2.3596e-01,  1.3647e-01],
          [-1.7371e-01, -9.3811e-02],
          [-3.7817e-01,  2.0459e-01],
          ...,
          [-9.5093e-02, -7.1350e-02],
          [ 4.9243e-01,  3.1982e-01],
          [ 1.8250e-01, -3.6652e-02]]]], dtype=torch.float16)

2025-07-09 13:52:58.648062 GPU 3 150451 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752040534 (unix time) try "date -d @1752040534" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24bb3) received by PID 150451 (TID 0x7f923ebbb740) from PID 150451 ***]


2025-07-09 13:53:02.401670 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1041295246 / 1369020828 (76.1%)
Greatest absolute difference: 0.6666170954704285 at index (1, 5, 29044580) (up to 0.01 allowed)
Greatest relative difference: 375496576.0 at index (0, 5, 105548356) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 6, 114085069]), dtype=torch.float32)
tensor([[[-0.2249, -0.3538, -0.3808,  ...,  0.3200,  0.2886,  0.3544],
         [-0.0152,  0.0006, -0.0042,  ..., -0.2641, -0.0780,  0.1941],
         [ 0.1256,  0.1495,  0.1920,  ..., -0.0955, -0.4484, -0.0176],
         [ 0.3937, -0.2777,  0.0390,  ...,  0.2228,  0.2875, -0.1307],
         [-0.0243, -0.3070,  0.0793,  ...,  0.2134, -0.2724, -0.2145],
         [-0.2922, -0.1650,  0.0574,  ..., -0.1625, -0.1429,  0.3184]],

        [[-0.1293, -0.0496, -0.4953,  ..., -0.3162, -0.2884, -0.0192],
         [-0.2172,  0.1909,  0.0311,  ...,  0.2661,  0.2018,  0.3771],
         [ 0.3418,  0.1046,  0.0737,  ...,  0.1029,  0.2816, -0.0532],
         [-0.3166, -0.1939, -0.1091,  ..., -0.4442, -0.3412, -0.3558],
         [-0.0724,  0.2658, -0.0563,  ...,  0.0759, -0.0565, -0.0683],
         [ 0.2642,  0.1919, -0.0280,  ...,  0.3677, -0.2530, -0.3026]]])
DESIRED: (shape=torch.Size([2, 6, 114085069]), dtype=torch.float32)
tensor([[[-0.2249, -0.3538, -0.3808,  ...,  0.3200,  0.2886,  0.3544],
         [-0.1090, -0.0240, -0.0852,  ..., -0.3099, -0.1558,  0.2356],
         [ 0.0021,  0.1448,  0.2591,  ..., -0.0697, -0.4662,  0.0794],
         [ 0.3042, -0.1649, -0.1213,  ...,  0.2849,  0.0253, -0.0237],
         [-0.1879, -0.4096,  0.3008,  ...,  0.0403, -0.2017, -0.2298],
         [ 0.0321, -0.0995, -0.4371,  ...,  0.1576, -0.2086,  0.1320]],

        [[-0.1293, -0.0496, -0.4953,  ..., -0.3162, -0.2884, -0.0192],
         [-0.2423,  0.3133,  0.0559,  ...,  0.2663,  0.2948,  0.3832],
         [ 0.3328,  0.1864,  0.1961,  ..., -0.1133,  0.2538, -0.0101],
         [-0.2908, -0.0403,  0.1077,  ..., -0.2062, -0.4007, -0.1308],
         [ 0.1138,  0.2616, -0.2504,  ...,  0.1004,  0.0347, -0.2202],
         [-0.2058,  0.3887,  0.0124,  ...,  0.4068,  0.1759, -0.3516]]])

2025-07-09 13:53:05.488280 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1131318770 / 1369020828 (82.6%)
Greatest absolute difference: 0.33332231640815735 at index (0, 0, 22584170) (up to 0.01 allowed)
Greatest relative difference: 304666080.0 at index (0, 4, 84467179) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 6, 114085069]), dtype=torch.float32)
tensor([[[ 0.0714,  0.0221,  0.1613,  ..., -0.3751,  0.1181,  0.1892],
         [ 0.1865, -0.3495, -0.1986,  ..., -0.0935, -0.1348,  0.3620],
         [ 0.0544, -0.1982, -0.1142,  ..., -0.1256, -0.0898,  0.2759],
         [-0.2097, -0.2659, -0.1293,  ...,  0.0338, -0.1600, -0.1200],
         [ 0.2433,  0.3999,  0.0186,  ..., -0.2410,  0.1686, -0.4712],
         [-0.1822,  0.2917, -0.2129,  ..., -0.2084, -0.4822, -0.4979]],

        [[ 0.3435,  0.1673, -0.3853,  ...,  0.1850, -0.1820,  0.0389],
         [ 0.2149,  0.1309, -0.3223,  ..., -0.0165,  0.1890, -0.1609],
         [-0.0440, -0.2332, -0.0565,  ..., -0.3345, -0.0588,  0.0706],
         [ 0.1917,  0.1480,  0.0463,  ...,  0.1238,  0.1046,  0.0259],
         [ 0.3709, -0.0186,  0.1048,  ...,  0.1103,  0.0320,  0.4054],
         [ 0.2635, -0.1470,  0.3251,  ..., -0.4960, -0.1104, -0.1422]]])
DESIRED: (shape=torch.Size([2, 6, 114085069]), dtype=torch.float32)
tensor([[[ 0.1287, -0.1427,  0.2386,  ..., -0.3534,  0.2190,  0.1140],
         [ 0.1722, -0.3189, -0.3466,  ..., -0.0394, -0.2738,  0.4617],
         [ 0.0375, -0.1832, -0.1343,  ..., -0.1157, -0.1521,  0.2943],
         [-0.2200, -0.2721, -0.1273,  ...,  0.0695, -0.1800, -0.1422],
         [ 0.3744,  0.4998, -0.0946,  ..., -0.2151,  0.1405, -0.4836],
         [-0.2151,  0.1946,  0.0153,  ..., -0.2537, -0.2279, -0.4726]],

        [[ 0.3208,  0.0789, -0.2532,  ...,  0.2680, -0.0986, -0.0287],
         [ 0.1998,  0.1881, -0.4057,  ..., -0.1291,  0.2192, -0.1602],
         [-0.0631, -0.2161, -0.0719,  ..., -0.3354, -0.0407,  0.1203],
         [ 0.1793,  0.1272,  0.0589,  ...,  0.1352,  0.1266,  0.0665],
         [ 0.4503,  0.0099,  0.0334,  ...,  0.0375,  0.1075,  0.4599],
         [ 0.1934, -0.1423,  0.3470,  ..., -0.1969, -0.1636, -0.0324]]])

2025-07-09 13:53:07.882333 GPU 4 151556 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.6,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752040544 (unix time) try "date -d @1752040544" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x25004) received by PID 151556 (TID 0x7fc5276ac740) from PID 151556 ***]


2025-07-09 13:53:15.721973 GPU 6 151383 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752040563 (unix time) try "date -d @1752040563" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24f57) received by PID 151383 (TID 0x7f87d8e0b740) from PID 151383 ***]


2025-07-09 13:53:23.571151 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1187475727 / 1597190966 (74.3%)
Greatest absolute difference: 0.42856383323669434 at index (1, 6, 81762810) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 1, 31139428) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 7, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.0732, -0.2355, -0.1094,  ...,  0.0477, -0.0696,  0.4553],
         [ 0.3434,  0.3488,  0.0587,  ...,  0.2199, -0.0570,  0.0479],
         ...,
         [ 0.0076, -0.0186, -0.1995,  ...,  0.0123,  0.0457,  0.2968],
         [-0.3106,  0.0307, -0.3878,  ...,  0.2954, -0.0296,  0.1435],
         [ 0.2182, -0.2462, -0.1280,  ..., -0.1165,  0.0018,  0.2417]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.2694,  0.3174, -0.0846,  ...,  0.2195,  0.1538, -0.1408],
         [ 0.1817,  0.0109,  0.2026,  ..., -0.1775,  0.1340,  0.0431],
         ...,
         [-0.3638,  0.2768, -0.2824,  ..., -0.2191, -0.2157,  0.1859],
         [ 0.0910, -0.1206, -0.3629,  ..., -0.4039, -0.2533,  0.2771],
         [ 0.2211, -0.3217, -0.1717,  ..., -0.0091, -0.1016, -0.1607]]])
DESIRED: (shape=torch.Size([2, 7, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.0569, -0.2678, -0.1117,  ...,  0.0518, -0.0312,  0.4596],
         [ 0.3911,  0.4892,  0.0898,  ...,  0.2432, -0.1060, -0.0257],
         ...,
         [ 0.0582,  0.0990, -0.4707,  ..., -0.0009, -0.0808,  0.3716],
         [-0.1528, -0.1435, -0.1282,  ...,  0.1524, -0.1396,  0.2802],
         [ 0.3306, -0.1403, -0.4004,  ..., -0.1681,  0.2234,  0.0693]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.2566,  0.2968, -0.0170,  ...,  0.2181,  0.1296, -0.1227],
         [ 0.2398, -0.0127,  0.1603,  ..., -0.2418,  0.1630,  0.0497],
         ...,
         [-0.4447,  0.2950, -0.3840,  ..., -0.2139, -0.4326,  0.0794],
         [-0.0058, -0.1126, -0.2510,  ..., -0.2921, -0.2877, -0.0441],
         [ 0.4929, -0.4869, -0.2299,  ...,  0.0858,  0.0741,  0.0891]]])

2025-07-09 13:55:43.852627 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1101165901 / 1597190966 (68.9%)
Greatest absolute difference: 0.21427509188652039 at index (0, 0, 20146436) (up to 0.01 allowed)
Greatest relative difference: 170683408.0 at index (0, 1, 4405629) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 7, 114085069]), dtype=torch.float32)
tensor([[[-2.2491e-01, -3.5380e-01, -3.8083e-01,  ...,  3.2002e-01,  2.8858e-01,  3.5436e-01],
         [ 1.0196e-01,  3.1414e-02,  9.7006e-02,  ..., -2.0693e-01,  1.9352e-02,  1.4214e-01],
         [ 2.8002e-01,  1.5525e-01,  1.0807e-01,  ..., -1.2773e-01, -4.2623e-01, -1.3893e-01],
         ...,
         [ 1.6982e-01,  4.3295e-03, -3.6177e-01,  ...,  3.7810e-01, -3.6807e-01,  1.3682e-01],
         [-2.8786e-01, -3.3019e-01,  3.0228e-01,  ..., -9.5779e-02, -1.6730e-01,  1.0751e-02],
         [ 3.2054e-02, -9.9496e-02, -4.3709e-01,  ...,  1.5756e-01, -2.0856e-01,  1.3196e-01]],

        [[-1.2933e-01, -4.9589e-02, -4.9528e-01,  ..., -3.1625e-01, -2.8841e-01, -1.9244e-02],
         [-1.8580e-01,  3.7879e-02,  6.0741e-05,  ...,  2.6580e-01,  8.5456e-02,  3.6943e-01],
         [ 3.5309e-01,  2.4914e-03, -7.9290e-02,  ...,  3.7315e-01,  3.1644e-01, -1.0695e-01],
         ...,
         [-2.5207e-01,  1.9015e-01,  4.3292e-01,  ...,  1.5077e-01, -4.9005e-01,  2.0667e-01],
         [ 2.5835e-01,  1.9855e-01, -1.7458e-01,  ...,  1.9333e-01, -1.5361e-01, -2.4194e-01],
         [-2.0582e-01,  3.8872e-01,  1.2420e-02,  ...,  4.0676e-01,  1.7593e-01, -3.5164e-01]]])
DESIRED: (shape=torch.Size([2, 7, 114085069]), dtype=torch.float32)
tensor([[[-0.0795, -0.2515, -0.2134,  ...,  0.2439,  0.2934,  0.2755],
         [ 0.0015,  0.0050,  0.0103,  ..., -0.2560, -0.0641,  0.1867],
         [ 0.2469,  0.1540,  0.1261,  ..., -0.1208, -0.4310, -0.1129],
         ...,
         [ 0.1858, -0.0158, -0.3331,  ...,  0.3670, -0.3212,  0.1177],
         [-0.2403, -0.3680,  0.3016,  ..., -0.0310, -0.1837, -0.1038],
         [-0.0722, -0.1206, -0.2781,  ...,  0.0547, -0.1875,  0.1919]],

        [[-0.1213, -0.1292, -0.4091,  ..., -0.1917, -0.2831,  0.0591],
         [-0.2127,  0.1690,  0.0266,  ...,  0.2661,  0.1852,  0.3760],
         [ 0.3507,  0.0244, -0.0465,  ...,  0.3152,  0.3090, -0.0954],
         ...,
         [-0.2567,  0.1627,  0.3942,  ...,  0.1083, -0.4794,  0.1665],
         [ 0.1895,  0.2286, -0.2107,  ...,  0.1491, -0.0639, -0.2316],
         [-0.0547,  0.3254, -0.0006,  ...,  0.3942,  0.0381, -0.3359]]])

2025-07-09 13:55:49.648932 GPU 7 151723 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752040713 (unix time) try "date -d @1752040713" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x250ab) received by PID 151723 (TID 0x7fce39de9740) from PID 151723 ***]


2025-07-09 13:55:49.711755 GPU 4 156249 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 8, 114085069]) != torch.Size([2, 7, 114085069]).
ACTUAL: (shape=torch.Size([2, 8, 114085069]), dtype=torch.float32)
tensor([[[ 0.3679, -0.3477, -0.3742,  ..., -0.1619,  0.0380, -0.1492],
         [-0.2825, -0.4112, -0.3233,  ...,  0.0809, -0.1424,  0.0844],
         [ 0.1606, -0.0330, -0.2673,  ...,  0.1342, -0.3851, -0.1889],
         ...,
         [-0.1204, -0.2037, -0.0466,  ...,  0.1565,  0.3292,  0.0854],
         [ 0.2020, -0.1306,  0.1475,  ..., -0.1099, -0.1301,  0.2897],
         [ 0.4450,  0.0934, -0.2560,  ..., -0.1235, -0.4200,  0.4847]],

        [[ 0.1840,  0.1155, -0.3650,  ..., -0.3561, -0.3912,  0.1762],
         [-0.2028, -0.1419, -0.4020,  ...,  0.3133, -0.1346, -0.0609],
         [ 0.2997,  0.0877, -0.3078,  ...,  0.1310, -0.1372, -0.0613],
         ...,
         [-0.0850, -0.1549,  0.2785,  ...,  0.1710, -0.4036,  0.0334],
         [ 0.0985,  0.2760,  0.4750,  ...,  0.2187,  0.1089,  0.2088],
         [ 0.1027,  0.1756,  0.0115,  ...,  0.3267,  0.1068, -0.1264]]])
DESIRED: (shape=torch.Size([2, 7, 114085069]), dtype=torch.float32)
tensor([[[ 0.3679, -0.3477, -0.3742,  ..., -0.1619,  0.0380, -0.1492],
         [-0.2825, -0.4112, -0.3233,  ...,  0.0809, -0.1424,  0.0844],
         [ 0.1606, -0.0330, -0.2673,  ...,  0.1342, -0.3851, -0.1889],
         ...,
         [-0.3391, -0.2316, -0.3362,  ..., -0.1365,  0.4495, -0.2436],
         [-0.1204, -0.2037, -0.0466,  ...,  0.1565,  0.3292,  0.0854],
         [ 0.2020, -0.1306,  0.1475,  ..., -0.1099, -0.1301,  0.2897]],

        [[ 0.1840,  0.1155, -0.3650,  ..., -0.3561, -0.3912,  0.1762],
         [-0.2028, -0.1419, -0.4020,  ...,  0.3133, -0.1346, -0.0609],
         [ 0.2997,  0.0877, -0.3078,  ...,  0.1310, -0.1372, -0.0613],
         ...,
         [ 0.2937, -0.2285, -0.0876,  ..., -0.0318,  0.3306, -0.2057],
         [-0.0850, -0.1549,  0.2785,  ...,  0.1710, -0.4036,  0.0334],
         [ 0.0985,  0.2760,  0.4750,  ...,  0.2187,  0.1089,  0.2088]]])

2025-07-09 13:56:09.549036 GPU 6 156412 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 8, 114085069]) != torch.Size([2, 7, 114085069]).
ACTUAL: (shape=torch.Size([2, 8, 114085069]), dtype=torch.float32)
tensor([[[ 0.2453, -0.3780,  0.4174,  ...,  0.4323, -0.3515, -0.2068],
         [ 0.1701, -0.1964, -0.3107,  ...,  0.3126,  0.0659, -0.3989],
         [-0.1895,  0.4212,  0.0054,  ..., -0.1794,  0.4154, -0.3161],
         ...,
         [ 0.2050, -0.1849,  0.3117,  ...,  0.1196, -0.3365, -0.4094],
         [ 0.1229, -0.1852, -0.0337,  ..., -0.0897, -0.3774, -0.2677],
         [ 0.2434, -0.3436, -0.3968,  ..., -0.1953, -0.4706, -0.0853]],

        [[-0.3751,  0.3461,  0.2788,  ...,  0.1286,  0.0210, -0.4097],
         [-0.0767,  0.0320,  0.1120,  ..., -0.4119,  0.2624, -0.1764],
         [-0.0597, -0.1744,  0.0336,  ..., -0.3987, -0.4532,  0.3950],
         ...,
         [-0.1547, -0.2758,  0.2015,  ..., -0.1112, -0.1155, -0.0930],
         [ 0.0261,  0.0030,  0.1635,  ..., -0.1375, -0.1853,  0.1931],
         [-0.3139, -0.1658,  0.2791,  ...,  0.2227,  0.2193,  0.3131]]])
DESIRED: (shape=torch.Size([2, 7, 114085069]), dtype=torch.float32)
tensor([[[ 0.2453, -0.3780,  0.4174,  ...,  0.4323, -0.3515, -0.2068],
         [ 0.0950, -0.0053, -0.3037,  ...,  0.1542,  0.1791, -0.3823],
         [-0.3238,  0.4654,  0.3004,  ..., -0.1961,  0.4253, -0.2832],
         ...,
         [ 0.1434, -0.2895,  0.3018,  ...,  0.1855, -0.3394, -0.4928],
         [ 0.1229, -0.1852, -0.0337,  ..., -0.0897, -0.3774, -0.2677],
         [ 0.3725, -0.2917, -0.3927,  ..., -0.2267, -0.4852,  0.0117]],

        [[-0.3751,  0.3461,  0.2788,  ...,  0.1286,  0.0210, -0.4097],
         [ 0.0486, -0.1321,  0.1492,  ..., -0.4342,  0.0321,  0.0143],
         [-0.4187,  0.1116, -0.1563,  ..., -0.3187, -0.4778,  0.3943],
         ...,
         [-0.1979, -0.2457,  0.1476,  ..., -0.1004, -0.0399, -0.1871],
         [ 0.0261,  0.0030,  0.1635,  ..., -0.1375, -0.1853,  0.1931],
         [-0.4443, -0.3451,  0.3841,  ...,  0.3407,  0.3018,  0.3517]]])

2025-07-09 13:56:19.469399 GPU 3 156571 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 8, 114085069]) != torch.Size([2, 7, 114085069]).
ACTUAL: (shape=torch.Size([2, 8, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [ 0.3093,  0.2395,  0.2691,  ..., -0.2336,  0.1027, -0.1759],
         [ 0.2896, -0.2526,  0.0303,  ..., -0.0475, -0.0528,  0.0174],
         ...,
         [-0.1636,  0.0800, -0.0204,  ..., -0.3555,  0.2244, -0.0761],
         [ 0.2530,  0.2260,  0.2086,  ..., -0.2087,  0.1313, -0.1783],
         [ 0.3280, -0.1366,  0.4958,  ...,  0.1776, -0.2365,  0.3795]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.4282, -0.3839, -0.3198,  ..., -0.1932,  0.1363,  0.2494],
         [ 0.0077,  0.0655,  0.2566,  ..., -0.0449,  0.1861, -0.1603],
         ...,
         [-0.0029, -0.0348,  0.0622,  ...,  0.1166,  0.0282,  0.0012],
         [-0.3497,  0.3084, -0.3285,  ...,  0.4532,  0.1462, -0.0534],
         [-0.0481, -0.1945,  0.2152,  ...,  0.3411, -0.0938,  0.2824]]])
DESIRED: (shape=torch.Size([2, 7, 114085069]), dtype=torch.float32)
tensor([[[-0.2369, -0.2779,  0.2274,  ..., -0.4611, -0.1381,  0.3707],
         [ 0.3102,  0.1728,  0.2841,  ..., -0.1810,  0.0449, -0.2022],
         [ 0.2870, -0.2487, -0.0034,  ..., -0.0695, -0.0240,  0.0553],
         ...,
         [ 0.2670,  0.4476, -0.1622,  ..., -0.2982, -0.3581, -0.3380],
         [-0.1610,  0.1239, -0.0549,  ..., -0.3676,  0.2300, -0.0998],
         [ 0.1975,  0.1492,  0.2260,  ..., -0.2108,  0.1354, -0.1341]],

        [[-0.0516, -0.0876, -0.0166,  ...,  0.1014, -0.0648, -0.1505],
         [-0.4278, -0.3570, -0.2670,  ..., -0.1698,  0.1182,  0.2539],
         [ 0.0483,  0.0875,  0.2711,  ..., -0.0486,  0.2044, -0.2021],
         ...,
         [-0.0061,  0.0333,  0.0682,  ...,  0.0272, -0.1173, -0.3302],
         [ 0.0145, -0.0839,  0.0714,  ...,  0.0866,  0.0139, -0.0130],
         [-0.3296,  0.3310, -0.2919,  ...,  0.4511,  0.1504, -0.0276]]])

2025-07-09 13:56:25.586584 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.7999999999999999,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 8, 114085069]) != torch.Size([2, 7, 114085069]).
ACTUAL: (shape=torch.Size([2, 8, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.1058, -0.1710, -0.1049,  ...,  0.0397, -0.1462,  0.4467],
         [ 0.2480,  0.0680, -0.0034,  ...,  0.1732,  0.0411,  0.1952],
         ...,
         [-0.1270,  0.0996, -0.4797,  ...,  0.1506, -0.0400,  0.2504],
         [-0.0580, -0.2481,  0.0275,  ...,  0.0666, -0.2056,  0.3622],
         [ 0.3306, -0.1403, -0.4004,  ..., -0.1681,  0.2234,  0.0693]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.2950,  0.3587, -0.2198,  ...,  0.2223,  0.2022, -0.1770],
         [ 0.0655,  0.0580,  0.2872,  ..., -0.0490,  0.0761,  0.0301],
         ...,
         [-0.1986,  0.1155, -0.3942,  ..., -0.3145, -0.3499,  0.2192],
         [-0.0638, -0.1079, -0.1838,  ..., -0.2251, -0.3084, -0.2368],
         [ 0.4929, -0.4869, -0.2299,  ...,  0.0858,  0.0741,  0.0891]]])
DESIRED: (shape=torch.Size([2, 7, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.0569, -0.2678, -0.1117,  ...,  0.0518, -0.0312,  0.4596],
         [ 0.3911,  0.4892,  0.0898,  ...,  0.2432, -0.1060, -0.0257],
         ...,
         [ 0.0582,  0.0990, -0.4707,  ..., -0.0009, -0.0808,  0.3716],
         [-0.1528, -0.1435, -0.1282,  ...,  0.1524, -0.1396,  0.2802],
         [ 0.3306, -0.1403, -0.4004,  ..., -0.1681,  0.2234,  0.0693]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.2566,  0.2968, -0.0170,  ...,  0.2181,  0.1296, -0.1227],
         [ 0.2398, -0.0127,  0.1603,  ..., -0.2418,  0.1630,  0.0497],
         ...,
         [-0.4447,  0.2950, -0.3840,  ..., -0.2139, -0.4326,  0.0794],
         [-0.0058, -0.1126, -0.2510,  ..., -0.2921, -0.2877, -0.0441],
         [ 0.4929, -0.4869, -0.2299,  ...,  0.0858,  0.0741,  0.0891]]])

2025-07-09 13:58:34.683229 GPU 5 150614 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752040895 (unix time) try "date -d @1752040895" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24c56) received by PID 150614 (TID 0x7f2e0e049740) from PID 150614 ***]


2025-07-09 13:59:01.247856 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1013384827 / 2053531242 (49.3%)
Greatest absolute difference: 0.11110863089561462 at index (1, 8, 81762810) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 4, 42463747) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 9, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.1457, -0.0922, -0.0993,  ...,  0.0298, -0.2400,  0.4362],
         [ 0.1315, -0.2752, -0.0792,  ...,  0.1162,  0.1609,  0.3751],
         ...,
         [-0.2298,  0.0999, -0.4846,  ...,  0.2348, -0.0173,  0.1831],
         [-0.0300, -0.2790,  0.0737,  ...,  0.0411, -0.2252,  0.3865],
         [ 0.3015, -0.1678, -0.3298,  ..., -0.1547,  0.1660,  0.1140]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.3264,  0.4092, -0.3851,  ...,  0.2258,  0.2613, -0.2212],
         [-0.0765,  0.1156,  0.3905,  ...,  0.1082,  0.0053,  0.0142],
         ...,
         [-0.0618,  0.0158, -0.3998,  ..., -0.3703, -0.3039,  0.2968],
         [-0.0810, -0.1065, -0.1638,  ..., -0.2052, -0.3145, -0.2939],
         [ 0.4224, -0.4441, -0.2148,  ...,  0.0612,  0.0285,  0.0243]]])
DESIRED: (shape=torch.Size([2, 9, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.1425, -0.0984, -0.0998,  ...,  0.0306, -0.2325,  0.4370],
         [ 0.1408, -0.2479, -0.0732,  ...,  0.1208,  0.1513,  0.3608],
         ...,
         [-0.2658,  0.1000, -0.4864,  ...,  0.2642, -0.0094,  0.1595],
         [ 0.0130, -0.3265,  0.1443,  ...,  0.0022, -0.2551,  0.4237],
         [ 0.3306, -0.1403, -0.4004,  ..., -0.1681,  0.2234,  0.0693]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.3239,  0.4052, -0.3720,  ...,  0.2255,  0.2566, -0.2177],
         [-0.0652,  0.1110,  0.3823,  ...,  0.0957,  0.0110,  0.0154],
         ...,
         [-0.0139, -0.0191, -0.4018,  ..., -0.3899, -0.2878,  0.3240],
         [-0.1073, -0.1043, -0.1334,  ..., -0.1748, -0.3239, -0.3813],
         [ 0.4929, -0.4869, -0.2299,  ...,  0.0858,  0.0741,  0.0891]]])

2025-07-09 13:59:16.902506 GPU 7 157406 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 690790203 / 2053531242 (33.6%)
Greatest absolute difference: 0.055553168058395386 at index (0, 0, 94862890) (up to 0.01 allowed)
Greatest relative difference: 312804480.0 at index (0, 5, 13318730) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 9, 114085069]), dtype=torch.float32)
tensor([[[ 0.2025,  0.2227, -0.4889,  ...,  0.3671, -0.4335, -0.3325],
         [-0.1334,  0.2629,  0.0181,  ...,  0.4615,  0.0450,  0.3927],
         [ 0.1426,  0.0463, -0.3250,  ...,  0.3660, -0.0062,  0.3087],
         ...,
         [-0.0533,  0.0792, -0.3144,  ..., -0.1869,  0.2847,  0.0783],
         [-0.3433,  0.3877,  0.3443,  ..., -0.3643, -0.2790,  0.0726],
         [ 0.2455,  0.3928, -0.3154,  ...,  0.3122,  0.4746, -0.3272]],

        [[-0.0320,  0.3474, -0.3906,  ...,  0.4619,  0.4530,  0.3269],
         [ 0.0729, -0.1098,  0.4580,  ...,  0.2275, -0.0622, -0.0275],
         [-0.2826,  0.3519,  0.1606,  ...,  0.3720,  0.3525, -0.3027],
         ...,
         [-0.0503,  0.3096, -0.1985,  ..., -0.1050,  0.0156,  0.2484],
         [ 0.1856,  0.0694,  0.0223,  ...,  0.1779, -0.2416, -0.1949],
         [ 0.1686,  0.4366, -0.4333,  ...,  0.3013,  0.1988, -0.4518]]])
DESIRED: (shape=torch.Size([2, 9, 114085069]), dtype=torch.float32)
tensor([[[ 0.1824,  0.2273, -0.4570,  ...,  0.3728, -0.4074, -0.2928],
         [-0.1252,  0.2488, -0.0040,  ...,  0.4589,  0.0482,  0.3967],
         [ 0.1541,  0.0549, -0.3116,  ...,  0.3615, -0.0193,  0.2901],
         ...,
         [-0.0667,  0.0738, -0.3092,  ..., -0.1779,  0.2736,  0.0655],
         [-0.3237,  0.3754,  0.3107,  ..., -0.3597, -0.2474,  0.0783],
         [ 0.2096,  0.3946, -0.2732,  ...,  0.2739,  0.4275, -0.3059]],

        [[-0.0223,  0.3177, -0.3428,  ...,  0.4473,  0.4203,  0.3104],
         [ 0.0498, -0.0835,  0.4539,  ...,  0.2366, -0.0376, -0.0468],
         [-0.2682,  0.3419,  0.1372,  ...,  0.3670,  0.3413, -0.2881],
         ...,
         [-0.0349,  0.2943, -0.1980,  ..., -0.1181,  0.0093,  0.2215],
         [ 0.1678,  0.0874,  0.0116,  ...,  0.1701, -0.2267, -0.1622],
         [ 0.1725,  0.4132, -0.4062,  ...,  0.2958,  0.1718, -0.4430]]])

2025-07-09 13:59:31.119005 GPU 4 156249 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[0.8999999999999999,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752040961 (unix time) try "date -d @1752040961" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26259) received by PID 156249 (TID 0x7ff0c0431740) from PID 156249 ***]


2025-07-09 13:59:59.310270 GPU 6 156412 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.0999999999999999,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.0999999999999999,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 11, 114085069]) != torch.Size([2, 10, 114085069]).
ACTUAL: (shape=torch.Size([2, 11, 114085069]), dtype=torch.float32)
tensor([[[ 0.2453, -0.3780,  0.4174,  ...,  0.4323, -0.3515, -0.2068],
         [ 0.2453, -0.3862, -0.2175,  ...,  0.4658, -0.0888, -0.3870],
         [ 0.0130,  0.2031, -0.2960,  ..., -0.0187,  0.3026, -0.3642],
         ...,
         [ 0.2685, -0.0138,  0.1711,  ..., -0.0834, -0.3504, -0.2086],
         [-0.0737, -0.4710, -0.4068,  ..., -0.1183, -0.4348, -0.3233],
         [ 0.3725, -0.2917, -0.3927,  ..., -0.2267, -0.4852,  0.0117]],

        [[-0.3751,  0.3461,  0.2788,  ...,  0.1286,  0.0210, -0.4097],
         [-0.2256,  0.2166,  0.1026,  ..., -0.3189,  0.4284, -0.3729],
         [ 0.1853, -0.3111,  0.1898,  ..., -0.4585, -0.2191,  0.2223],
         ...,
         [-0.0019, -0.1983,  0.2724,  ..., -0.1407, -0.2709,  0.1909],
         [ 0.0064,  0.2743,  0.0211,  ..., -0.0671,  0.0168,  0.2181],
         [-0.4443, -0.3451,  0.3841,  ...,  0.3407,  0.3018,  0.3517]]])
DESIRED: (shape=torch.Size([2, 10, 114085069]), dtype=torch.float32)
tensor([[[ 0.2453, -0.3780,  0.4174,  ...,  0.4323, -0.3515, -0.2068],
         [ 0.2453, -0.3875, -0.3178,  ...,  0.4711, -0.0474, -0.4155],
         [-0.0553,  0.3769, -0.2896,  ..., -0.1627,  0.4055, -0.3491],
         ...,
         [ 0.3899,  0.1289,  0.3417,  ..., -0.0782, -0.3279, -0.1592],
         [-0.1441, -0.4993, -0.4090,  ..., -0.1012, -0.4269, -0.3762],
         [ 0.3725, -0.2917, -0.3927,  ..., -0.2267, -0.4852,  0.0117]],

        [[-0.3751,  0.3461,  0.2788,  ...,  0.1286,  0.0210, -0.4097],
         [-0.2020,  0.1961,  0.0748,  ..., -0.3896,  0.4927, -0.3671],
         [ 0.2993, -0.4603,  0.2236,  ..., -0.4788, -0.4285,  0.3957],
         ...,
         [-0.0253, -0.3660,  0.3632,  ..., -0.1434, -0.3423,  0.1891],
         [ 0.0776,  0.3721, -0.0362,  ..., -0.1315, -0.0282,  0.1970],
         [-0.4443, -0.3451,  0.3841,  ...,  0.3407,  0.3018,  0.3517]]])

2025-07-09 13:59:59.800106 GPU 3 156571 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.0999999999999999,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.0999999999999999,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 11, 114085069]) != torch.Size([2, 10, 114085069]).
ACTUAL: (shape=torch.Size([2, 11, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [ 0.2497,  0.3770,  0.2217,  ..., -0.4082,  0.2436, -0.0435],
         [ 0.3151, -0.1582,  0.3586,  ...,  0.0803, -0.2423, -0.3331],
         ...,
         [-0.0213, -0.1538,  0.2946,  ..., -0.2194,  0.1515,  0.0406],
         [ 0.4119,  0.3611,  0.2152,  ..., -0.1327,  0.0537, -0.1928],
         [ 0.3280, -0.1366,  0.4958,  ...,  0.1776, -0.2365,  0.3795]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.3901, -0.4302, -0.4397,  ..., -0.2298,  0.1675,  0.1949],
         [-0.4260, -0.2236, -0.0049,  ..., -0.0535,  0.0280,  0.2764],
         ...,
         [-0.2501,  0.4199, -0.1475,  ...,  0.4429,  0.1672,  0.0741],
         [-0.3476,  0.1580, -0.3255,  ...,  0.4383,  0.0914, -0.0598],
         [-0.0481, -0.1945,  0.2152,  ...,  0.3411, -0.0938,  0.2824]]])
DESIRED: (shape=torch.Size([2, 10, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [ 0.3062,  0.4530,  0.2211,  ..., -0.4021,  0.2879, -0.0915],
         [ 0.3171, -0.2940,  0.3891,  ...,  0.1875, -0.3602, -0.3868],
         ...,
         [-0.1908, -0.3884,  0.3478,  ..., -0.2261,  0.1640,  0.1758],
         [ 0.4305,  0.4717,  0.1529,  ..., -0.2017,  0.1182, -0.3200],
         [ 0.3280, -0.1366,  0.4958,  ...,  0.1776, -0.2365,  0.3795]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.4293, -0.4700, -0.4888,  ..., -0.2682,  0.1944,  0.2349],
         [-0.4253, -0.1688,  0.1026,  ..., -0.0058, -0.0090,  0.2856],
         ...,
         [-0.1885,  0.4888, -0.0357,  ...,  0.4365,  0.1802,  0.1528],
         [-0.4142,  0.2363, -0.4456,  ...,  0.4599,  0.1325, -0.1358],
         [-0.0481, -0.1945,  0.2152,  ...,  0.3411, -0.0938,  0.2824]]])

2025-07-09 14:02:20.210436 GPU 5 158717 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.0999999999999999,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.0999999999999999,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 11, 114085069]) != torch.Size([2, 10, 114085069]).
ACTUAL: (shape=torch.Size([2, 11, 114085069]), dtype=torch.float32)
tensor([[[ 0.1553, -0.0451, -0.1985,  ...,  0.0481,  0.1804, -0.0154],
         [-0.1511,  0.1606,  0.1778,  ...,  0.3113, -0.4248,  0.2337],
         [ 0.0135, -0.0863, -0.1870,  ..., -0.0183, -0.1573, -0.2662],
         ...,
         [ 0.2497,  0.2586,  0.4611,  ...,  0.0527,  0.0535,  0.2843],
         [ 0.3319, -0.1667,  0.3547,  ..., -0.1036,  0.1429,  0.1640],
         [-0.3421,  0.1007, -0.3396,  ..., -0.3475,  0.2707,  0.1130]],

        [[-0.3549, -0.0836,  0.2692,  ..., -0.2883,  0.3938, -0.2152],
         [ 0.0435, -0.0038, -0.0284,  ..., -0.0411,  0.0900, -0.2870],
         [-0.1755, -0.1221,  0.3650,  ...,  0.2900, -0.1859, -0.3063],
         ...,
         [ 0.0842, -0.3487, -0.2081,  ...,  0.0034,  0.3819, -0.0821],
         [-0.0154,  0.0047,  0.0260,  ..., -0.2041,  0.0837,  0.1280],
         [ 0.4407, -0.3031, -0.2291,  ..., -0.3013, -0.4915,  0.3774]]])
DESIRED: (shape=torch.Size([2, 10, 114085069]), dtype=torch.float32)
tensor([[[ 0.1553, -0.0451, -0.1985,  ...,  0.0481,  0.1804, -0.0154],
         [-0.1851,  0.1834,  0.2196,  ...,  0.3405, -0.4921,  0.2613],
         [ 0.0632, -0.1538, -0.2886,  ..., -0.1080, -0.0736, -0.3981],
         ...,
         [ 0.2105,  0.3723,  0.4684,  ...,  0.0851,  0.0347,  0.3129],
         [ 0.4068, -0.1965,  0.4319,  ..., -0.0765,  0.1288,  0.1696],
         [-0.3421,  0.1007, -0.3396,  ..., -0.3475,  0.2707,  0.1130]],

        [[-0.3549, -0.0836,  0.2692,  ..., -0.2883,  0.3938, -0.2152],
         [ 0.0878,  0.0051, -0.0614,  ..., -0.0136,  0.0562, -0.2950],
         [-0.2413, -0.1540,  0.4716,  ...,  0.3659, -0.2465, -0.3092],
         ...,
         [ 0.1218, -0.4456, -0.2738,  ...,  0.0525,  0.4404, -0.1277],
         [-0.0661,  0.0390,  0.0544,  ..., -0.1933,  0.1476,  0.1003],
         [ 0.4407, -0.3031, -0.2291,  ..., -0.3013, -0.4915,  0.3774]]])

2025-07-09 14:02:24.298372 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.0999999999999999,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.0999999999999999,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 11, 114085069]) != torch.Size([2, 10, 114085069]).
ACTUAL: (shape=torch.Size([2, 11, 114085069]), dtype=torch.float32)
tensor([[[-2.8105e-01,  3.7345e-01,  1.6189e-01,  ..., -2.7483e-02, -3.4301e-01, -3.6034e-01],
         [-1.8209e-01, -4.2624e-04, -7.0053e-02,  ...,  1.8454e-02, -3.0393e-01,  3.5052e-01],
         [ 1.1641e-02, -4.0332e-01, -1.2120e-01,  ...,  6.8679e-02,  1.2977e-01,  4.7761e-01],
         ...,
         [-2.8538e-01,  2.8188e-03, -3.4624e-01,  ...,  2.7254e-01, -4.7183e-02,  1.6536e-01],
         [ 9.4515e-02, -3.6273e-01,  1.7162e-01,  ..., -5.9868e-02, -2.4195e-01,  4.3134e-01],
         [ 3.3064e-01, -1.4032e-01, -4.0038e-01,  ..., -1.6807e-01,  2.2344e-01,  6.9277e-02]],

        [[ 2.4912e-02, -1.1448e-01, -4.6302e-01,  ..., -3.0197e-01,  3.5578e-01, -2.6051e-01],
         [-3.0918e-01,  3.8574e-01, -4.8756e-01,  ...,  1.7496e-01,  3.0465e-01, -2.5049e-01],
         [-2.0274e-01,  2.1002e-01,  2.6699e-01,  ...,  2.1215e-01,  2.8041e-02, -4.6663e-02],
         ...,
         [ 7.5480e-02, -1.1931e-01, -3.4503e-01,  ..., -3.8600e-01, -2.5882e-01,  2.2571e-01],
         [-7.7780e-02, -1.4006e-01, -1.0772e-01,  ..., -1.1354e-01, -2.9490e-01, -4.3545e-01],
         [ 4.9289e-01, -4.8687e-01, -2.2985e-01,  ...,  8.5767e-02,  7.4062e-02,  8.9064e-02]]])
DESIRED: (shape=torch.Size([2, 10, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.1711, -0.0420, -0.0958,  ...,  0.0236, -0.2996,  0.4295],
         [ 0.0573, -0.4937, -0.1275,  ...,  0.0800,  0.2371,  0.4896],
         ...,
         [-0.3738,  0.1004, -0.4916,  ...,  0.3526,  0.0144,  0.0888],
         [ 0.0683, -0.3874,  0.2352,  ..., -0.0478, -0.2937,  0.4716],
         [ 0.3306, -0.1403, -0.4004,  ..., -0.1681,  0.2234,  0.0693]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.3463,  0.4413, -0.4903,  ...,  0.2280,  0.2990, -0.2494],
         [-0.1668,  0.1522,  0.4563,  ...,  0.2082, -0.0397,  0.0040],
         ...,
         [ 0.1296, -0.1238, -0.4077,  ..., -0.4486, -0.2396,  0.4056],
         [-0.1412, -0.1015, -0.0942,  ..., -0.1357, -0.3359, -0.4937],
         [ 0.4929, -0.4869, -0.2299,  ...,  0.0858,  0.0741,  0.0891]]])

2025-07-09 14:02:52.530857 GPU 3 156571 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.1999999999999997,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.1999999999999997,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 12, 114085069]) != torch.Size([2, 11, 114085069]).
ACTUAL: (shape=torch.Size([2, 12, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [ 0.1510,  0.2442,  0.2229,  ..., -0.4189,  0.1662,  0.0406],
         [ 0.3125,  0.0172,  0.3191,  ..., -0.0582, -0.0901, -0.2638],
         ...,
         [ 0.0681, -0.0300,  0.2666,  ..., -0.2159,  0.1449, -0.0308],
         [ 0.4049,  0.3196,  0.2386,  ..., -0.1069,  0.0296, -0.1451],
         [ 0.3280, -0.1366,  0.4958,  ...,  0.1776, -0.2365,  0.3795]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.3214, -0.3607, -0.3539,  ..., -0.1626,  0.1204,  0.1248],
         [-0.4270, -0.2943, -0.1438,  ..., -0.1151,  0.0758,  0.2645],
         ...,
         [-0.2825,  0.3836, -0.2065,  ...,  0.4463,  0.1604,  0.0325],
         [-0.3227,  0.1286, -0.2804,  ...,  0.4302,  0.0759, -0.0313],
         [-0.0481, -0.1945,  0.2152,  ...,  0.3411, -0.0938,  0.2824]]])
DESIRED: (shape=torch.Size([2, 11, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [ 0.1510,  0.2442,  0.2229,  ..., -0.4189,  0.1662,  0.0406],
         [ 0.3125,  0.0172,  0.3191,  ..., -0.0582, -0.0901, -0.2638],
         ...,
         [-0.1710, -0.0468,  0.0793,  ..., -0.3204,  0.2080, -0.0079],
         [ 0.0681, -0.0300,  0.2666,  ..., -0.2159,  0.1449, -0.0308],
         [ 0.4049,  0.3196,  0.2386,  ..., -0.1069,  0.0296, -0.1451]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.3214, -0.3607, -0.3539,  ..., -0.1626,  0.1204,  0.1248],
         [-0.4270, -0.2943, -0.1438,  ..., -0.1151,  0.0758,  0.2645],
         ...,
         [-0.0531,  0.1070,  0.0357,  ...,  0.2032,  0.0694,  0.0422],
         [-0.2825,  0.3836, -0.2065,  ...,  0.4463,  0.1604,  0.0325],
         [-0.3227,  0.1286, -0.2804,  ...,  0.4302,  0.0759, -0.0313]]])

2025-07-09 14:02:53.012857 GPU 6 156412 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.1999999999999997,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.1999999999999997,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 12, 114085069]) != torch.Size([2, 11, 114085069]).
ACTUAL: (shape=torch.Size([2, 12, 114085069]), dtype=torch.float32)
tensor([[[ 0.2453, -0.3780,  0.4174,  ...,  0.4323, -0.3515, -0.2068],
         [ 0.2453, -0.3859, -0.1952,  ...,  0.4646, -0.0981, -0.3807],
         [ 0.0449,  0.1221, -0.2990,  ...,  0.0486,  0.2546, -0.3712],
         ...,
         [ 0.1229, -0.1852, -0.0337,  ..., -0.0897, -0.3774, -0.2677],
         [ 0.0281, -0.4301, -0.4036,  ..., -0.1430, -0.4463, -0.2469],
         [ 0.3725, -0.2917, -0.3927,  ..., -0.2267, -0.4852,  0.0117]],

        [[-0.3751,  0.3461,  0.2788,  ...,  0.1286,  0.0210, -0.4097],
         [-0.2308,  0.2211,  0.1088,  ..., -0.3032,  0.4141, -0.3742],
         [ 0.1322, -0.2415,  0.1740,  ..., -0.4490, -0.1214,  0.1414],
         ...,
         [ 0.0261,  0.0030,  0.1635,  ..., -0.1375, -0.1853,  0.1931],
         [-0.0964,  0.1330,  0.1039,  ...,  0.0259,  0.0818,  0.2486],
         [-0.4443, -0.3451,  0.3841,  ...,  0.3407,  0.3018,  0.3517]]])
DESIRED: (shape=torch.Size([2, 11, 114085069]), dtype=torch.float32)
tensor([[[ 0.2453, -0.3780,  0.4174,  ...,  0.4323, -0.3515, -0.2068],
         [ 0.2453, -0.3865, -0.2442,  ...,  0.4672, -0.0778, -0.3946],
         [ 0.0048,  0.2240, -0.2952,  ..., -0.0359,  0.3149, -0.3624],
         ...,
         [ 0.2831,  0.0033,  0.1915,  ..., -0.0828, -0.3477, -0.2026],
         [-0.0925, -0.4785, -0.4074,  ..., -0.1137, -0.4327, -0.3375],
         [ 0.3725, -0.2917, -0.3927,  ..., -0.2267, -0.4852,  0.0117]],

        [[-0.3751,  0.3461,  0.2788,  ...,  0.1286,  0.0210, -0.4097],
         [-0.2193,  0.2111,  0.0952,  ..., -0.3378,  0.4455, -0.3713],
         [ 0.1990, -0.3290,  0.1938,  ..., -0.4609, -0.2443,  0.2431],
         ...,
         [-0.0048, -0.2184,  0.2833,  ..., -0.1410, -0.2795,  0.1907],
         [ 0.0254,  0.3004,  0.0058,  ..., -0.0843,  0.0048,  0.2125],
         [-0.4443, -0.3451,  0.3841,  ...,  0.3407,  0.3018,  0.3517]]])

2025-07-09 14:03:26.367504 GPU 4 159314 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.1999999999999997,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.1999999999999997,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 12, 114085069]) != torch.Size([2, 11, 114085069]).
ACTUAL: (shape=torch.Size([2, 12, 114085069]), dtype=torch.float32)
tensor([[[-0.3590,  0.4389, -0.2580,  ...,  0.1361, -0.3949,  0.4816],
         [ 0.0741,  0.2030,  0.2526,  ...,  0.2201,  0.2282,  0.3399],
         [-0.1865, -0.0579, -0.0583,  ..., -0.1667, -0.0853, -0.1247],
         ...,
         [ 0.1660, -0.0057,  0.2081,  ..., -0.0732,  0.0519,  0.3387],
         [ 0.3391, -0.2414,  0.2615,  ...,  0.0696, -0.3280,  0.2267],
         [ 0.4715, -0.1046, -0.3353,  ..., -0.3015, -0.2857,  0.0745]],

        [[-0.1440,  0.2287, -0.1257,  ..., -0.0505, -0.3637, -0.2437],
         [ 0.3697,  0.0481,  0.3144,  ...,  0.1349, -0.3895,  0.0041],
         [ 0.1519, -0.2787, -0.1682,  ...,  0.0143, -0.1333, -0.2825],
         ...,
         [-0.0705, -0.3268,  0.1179,  ...,  0.2276, -0.0098,  0.2092],
         [-0.2873, -0.2926,  0.0708,  ...,  0.1515,  0.0528,  0.0796],
         [-0.4751,  0.3213,  0.1467,  ..., -0.1257, -0.3660, -0.1627]]])
DESIRED: (shape=torch.Size([2, 11, 114085069]), dtype=torch.float32)
tensor([[[-0.3590,  0.4389, -0.2580,  ...,  0.1361, -0.3949,  0.4816],
         [ 0.0380,  0.2227,  0.2101,  ...,  0.2131,  0.1763,  0.3517],
         [-0.1568, -0.0405, -0.0229,  ..., -0.1329, -0.0476, -0.0886],
         ...,
         [ 0.2386, -0.0943, -0.0623,  ..., -0.1720,  0.2161,  0.4108],
         [ 0.1780, -0.0279,  0.2236,  ..., -0.0544,  0.0194,  0.3322],
         [ 0.3501, -0.2300,  0.2117,  ...,  0.0387, -0.3245,  0.2141]],

        [[-0.1440,  0.2287, -0.1257,  ..., -0.0505, -0.3637, -0.2437],
         [ 0.3269,  0.0632,  0.2777,  ...,  0.1195, -0.3873, -0.0165],
         [ 0.1796, -0.2548, -0.1198,  ...,  0.0278, -0.1551, -0.2541],
         ...,
         [ 0.1816, -0.3216, -0.0119,  ..., -0.0435, -0.0970,  0.0222],
         [-0.0851, -0.3353,  0.1126,  ...,  0.2264,  0.0032,  0.2029],
         [-0.3029, -0.2414,  0.0772,  ...,  0.1284,  0.0179,  0.0594]]])

2025-07-09 14:03:36.251353 GPU 7 157406 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.1999999999999997,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.1999999999999997,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 12, 114085069]) != torch.Size([2, 11, 114085069]).
ACTUAL: (shape=torch.Size([2, 12, 114085069]), dtype=torch.float32)
tensor([[[ 0.2025,  0.2227, -0.4889,  ...,  0.3671, -0.4335, -0.3325],
         [-0.0925,  0.2900, -0.0198,  ...,  0.4508, -0.0498,  0.2510],
         [-0.0327,  0.0909, -0.2531,  ...,  0.4295,  0.0840,  0.4418],
         ...,
         [-0.1035,  0.2360, -0.0680,  ..., -0.3076,  0.1089,  0.1429],
         [-0.2842,  0.4190,  0.3069,  ..., -0.2527, -0.2195, -0.0141],
         [ 0.2455,  0.3928, -0.3154,  ...,  0.3122,  0.4746, -0.3272]],

        [[-0.0320,  0.3474, -0.3906,  ...,  0.4619,  0.4530,  0.3269],
         [ 0.1105, -0.0911,  0.3137,  ...,  0.2480, -0.0288,  0.0844],
         [-0.2107,  0.2125,  0.4077,  ...,  0.3381,  0.2393, -0.2648],
         ...,
         [-0.0331,  0.2901, -0.1092,  ...,  0.0814, -0.0579,  0.2058],
         [ 0.2263,  0.0920, -0.0342,  ...,  0.2197, -0.1983, -0.3217],
         [ 0.1686,  0.4366, -0.4333,  ...,  0.3013,  0.1988, -0.4518]]])
DESIRED: (shape=torch.Size([2, 11, 114085069]), dtype=torch.float32)
tensor([[[ 0.2025,  0.2227, -0.4889,  ...,  0.3671, -0.4335, -0.3325],
         [-0.1219,  0.2967,  0.0271,  ...,  0.4591, -0.0114,  0.3094],
         [-0.0005,  0.0359, -0.3399,  ...,  0.4193,  0.0965,  0.4575],
         ...,
         [-0.0268,  0.1874, -0.2000,  ..., -0.2894,  0.2330,  0.1655],
         [-0.3371,  0.4216,  0.3691,  ..., -0.3092, -0.2890,  0.0172],
         [ 0.2455,  0.3928, -0.3154,  ...,  0.3122,  0.4746, -0.3272]],

        [[-0.0320,  0.3474, -0.3906,  ...,  0.4619,  0.4530,  0.3269],
         [ 0.1248, -0.1349,  0.3842,  ...,  0.2266, -0.0770,  0.0601],
         [-0.3014,  0.3156,  0.3917,  ...,  0.3735,  0.3358, -0.3407],
         ...,
         [-0.1031,  0.3607, -0.1513,  ...,  0.0505,  0.0009,  0.3340],
         [ 0.2320,  0.0575,  0.0057,  ...,  0.2115, -0.2380, -0.3087],
         [ 0.1686,  0.4366, -0.4333,  ...,  0.3013,  0.1988, -0.4518]]])

2025-07-09 14:05:27.096497 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.2999999999999998,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.2999999999999998,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 13, 114085069]) != torch.Size([2, 12, 114085069]).
ACTUAL: (shape=torch.Size([2, 13, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.2092,  0.1018, -0.0066,  ...,  0.0059, -0.3146,  0.1561],
         [-0.0745, -0.2331, -0.1092,  ...,  0.0474, -0.0725,  0.4549],
         ...,
         [-0.1188, -0.1811, -0.0723,  ...,  0.1216, -0.1633,  0.3096],
         [ 0.1591, -0.3019,  0.0152,  ..., -0.0895, -0.1147,  0.3323],
         [ 0.3306, -0.1403, -0.4004,  ..., -0.1681,  0.2234,  0.0693]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.2178,  0.2489, -0.4808,  ...,  0.0445,  0.3186, -0.2532],
         [-0.2704,  0.3190, -0.0898,  ...,  0.2196,  0.1557, -0.1422],
         ...,
         [-0.0266, -0.1109, -0.2268,  ..., -0.2681, -0.2951, -0.1133],
         [ 0.0783, -0.2349, -0.1411,  ..., -0.0590, -0.1940, -0.2920],
         [ 0.4929, -0.4869, -0.2299,  ...,  0.0858,  0.0741,  0.0891]]])
DESIRED: (shape=torch.Size([2, 12, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.2092,  0.1018, -0.0066,  ...,  0.0059, -0.3146,  0.1561],
         [-0.0745, -0.2331, -0.1092,  ...,  0.0474, -0.0725,  0.4549],
         ...,
         [-0.2907,  0.1001, -0.4876,  ...,  0.2846, -0.0039,  0.1432],
         [-0.1188, -0.1811, -0.0723,  ...,  0.1216, -0.1633,  0.3096],
         [ 0.1591, -0.3019,  0.0152,  ..., -0.0895, -0.1147,  0.3323]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.2178,  0.2489, -0.4808,  ...,  0.0445,  0.3186, -0.2532],
         [-0.2704,  0.3190, -0.0898,  ...,  0.2196,  0.1557, -0.1422],
         ...,
         [ 0.0192, -0.0432, -0.4032,  ..., -0.4034, -0.2767,  0.3428],
         [-0.0266, -0.1109, -0.2268,  ..., -0.2681, -0.2951, -0.1133],
         [ 0.0783, -0.2349, -0.1411,  ..., -0.0590, -0.1940, -0.2920]]])

2025-07-09 14:05:53.204341 GPU 3 156571 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.2999999999999998,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.2999999999999998,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 13, 114085069]) != torch.Size([2, 12, 114085069]).
ACTUAL: (shape=torch.Size([2, 13, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [ 0.1629,  0.2602,  0.2228,  ..., -0.4176,  0.1755,  0.0304],
         [ 0.3120,  0.0507,  0.3116,  ..., -0.0846, -0.0611, -0.2505],
         ...,
         [ 0.2394,  0.2070,  0.2129,  ..., -0.2092,  0.1323, -0.1674],
         [ 0.3832,  0.1910,  0.3111,  ..., -0.0266, -0.0455,  0.0029],
         [ 0.3280, -0.1366,  0.4958,  ...,  0.1776, -0.2365,  0.3795]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.3297, -0.3691, -0.3642,  ..., -0.1707,  0.1261,  0.1333],
         [-0.4272, -0.3078, -0.1703,  ..., -0.1269,  0.0849,  0.2622],
         ...,
         [-0.3447,  0.3140, -0.3195,  ...,  0.4527,  0.1472, -0.0470],
         [-0.2452,  0.0375, -0.1406,  ...,  0.4050,  0.0281,  0.0572],
         [-0.0481, -0.1945,  0.2152,  ...,  0.3411, -0.0938,  0.2824]]])
DESIRED: (shape=torch.Size([2, 12, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [ 0.1933,  0.3011,  0.2224,  ..., -0.4143,  0.1994,  0.0046],
         [ 0.3131, -0.0224,  0.3280,  ..., -0.0269, -0.1245, -0.2794],
         ...,
         [ 0.0351, -0.0756,  0.2769,  ..., -0.2172,  0.1473, -0.0045],
         [ 0.4119,  0.3611,  0.2152,  ..., -0.1327,  0.0537, -0.1928],
         [ 0.3280, -0.1366,  0.4958,  ...,  0.1776, -0.2365,  0.3795]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.3508, -0.3905, -0.3907,  ..., -0.1914,  0.1406,  0.1548],
         [-0.4268, -0.2783, -0.1124,  ..., -0.1012,  0.0650,  0.2672],
         ...,
         [-0.2706,  0.3970, -0.1848,  ...,  0.4450,  0.1629,  0.0479],
         [-0.3476,  0.1580, -0.3255,  ...,  0.4383,  0.0914, -0.0598],
         [-0.0481, -0.1945,  0.2152,  ...,  0.3411, -0.0938,  0.2824]]])

2025-07-09 14:05:55.862650 GPU 6 156412 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.2999999999999998,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.2999999999999998,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 13, 114085069]) != torch.Size([2, 12, 114085069]).
ACTUAL: (shape=torch.Size([2, 13, 114085069]), dtype=torch.float32)
tensor([[[ 0.2453, -0.3780,  0.4174,  ...,  0.4323, -0.3515, -0.2068],
         [ 0.2453, -0.3851, -0.1340,  ...,  0.4614, -0.1234, -0.3633],
         [ 0.0950, -0.0053, -0.3037,  ...,  0.1542,  0.1791, -0.3823],
         ...,
         [ 0.1229, -0.1852, -0.0337,  ..., -0.0897, -0.3774, -0.2677],
         [-0.0150, -0.4474, -0.4050,  ..., -0.1326, -0.4415, -0.2793],
         [ 0.3725, -0.2917, -0.3927,  ..., -0.2267, -0.4852,  0.0117]],

        [[-0.3751,  0.3461,  0.2788,  ...,  0.1286,  0.0210, -0.4097],
         [-0.2453,  0.2336,  0.1258,  ..., -0.2601,  0.3748, -0.3777],
         [ 0.0486, -0.1321,  0.1492,  ..., -0.4342,  0.0321,  0.0143],
         ...,
         [ 0.0261,  0.0030,  0.1635,  ..., -0.1375, -0.1853,  0.1931],
         [-0.0529,  0.1928,  0.0689,  ..., -0.0134,  0.0543,  0.2357],
         [-0.4443, -0.3451,  0.3841,  ...,  0.3407,  0.3018,  0.3517]]])
DESIRED: (shape=torch.Size([2, 12, 114085069]), dtype=torch.float32)
tensor([[[ 0.2453, -0.3780,  0.4174,  ...,  0.4323, -0.3515, -0.2068],
         [ 0.2453, -0.3842, -0.0633,  ...,  0.4577, -0.1527, -0.3432],
         [ 0.1181, -0.0641, -0.3058,  ...,  0.2029,  0.1442, -0.3874],
         ...,
         [ 0.3425,  0.0485,  0.3340,  ..., -0.0275, -0.3301, -0.2234],
         [ 0.0818, -0.2335, -0.0914,  ..., -0.0914, -0.3850, -0.2844],
         [ 0.0347, -0.4274, -0.4034,  ..., -0.1446, -0.4471, -0.2420]],

        [[-0.3751,  0.3461,  0.2788,  ...,  0.1286,  0.0210, -0.4097],
         [-0.2619,  0.2480,  0.1454,  ..., -0.2102,  0.3294, -0.3818],
         [ 0.0101, -0.0816,  0.1378,  ..., -0.4273,  0.1030, -0.0444],
         ...,
         [-0.0585, -0.3429,  0.3218,  ..., -0.1351, -0.2841,  0.1168],
         [ 0.0340,  0.0598,  0.1328,  ..., -0.1365, -0.1611,  0.1937],
         [-0.1031,  0.1238,  0.1093,  ...,  0.0320,  0.0860,  0.2506]]])

2025-07-09 14:06:35.626527 GPU 7 157406 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.2999999999999998,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.2999999999999998,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 13, 114085069]) != torch.Size([2, 12, 114085069]).
ACTUAL: (shape=torch.Size([2, 13, 114085069]), dtype=torch.float32)
tensor([[[ 0.2025,  0.2227, -0.4889,  ...,  0.3671, -0.4335, -0.3325],
         [-0.0679,  0.2844, -0.0589,  ...,  0.4438, -0.0818,  0.2024],
         [-0.0596,  0.1367, -0.1808,  ...,  0.4381,  0.0736,  0.4287],
         ...,
         [-0.1674,  0.2764,  0.0419,  ..., -0.3227,  0.0054,  0.1242],
         [-0.2400,  0.4168,  0.2550,  ..., -0.2056, -0.1617, -0.0402],
         [ 0.2455,  0.3928, -0.3154,  ...,  0.3122,  0.4746, -0.3272]],

        [[-0.0320,  0.3474, -0.3906,  ...,  0.4619,  0.4530,  0.3269],
         [ 0.0987, -0.0546,  0.2550,  ...,  0.2658,  0.0113,  0.1046],
         [-0.1351,  0.1265,  0.4211,  ...,  0.3087,  0.1589, -0.2015],
         ...,
         [ 0.0252,  0.2312, -0.0741,  ...,  0.1071, -0.1069,  0.0989],
         [ 0.2215,  0.1207, -0.0675,  ...,  0.2265, -0.1652, -0.3325],
         [ 0.1686,  0.4366, -0.4333,  ...,  0.3013,  0.1988, -0.4518]]])
DESIRED: (shape=torch.Size([2, 12, 114085069]), dtype=torch.float32)
tensor([[[ 0.2025,  0.2227, -0.4889,  ...,  0.3671, -0.4335, -0.3325],
         [-0.0925,  0.2900, -0.0198,  ...,  0.4508, -0.0498,  0.2510],
         [-0.0327,  0.0909, -0.2531,  ...,  0.4295,  0.0840,  0.4418],
         ...,
         [-0.1035,  0.2360, -0.0680,  ..., -0.3076,  0.1089,  0.1429],
         [-0.2842,  0.4190,  0.3069,  ..., -0.2527, -0.2195, -0.0141],
         [ 0.2455,  0.3928, -0.3154,  ...,  0.3122,  0.4746, -0.3272]],

        [[-0.0320,  0.3474, -0.3906,  ...,  0.4619,  0.4530,  0.3269],
         [ 0.1105, -0.0911,  0.3137,  ...,  0.2480, -0.0288,  0.0844],
         [-0.2107,  0.2125,  0.4077,  ...,  0.3381,  0.2393, -0.2648],
         ...,
         [-0.0331,  0.2901, -0.1092,  ...,  0.0814, -0.0579,  0.2058],
         [ 0.2263,  0.0920, -0.0342,  ...,  0.2197, -0.1983, -0.3217],
         [ 0.1686,  0.4366, -0.4333,  ...,  0.3013,  0.1988, -0.4518]]])

2025-07-09 14:06:40.060859 GPU 5 158717 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752041420 (unix time) try "date -d @1752041420" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26bfd) received by PID 158717 (TID 0x7f6a8bdba740) from PID 158717 ***]


2025-07-09 14:07:49.008647 GPU 4 159314 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2057960464 / 3194381932 (64.4%)
Greatest absolute difference: 0.2637062668800354 at index (0, 12, 90171940) (up to 0.01 allowed)
Greatest relative difference: 18998693888.0 at index (0, 11, 96031253) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[-3.5897e-01,  4.3886e-01, -2.5798e-01,  ...,  1.3611e-01, -3.9493e-01,  4.8161e-01],
         [ 1.9099e-02,  2.3296e-01,  1.8776e-01,  ...,  2.0945e-01,  1.4911e-01,  3.5792e-01],
         [-6.9982e-02,  1.0178e-02,  8.0259e-02,  ..., -3.4277e-02,  6.2294e-02,  1.6744e-02],
         ...,
         [ 2.7745e-01, -2.1209e-01,  3.5231e-01,  ...,  1.0154e-01, -2.5006e-01,  2.7811e-01],
         [ 4.0213e-01, -1.7625e-01, -2.2697e-02,  ..., -1.0710e-01, -3.0787e-01,  1.5423e-01],
         [ 4.7146e-01, -1.0456e-01, -3.3526e-01,  ..., -3.0152e-01, -2.8568e-01,  7.4465e-02]],

        [[-1.4398e-01,  2.2870e-01, -1.2568e-01,  ..., -5.0473e-02, -3.6365e-01, -2.4368e-01],
         [ 3.0445e-01,  7.1067e-02,  2.5850e-01,  ...,  1.1140e-01, -3.8621e-01, -2.7338e-02],
         [ 2.6028e-01, -1.8509e-01,  2.1316e-02,  ...,  6.7163e-02, -2.1884e-01, -1.7095e-01],
         ...,
         [-2.0624e-01, -4.0605e-01,  6.8336e-02,  ...,  2.1638e-01,  1.1095e-01,  1.5048e-01],
         [-3.7671e-01, -2.6392e-04,  1.0695e-01,  ...,  1.9490e-02, -1.4660e-01, -3.5767e-02],
         [-4.7509e-01,  3.2127e-01,  1.4666e-01,  ..., -1.2574e-01, -3.6597e-01, -1.6270e-01]]])
DESIRED: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[-0.3590,  0.4389, -0.2580,  ...,  0.1361, -0.3949,  0.4816],
         [ 0.0075,  0.2393,  0.1740,  ...,  0.2072,  0.1324,  0.3617],
         [-0.0453,  0.0246,  0.1096,  ..., -0.0063,  0.0935,  0.0467],
         ...,
         [ 0.2229, -0.1110,  0.2817,  ...,  0.0159, -0.1022,  0.3078],
         [ 0.3595, -0.2204,  0.1697,  ...,  0.0125, -0.3215,  0.2033],
         [ 0.4715, -0.1046, -0.3353,  ..., -0.3015, -0.2857,  0.0745]],

        [[-0.1440,  0.2287, -0.1257,  ..., -0.0505, -0.3637, -0.2437],
         [ 0.2907,  0.0759,  0.2467,  ...,  0.1064, -0.3855, -0.0340],
         [ 0.2832, -0.1653,  0.0614,  ...,  0.0783, -0.2369, -0.1473],
         ...,
         [-0.1397, -0.3672,  0.0926,  ...,  0.2219,  0.0518,  0.1792],
         [-0.3162, -0.1981,  0.0825,  ...,  0.1089, -0.0116,  0.0423],
         [-0.4751,  0.3213,  0.1467,  ..., -0.1257, -0.3660, -0.1627]]])

2025-07-09 14:08:48.225497 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1514558156 / 3194381932 (47.4%)
Greatest absolute difference: 0.12087695300579071 at index (1, 12, 81762810) (up to 0.01 allowed)
Greatest relative difference: 174441488.0 at index (1, 11, 9434222) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.2049,  0.0859, -0.0165,  ...,  0.0079, -0.3130,  0.1865],
         [-0.0832, -0.2157, -0.1080,  ...,  0.0453, -0.0932,  0.4526],
         ...,
         [-0.1017, -0.1998, -0.0443,  ...,  0.1062, -0.1752,  0.3244],
         [ 0.1490, -0.3114,  0.0396,  ..., -0.0848, -0.1345,  0.3478],
         [ 0.3306, -0.1403, -0.4004,  ..., -0.1681,  0.2234,  0.0693]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.2321,  0.2703, -0.4819,  ...,  0.0649,  0.3164, -0.2528],
         [-0.2773,  0.3301, -0.1262,  ...,  0.2204,  0.1687, -0.1519],
         ...,
         [-0.0370, -0.1101, -0.2148,  ..., -0.2560, -0.2988, -0.1478],
         [ 0.0539, -0.2201, -0.1359,  ..., -0.0675, -0.2098, -0.3144],
         [ 0.4929, -0.4869, -0.2299,  ...,  0.0858,  0.0741,  0.0891]]])
DESIRED: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.2182,  0.1361,  0.0146,  ...,  0.0017, -0.3182,  0.0910],
         [-0.1058, -0.1710, -0.1049,  ...,  0.0397, -0.1462,  0.4467],
         ...,
         [-0.0580, -0.2481,  0.0275,  ...,  0.0666, -0.2056,  0.3622],
         [ 0.1807, -0.2815, -0.0372,  ..., -0.0994, -0.0720,  0.2992],
         [ 0.3306, -0.1403, -0.4004,  ..., -0.1681,  0.2234,  0.0693]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.1872,  0.2031, -0.4786,  ...,  0.0008,  0.3233, -0.2541],
         [-0.2950,  0.3587, -0.2198,  ...,  0.2223,  0.2022, -0.1770],
         ...,
         [-0.0638, -0.1079, -0.1838,  ..., -0.2251, -0.3084, -0.2368],
         [ 0.1306, -0.2667, -0.1523,  ..., -0.0408, -0.1602, -0.2440],
         [ 0.4929, -0.4869, -0.2299,  ...,  0.0858,  0.0741,  0.0891]]])

2025-07-09 14:09:03.021936 GPU 6 156412 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752041580 (unix time) try "date -d @1752041580" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x262fc) received by PID 156412 (TID 0x7f7e7e5eb740) from PID 156412 ***]


2025-07-09 14:09:03.935213 GPU 3 156571 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4999999999999998,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4999999999999998,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 15, 114085069]) != torch.Size([2, 14, 114085069]).
ACTUAL: (shape=torch.Size([2, 15, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [-0.0042,  0.0353,  0.2247,  ..., -0.4358,  0.0444,  0.1726],
         [ 0.3080,  0.3285,  0.2491,  ..., -0.3038,  0.1799, -0.1407],
         ...,
         [ 0.3270,  0.3283,  0.1854,  ..., -0.2058,  0.1259, -0.2373],
         [ 0.3792,  0.1676,  0.3243,  ..., -0.0120, -0.0591,  0.0298],
         [ 0.3280, -0.1366,  0.4958,  ...,  0.1776, -0.2365,  0.3795]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.2135, -0.2515, -0.2190,  ..., -0.0570,  0.0463,  0.0147],
         [-0.4287, -0.4198, -0.3902,  ..., -0.2245,  0.1605,  0.2434],
         ...,
         [-0.3766,  0.2784, -0.3773,  ...,  0.4560,  0.1405, -0.0877],
         [-0.2312,  0.0209, -0.1152,  ...,  0.4005,  0.0193,  0.0733],
         [-0.0481, -0.1945,  0.2152,  ...,  0.3411, -0.0938,  0.2824]]])
DESIRED: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [-0.0042,  0.0353,  0.2247,  ..., -0.4358,  0.0444,  0.1726],
         [ 0.3080,  0.3285,  0.2491,  ..., -0.3038,  0.1799, -0.1407],
         ...,
         [-0.0872, -0.2450,  0.3153,  ..., -0.2220,  0.1563,  0.0932],
         [ 0.3270,  0.3283,  0.1854,  ..., -0.2058,  0.1259, -0.2373],
         [ 0.3792,  0.1676,  0.3243,  ..., -0.0120, -0.0591,  0.0298]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.2135, -0.2515, -0.2190,  ..., -0.0570,  0.0463,  0.0147],
         [-0.4287, -0.4198, -0.3902,  ..., -0.2245,  0.1605,  0.2434],
         ...,
         [-0.2261,  0.4467, -0.1040,  ...,  0.4404,  0.1723,  0.1047],
         [-0.3766,  0.2784, -0.3773,  ...,  0.4560,  0.1405, -0.0877],
         [-0.2312,  0.0209, -0.1152,  ...,  0.4005,  0.0193,  0.0733]]])

2025-07-09 14:09:38.749308 GPU 7 157406 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4999999999999998,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4999999999999998,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 15, 114085069]) != torch.Size([2, 14, 114085069]).
ACTUAL: (shape=torch.Size([2, 15, 114085069]), dtype=torch.float32)
tensor([[[ 0.2025,  0.2227, -0.4889,  ...,  0.3671, -0.4335, -0.3325],
         [-0.0378,  0.2775, -0.1067,  ...,  0.4353, -0.1209,  0.1430],
         [-0.0924,  0.1928, -0.0924,  ...,  0.4485,  0.0609,  0.4127],
         ...,
         [-0.4019,  0.4248,  0.4452,  ..., -0.3782, -0.3738,  0.0554],
         [ 0.0297,  0.4035, -0.0619,  ...,  0.0821,  0.1918, -0.1996],
         [ 0.2455,  0.3928, -0.3154,  ...,  0.3122,  0.4746, -0.3272]],

        [[-0.0320,  0.3474, -0.3906,  ...,  0.4619,  0.4530,  0.3269],
         [ 0.0841, -0.0099,  0.1833,  ...,  0.2876,  0.0604,  0.1293],
         [-0.0426,  0.0215,  0.4375,  ...,  0.2726,  0.0607, -0.1242],
         ...,
         [ 0.2391,  0.0154,  0.0544,  ...,  0.2015, -0.2865, -0.2928],
         [ 0.1921,  0.2962, -0.2707,  ...,  0.2680,  0.0370, -0.3988],
         [ 0.1686,  0.4366, -0.4333,  ...,  0.3013,  0.1988, -0.4518]]])
DESIRED: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[ 0.2025,  0.2227, -0.4889,  ...,  0.3671, -0.4335, -0.3325],
         [-0.0471,  0.2796, -0.0920,  ...,  0.4379, -0.1088,  0.1612],
         [-0.0823,  0.1756, -0.1196,  ...,  0.4453,  0.0648,  0.4176],
         ...,
         [-0.2215,  0.3107,  0.1350,  ..., -0.3355, -0.0821,  0.1083],
         [-0.2027,  0.4150,  0.2111,  ..., -0.1658, -0.1127, -0.0623],
         [ 0.2455,  0.3928, -0.3154,  ...,  0.3122,  0.4746, -0.3272]],

        [[-0.0320,  0.3474, -0.3906,  ...,  0.4619,  0.4530,  0.3269],
         [ 0.0886, -0.0236,  0.2054,  ...,  0.2809,  0.0453,  0.1217],
         [-0.0711,  0.0538,  0.4325,  ...,  0.2837,  0.0909, -0.1480],
         ...,
         [ 0.0746,  0.1814, -0.0445,  ...,  0.1289, -0.1483,  0.0085],
         [ 0.2174,  0.1450, -0.0956,  ...,  0.2322, -0.1372, -0.3417],
         [ 0.1686,  0.4366, -0.4333,  ...,  0.3013,  0.1988, -0.4518]]])

2025-07-09 14:11:06.429325 GPU 5 160556 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4999999999999998,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4999999999999998,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 15, 114085069]) != torch.Size([2, 14, 114085069]).
ACTUAL: (shape=torch.Size([2, 15, 114085069]), dtype=torch.float32)
tensor([[[-0.0811,  0.0005, -0.2664,  ..., -0.0649,  0.1573,  0.2875],
         [-0.2488, -0.3022, -0.1646,  ...,  0.0875,  0.0154, -0.1682],
         [-0.1151, -0.2771, -0.1938,  ...,  0.1972,  0.0700, -0.1743],
         ...,
         [-0.0971, -0.1496,  0.1910,  ..., -0.0533,  0.2127,  0.1687],
         [-0.1433, -0.1221,  0.1907,  ..., -0.1186,  0.3560,  0.3452],
         [-0.4171, -0.0115,  0.0187,  ..., -0.2304,  0.3502,  0.4198]],

        [[ 0.1603,  0.2201,  0.4018,  ...,  0.2288, -0.3050,  0.2042],
         [-0.1165, -0.0478, -0.1600,  ...,  0.0529, -0.3131, -0.0534],
         [-0.3323, -0.2486, -0.3087,  ..., -0.1246, -0.2777, -0.0889],
         ...,
         [-0.2306, -0.3771,  0.0746,  ...,  0.4725, -0.0242, -0.0330],
         [-0.2565, -0.1337,  0.0018,  ...,  0.4701, -0.1440,  0.1276],
         [ 0.1776,  0.4010, -0.3580,  ...,  0.4413, -0.3877,  0.2089]]])
DESIRED: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[-0.0811,  0.0005, -0.2664,  ..., -0.0649,  0.1573,  0.2875],
         [-0.2115, -0.2349, -0.1872,  ...,  0.0537,  0.0470, -0.0669],
         [-0.2096, -0.3576, -0.1580,  ...,  0.1868,  0.0144, -0.2772],
         ...,
         [-0.3000, -0.0845,  0.0086,  ..., -0.0472, -0.0681, -0.0903],
         [-0.0530, -0.1637,  0.2307,  ..., -0.0546,  0.2737,  0.2250],
         [-0.2041, -0.0975,  0.1524,  ..., -0.1435,  0.3547,  0.3618]],

        [[ 0.1603,  0.2201,  0.4018,  ...,  0.2288, -0.3050,  0.2042],
         [-0.0550,  0.0117, -0.0352,  ...,  0.0920, -0.3113,  0.0038],
         [-0.3065, -0.2269, -0.3768,  ..., -0.0913, -0.2943, -0.1337],
         ...,
         [ 0.2812, -0.2743, -0.1690,  ...,  0.4465, -0.0539, -0.2544],
         [-0.3419, -0.3995,  0.1276,  ...,  0.4782, -0.0177,  0.0151],
         [-0.1600, -0.0149, -0.0781,  ...,  0.4637, -0.1982,  0.1457]]])

2025-07-09 14:11:40.152118 GPU 4 159314 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4999999999999998,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.4999999999999998,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 15, 114085069]) != torch.Size([2, 14, 114085069]).
ACTUAL: (shape=torch.Size([2, 15, 114085069]), dtype=torch.float32)
tensor([[[-0.3590,  0.4389, -0.2580,  ...,  0.1361, -0.3949,  0.4816],
         [-0.0187,  0.2535,  0.1432,  ...,  0.2021,  0.0947,  0.3703],
         [ 0.0101,  0.0570,  0.1755,  ...,  0.0567,  0.1638,  0.1140],
         ...,
         [ 0.2452, -0.1524,  0.3106,  ...,  0.0510, -0.1627,  0.2956],
         [ 0.3675, -0.2121,  0.1336,  ..., -0.0099, -0.3190,  0.1941],
         [ 0.4715, -0.1046, -0.3353,  ..., -0.3015, -0.2857,  0.0745]],

        [[-0.1440,  0.2287, -0.1257,  ..., -0.0505, -0.3637, -0.2437],
         [ 0.2596,  0.0868,  0.2201,  ...,  0.0952, -0.3840, -0.0490],
         [ 0.3348, -0.1207,  0.1516,  ...,  0.1035, -0.2776, -0.0942],
         ...,
         [-0.1669, -0.3831,  0.0827,  ...,  0.2196,  0.0760,  0.1675],
         [-0.3275, -0.1610,  0.0871,  ...,  0.0921, -0.0369,  0.0277],
         [-0.4751,  0.3213,  0.1467,  ..., -0.1257, -0.3660, -0.1627]]])
DESIRED: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[-0.3590,  0.4389, -0.2580,  ...,  0.1361, -0.3949,  0.4816],
         [ 0.0075,  0.2393,  0.1740,  ...,  0.2072,  0.1324,  0.3617],
         [-0.0453,  0.0246,  0.1096,  ..., -0.0063,  0.0935,  0.0467],
         ...,
         [ 0.2229, -0.1110,  0.2817,  ...,  0.0159, -0.1022,  0.3078],
         [ 0.3595, -0.2204,  0.1697,  ...,  0.0125, -0.3215,  0.2033],
         [ 0.4715, -0.1046, -0.3353,  ..., -0.3015, -0.2857,  0.0745]],

        [[-0.1440,  0.2287, -0.1257,  ..., -0.0505, -0.3637, -0.2437],
         [ 0.2907,  0.0759,  0.2467,  ...,  0.1064, -0.3855, -0.0340],
         [ 0.2832, -0.1653,  0.0614,  ...,  0.0783, -0.2369, -0.1473],
         ...,
         [-0.1397, -0.3672,  0.0926,  ...,  0.2219,  0.0518,  0.1792],
         [-0.3162, -0.1981,  0.0825,  ...,  0.1089, -0.0116,  0.0423],
         [-0.4751,  0.3213,  0.1467,  ..., -0.1257, -0.3660, -0.1627]]])

2025-07-09 14:12:29.802122 GPU 3 156571 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.5999999999999996,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.5999999999999996,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 16, 114085069]) != torch.Size([2, 15, 114085069]).
ACTUAL: (shape=torch.Size([2, 16, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [-0.0430, -0.0169,  0.2252,  ..., -0.4400,  0.0140,  0.2057],
         [ 0.3068,  0.4063,  0.2316,  ..., -0.3652,  0.2474, -0.1099],
         ...,
         [ 0.3917,  0.4179,  0.1651,  ..., -0.2032,  0.1211, -0.2890],
         [ 0.3728,  0.1295,  0.3458,  ...,  0.0117, -0.0813,  0.0735],
         [ 0.3280, -0.1366,  0.4958,  ...,  0.1776, -0.2365,  0.3795]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.1865, -0.2242, -0.1852,  ..., -0.0306,  0.0278, -0.0128],
         [-0.4291, -0.4511, -0.4518,  ..., -0.2518,  0.1817,  0.2381],
         ...,
         [-0.4001,  0.2521, -0.4200,  ...,  0.4584,  0.1355, -0.1178],
         [-0.2083, -0.0060, -0.0739,  ...,  0.3930,  0.0052,  0.0994],
         [-0.0481, -0.1945,  0.2152,  ...,  0.3411, -0.0938,  0.2824]]])
DESIRED: (shape=torch.Size([2, 15, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [-0.0430, -0.0169,  0.2252,  ..., -0.4400,  0.0140,  0.2057],
         [ 0.3068,  0.4063,  0.2316,  ..., -0.3652,  0.2474, -0.1099],
         ...,
         [ 0.0034, -0.1196,  0.2869,  ..., -0.2184,  0.1497,  0.0209],
         [ 0.3917,  0.4179,  0.1651,  ..., -0.2032,  0.1211, -0.2890],
         [ 0.3728,  0.1295,  0.3458,  ...,  0.0117, -0.0813,  0.0735]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.1865, -0.2242, -0.1852,  ..., -0.0306,  0.0278, -0.0128],
         [-0.4291, -0.4511, -0.4518,  ..., -0.2518,  0.1817,  0.2381],
         ...,
         [-0.2590,  0.4099, -0.1638,  ...,  0.4438,  0.1653,  0.0626],
         [-0.4001,  0.2521, -0.4200,  ...,  0.4584,  0.1355, -0.1178],
         [-0.2083, -0.0060, -0.0739,  ...,  0.3930,  0.0052,  0.0994]]])

2025-07-09 14:12:56.181204 GPU 7 157406 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.5999999999999996,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.5999999999999996,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 16, 114085069]) != torch.Size([2, 15, 114085069]).
ACTUAL: (shape=torch.Size([2, 16, 114085069]), dtype=torch.float32)
tensor([[[ 0.2025,  0.2227, -0.4889,  ...,  0.3671, -0.4335, -0.3325],
         [-0.0228,  0.2741, -0.1306,  ...,  0.4310, -0.1404,  0.1132],
         [-0.1088,  0.2208, -0.0482,  ...,  0.4537,  0.0545,  0.4047],
         ...,
         [-0.3209,  0.4208,  0.3501,  ..., -0.2919, -0.2677,  0.0076],
         [ 0.0837,  0.4008, -0.1253,  ...,  0.1396,  0.2625, -0.2315],
         [ 0.2455,  0.3928, -0.3154,  ...,  0.3122,  0.4746, -0.3272]],

        [[-0.0320,  0.3474, -0.3906,  ...,  0.4619,  0.4530,  0.3269],
         [ 0.0769,  0.0124,  0.1474,  ...,  0.2985,  0.0850,  0.1416],
         [ 0.0036, -0.0310,  0.4457,  ...,  0.2546,  0.0115, -0.0855],
         ...,
         [ 0.2303,  0.0681, -0.0065,  ...,  0.2140, -0.2259, -0.3127],
         [ 0.1862,  0.3313, -0.3114,  ...,  0.2764,  0.0774, -0.4121],
         [ 0.1686,  0.4366, -0.4333,  ...,  0.3013,  0.1988, -0.4518]]])
DESIRED: (shape=torch.Size([2, 15, 114085069]), dtype=torch.float32)
tensor([[[ 0.2025,  0.2227, -0.4889,  ...,  0.3671, -0.4335, -0.3325],
         [-0.0293,  0.2756, -0.1203,  ...,  0.4328, -0.1320,  0.1260],
         [-0.1017,  0.2088, -0.0671,  ...,  0.4515,  0.0573,  0.4081],
         ...,
         [-0.2679,  0.3400,  0.2147,  ..., -0.3465, -0.1571,  0.0947],
         [-0.1707,  0.4134,  0.1735,  ..., -0.1316, -0.0708, -0.0812],
         [ 0.2455,  0.3928, -0.3154,  ...,  0.3122,  0.4746, -0.3272]],

        [[-0.0320,  0.3474, -0.3906,  ...,  0.4619,  0.4530,  0.3269],
         [ 0.0800,  0.0029,  0.1628,  ...,  0.2939,  0.0744,  0.1364],
         [-0.0162, -0.0085,  0.4422,  ...,  0.2623,  0.0326, -0.1021],
         ...,
         [ 0.1169,  0.1388, -0.0190,  ...,  0.1476, -0.1839, -0.0689],
         [ 0.2139,  0.1658, -0.1197,  ...,  0.2372, -0.1132, -0.3496],
         [ 0.1686,  0.4366, -0.4333,  ...,  0.3013,  0.1988, -0.4518]]])

2025-07-09 14:13:10.455210 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.5999999999999996,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.5999999999999996,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 16, 114085069]) != torch.Size([2, 15, 114085069]).
ACTUAL: (shape=torch.Size([2, 16, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.2151,  0.1242,  0.0073,  ...,  0.0031, -0.3170,  0.1136],
         [-0.1254, -0.1323, -0.1022,  ...,  0.0348, -0.1922,  0.4415],
         ...,
         [-0.0201, -0.2899,  0.0898,  ...,  0.0322, -0.2320,  0.3950],
         [ 0.1732, -0.2886, -0.0190,  ..., -0.0959, -0.0868,  0.3107],
         [ 0.3306, -0.1403, -0.4004,  ..., -0.1681,  0.2234,  0.0693]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.1978,  0.2190, -0.4794,  ...,  0.0160,  0.3217, -0.2538],
         [-0.3104,  0.3835, -0.3010,  ...,  0.2240,  0.2312, -0.1987],
         ...,
         [-0.0870, -0.1060, -0.1569,  ..., -0.1983, -0.3166, -0.3139],
         [ 0.1124, -0.2557, -0.1484,  ..., -0.0471, -0.1719, -0.2606],
         [ 0.4929, -0.4869, -0.2299,  ...,  0.0858,  0.0741,  0.0891]]])
DESIRED: (shape=torch.Size([2, 15, 114085069]), dtype=torch.float32)
tensor([[[-0.2811,  0.3735,  0.1619,  ..., -0.0275, -0.3430, -0.3603],
         [-0.2329,  0.1917,  0.0491,  ..., -0.0052, -0.3240, -0.0148],
         [-0.1568, -0.0702, -0.0978,  ...,  0.0271, -0.2660,  0.4333],
         ...,
         [-0.2356, -0.0521, -0.2645,  ...,  0.2275, -0.0818,  0.2084],
         [ 0.0406, -0.3570,  0.1898,  ..., -0.0228, -0.2744,  0.4477],
         [ 0.2159, -0.2484, -0.1223,  ..., -0.1155, -0.0028,  0.2453]],

        [[ 0.0249, -0.1145, -0.4630,  ..., -0.3020,  0.3558, -0.2605],
         [-0.1375,  0.1287, -0.4749,  ..., -0.0701,  0.3309, -0.2556],
         [-0.3351,  0.4232, -0.4311,  ...,  0.2267,  0.2778, -0.2335],
         ...,
         [ 0.0450, -0.1168, -0.3097,  ..., -0.3508, -0.2697,  0.1245],
         [-0.1243, -0.1029, -0.1138,  ..., -0.1552, -0.3299, -0.4375],
         [ 0.2155, -0.3183, -0.1705,  ..., -0.0111, -0.1053, -0.1659]]])

2025-07-09 14:13:44.960652 GPU 6 161328 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.5999999999999996,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.5999999999999996,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 16, 114085069]) != torch.Size([2, 15, 114085069]).
ACTUAL: (shape=torch.Size([2, 16, 114085069]), dtype=torch.float32)
tensor([[[ 1.3581e-01,  1.8452e-01,  1.4160e-02,  ..., -4.2689e-01, -4.4357e-01, -2.6336e-01],
         [-2.3325e-01, -7.9422e-02, -1.2839e-01,  ..., -2.6520e-01, -3.4846e-01, -1.9332e-01],
         [-4.6580e-01, -1.5111e-01, -1.8064e-01,  ..., -3.3570e-02, -1.6485e-01, -1.1519e-01],
         ...,
         [-1.9126e-01,  1.8886e-01,  3.5790e-01,  ..., -2.5357e-01, -3.4588e-01, -1.2328e-01],
         [ 3.6318e-02,  2.7107e-01,  2.1521e-01,  ..., -9.4449e-02, -1.3711e-01,  5.2230e-02],
         [ 2.7267e-01,  4.9409e-01,  2.6460e-02,  ...,  2.9745e-01,  1.4761e-01,  1.7849e-01]],

        [[-6.2856e-02, -1.8685e-01,  1.3960e-01,  ..., -4.9468e-01, -1.1149e-02, -2.9585e-01],
         [-1.9631e-01,  1.9242e-01,  3.5290e-01,  ...,  9.2056e-02,  2.4481e-01, -3.7649e-01],
         [-1.6066e-01,  4.4998e-01,  3.0383e-01,  ...,  4.2928e-01,  2.8915e-01, -2.8048e-01],
         ...,
         [ 3.2286e-01,  2.8155e-01,  9.9757e-02,  ..., -4.5566e-01, -2.7469e-01, -4.8749e-01],
         [ 1.0117e-01,  2.5632e-01, -3.6425e-02,  ..., -2.7352e-01, -1.9144e-01, -2.4188e-01],
         [-4.9048e-01, -3.4923e-04, -3.8154e-01,  ...,  8.1403e-03,  1.5223e-01,  1.4275e-01]]])
DESIRED: (shape=torch.Size([2, 15, 114085069]), dtype=torch.float32)
tensor([[[ 1.3581e-01,  1.8452e-01,  1.4160e-02,  ..., -4.2689e-01, -4.4357e-01, -2.6336e-01],
         [-2.5961e-01, -9.8275e-02, -1.3857e-01,  ..., -2.5365e-01, -3.4166e-01, -1.8831e-01],
         [-4.6003e-01, -1.0643e-01, -1.6230e-01,  ...,  1.9501e-02, -1.1334e-01, -1.0172e-01],
         ...,
         [-2.2127e-01,  2.1735e-01,  3.6512e-01,  ..., -2.0980e-01, -3.5400e-01, -1.6242e-01],
         [ 1.9435e-02,  2.5514e-01,  2.2869e-01,  ..., -1.2244e-01, -1.5745e-01,  4.3211e-02],
         [ 2.7267e-01,  4.9409e-01,  2.6460e-02,  ...,  2.9745e-01,  1.4761e-01,  1.7849e-01]],

        [[-6.2856e-02, -1.8685e-01,  1.3960e-01,  ..., -4.9468e-01, -1.1149e-02, -2.9585e-01],
         [-2.0584e-01,  2.1952e-01,  3.6814e-01,  ...,  1.3397e-01,  2.6309e-01, -3.8225e-01],
         [-1.0725e-01,  4.5200e-01,  2.2186e-01,  ...,  4.0617e-01,  2.3502e-01, -2.1630e-01],
         ...,
         [ 2.4882e-01,  2.1902e-01,  5.9516e-02,  ..., -4.5324e-01, -2.1217e-01, -4.8286e-01],
         [ 1.4343e-01,  2.7465e-01, -1.1773e-02,  ..., -2.9364e-01, -2.1599e-01, -2.6936e-01],
         [-4.9048e-01, -3.4923e-04, -3.8154e-01,  ...,  8.1403e-03,  1.5223e-01,  1.4275e-01]]])

2025-07-09 14:14:59.836157 GPU 4 159314 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.6999999999999997,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.6999999999999997,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 17, 114085069]) != torch.Size([2, 16, 114085069]).
ACTUAL: (shape=torch.Size([2, 17, 114085069]), dtype=torch.float32)
tensor([[[-0.3590,  0.4389, -0.2580,  ...,  0.1361, -0.3949,  0.4816],
         [-0.1566,  0.3286, -0.0194,  ...,  0.1754, -0.1037,  0.4154],
         [ 0.1548,  0.1591,  0.3477,  ...,  0.2358,  0.3443,  0.3135],
         ...,
         [ 0.3145, -0.2669,  0.3726,  ...,  0.1388, -0.3359,  0.2551],
         [ 0.4096, -0.1685, -0.0564,  ..., -0.1281, -0.3055,  0.1456],
         [ 0.4715, -0.1046, -0.3353,  ..., -0.3015, -0.2857,  0.0745]],

        [[-0.1440,  0.2287, -0.1257,  ..., -0.0505, -0.3637, -0.2437],
         [ 0.0961,  0.1443,  0.0800,  ...,  0.0362, -0.3757, -0.1279],
         [ 0.4654,  0.0145,  0.3964,  ...,  0.1695, -0.3943,  0.0503],
         ...,
         [-0.2523, -0.4069,  0.0567,  ...,  0.2032,  0.1309,  0.1248],
         [-0.3873,  0.0344,  0.1112,  ...,  0.0038, -0.1703, -0.0495],
         [-0.4751,  0.3213,  0.1467,  ..., -0.1257, -0.3660, -0.1627]]])
DESIRED: (shape=torch.Size([2, 16, 114085069]), dtype=torch.float32)
tensor([[[-0.3590,  0.4389, -0.2580,  ...,  0.1361, -0.3949,  0.4816],
         [-0.1566,  0.3286, -0.0194,  ...,  0.1754, -0.1037,  0.4154],
         [ 0.1548,  0.1591,  0.3477,  ...,  0.2358,  0.3443,  0.3135],
         ...,
         [ 0.1835, -0.0382,  0.2308,  ..., -0.0458,  0.0044,  0.3292],
         [ 0.3145, -0.2669,  0.3726,  ...,  0.1388, -0.3359,  0.2551],
         [ 0.4096, -0.1685, -0.0564,  ..., -0.1281, -0.3055,  0.1456]],

        [[-0.1440,  0.2287, -0.1257,  ..., -0.0505, -0.3637, -0.2437],
         [ 0.0961,  0.1443,  0.0800,  ...,  0.0362, -0.3757, -0.1279],
         [ 0.4654,  0.0145,  0.3964,  ...,  0.1695, -0.3943,  0.0503],
         ...,
         [-0.0918, -0.3393,  0.1101,  ...,  0.2258,  0.0092,  0.2000],
         [-0.2523, -0.4069,  0.0567,  ...,  0.2032,  0.1308,  0.1248],
         [-0.3873,  0.0344,  0.1112,  ...,  0.0038, -0.1703, -0.0495]]])

2025-07-09 14:15:37.156749 GPU 5 160556 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.6999999999999997,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.6999999999999997,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 17, 114085069]) != torch.Size([2, 16, 114085069]).
ACTUAL: (shape=torch.Size([2, 17, 114085069]), dtype=torch.float32)
tensor([[[-0.0811,  0.0005, -0.2664,  ..., -0.0649,  0.1573,  0.2875],
         [-0.2346, -0.2765, -0.1732,  ...,  0.0746,  0.0275, -0.1294],
         [-0.2019, -0.3510, -0.1610,  ...,  0.1877,  0.0190, -0.2688],
         ...,
         [-0.0914, -0.1431,  0.2233,  ..., -0.0974,  0.3570,  0.3311],
         [-0.3419, -0.0419,  0.0659,  ..., -0.1997,  0.3518,  0.3993],
         [-0.4171, -0.0115,  0.0187,  ..., -0.2304,  0.3502,  0.4198]],

        [[ 0.1603,  0.2201,  0.4018,  ...,  0.2288, -0.3050,  0.2042],
         [-0.0930, -0.0250, -0.1123,  ...,  0.0678, -0.3124, -0.0315],
         [-0.3086, -0.2287, -0.3712,  ..., -0.0941, -0.2930, -0.1300],
         ...,
         [-0.3388, -0.2351,  0.0700,  ...,  0.4756, -0.0978,  0.1122],
         [ 0.0584,  0.2542, -0.2592,  ...,  0.4492, -0.3208,  0.1866],
         [ 0.1776,  0.4010, -0.3580,  ...,  0.4413, -0.3877,  0.2089]]])
DESIRED: (shape=torch.Size([2, 16, 114085069]), dtype=torch.float32)
tensor([[[-0.0811,  0.0005, -0.2664,  ..., -0.0649,  0.1573,  0.2875],
         [-0.2376, -0.2820, -0.1714,  ...,  0.0774,  0.0249, -0.1378],
         [-0.1832, -0.3351, -0.1680,  ...,  0.1897,  0.0300, -0.2484],
         ...,
         [-0.0653, -0.1598,  0.2196,  ..., -0.0543,  0.2566,  0.2092],
         [-0.1616, -0.1147,  0.1792,  ..., -0.1261,  0.3556,  0.3502],
         [-0.4171, -0.0115,  0.0187,  ..., -0.2304,  0.3502,  0.4198]],

        [[ 0.1603,  0.2201,  0.4018,  ...,  0.2288, -0.3050,  0.2042],
         [-0.0981, -0.0299, -0.1225,  ...,  0.0646, -0.3126, -0.0362],
         [-0.3137, -0.2330, -0.3577,  ..., -0.1006, -0.2897, -0.1212],
         ...,
         [-0.3107, -0.3932,  0.1127,  ...,  0.4766, -0.0195,  0.0016],
         [-0.2276, -0.0981, -0.0222,  ...,  0.4682, -0.1603,  0.1331],
         [ 0.1776,  0.4010, -0.3580,  ...,  0.4413, -0.3877,  0.2089]]])

2025-07-09 14:16:06.781775 GPU 3 156571 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.6999999999999997,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.6999999999999997,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 17, 114085069]) != torch.Size([2, 16, 114085069]).
ACTUAL: (shape=torch.Size([2, 17, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [ 0.0346,  0.0875,  0.2243,  ..., -0.4316,  0.0749,  0.1396],
         [ 0.3075,  0.3596,  0.2421,  ..., -0.3284,  0.2069, -0.1284],
         ...,
         [ 0.3529,  0.3642,  0.1773,  ..., -0.2047,  0.1239, -0.2580],
         [ 0.3857,  0.2056,  0.3029,  ..., -0.0357, -0.0369, -0.0140],
         [ 0.3280, -0.1366,  0.4958,  ...,  0.1776, -0.2365,  0.3795]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.2405, -0.2788, -0.2527,  ..., -0.0834,  0.0648,  0.0422],
         [-0.4288, -0.4323, -0.4149,  ..., -0.2354,  0.1690,  0.2413],
         ...,
         [-0.3860,  0.2679, -0.3944,  ...,  0.4570,  0.1385, -0.0998],
         [-0.2540,  0.0478, -0.1565,  ...,  0.4079,  0.0335,  0.0471],
         [-0.0481, -0.1945,  0.2152,  ...,  0.3411, -0.0938,  0.2824]]])
DESIRED: (shape=torch.Size([2, 16, 114085069]), dtype=torch.float32)
tensor([[[-0.3145, -0.3823,  0.2283,  ..., -0.4695, -0.1990,  0.4368],
         [-0.0772, -0.0629,  0.2256,  ..., -0.4437, -0.0128,  0.2348],
         [ 0.2879,  0.4284,  0.2213,  ..., -0.4041,  0.2736, -0.0759],
         ...,
         [ 0.0833, -0.0089,  0.2618,  ..., -0.2153,  0.1438, -0.0429],
         [ 0.4275,  0.4538,  0.1630,  ..., -0.1905,  0.1078, -0.2994],
         [ 0.3672,  0.0960,  0.3647,  ...,  0.0326, -0.1008,  0.1120]],

        [[ 0.0024, -0.0330,  0.0509,  ...,  0.1542, -0.1018, -0.2055],
         [-0.1627, -0.2001, -0.1555,  ..., -0.0073,  0.0115, -0.0371],
         [-0.4166, -0.4571, -0.4729,  ..., -0.2558,  0.1857,  0.2220],
         ...,
         [-0.2881,  0.3774, -0.2165,  ...,  0.4468,  0.1592,  0.0255],
         [-0.4034,  0.2236, -0.4262,  ...,  0.4564,  0.1259, -0.1235],
         [-0.1881, -0.0298, -0.0375,  ...,  0.3865, -0.0073,  0.1225]]])

2025-07-09 14:16:21.157249 GPU 7 157406 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.6999999999999997,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.6999999999999997,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([2, 17, 114085069]) != torch.Size([2, 16, 114085069]).
ACTUAL: (shape=torch.Size([2, 17, 114085069]), dtype=torch.float32)
tensor([[[ 2.0246e-01,  2.2274e-01, -4.8886e-01,  ...,  3.6709e-01, -4.3346e-01, -3.3246e-01],
         [-2.9297e-04,  2.6895e-01, -1.6641e-01,  ...,  4.2461e-01, -1.6970e-01,  6.8679e-02],
         [-1.3338e-01,  2.6286e-01,  1.8095e-02,  ...,  4.6153e-01,  4.4990e-02,  3.9267e-01],
         ...,
         [-3.4326e-01,  3.8774e-01,  3.4434e-01,  ..., -3.6432e-01, -2.7898e-01,  7.2616e-02],
         [-1.1863e-01,  4.1083e-01,  1.1241e-01,  ..., -7.6129e-02, -2.6178e-03, -1.1195e-01],
         [ 2.4552e-01,  3.9283e-01, -3.1541e-01,  ...,  3.1224e-01,  4.7461e-01, -3.2715e-01]],

        [[-3.1964e-02,  3.4743e-01, -3.9064e-01,  ...,  4.6186e-01,  4.5304e-01,  3.2691e-01],
         [ 6.6006e-02,  4.5946e-02,  9.3603e-02,  ...,  3.1485e-01,  1.2177e-01,  1.6017e-01],
         [ 7.2885e-02, -1.0977e-01,  4.5796e-01,  ...,  2.2755e-01, -6.2188e-02, -2.7515e-02],
         ...,
         [ 1.8563e-01,  6.9385e-02,  2.2302e-02,  ...,  1.7794e-01, -2.4162e-01, -1.9485e-01],
         [ 2.0824e-01,  1.9969e-01, -1.5894e-01,  ...,  2.4519e-01, -7.4217e-02, -3.6236e-01],
         [ 1.6856e-01,  4.3660e-01, -4.3329e-01,  ...,  3.0129e-01,  1.9877e-01, -4.5182e-01]]])
DESIRED: (shape=torch.Size([2, 16, 114085069]), dtype=torch.float32)
tensor([[[ 0.2025,  0.2227, -0.4889,  ...,  0.3671, -0.4335, -0.3325],
         [-0.0138,  0.2720, -0.1449,  ...,  0.4284, -0.1521,  0.0954],
         [-0.1186,  0.2376, -0.0217,  ...,  0.4568,  0.0507,  0.3999],
         ...,
         [-0.3081,  0.3655,  0.2839,  ..., -0.3560, -0.2221,  0.0829],
         [-0.1429,  0.4120,  0.1409,  ..., -0.1020, -0.0344, -0.0976],
         [ 0.2455,  0.3928, -0.3154,  ...,  0.3122,  0.4746, -0.3272]],

        [[-0.0320,  0.3474, -0.3906,  ...,  0.4619,  0.4530,  0.3269],
         [ 0.0725,  0.0258,  0.1259,  ...,  0.3051,  0.0997,  0.1491],
         [ 0.0313, -0.0625,  0.4506,  ...,  0.2438, -0.0180, -0.0623],
         ...,
         [ 0.1535,  0.1018,  0.0030,  ...,  0.1638, -0.2147, -0.1361],
         [ 0.2109,  0.1839, -0.1406,  ...,  0.2414, -0.0924, -0.3564],
         [ 0.1686,  0.4366, -0.4333,  ...,  0.3013,  0.1988, -0.4518]]])

2025-07-09 14:16:46.593891 GPU 2 148875 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.7999999999999998,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752042090 (unix time) try "date -d @1752042090" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2458b) received by PID 148875 (TID 0x7fab59447740) from PID 148875 ***]


2025-07-09 14:18:43.177794 GPU 6 161328 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.7999999999999998,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.7999999999999998,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2992535051 / 4107062484 (72.9%)
Greatest absolute difference: 0.418281614780426 at index (0, 16, 3086955) (up to 0.01 allowed)
Greatest relative difference: 316056512.0 at index (0, 14, 1523653) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 18, 114085069]), dtype=torch.float32)
tensor([[[ 1.3581e-01,  1.8452e-01,  1.4160e-02,  ..., -4.2689e-01, -4.4357e-01, -2.6336e-01],
         [-2.0591e-01, -5.9870e-02, -1.1783e-01,  ..., -2.7718e-01, -3.5550e-01, -1.9851e-01],
         [-4.7179e-01, -1.9746e-01, -1.9965e-01,  ..., -8.8607e-02, -2.1827e-01, -1.2916e-01],
         ...,
         [ 1.0056e-02,  2.4630e-01,  2.3618e-01,  ..., -1.3799e-01, -1.6875e-01,  3.8201e-02],
         [ 2.2890e-01,  4.5279e-01,  6.1414e-02,  ...,  2.2487e-01,  9.4886e-02,  1.5511e-01],
         [ 2.7267e-01,  4.9409e-01,  2.6460e-02,  ...,  2.9745e-01,  1.4761e-01,  1.7849e-01]],

        [[-6.2856e-02, -1.8685e-01,  1.3960e-01,  ..., -4.9468e-01, -1.1149e-02, -2.9585e-01],
         [-1.8642e-01,  1.6433e-01,  3.3710e-01,  ...,  4.8594e-02,  2.2585e-01, -3.7052e-01],
         [-2.1604e-01,  4.4789e-01,  3.8884e-01,  ...,  4.5325e-01,  3.4528e-01, -3.4705e-01],
         ...,
         [ 1.6691e-01,  2.8484e-01,  1.9212e-03,  ..., -3.0482e-01, -2.2963e-01, -2.8462e-01],
         [-3.8092e-01,  4.7182e-02, -3.1763e-01,  ..., -4.4020e-02,  8.8588e-02,  7.1523e-02],
         [-4.9048e-01, -3.4923e-04, -3.8154e-01,  ...,  8.1403e-03,  1.5223e-01,  1.4275e-01]]])
DESIRED: (shape=torch.Size([2, 18, 114085069]), dtype=torch.float32)
tensor([[[ 1.3581e-01,  1.8452e-01,  1.4160e-02,  ..., -4.2689e-01, -4.4357e-01, -2.6336e-01],
         [-1.8983e-01, -4.8369e-02, -1.1162e-01,  ..., -2.8422e-01, -3.5965e-01, -2.0156e-01],
         [-4.7532e-01, -2.2472e-01, -2.1084e-01,  ..., -1.2098e-01, -2.4970e-01, -1.3738e-01],
         ...,
         [-1.4184e-01,  1.4195e-01,  3.4600e-01,  ..., -3.2567e-01, -3.3250e-01, -5.8808e-02],
         [ 6.4124e-02,  2.9731e-01,  1.9300e-01,  ..., -4.8344e-02, -1.0361e-01,  6.7084e-02],
         [ 2.7267e-01,  4.9409e-01,  2.6460e-02,  ...,  2.9745e-01,  1.4761e-01,  1.7849e-01]],

        [[-6.2856e-02, -1.8685e-01,  1.3960e-01,  ..., -4.9468e-01, -1.1149e-02, -2.9585e-01],
         [-1.8061e-01,  1.4780e-01,  3.2781e-01,  ...,  2.3028e-02,  2.1469e-01, -3.6701e-01],
         [-2.4863e-01,  4.4666e-01,  4.3884e-01,  ...,  4.6735e-01,  3.7830e-01, -3.8620e-01],
         ...,
         [ 4.4479e-01,  3.8453e-01,  1.6604e-01,  ..., -4.5964e-01, -3.7766e-01, -4.9513e-01],
         [ 3.1561e-02,  2.2612e-01, -7.7027e-02,  ..., -2.4039e-01, -1.5101e-01, -1.9663e-01],
         [-4.9048e-01, -3.4923e-04, -3.8154e-01,  ...,  8.1403e-03,  1.5223e-01,  1.4275e-01]]])

2025-07-09 14:18:54.873386 GPU 4 159314 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.7999999999999998,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
torch_assert failed, try np_assert
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=None, scale_factor=list[1.7999999999999998,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2375846729 / 4107062484 (57.8%)
Max absolute difference among violations: 0.19606625
Max relative difference among violations: 2.665767e+09
 ACTUAL: array([[[-0.358968,  0.438861, -0.257982, ...,  0.136106, -0.394929,
          0.481608],
        [-0.078754,  0.286251,  0.072394, ...,  0.190466,  0.0083  ,...
 DESIRED: array([[[-0.358968,  0.438861, -0.257982, ...,  0.136106, -0.394929,
          0.481608],
        [-0.182537,  0.342773, -0.049967, ...,  0.170333, -0.141044,...

2025-07-09 14:22:17.425660 GPU 2 162768 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[11,], scale_factor=None, mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[11,], scale_factor=None, mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 424131279 / 2509871518 (16.9%)
Greatest absolute difference: 0.03636243939399719 at index (1, 9, 59441573) (up to 0.01 allowed)
Greatest relative difference: 11627733.0 at index (0, 9, 25936000) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 11, 114085069]), dtype=torch.float32)
tensor([[[ 0.2579, -0.2842,  0.1310,  ..., -0.0591, -0.2615, -0.1745],
         [ 0.4628, -0.1088,  0.0694,  ..., -0.2909, -0.0160, -0.3561],
         [-0.2430, -0.2094, -0.3012,  ...,  0.0684, -0.1223, -0.1361],
         ...,
         [ 0.2425, -0.3072,  0.0873,  ..., -0.2667, -0.1477,  0.4011],
         [-0.0182, -0.2873,  0.0960,  ...,  0.3864, -0.1590,  0.3973],
         [-0.3971,  0.1099, -0.0277,  ...,  0.3457,  0.1986, -0.3998]],

        [[ 0.1063,  0.1766,  0.0746,  ...,  0.3537,  0.2293, -0.1953],
         [ 0.2808,  0.0349, -0.1233,  ...,  0.0835, -0.0989,  0.1378],
         [ 0.2335,  0.2044, -0.1258,  ...,  0.1100, -0.0938, -0.0988],
         ...,
         [ 0.3236, -0.1068, -0.2444,  ..., -0.4103,  0.4060,  0.2769],
         [ 0.1756,  0.0325,  0.3112,  ..., -0.1044,  0.2968,  0.2675],
         [ 0.1298, -0.1520,  0.0791,  ...,  0.4545,  0.3458, -0.2326]]])
DESIRED: (shape=torch.Size([2, 11, 114085069]), dtype=torch.float32)
tensor([[[ 0.2579, -0.2842,  0.1310,  ..., -0.0591, -0.2615, -0.1745],
         [ 0.4545, -0.1159,  0.0719,  ..., -0.2815, -0.0259, -0.3488],
         [-0.2181, -0.2053, -0.2888,  ...,  0.0552, -0.1178, -0.1443],
         ...,
         [ 0.2350, -0.3080,  0.0880,  ..., -0.2443, -0.1494,  0.4040],
         [-0.0335, -0.2713,  0.0910,  ...,  0.3848, -0.1445,  0.3651],
         [-0.3971,  0.1099, -0.0277,  ...,  0.3457,  0.1986, -0.3998]],

        [[ 0.1063,  0.1766,  0.0746,  ...,  0.3537,  0.2293, -0.1953],
         [ 0.2737,  0.0407, -0.1153,  ...,  0.0944, -0.0857,  0.1244],
         [ 0.2357,  0.1981, -0.1265,  ...,  0.1081, -0.0952, -0.0895],
         ...,
         [ 0.3187, -0.1014, -0.2246,  ..., -0.4020,  0.4021,  0.2785],
         [ 0.1738,  0.0251,  0.3018,  ..., -0.0818,  0.2988,  0.2473],
         [ 0.1298, -0.1520,  0.0791,  ...,  0.4545,  0.3458, -0.2326]]])

2025-07-09 14:23:00.410357 GPU 3 156571 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[11,], scale_factor=None, mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752042384 (unix time) try "date -d @1752042384" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2639b) received by PID 156571 (TID 0x7f095611e740) from PID 156571 ***]


2025-07-09 14:23:07.325502 GPU 7 157406 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[14,], scale_factor=None, mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752042422 (unix time) try "date -d @1752042422" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x266de) received by PID 157406 (TID 0x7fad9d6cb740) from PID 157406 ***]


2025-07-09 14:23:19.124527 GPU 5 160556 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[14,], scale_factor=None, mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[14,], scale_factor=None, mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2057968367 / 3194381932 (64.4%)
Greatest absolute difference: 0.26371660828590393 at index (0, 12, 3603378) (up to 0.01 allowed)
Greatest relative difference: 383442080.0 at index (1, 12, 26392473) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[-8.1091e-02,  5.3013e-04, -2.6644e-01,  ..., -6.4884e-02,  1.5735e-01,  2.8753e-01],
         [-2.6746e-01, -3.3587e-01, -1.5328e-01,  ...,  1.0446e-01, -3.1686e-04, -2.1879e-01],
         [-1.6545e-03, -1.8037e-01, -2.3665e-01,  ...,  2.0976e-01,  1.3666e-01, -5.0805e-02],
         ...,
         [-4.4137e-02, -1.6657e-01,  2.3861e-01,  ..., -5.4899e-02,  2.8589e-01,  2.3627e-01],
         [-2.3457e-01, -8.5237e-02,  1.3334e-01,  ..., -1.5589e-01,  3.5404e-01,  3.7009e-01],
         [-4.1710e-01, -1.1503e-02,  1.8691e-02,  ..., -2.3045e-01,  3.5023e-01,  4.1978e-01]],

        [[ 1.6031e-01,  2.2013e-01,  4.0177e-01,  ...,  2.2876e-01, -3.0499e-01,  2.0418e-01],
         [-1.4730e-01, -7.7575e-02, -2.2242e-01,  ...,  3.3318e-02, -3.1402e-01, -8.2035e-02],
         [-3.6329e-01, -2.7450e-01, -2.2696e-01,  ..., -1.6440e-01, -2.5771e-01, -3.5074e-02],
         ...,
         [-3.6414e-01, -4.0395e-01,  1.3817e-01,  ...,  4.7933e-01, -1.6402e-02,  2.4728e-02],
         [-1.1179e-01,  4.4527e-02, -1.1812e-01,  ...,  4.6050e-01, -2.2526e-01,  1.5472e-01],
         [ 1.7761e-01,  4.0101e-01, -3.5801e-01,  ...,  4.4128e-01, -3.8772e-01,  2.0891e-01]]])
DESIRED: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[-0.0811,  0.0005, -0.2664,  ..., -0.0649,  0.1573,  0.2875],
         [-0.2617, -0.3255, -0.1568,  ...,  0.0992,  0.0045, -0.2032],
         [-0.0366, -0.2101, -0.2235,  ...,  0.2059,  0.1161, -0.0888],
         ...,
         [-0.1337, -0.1378,  0.1581,  ..., -0.0522,  0.1619,  0.1219],
         [-0.1222, -0.1306,  0.2039,  ..., -0.1100,  0.3564,  0.3395],
         [-0.4171, -0.0115,  0.0187,  ..., -0.2304,  0.3502,  0.4198]],

        [[ 0.1603,  0.2201,  0.4018,  ...,  0.2288, -0.3050,  0.2042],
         [-0.1378, -0.0684, -0.2032,  ...,  0.0393, -0.3137, -0.0732],
         [-0.3538, -0.2665, -0.2521,  ..., -0.1521, -0.2639, -0.0516],
         ...,
         [-0.1382, -0.3586,  0.0306,  ...,  0.4678, -0.0295, -0.0730],
         [-0.2899, -0.1748,  0.0295,  ...,  0.4723, -0.1253,  0.1214],
         [ 0.1776,  0.4010, -0.3580,  ...,  0.4413, -0.3877,  0.2089]]])

2025-07-09 14:24:06.327800 GPU 6 161328 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[14,], scale_factor=None, mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[14,], scale_factor=None, mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1514577888 / 3194381932 (47.4%)
Greatest absolute difference: 0.1208745688199997 at index (0, 1, 37666736) (up to 0.01 allowed)
Greatest relative difference: 893018688.0 at index (0, 12, 38663436) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[ 1.3581e-01,  1.8452e-01,  1.4160e-02,  ..., -4.2689e-01, -4.4357e-01, -2.6336e-01],
         [-2.9002e-01, -1.2003e-01, -1.5032e-01,  ..., -2.4032e-01, -3.3382e-01, -1.8254e-01],
         [-4.5336e-01, -5.4863e-02, -1.4114e-01,  ...,  8.0737e-02, -5.3903e-02, -8.6172e-02],
         ...,
         [-2.5589e-01,  2.5022e-01,  3.7345e-01,  ..., -1.5929e-01, -3.6337e-01, -2.0759e-01],
         [-4.4533e-05,  2.3676e-01,  2.4425e-01,  ..., -1.5474e-01, -1.8091e-01,  3.2805e-02],
         [ 2.7267e-01,  4.9409e-01,  2.6460e-02,  ...,  2.9745e-01,  1.4761e-01,  1.7849e-01]],

        [[-6.2856e-02, -1.8685e-01,  1.3960e-01,  ..., -4.9468e-01, -1.1149e-02, -2.9585e-01],
         [-2.1684e-01,  2.5077e-01,  3.8571e-01,  ...,  1.8232e-01,  2.8419e-01, -3.8890e-01],
         [-4.5619e-02,  4.5433e-01,  1.2727e-01,  ...,  3.7950e-01,  1.7257e-01, -1.4224e-01],
         ...,
         [ 1.6340e-01,  1.4688e-01,  1.3084e-02,  ..., -4.5045e-01, -1.4003e-01, -4.7751e-01],
         [ 1.9219e-01,  2.9581e-01,  1.6670e-02,  ..., -3.1686e-01, -2.4432e-01, -3.0106e-01],
         [-4.9048e-01, -3.4923e-04, -3.8154e-01,  ...,  8.1403e-03,  1.5223e-01,  1.4275e-01]]])
DESIRED: (shape=torch.Size([2, 14, 114085069]), dtype=torch.float32)
tensor([[[ 1.3581e-01,  1.8452e-01,  1.4160e-02,  ..., -4.2689e-01, -4.4357e-01, -2.6336e-01],
         [-2.1567e-01, -6.6853e-02, -1.2160e-01,  ..., -2.7290e-01, -3.5299e-01, -1.9665e-01],
         [-4.6003e-01, -1.0643e-01, -1.6230e-01,  ...,  1.9501e-02, -1.1334e-01, -1.0172e-01],
         ...,
         [-2.2127e-01,  2.1735e-01,  3.6512e-01,  ..., -2.0980e-01, -3.5400e-01, -1.6242e-01],
         [ 4.7572e-02,  2.8169e-01,  2.0622e-01,  ..., -7.5788e-02, -1.2355e-01,  5.8242e-02],
         [ 2.7267e-01,  4.9409e-01,  2.6460e-02,  ...,  2.9745e-01,  1.4761e-01,  1.7849e-01]],

        [[-6.2856e-02, -1.8685e-01,  1.3960e-01,  ..., -4.9468e-01, -1.1149e-02, -2.9585e-01],
         [-1.8996e-01,  1.7436e-01,  3.4274e-01,  ...,  6.4116e-02,  2.3262e-01, -3.7265e-01],
         [-1.0725e-01,  4.5200e-01,  2.2186e-01,  ...,  4.0617e-01,  2.3502e-01, -2.1630e-01],
         ...,
         [ 2.4882e-01,  2.1902e-01,  5.9516e-02,  ..., -4.5324e-01, -2.1217e-01, -4.8286e-01],
         [ 7.2993e-02,  2.4410e-01, -5.2859e-02,  ..., -2.6011e-01, -1.7508e-01, -2.2357e-01],
         [-4.9048e-01, -3.4923e-04, -3.8154e-01,  ...,  8.1403e-03,  1.5223e-01,  1.4275e-01]]])

2025-07-09 14:27:09.282753 GPU 7 558 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[14,], scale_factor=None, mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752042722 (unix time) try "date -d @1752042722" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22e) received by PID 558 (TID 0x7fdcd0476740) from PID 558 ***]


2025-07-09 14:27:13.506388 GPU 3 716 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[17,], scale_factor=None, mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752042758 (unix time) try "date -d @1752042758" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2cc) received by PID 716 (TID 0x7f1edb394740) from PID 716 ***]


2025-07-09 14:27:32.782284 GPU 2 162768 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[17,], scale_factor=None, mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[17,], scale_factor=None, mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2763152868 / 3878892346 (71.2%)
Greatest absolute difference: 0.38600775599479675 at index (1, 15, 59441573) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 3, 18729854) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 17, 114085069]), dtype=torch.float32)
tensor([[[ 0.2579, -0.2842,  0.1310,  ..., -0.0591, -0.2615, -0.1745],
         [ 0.3918, -0.1695,  0.0907,  ..., -0.2106, -0.1010, -0.2932],
         [ 0.3248, -0.1158, -0.0177,  ..., -0.2317, -0.0182, -0.3233],
         ...,
         [-0.0751, -0.2276,  0.0774,  ...,  0.3803, -0.1052,  0.2774],
         [-0.3228,  0.0320, -0.0035,  ...,  0.3537,  0.1285, -0.2435],
         [-0.3971,  0.1099, -0.0277,  ...,  0.3457,  0.1986, -0.3998]],

        [[ 0.1063,  0.1766,  0.0746,  ...,  0.3537,  0.2293, -0.1953],
         [ 0.2203,  0.0840, -0.0548,  ...,  0.1771,  0.0148,  0.0224],
         [ 0.2855,  0.0601, -0.1410,  ...,  0.0659, -0.1262,  0.1145],
         ...,
         [ 0.1687,  0.0048,  0.2763,  ..., -0.0203,  0.3042,  0.1924],
         [ 0.1388, -0.1158,  0.1246,  ...,  0.3449,  0.3362, -0.1345],
         [ 0.1298, -0.1520,  0.0791,  ...,  0.4545,  0.3458, -0.2326]]])
DESIRED: (shape=torch.Size([2, 17, 114085069]), dtype=torch.float32)
tensor([[[ 0.2579, -0.2842,  0.1310,  ..., -0.0591, -0.2615, -0.1745],
         [ 0.3859, -0.1746,  0.0925,  ..., -0.2039, -0.1081, -0.2880],
         [ 0.3717, -0.1081,  0.0057,  ..., -0.2565, -0.0096, -0.3388],
         ...,
         [ 0.0581, -0.3276,  0.1062,  ...,  0.2882, -0.1907,  0.4726],
         [-0.1603, -0.1384,  0.0496,  ...,  0.3711, -0.0249,  0.0984],
         [-0.3971,  0.1099, -0.0277,  ...,  0.3457,  0.1986, -0.3998]],

        [[ 0.1063,  0.1766,  0.0746,  ...,  0.3537,  0.2293, -0.1953],
         [ 0.2153,  0.0881, -0.0491,  ...,  0.1848,  0.0242,  0.0129],
         [ 0.2898,  0.0481, -0.1423,  ...,  0.0623, -0.1289,  0.1321],
         ...,
         [ 0.2030,  0.0281,  0.2462,  ..., -0.2045,  0.3093,  0.3159],
         [ 0.1584, -0.0367,  0.2242,  ...,  0.1052,  0.3152,  0.0800],
         [ 0.1298, -0.1520,  0.0791,  ...,  0.4545,  0.3458, -0.2326]]])

2025-07-09 14:27:45.586953 GPU 5 160556 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[17,], scale_factor=None, mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[17,], scale_factor=None, mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2216047997 / 3878892346 (57.1%)
Greatest absolute difference: 0.18013408780097961 at index (0, 15, 3603378) (up to 0.01 allowed)
Greatest relative difference: 406375520.0 at index (1, 15, 70505550) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 17, 114085069]), dtype=torch.float32)
tensor([[[-0.0811,  0.0005, -0.2664,  ..., -0.0649,  0.1573,  0.2875],
         [-0.2279, -0.2644, -0.1773,  ...,  0.0685,  0.0332, -0.1112],
         [-0.2427, -0.3858, -0.1455,  ...,  0.1831, -0.0050, -0.3133],
         ...,
         [-0.0375, -0.1687,  0.2446,  ..., -0.0551,  0.2950,  0.2447],
         [-0.1775, -0.1083,  0.1692,  ..., -0.1326,  0.3552,  0.3546],
         [-0.4171, -0.0115,  0.0187,  ..., -0.2304,  0.3502,  0.4198]],

        [[ 0.1603,  0.2201,  0.4018,  ...,  0.2288, -0.3050,  0.2042],
         [-0.0819, -0.0143, -0.0898,  ...,  0.0749, -0.3121, -0.0212],
         [-0.2975, -0.2194, -0.4006,  ..., -0.0797, -0.3002, -0.1494],
         ...,
         [-0.3808, -0.4073,  0.1461,  ...,  0.4802, -0.0154,  0.0319],
         [-0.2022, -0.0669, -0.0432,  ...,  0.4665, -0.1745,  0.1378],
         [ 0.1776,  0.4010, -0.3580,  ...,  0.4413, -0.3877,  0.2089]]])
DESIRED: (shape=torch.Size([2, 17, 114085069]), dtype=torch.float32)
tensor([[[-0.0811,  0.0005, -0.2664,  ..., -0.0649,  0.1573,  0.2875],
         [-0.1809, -0.1795, -0.2059,  ...,  0.0258,  0.0729,  0.0165],
         [-0.3343, -0.4566, -0.1127,  ...,  0.1652, -0.0569, -0.4005],
         ...,
         [-0.0037, -0.1785,  0.2783,  ..., -0.0616,  0.3589,  0.3072],
         [-0.2543, -0.0773,  0.1210,  ..., -0.1639,  0.3536,  0.3754],
         [-0.4171, -0.0115,  0.0187,  ..., -0.2304,  0.3502,  0.4198]],

        [[ 0.1603,  0.2201,  0.4018,  ...,  0.2288, -0.3050,  0.2042],
         [-0.0044,  0.0608,  0.0676,  ...,  0.1241, -0.3098,  0.0510],
         [-0.2577, -0.1844, -0.4464,  ..., -0.0368, -0.3173, -0.1847],
         ...,
         [-0.4778, -0.4063,  0.1853,  ...,  0.4848, -0.0198,  0.0862],
         [-0.0806,  0.0830, -0.1440,  ...,  0.4584, -0.2428,  0.1606],
         [ 0.1776,  0.4010, -0.3580,  ...,  0.4413, -0.3877,  0.2089]]])

2025-07-09 14:28:25.308559 GPU 6 161328 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[17,], scale_factor=None, mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752042758 (unix time) try "date -d @1752042758" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27630) received by PID 161328 (TID 0x7fb241b7d740) from PID 161328 ***]


2025-07-09 14:29:51.962380 GPU 4 159314 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[5,], scale_factor=None, mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752042737 (unix time) try "date -d @1752042737" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26e52) received by PID 159314 (TID 0x7fb244a9c740) from PID 159314 ***]


2025-07-09 14:31:50.741184 GPU 2 162768 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[5,], scale_factor=None, mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[5,], scale_factor=None, mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 866294343 / 1140850690 (75.9%)
Greatest absolute difference: 0.9999418258666992 at index (1, 4, 59441573) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 2, 56679418) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 5, 114085069]), dtype=torch.float32)
tensor([[[ 0.2579, -0.2842,  0.1310,  ..., -0.0591, -0.2615, -0.1745],
         [-0.4251, -0.2394, -0.3922,  ...,  0.1646, -0.1557, -0.0761],
         [ 0.2200, -0.1742,  0.0566,  ...,  0.3574, -0.4979, -0.0108],
         [ 0.3164,  0.4228,  0.1048,  ..., -0.2975,  0.1963, -0.3562],
         [ 0.0239, -0.3314,  0.1097,  ...,  0.3909, -0.1987,  0.4858]],

        [[ 0.1063,  0.1766,  0.0746,  ...,  0.3537,  0.2293, -0.1953],
         [ 0.2168,  0.2507, -0.1209,  ...,  0.1241, -0.0834, -0.1672],
         [-0.0130, -0.0791,  0.0431,  ..., -0.3362, -0.4918,  0.1041],
         [ 0.2963,  0.4395, -0.3726,  ...,  0.4329, -0.4238, -0.2836],
         [ 0.1807,  0.0530,  0.3370,  ..., -0.1664,  0.2914,  0.3231]]])
DESIRED: (shape=torch.Size([2, 5, 114085069]), dtype=torch.float32)
tensor([[[ 2.5788e-01, -2.8417e-01,  1.3105e-01,  ..., -5.9087e-02, -2.6152e-01, -1.7448e-01],
         [-2.2470e-01, -1.8617e-01, -3.7847e-01,  ...,  7.3361e-02, -2.6350e-02, -3.3485e-02],
         [ 1.4067e-01, -1.2276e-01, -1.8132e-01,  ..., -4.2877e-02, -7.5178e-02,  3.2119e-02],
         [ 3.0192e-01, -1.2015e-01,  8.7451e-02,  ..., -3.9769e-01, -5.2117e-02,  1.9592e-01],
         [-3.9707e-01,  1.0988e-01, -2.7743e-02,  ...,  3.4570e-01,  1.9859e-01, -3.9976e-01]],

        [[ 1.0625e-01,  1.7657e-01,  7.4563e-02,  ...,  3.5370e-01,  2.2932e-01, -1.9532e-01],
         [ 1.8992e-01,  1.5949e-01, -2.8967e-03,  ...,  1.3826e-01,  6.8916e-03, -2.3302e-02],
         [ 9.8480e-02,  1.4916e-01,  6.7062e-03,  ..., -2.3016e-01, -4.7177e-01,  2.4344e-01],
         [ 3.4354e-01, -2.2760e-04, -3.8546e-01,  ..., -2.4519e-01,  2.2003e-01,  1.2814e-01],
         [ 1.2978e-01, -1.5202e-01,  7.9085e-02,  ...,  4.5448e-01,  3.4581e-01, -2.3258e-01]]])

2025-07-09 14:32:23.714627 GPU 4 2573 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[5,], scale_factor=None, mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[5,], scale_factor=None, mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 846233519 / 1140850690 (74.2%)
Greatest absolute difference: 0.4999866485595703 at index (1, 0, 58193218) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 28725341) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 5, 114085069]), dtype=torch.float32)
tensor([[[ 0.3711,  0.1078,  0.0150,  ..., -0.3454, -0.2921,  0.4434],
         [-0.3079,  0.2845,  0.1364,  ..., -0.1282,  0.2904, -0.1656],
         [ 0.1069,  0.1502,  0.0665,  ..., -0.2968, -0.0146,  0.1431],
         [ 0.0375, -0.0306, -0.2624,  ...,  0.3294, -0.3380, -0.1849],
         [-0.1483,  0.1673,  0.2302,  ...,  0.1464,  0.4414,  0.4759]],

        [[ 0.1844, -0.0091, -0.3163,  ..., -0.2369, -0.4141,  0.4305],
         [-0.3659, -0.1926, -0.1981,  ...,  0.3393, -0.2547, -0.1184],
         [-0.1829,  0.0933,  0.2161,  ..., -0.2122,  0.1194, -0.1476],
         [ 0.3853, -0.2986, -0.2999,  ...,  0.2405,  0.1696,  0.2971],
         [-0.4785,  0.2502, -0.4250,  ..., -0.3894,  0.0858,  0.2140]]])
DESIRED: (shape=torch.Size([2, 5, 114085069]), dtype=torch.float32)
tensor([[[ 0.2425, -0.0202,  0.0107,  ..., -0.1099, -0.1137,  0.4207],
         [-0.1417,  0.0693,  0.1551,  ..., -0.2521,  0.1246,  0.0274],
         [ 0.1069,  0.1502,  0.0665,  ..., -0.2968, -0.0146,  0.1431],
         [ 0.0703, -0.0747, -0.2386,  ...,  0.3001, -0.3339,  0.0345],
         [-0.3185,  0.2419,  0.2800,  ...,  0.2634,  0.4111,  0.2406]],

        [[ 0.3395,  0.0997,  0.0511,  ..., -0.0230, -0.3848,  0.3808],
         [-0.2365, -0.0394, -0.2763,  ...,  0.3287, -0.0653, -0.2441],
         [-0.1829,  0.0933,  0.2161,  ..., -0.2122,  0.1194, -0.1476],
         [ 0.2741, -0.1040, -0.3650,  ...,  0.3120,  0.0034,  0.3404],
         [-0.3716,  0.3697, -0.1487,  ...,  0.0127,  0.0010,  0.1591]]])

2025-07-09 14:32:31.337589 GPU 5 160556 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[5,], scale_factor=None, mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752042892 (unix time) try "date -d @1752042892" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2732c) received by PID 160556 (TID 0x7fbf2e1ac740) from PID 160556 ***]


2025-07-09 14:32:43.857099 GPU 6 2734 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[8,], scale_factor=None, mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752043054 (unix time) try "date -d @1752043054" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xaae) received by PID 2734 (TID 0x7f84c8669740) from PID 2734 ***]


2025-07-09 14:32:44.266332 GPU 3 2816 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[8,], scale_factor=None, mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[8,], scale_factor=None, mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1237055037 / 1825361104 (67.8%)
Greatest absolute difference: 0.2499721646308899 at index (1, 7, 105901697) (up to 0.01 allowed)
Greatest relative difference: 602422720.0 at index (1, 6, 98659307) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 8, 114085069]), dtype=torch.float32)
tensor([[[-0.0496,  0.4756, -0.2489,  ..., -0.4713,  0.4126,  0.3364],
         [-0.3280, -0.0872, -0.2746,  ..., -0.2392, -0.0085, -0.3061],
         [ 0.2352,  0.0831, -0.1156,  ...,  0.3188,  0.0529, -0.1910],
         ...,
         [ 0.2326, -0.0029,  0.1104,  ..., -0.3795, -0.3509,  0.3713],
         [-0.0945,  0.0975, -0.4045,  ..., -0.2314, -0.4062,  0.3235],
         [-0.0990, -0.3593,  0.2639,  ...,  0.1096, -0.4500,  0.3651]],

        [[-0.3164,  0.2489,  0.0455,  ..., -0.0120,  0.0330,  0.2755],
         [ 0.1057, -0.2331, -0.2594,  ...,  0.2965,  0.3774, -0.1475],
         [-0.1591, -0.1218, -0.2177,  ...,  0.4192, -0.0227, -0.0621],
         ...,
         [ 0.3197,  0.0351,  0.0561,  ...,  0.0811,  0.1714, -0.0689],
         [ 0.3221,  0.0091, -0.0500,  ...,  0.3576, -0.0357,  0.3063],
         [ 0.2240,  0.1839,  0.2485,  ..., -0.1722, -0.3809,  0.3327]]])
DESIRED: (shape=torch.Size([2, 8, 114085069]), dtype=torch.float32)
tensor([[[-0.0496,  0.4756, -0.2489,  ..., -0.4713,  0.4126,  0.3364],
         [-0.3119, -0.0989, -0.2603,  ..., -0.2088, -0.0225, -0.3050],
         [ 0.2673,  0.1424, -0.1356,  ...,  0.3073,  0.1038, -0.1779],
         ...,
         [ 0.0599,  0.0398, -0.0244,  ..., -0.3629, -0.3727,  0.3880],
         [ 0.0763,  0.0637, -0.3826,  ..., -0.1977, -0.3907,  0.2729],
         [-0.2333, -0.4853,  0.4696,  ...,  0.1970, -0.4767,  0.4182]],

        [[-0.3164,  0.2489,  0.0455,  ..., -0.0120,  0.0330,  0.2755],
         [ 0.0799, -0.2169, -0.2657,  ...,  0.3018,  0.3699, -0.1439],
         [-0.1197, -0.1547, -0.1931,  ...,  0.4207, -0.0574, -0.0606],
         ...,
         [ 0.3625, -0.0304,  0.1373,  ...,  0.1755,  0.1752, -0.0239],
         [ 0.2461,  0.1157, -0.2417,  ...,  0.3061, -0.1313,  0.3861],
         [ 0.2504,  0.1592,  0.4971,  ..., -0.3087, -0.4216,  0.2795]]])

2025-07-09 14:32:47.913528 GPU 7 3050 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[8,], scale_factor=None, mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[8,], scale_factor=None, mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1032102681 / 1825361104 (56.5%)
Greatest absolute difference: 0.12499624490737915 at index (1, 7, 78720266) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 5, 33997119) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 8, 114085069]), dtype=torch.float32)
tensor([[[-0.4917,  0.2214,  0.0095,  ..., -0.4915, -0.4995,  0.3490],
         [ 0.2839, -0.2356,  0.0068,  ...,  0.1956, -0.2388,  0.0257],
         [-0.0264,  0.0342,  0.1823,  ...,  0.2380, -0.0421, -0.1622],
         ...,
         [ 0.3285,  0.1382, -0.0447,  ..., -0.1347,  0.3610,  0.0461],
         [ 0.2259,  0.0406, -0.0449,  ...,  0.2333,  0.1559, -0.2738],
         [ 0.2268,  0.4348,  0.4289,  ...,  0.0652,  0.2125, -0.2617]],

        [[-0.4471, -0.2519, -0.0041,  ...,  0.1800, -0.0721,  0.0730],
         [ 0.3404,  0.1088, -0.2981,  ..., -0.1014,  0.4025, -0.2160],
         [-0.0098, -0.2830, -0.0444,  ...,  0.1403,  0.1139, -0.2174],
         ...,
         [ 0.2037, -0.1839, -0.0869,  ...,  0.1566,  0.1340,  0.1050],
         [ 0.0877, -0.0044, -0.0914,  ...,  0.0143,  0.3536, -0.0110],
         [ 0.2545,  0.2196, -0.1952,  ...,  0.4993, -0.1070,  0.4058]]])
DESIRED: (shape=torch.Size([2, 8, 114085069]), dtype=torch.float32)
tensor([[[-0.3845,  0.1378,  0.0190,  ..., -0.3993, -0.4767,  0.3020],
         [ 0.2583, -0.1693, -0.0178,  ...,  0.1798, -0.2143,  0.0422],
         [-0.0363,  0.0098,  0.2171,  ...,  0.2539, -0.0421, -0.1922],
         ...,
         [ 0.3433,  0.1407, -0.0330,  ..., -0.1356,  0.3567,  0.0570],
         [ 0.2189,  0.0495, -0.0604,  ...,  0.1885,  0.1872, -0.2483],
         [ 0.2295,  0.3819,  0.3759,  ...,  0.1042,  0.1929, -0.2734]],

        [[-0.3484, -0.1937, -0.0432,  ...,  0.1563, -0.0034,  0.0205],
         [ 0.3398,  0.0761, -0.2922,  ..., -0.1300,  0.3789, -0.1749],
         [-0.0422, -0.2952, -0.0250,  ...,  0.1844,  0.1045, -0.2484],
         ...,
         [ 0.1943, -0.1833, -0.0827,  ...,  0.1654,  0.1263,  0.1278],
         [ 0.1147, -0.0276, -0.0964,  ...,  0.0204,  0.3364, -0.0269],
         [ 0.2229,  0.2010, -0.1802,  ...,  0.4362, -0.0426,  0.3600]]])

2025-07-09 14:34:15.926921 GPU 2 162768 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 114085069],"float32"), size=list[8,], scale_factor=None, mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752043051 (unix time) try "date -d @1752043051" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27bd0) received by PID 162768 (TID 0x7fd33f310740) from PID 162768 ***]


2025-07-09 14:35:40.928561 GPU 5 3961 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752043623 (unix time) try "date -d @1752043623" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf79) received by PID 3961 (TID 0x7fa837ce4740) from PID 3961 ***]


2025-07-09 14:36:19.858857 GPU 4 2573 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1246711611 / 1717986920 (72.6%)
Greatest absolute difference: 1.0 at index (0, 2, 4841043) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 1, 321834) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 4, 214748365]), dtype=torch.float16)
tensor([[[-0.0638,  0.1443,  0.1000,  ..., -0.0867, -0.4656, -0.4275],
         [-0.1489,  0.1575, -0.1001,  ...,  0.1243, -0.1255, -0.0764],
         [ 0.3069, -0.2260, -0.1432,  ...,  0.0570, -0.0616,  0.0096],
         [ 0.3906, -0.3584,  0.0496,  ...,  0.0474, -0.0378, -0.3130]],

        [[ 0.1932, -0.2434,  0.2307,  ...,  0.0343, -0.2783,  0.2573],
         [-0.0901, -0.1160, -0.2935,  ..., -0.0865, -0.0453, -0.2639],
         [-0.4106,  0.2363,  0.4829,  ..., -0.1674,  0.1611, -0.2198],
         [ 0.1244, -0.0212, -0.2008,  ..., -0.2969,  0.2219,  0.1031]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 4, 214748365]), dtype=torch.float16)
tensor([[[-0.0638,  0.1443,  0.1000,  ..., -0.0867, -0.4656, -0.4275],
         [ 0.1908,  0.4253,  0.0712,  ...,  0.1377, -0.4775, -0.3884],
         [-0.4387, -0.1602, -0.2615,  ...,  0.0195,  0.1730,  0.2981],
         [ 0.0019,  0.3992, -0.0531,  ...,  0.1052,  0.1998, -0.2903]],

        [[ 0.1932, -0.2434,  0.2307,  ...,  0.0343, -0.2783,  0.2573],
         [ 0.0234, -0.4126, -0.1570,  ..., -0.0145, -0.4065, -0.0967],
         [ 0.0341, -0.1321, -0.1053,  ..., -0.2106, -0.2267,  0.3872],
         [ 0.2120, -0.1022, -0.4792,  ..., -0.2825, -0.1921, -0.4880]]], dtype=torch.float16)

2025-07-09 14:37:46.504612 GPU 7 3050 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1606747412 / 1717986920 (93.5%)
Greatest absolute difference: 0.75 at index (0, 0, 3178932) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 88154) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 4, 214748365]), dtype=torch.float16)
tensor([[[-0.0621,  0.2273, -0.1892,  ..., -0.1732, -0.3596,  0.0536],
         [-0.4685,  0.2568, -0.1329,  ..., -0.0295, -0.4421,  0.0877],
         [-0.1532,  0.3398, -0.1349,  ...,  0.2915, -0.3369, -0.2847],
         [-0.0513,  0.2708, -0.0692,  ...,  0.2651,  0.4519,  0.3645]],

        [[-0.1337, -0.4683, -0.2006,  ...,  0.4993,  0.3481,  0.2490],
         [ 0.1609,  0.3315,  0.2646,  ...,  0.3452,  0.4858, -0.0935],
         [-0.0164, -0.1299, -0.1759,  ...,  0.0594,  0.4238,  0.2108],
         [ 0.1138, -0.1428,  0.1242,  ...,  0.0651,  0.1571,  0.0356]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 4, 214748365]), dtype=torch.float16)
tensor([[[ 0.3213, -0.0068, -0.0942,  ..., -0.1279,  0.2021,  0.2358],
         [-0.2786,  0.0966, -0.0327,  ..., -0.0057, -0.3232,  0.1328],
         [-0.1108,  0.2249, -0.1602,  ...,  0.3318, -0.2920, -0.3374],
         [-0.2263,  0.1099,  0.0055,  ..., -0.1523,  0.4583,  0.3411]],

        [[-0.1154,  0.0421,  0.0626,  ...,  0.3604,  0.4453,  0.4324],
         [ 0.2102,  0.3665,  0.1411,  ...,  0.3110,  0.3025, -0.0322],
         [ 0.0303, -0.2112, -0.1292,  ...,  0.0344,  0.2744,  0.0747],
         [ 0.2554, -0.3545,  0.0685,  ..., -0.3352,  0.0238, -0.2167]]], dtype=torch.float16)

2025-07-09 14:37:47.038095 GPU 6 4527 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.4,], mode="linear", align_corners=True, align_mode=1, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752043740 (unix time) try "date -d @1752043740" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x11af) received by PID 4527 (TID 0x7f521b909740) from PID 4527 ***]


2025-07-09 14:38:25.059280 GPU 2 4955 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.6000000000000001,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.6000000000000001,], mode="linear", align_corners=False, align_mode=1, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 1959968935 / 2576980380 (76.1%)
Greatest absolute difference: 0.66650390625 at index (0, 5, 2030517) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 1, 4708) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 6, 214748365]), dtype=torch.float16)
tensor([[[-0.2418,  0.3477, -0.3223,  ...,  0.1272, -0.2448,  0.1598],
         [-0.1964, -0.1425,  0.0670,  ...,  0.1525, -0.0421,  0.0456],
         [ 0.3052, -0.3081, -0.0718,  ..., -0.1292,  0.1902,  0.1281],
         [-0.0300,  0.4463,  0.4143,  ...,  0.2214,  0.0289, -0.2123],
         [-0.1851, -0.2634, -0.0738,  ..., -0.2471,  0.0333, -0.0390],
         [-0.0847,  0.0594,  0.1715,  ...,  0.0790,  0.3418, -0.0355]],

        [[-0.2228,  0.2163,  0.3796,  ...,  0.4646, -0.4431, -0.2281],
         [ 0.0031,  0.3550,  0.3147,  ...,  0.1826, -0.3318, -0.4624],
         [-0.0405,  0.0390,  0.0950,  ..., -0.2749, -0.1020, -0.0496],
         [ 0.3584, -0.0433, -0.1065,  ..., -0.2245, -0.2751,  0.4436],
         [ 0.1715,  0.1377,  0.2374,  ..., -0.0476, -0.0380, -0.3237],
         [-0.2118, -0.1526, -0.1454,  ...,  0.2340,  0.1324, -0.0372]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 6, 214748365]), dtype=torch.float16)
tensor([[[-0.2418,  0.3477, -0.3223,  ...,  0.1272, -0.2448,  0.1598],
         [-0.1385, -0.1024,  0.1006,  ...,  0.1284,  0.0057, -0.0163],
         [ 0.2683, -0.3069,  0.1316,  ..., -0.0782,  0.0080,  0.1980],
         [ 0.0514,  0.1482,  0.0655,  ...,  0.0553, -0.0474, -0.2642],
         [-0.3601, -0.2208,  0.1471,  ..., -0.2183,  0.1636,  0.1276],
         [ 0.4341,  0.4202, -0.0104,  ...,  0.2300,  0.4360, -0.4827]],

        [[-0.2228,  0.2163,  0.3796,  ...,  0.4646, -0.4431, -0.2281],
         [ 0.0485,  0.3728,  0.3882,  ...,  0.2048, -0.3699, -0.4583],
         [ 0.1506, -0.0604,  0.0503,  ..., -0.2830, -0.2087,  0.0066],
         [ 0.1396,  0.0775,  0.1180,  ..., -0.1287, -0.1166,  0.2169],
         [ 0.2479,  0.0518,  0.0466,  ..., -0.0133, -0.0205, -0.3069],
         [-0.3020, -0.3582,  0.1279,  ...,  0.2041, -0.3386, -0.4360]]], dtype=torch.float16)

2025-07-09 14:47:44.632887 GPU 4 2573 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.6000000000000001,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, )
[accuracy error] paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.6000000000000001,], mode="linear", align_corners=True, align_mode=0, data_format="NWC", name=None, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 2129325479 / 2576980380 (82.6%)
Greatest absolute difference: 0.33349609375 at index (0, 0, 22157578) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 11073) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 6, 214748365]), dtype=torch.float16)
tensor([[[-0.0638,  0.1443,  0.1000,  ..., -0.0867, -0.4656, -0.4275],
         [-0.3933, -0.0699, -0.2227,  ..., -0.0011,  0.2235,  0.1849],
         [-0.0456,  0.2408, -0.2086,  ...,  0.2004, -0.0256, -0.2598],
         [ 0.0086, -0.1996, -0.1906,  ...,  0.0420,  0.0323,  0.1250],
         [ 0.3396, -0.3572, -0.1975,  ...,  0.3069,  0.2257, -0.3928],
         [ 0.0019,  0.3992, -0.0531,  ...,  0.1052,  0.1998, -0.2903]],

        [[ 0.1932, -0.2434,  0.2307,  ...,  0.0343, -0.2783,  0.2573],
         [-0.1908,  0.1896, -0.2986,  ..., -0.0621,  0.1754, -0.3201],
         [ 0.1802, -0.2206, -0.0274,  ..., -0.2532,  0.0558,  0.1541],
         [-0.2328,  0.0890,  0.2476,  ..., -0.1847,  0.0060,  0.0230],
         [-0.0026,  0.0338, -0.2047,  ..., -0.2520,  0.2639,  0.1959],
         [ 0.2120, -0.1022, -0.4792,  ..., -0.2825, -0.1921, -0.4880]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 6, 214748365]), dtype=torch.float16)
tensor([[[-0.0464,  0.1271,  0.0576,  ..., -0.2076, -0.2397, -0.2910],
         [-0.4888, -0.1105, -0.2715,  ...,  0.1110,  0.2264,  0.2357],
         [-0.0718,  0.2203, -0.2397,  ...,  0.2074,  0.0246, -0.2454],
         [ 0.0583, -0.2040, -0.1826,  ...,  0.0445,  0.0166,  0.1058],
         [ 0.3057, -0.3564, -0.3623,  ...,  0.4797,  0.4014, -0.4458],
         [ 0.1599,  0.1461,  0.1184,  ..., -0.0582, -0.0258, -0.2537]],

        [[ 0.0822, -0.0870,  0.2292,  ...,  0.1309, -0.3145,  0.2126],
         [-0.2036,  0.1805, -0.4299,  ..., -0.1586,  0.3159, -0.4309],
         [ 0.1976, -0.1993, -0.0130,  ..., -0.2795,  0.1072,  0.1819],
         [-0.2625,  0.1135,  0.2869,  ..., -0.1818,  0.0319, -0.0175],
         [-0.0872,  0.0705, -0.2073,  ..., -0.2220,  0.2920,  0.2578],
         [ 0.2534, -0.1058, -0.3843,  ..., -0.3123, -0.0775, -0.3425]]], dtype=torch.float16)

2025-07-09 14:49:08.000386 GPU 7 3050 test begin: paddle.nn.functional.interpolate(Tensor([2, 10, 214748365],"float16"), size=None, scale_factor=list[0.8000000000000002,], mode="linear", align_corners=False, align_mode=0, data_format="NWC", name=None, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752044537 (unix time) try "date -d @1752044537" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xbea) received by PID 3050 (TID 0x7f88c9db1740) from PID 3050 ***]


2025-07-09 14:57:27.566276 GPU 2 12135 test begin: paddle.nn.functional.max_pool2d(Tensor([268435456, 1, 4, 4],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, )
element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.max_pool2d(Tensor([268435456, 1, 4, 4],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ceil_mode=False, data_format="NCHW", name=None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<MaxPool2dWithIndexGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752044410 (unix time) try "date -d @1752044410" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2f67) received by PID 12135 (TID 0x7fcaa8db1740) from PID 12135 ***]


2025-07-09 14:57:39.841414 GPU 5 12294 test begin: paddle.nn.functional.max_pool2d(Tensor([268435456, 1, 4, 4],"float32"), kernel_size=2, stride=2, return_mask=True, )
element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.max_pool2d(Tensor([268435456, 1, 4, 4],"float32"), kernel_size=2, stride=2, return_mask=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<MaxPool2dWithIndexGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752044428 (unix time) try "date -d @1752044428" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3006) received by PID 12294 (TID 0x7f587af37740) from PID 12294 ***]


2025-07-09 15:04:40.927721 GPU 4 2573 test begin: paddle.nn.functional.max_pool2d(Tensor([39768216, 3, 6, 6],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, )
element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.max_pool2d(Tensor([39768216, 3, 6, 6],"float32"), kernel_size=2, stride=2, padding=0, return_mask=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<MaxPool2dWithIndexGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752044786 (unix time) try "date -d @1752044786" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xa0d) received by PID 2573 (TID 0x7f2c96bf4740) from PID 2573 ***]


2025-07-09 15:31:38.818813 GPU 3 13700 test begin: paddle.nn.functional.max_pool3d(Tensor([1, 3, 59652324, 4, 6],"float32"), kernel_size=2, stride=2, return_mask=True, )
element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.max_pool3d(Tensor([1, 3, 59652324, 4, 6],"float32"), kernel_size=2, stride=2, return_mask=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<MaxPool3dWithIndexGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752046320 (unix time) try "date -d @1752046320" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3584) received by PID 13700 (TID 0x7f3e46253740) from PID 13700 ***]


2025-07-09 15:32:51.343535 GPU 3 28148 test begin: paddle.nn.functional.max_pool3d(Tensor([1, 4, 4, 67108864, 4],"float32"), list[3,3,3,], stride=1, padding=list[0,0,0,], data_format="NDHWC", )
[accuracy error] paddle.nn.functional.max_pool3d(Tensor([1, 4, 4, 67108864, 4],"float32"), list[3,3,3,], stride=1, padding=list[0,0,0,], data_format="NDHWC", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 422968351 / 1073741792 (39.4%)
Greatest absolute difference: 0.49999985098838806 at index (0, 1, 1, 66170309, 1) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0, 0, 78557, 3) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 2, 2, 67108862, 4]), dtype=torch.float32)
tensor([[[[[0.4738, 0.4498, 0.4747, 0.4643],
           [0.4797, 0.4296, 0.4909, 0.4025],
           [0.4797, 0.4947, 0.4920, 0.4025],
           ...,
           [0.4933, 0.4813, 0.4902, 0.4749],
           [0.4695, 0.4813, 0.4902, 0.4749],
           [0.4932, 0.4813, 0.4902, 0.4749]],

          [[0.4738, 0.4498, 0.4571, 0.4946],
           [0.4797, 0.4296, 0.4745, 0.4873],
           [0.4797, 0.4172, 0.4745, 0.4873],
           ...,
           [0.4933, 0.4991, 0.4902, 0.4700],
           [0.4695, 0.4991, 0.4902, 0.4723],
           [0.4695, 0.4813, 0.4902, 0.4723]]],


         [[[0.4738, 0.4812, 0.4747, 0.4643],
           [0.4873, 0.4461, 0.4571, 0.4199],
           [0.4873, 0.4947, 0.4920, 0.4199],
           ...,
           [0.4933, 0.4806, 0.4902, 0.4749],
           [0.4933, 0.4213, 0.4902, 0.4749],
           [0.4167, 0.4557, 0.4902, 0.4749]],

          [[0.4738, 0.4461, 0.4571, 0.4946],
           [0.4873, 0.4461, 0.4745, 0.4785],
           [0.4873, 0.4643, 0.4745, 0.4918],
           ...,
           [0.4933, 0.4991, 0.4902, 0.4700],
           [0.4933, 0.4991, 0.4902, 0.4723],
           [0.4690, 0.4557, 0.4902, 0.4723]]]]])
DESIRED: (shape=torch.Size([1, 2, 2, 67108862, 4]), dtype=torch.float32)
tensor([[[[[0.4738, 0.4498, 0.4571, 0.4643],
           [0.4797, 0.3165, 0.4909, 0.4025],
           [0.4797, 0.4947, 0.4920, 0.4025],
           ...,
           [0.4695, 0.4813, 0.4440, 0.4749],
           [0.4695, 0.4813, 0.4405, 0.4749],
           [0.4932, 0.4813, 0.4405, 0.4749]],

          [[0.4738, 0.4498, 0.4571, 0.4946],
           [0.4797, 0.4172, 0.4571, 0.4873],
           [0.4797, 0.4172, 0.4571, 0.4873],
           ...,
           [0.4695, 0.4991, 0.4563, 0.4356],
           [0.4695, 0.4991, 0.4563, 0.4723],
           [0.4695, 0.4813, 0.4405, 0.4723]]],


         [[[0.4864, 0.4947, 0.4920, 0.4944],
           [0.4864, 0.4947, 0.4920, 0.4944],
           [0.4864, 0.4947, 0.4920, 0.4944],
           ...,
           [0.4932, 0.4947, 0.4920, 0.4944],
           [0.4932, 0.4947, 0.4920, 0.4946],
           [0.4864, 0.4947, 0.4920, 0.4946]],

          [[0.4864, 0.4603, 0.4571, 0.4946],
           [0.4864, 0.4603, 0.4571, 0.4944],
           [0.4864, 0.4603, 0.4571, 0.4944],
           ...,
           [0.4864, 0.4991, 0.4920, 0.4944],
           [0.4864, 0.4991, 0.4920, 0.4944],
           [0.4864, 0.4947, 0.4920, 0.4944]]]]])

2025-07-09 15:34:08.506931 GPU 2 28912 test begin: paddle.nn.functional.max_pool3d(Tensor([1, 44739243, 4, 4, 6],"float32"), kernel_size=2, stride=2, return_mask=True, )
element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.max_pool3d(Tensor([1, 44739243, 4, 4, 6],"float32"), kernel_size=2, stride=2, return_mask=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<MaxPool3dWithIndexGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752046623 (unix time) try "date -d @1752046623" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x70f0) received by PID 28912 (TID 0x7faf16306740) from PID 28912 ***]


2025-07-09 15:36:16.345412 GPU 5 30319 test begin: paddle.nn.functional.max_pool3d(Tensor([1, 657326, 6, 33, 33],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, )
element 1 of tensors does not require grad and does not have a grad_fn
[cuda error] paddle.nn.functional.max_pool3d(Tensor([1, 657326, 6, 33, 33],"float32"), kernel_size=5, stride=5, padding=0, ceil_mode=True, return_mask=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/fluid/pybind/eager_functions.cc:1388)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   std::_Sp_counted_ptr_inplace<egr::AutogradMeta, std::allocator<egr::AutogradMeta>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
2   std::_Sp_counted_ptr<MaxPool3dWithIndexGradNode*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752046765 (unix time) try "date -d @1752046765" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x766f) received by PID 30319 (TID 0x7fe06c6c7740) from PID 30319 ***]


2025-07-09 16:18:34.192847 GPU 5 58578 test begin: paddle.nn.functional.pad(Tensor([13, 47197443, 7],"float32"), tuple(-3,0,), data_format="NCL", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752049240 (unix time) try "date -d @1752049240" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe4d2) received by PID 58578 (TID 0x7f8fc5f3b740) from PID 58578 ***]


2025-07-09 16:19:11.181076 GPU 6 58753 test begin: paddle.nn.functional.pad(Tensor([14, 43826197, 7],"float32"), tuple(-3,0,), data_format="NCL", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752049302 (unix time) try "date -d @1752049302" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe581) received by PID 58753 (TID 0x7fb1025ca740) from PID 58753 ***]


2025-07-09 16:19:12.987560 GPU 4 58839 test begin: paddle.nn.functional.pad(Tensor([9586981, 64, 7],"float32"), tuple(-3,0,), data_format="NCL", )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >::~vector()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752049298 (unix time) try "date -d @1752049298" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe5d7) received by PID 58839 (TID 0x7f8680e00740) from PID 58839 ***]


2025-07-09 17:44:15.987729 GPU 4 107573 test begin: paddle.nn.functional.temporal_shift(x=Tensor([2, 95070891, 4, 3],"float16"), seg_num=2, )
[accuracy error] paddle.nn.functional.temporal_shift(x=Tensor([2, 95070891, 4, 3],"float16"), seg_num=2, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 24 / 2281701384 (0.0%)
Greatest absolute difference: 0.76953125 at index (0, 47535444, 0, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 47535444, 0, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([2, 95070891, 4, 3]), dtype=torch.float16)
tensor([[[[ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000]],

         ...,

         [[-0.3984,  0.3818, -0.3882],
          [ 0.2061, -0.2292, -0.4185],
          [-0.4521, -0.3884,  0.3855],
          [ 0.1771,  0.1735,  0.0927]],

         [[-0.3816,  0.3904,  0.1281],
          [-0.0041,  0.0866, -0.1488],
          [ 0.2319,  0.3496, -0.0068],
          [-0.2620, -0.4592,  0.4807]],

         [[-0.3372,  0.2744,  0.2118],
          [ 0.4546, -0.4402,  0.0155],
          [-0.4321,  0.4971, -0.0872],
          [ 0.2396,  0.2500,  0.2788]]],


        [[[ 0.1844,  0.3372,  0.3882],
          [ 0.1323, -0.3140,  0.2393],
          [ 0.1637,  0.4270,  0.2532],
          [-0.1377,  0.2988,  0.1833]],

         [[ 0.1655,  0.4299,  0.4202],
          [-0.3931,  0.2094, -0.1322],
          [-0.2927, -0.2023, -0.3293],
          [ 0.2646,  0.0572, -0.2092]],

         [[-0.2273, -0.2007, -0.2280],
          [-0.0757,  0.3442, -0.0106],
          [-0.1772, -0.3633,  0.2332],
          [-0.1008,  0.0645, -0.2654]],

         ...,

         [[ 0.3872, -0.1611,  0.3828],
          [-0.3262, -0.0396,  0.2727],
          [-0.2871,  0.0482, -0.0601],
          [ 0.3970, -0.4473,  0.2479]],

         [[-0.1181, -0.3635, -0.1699],
          [ 0.3838, -0.1082, -0.3037],
          [-0.3992, -0.0363, -0.0036],
          [ 0.2761,  0.0118,  0.1581]],

         [[ 0.0159, -0.3965, -0.4607],
          [-0.3879,  0.2747, -0.4268],
          [-0.2273, -0.3826, -0.3999],
          [-0.2157, -0.3306, -0.2598]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([2, 95070891, 4, 3]), dtype=torch.float16)
tensor([[[[ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000]],

         [[ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000]],

         ...,

         [[-0.3984,  0.3818, -0.3882],
          [ 0.2061, -0.2292, -0.4185],
          [-0.4521, -0.3884,  0.3855],
          [ 0.1771,  0.1735,  0.0927]],

         [[-0.3816,  0.3904,  0.1281],
          [-0.0041,  0.0866, -0.1488],
          [ 0.2319,  0.3496, -0.0068],
          [-0.2620, -0.4592,  0.4807]],

         [[-0.3372,  0.2744,  0.2118],
          [ 0.4546, -0.4402,  0.0155],
          [-0.4321,  0.4971, -0.0872],
          [ 0.2396,  0.2500,  0.2788]]],


        [[[ 0.1844,  0.3372,  0.3882],
          [ 0.1323, -0.3140,  0.2393],
          [ 0.1637,  0.4270,  0.2532],
          [-0.1377,  0.2988,  0.1833]],

         [[ 0.1655,  0.4299,  0.4202],
          [-0.3931,  0.2094, -0.1322],
          [-0.2927, -0.2023, -0.3293],
          [ 0.2646,  0.0572, -0.2092]],

         [[-0.2273, -0.2007, -0.2280],
          [-0.0757,  0.3442, -0.0106],
          [-0.1772, -0.3633,  0.2332],
          [-0.1008,  0.0645, -0.2654]],

         ...,

         [[ 0.3872, -0.1611,  0.3828],
          [-0.3262, -0.0396,  0.2727],
          [-0.2871,  0.0482, -0.0601],
          [ 0.3970, -0.4473,  0.2479]],

         [[-0.1181, -0.3635, -0.1699],
          [ 0.3838, -0.1082, -0.3037],
          [-0.3992, -0.0363, -0.0036],
          [ 0.2761,  0.0118,  0.1581]],

         [[ 0.0159, -0.3965, -0.4607],
          [-0.3879,  0.2747, -0.4268],
          [-0.2273, -0.3826, -0.3999],
          [-0.2157, -0.3306, -0.2598]]]], dtype=torch.float16)

2025-07-09 17:48:12.632670 GPU 5 107637 test begin: paddle.nn.functional.upsample(x=Tensor([1, 1073741825, 2, 2, 1],"float16"), size=Tensor([3],"float16"), scale_factor=None, mode="trilinear", align_corners=True, align_mode=1, data_format="NDHWC", )
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.upsample(x=Tensor([1, 1073741825, 2, 2, 1],"float16"), size=Tensor([3],"float16"), scale_factor=None, mode="trilinear", align_corners=True, align_mode=1, data_format="NDHWC", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 26052 / 41400 (62.9%)
Greatest absolute difference: 2472.0 at index (0, 7, 39, 0, 0) (up to 0.01 allowed)
Greatest relative difference: 360.0 at index (0, 9, 39, 68, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 15, 40, 69, 1]), dtype=torch.float16)
tensor([[[[[   44.0000],
           [   43.7812],
           [   43.5625],
           ...,
           [   29.4375],
           [   29.2188],
           [   29.0000]],

          [[   43.8438],
           [   43.5938],
           [   43.3750],
           ...,
           [   28.8906],
           [   28.6719],
           [   28.4375]],

          [[   43.6250],
           [   43.4062],
           [   43.1875],
           ...,
           [   28.3281],
           [   28.1094],
           [   27.8750]],

          ...,

          [[   37.3438],
           [   36.9375],
           [   36.5000],
           ...,
           [    8.9922],
           [    8.5547],
           [    8.1250]],

          [[   37.1562],
           [   36.7500],
           [   36.3125],
           ...,
           [    8.4453],
           [    8.0078],
           [    7.5703]],

          [[   37.0000],
           [   36.5625],
           [   36.1250],
           ...,
           [    7.8828],
           [    7.4414],
           [    7.0000]]],


         [[[  126.0000],
           [  125.8125],
           [  125.6250],
           ...,
           [  113.3750],
           [  113.2500],
           [  113.0000]],

          [[  124.9375],
           [  124.7500],
           [  124.5625],
           ...,
           [  112.1250],
           [  112.0000],
           [  111.7500]],

          [[  123.8125],
           [  123.6250],
           [  123.4375],
           ...,
           [  110.8750],
           [  110.7500],
           [  110.5000]],

          ...,

          [[   85.1875],
           [   84.9375],
           [   84.6250],
           ...,
           [   67.0625],
           [   66.7500],
           [   66.5000]],

          [[   84.1250],
           [   83.8750],
           [   83.5625],
           ...,
           [   65.8125],
           [   65.5000],
           [   65.2500]],

          [[   83.0000],
           [   82.7500],
           [   82.4375],
           ...,
           [   64.5625],
           [   64.2500],
           [   64.0000]]],


         [[[  121.0000],
           [  119.5000],
           [  117.9375],
           ...,
           [   20.0469],
           [   18.5156],
           [   17.0000]],

          [[  118.3750],
           [  116.9375],
           [  115.4375],
           ...,
           [   20.9688],
           [   19.4844],
           [   18.0312]],

          [[  115.6875],
           [  114.2500],
           [  112.8125],
           ...,
           [   21.8750],
           [   20.4531],
           [   19.0469]],

          ...,

          [[   22.3281],
           [   22.8125],
           [   23.2812],
           ...,
           [   54.0000],
           [   54.4375],
           [   54.9375]],

          [[   19.6875],
           [   20.2344],
           [   20.7500],
           ...,
           [   54.9375],
           [   55.4062],
           [   55.9688]],

          [[   17.0000],
           [   17.5938],
           [   18.1719],
           ...,
           [   55.8438],
           [   56.4062],
           [   57.0000]]],


         ...,


         [[[-2003.0000],
           [-1999.0000],
           [-1995.0000],
           ...,
           [-1693.0000],
           [-1689.0000],
           [-1683.0000]],

          [[-1951.0000],
           [-1947.0000],
           [-1944.0000],
           ...,
           [-1676.0000],
           [-1673.0000],
           [-1668.0000]],

          [[-1896.0000],
           [-1893.0000],
           [-1892.0000],
           ...,
           [-1661.0000],
           [-1659.0000],
           [-1652.0000]],

          ...,

          [[  -41.5625],
           [  -58.4375],
           [  -73.3750],
           ...,
           [-1089.0000],
           [-1105.0000],
           [-1122.0000]],

          [[   11.7500],
           [   -5.1875],
           [  -24.1875],
           ...,
           [-1073.0000],
           [-1092.0000],
           [-1105.0000]],

          [[   63.0000],
           [   46.0625],
           [   29.0625],
           ...,
           [-1057.0000],
           [-1075.0000],
           [-1091.0000]]],


         [[[  156.0000],
           [  155.7500],
           [  155.5000],
           ...,
           [  148.5000],
           [  147.7500],
           [  148.0000]],

          [[  151.0000],
           [  150.7500],
           [  150.5000],
           ...,
           [  134.7500],
           [  134.0000],
           [  133.7500]],

          [[  145.2500],
           [  145.0000],
           [  144.3750],
           ...,
           [  120.8125],
           [  119.6875],
           [  119.4375]],

          ...,

          [[  -36.5312],
           [  -41.5938],
           [  -46.6250],
           ...,
           [ -365.7500],
           [ -371.0000],
           [ -375.5000]],

          [[  -41.8750],
           [  -47.0312],
           [  -51.7188],
           ...,
           [ -379.2500],
           [ -385.0000],
           [ -389.2500]],

          [[  -47.0000],
           [  -52.3750],
           [  -57.4062],
           ...,
           [ -393.2500],
           [ -399.2500],
           [ -404.0000]]],


         [[[  101.0000],
           [   98.6875],
           [   99.3125],
           ...,
           [   12.7891],
           [   11.5781],
           [   10.0000]],

          [[   98.9375],
           [  100.6250],
           [   98.3125],
           ...,
           [   13.3047],
           [   12.1250],
           [   10.9453]],

          [[   99.8125],
           [   98.5625],
           [   96.2500],
           ...,
           [   14.3203],
           [   13.0391],
           [   11.8906]],

          ...,

          [[   59.7188],
           [   58.9375],
           [   59.6250],
           ...,
           [   40.9375],
           [   41.1562],
           [   39.8438]],

          [[   59.1250],
           [   58.8750],
           [   59.5625],
           ...,
           [   41.6875],
           [   41.9062],
           [   41.6562]],

          [[   58.0000],
           [   58.2812],
           [   58.0312],
           ...,
           [   41.9688],
           [   41.7188],
           [   42.0000]]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([1, 15, 40, 69, 1]), dtype=torch.float16)
tensor([[[[[ 44.0000],
           [ 43.7812],
           [ 43.5625],
           ...,
           [ 29.4375],
           [ 29.2188],
           [ 29.0000]],

          [[ 43.8125],
           [ 43.5938],
           [ 43.3750],
           ...,
           [ 28.8906],
           [ 28.6562],
           [ 28.4375]],

          [[ 43.6562],
           [ 43.4062],
           [ 43.1875],
           ...,
           [ 28.3281],
           [ 28.1094],
           [ 27.8750]],

          ...,

          [[ 37.3438],
           [ 36.9375],
           [ 36.5000],
           ...,
           [  8.9844],
           [  8.5547],
           [  8.1250]],

          [[ 37.1875],
           [ 36.7500],
           [ 36.3125],
           ...,
           [  8.4375],
           [  8.0000],
           [  7.5625]],

          [[ 37.0000],
           [ 36.5625],
           [ 36.1250],
           ...,
           [  7.8828],
           [  7.4414],
           [  7.0000]]],


         [[[126.0000],
           [125.8125],
           [125.6250],
           ...,
           [113.3750],
           [113.1875],
           [113.0000]],

          [[124.8750],
           [124.6875],
           [124.5000],
           ...,
           [112.1250],
           [111.9375],
           [111.7500]],

          [[123.8125],
           [123.6250],
           [123.3750],
           ...,
           [110.8750],
           [110.6875],
           [110.5000]],

          ...,

          [[ 85.1875],
           [ 84.9375],
           [ 84.6250],
           ...,
           [ 67.0625],
           [ 66.8125],
           [ 66.5000]],

          [[ 84.1250],
           [ 83.8125],
           [ 83.5625],
           ...,
           [ 65.8125],
           [ 65.5625],
           [ 65.2500]],

          [[ 83.0000],
           [ 82.7500],
           [ 82.4375],
           ...,
           [ 64.5625],
           [ 64.2500],
           [ 64.0000]]],


         [[[121.0000],
           [119.5000],
           [117.9375],
           ...,
           [ 20.0625],
           [ 18.5312],
           [ 17.0000]],

          [[118.3125],
           [116.8750],
           [115.3750],
           ...,
           [ 20.9688],
           [ 19.5000],
           [ 18.0312]],

          [[115.6875],
           [114.2500],
           [112.8125],
           ...,
           [ 21.8906],
           [ 20.4688],
           [ 19.0469]],

          ...,

          [[ 22.3281],
           [ 22.8125],
           [ 23.2969],
           ...,
           [ 54.0000],
           [ 54.4688],
           [ 54.9375]],

          [[ 19.6719],
           [ 20.2031],
           [ 20.7344],
           ...,
           [ 54.9062],
           [ 55.4375],
           [ 55.9688]],

          [[ 17.0000],
           [ 17.5938],
           [ 18.1719],
           ...,
           [ 55.8125],
           [ 56.4062],
           [ 57.0000]]],


         ...,


         [[[ 45.0000],
           [ 45.0000],
           [ 45.0000],
           ...,
           [ 45.0000],
           [ 45.0000],
           [ 45.0000]],

          [[ 46.2812],
           [ 46.2500],
           [ 46.2188],
           ...,
           [ 44.6250],
           [ 44.6250],
           [ 44.5938]],

          [[ 47.5625],
           [ 47.5000],
           [ 47.4688],
           ...,
           [ 44.2812],
           [ 44.2188],
           [ 44.1875]],

          ...,

          [[ 92.4375],
           [ 91.5000],
           [ 90.6250],
           ...,
           [ 31.6562],
           [ 30.7344],
           [ 29.8281]],

          [[ 93.6875],
           [ 92.7500],
           [ 91.8125],
           ...,
           [ 31.2969],
           [ 30.3594],
           [ 29.4062]],

          [[ 95.0000],
           [ 94.0000],
           [ 93.0625],
           ...,
           [ 30.9375],
           [ 29.9688],
           [ 29.0000]]],


         [[[ 84.0000],
           [ 84.2500],
           [ 84.5000],
           ...,
           [ 99.5000],
           [ 99.7500],
           [100.0000]],

          [[ 82.8750],
           [ 83.1250],
           [ 83.4375],
           ...,
           [ 99.6875],
           [ 99.9375],
           [100.1875]],

          [[ 81.8125],
           [ 82.0625],
           [ 82.3125],
           ...,
           [ 99.8750],
           [100.1250],
           [100.4375]],

          ...,

          [[ 43.2188],
           [ 44.1562],
           [ 45.0938],
           ...,
           [105.6875],
           [106.6250],
           [107.5625]],

          [[ 42.0938],
           [ 43.0625],
           [ 44.0312],
           ...,
           [105.8750],
           [106.8125],
           [107.8125]],

          [[ 41.0000],
           [ 42.0000],
           [ 42.9688],
           ...,
           [106.0000],
           [107.0000],
           [108.0000]]],


         [[[101.0000],
           [ 99.6875],
           [ 98.3125],
           ...,
           [ 12.6797],
           [ 11.3359],
           [ 10.0000]],

          [[ 99.8750],
           [ 98.5625],
           [ 97.2500],
           ...,
           [ 13.4375],
           [ 12.1328],
           [ 10.8203]],

          [[ 98.8125],
           [ 97.5000],
           [ 96.2500],
           ...,
           [ 14.2031],
           [ 12.9219],
           [ 11.6406]],

          ...,

          [[ 60.2188],
           [ 59.9062],
           [ 59.6250],
           ...,
           [ 40.9375],
           [ 40.6562],
           [ 40.3438]],

          [[ 59.0938],
           [ 58.8438],
           [ 58.5625],
           ...,
           [ 41.7188],
           [ 41.4375],
           [ 41.1875]],

          [[ 58.0000],
           [ 57.7500],
           [ 57.5312],
           ...,
           [ 42.4688],
           [ 42.2500],
           [ 42.0000]]]]], dtype=torch.float16)

2025-07-09 17:48:19.761599 GPU 4 107573 test begin: paddle.nn.functional.upsample(x=Tensor([1, 2, 1073741825, 2, 1],"float16"), size=Tensor([3],"float16"), scale_factor=None, mode="trilinear", align_corners=True, align_mode=1, data_format="NDHWC", )
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.upsample(x=Tensor([1, 2, 1073741825, 2, 1],"float16"), size=Tensor([3],"float16"), scale_factor=None, mode="trilinear", align_corners=True, align_mode=1, data_format="NDHWC", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 31625 / 44000 (71.9%)
Greatest absolute difference: 2800.0 at index (0, 0, 21, 19, 0) (up to 0.01 allowed)
Greatest relative difference: 117.3125 at index (0, 0, 6, 19, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 88, 25, 20, 1]), dtype=torch.float16)
tensor([[[[[  54.0000],
           [  56.3750],
           [  58.7188],
           ...,
           [  94.2500],
           [  96.6250],
           [  99.0000]],

          [[  32.0000],
           [  31.3125],
           [  30.6250],
           ...,
           [  20.3750],
           [  19.6875],
           [  19.0000]],

          [[  86.0000],
           [  87.8750],
           [  89.7500],
           ...,
           [ 118.1875],
           [ 120.1250],
           [ 122.0000]],

          ...,

          [[-148.0000],
           [-138.5000],
           [-129.2500],
           ...,
           [  12.3125],
           [  22.1250],
           [  31.0000]],

          [[-157.0000],
           [-186.0000],
           [-216.0000],
           ...,
           [-658.0000],
           [-687.5000],
           [-717.0000]],

          [[  51.0000],
           [  48.8750],
           [  46.7812],
           ...,
           [  15.2188],
           [  13.1094],
           [  11.0000]]],


         [[[  53.9062],
           [  56.3125],
           [  58.6562],
           ...,
           [  94.3750],
           [  96.7500],
           [  99.1250]],

          [[  31.7812],
           [  31.1250],
           [  30.4531],
           ...,
           [  20.5156],
           [  19.8438],
           [  19.1875]],

          [[  86.4375],
           [  88.2500],
           [  90.1250],
           ...,
           [ 118.1250],
           [ 120.0625],
           [ 121.8750]],

          ...,

          [[-147.6250],
           [-137.7500],
           [-128.1250],
           ...,
           [  19.1250],
           [  29.3125],
           [  38.5625]],

          [[-162.6250],
           [-190.5000],
           [-219.5000],
           ...,
           [-645.5000],
           [-674.0000],
           [-702.0000]],

          [[  51.2188],
           [  49.0625],
           [  46.9688],
           ...,
           [  15.2891],
           [  13.1719],
           [  11.0547]]],


         [[[  53.8750],
           [  56.2500],
           [  58.6250],
           ...,
           [  94.5000],
           [  96.9375],
           [  99.3125]],

          [[  31.5938],
           [  30.9375],
           [  30.2969],
           ...,
           [  20.6562],
           [  20.0156],
           [  19.3750]],

          [[  86.8750],
           [  88.7500],
           [  90.5625],
           ...,
           [ 118.1250],
           [ 120.0000],
           [ 121.8750]],

          ...,

          [[-147.2500],
           [-137.0000],
           [-126.9375],
           ...,
           [  25.9531],
           [  36.5312],
           [  46.1562]],

          [[-168.3750],
           [-195.2500],
           [-223.0000],
           ...,
           [-633.0000],
           [-660.5000],
           [-688.0000]],

          [[  51.4375],
           [  49.3125],
           [  47.1875],
           ...,
           [  15.3672],
           [  13.2422],
           [  11.1172]]],


         ...,


         [[[  48.1562],
           [  51.5000],
           [  54.9375],
           ...,
           [ 105.8750],
           [ 109.3125],
           [ 112.6875]],

          [[  14.4141],
           [  15.4766],
           [  16.5312],
           ...,
           [  32.5000],
           [  33.5625],
           [  34.6250]],

          [[ 124.1250],
           [ 123.5625],
           [ 123.1250],
           ...,
           [ 116.0625],
           [ 115.6250],
           [ 115.1875]],

          ...,

          [[-113.8125],
           [ -72.0625],
           [ -30.8438],
           ...,
           [ 592.0000],
           [ 633.5000],
           [ 674.5000]],

          [[-637.5000],
           [-576.5000],
           [-515.0000],
           ...,
           [ 403.0000],
           [ 463.0000],
           [ 526.0000]],

          [[  69.5625],
           [  66.6875],
           [  63.9062],
           ...,
           [  21.5312],
           [  18.7031],
           [  15.8828]]],


         [[[  48.0625],
           [  51.4688],
           [  54.8750],
           ...,
           [ 106.0000],
           [ 109.4375],
           [ 112.8750]],

          [[  14.2109],
           [  15.2891],
           [  16.3750],
           ...,
           [  32.6562],
           [  33.7188],
           [  34.8125]],

          [[ 124.5625],
           [ 124.0000],
           [ 123.5625],
           ...,
           [ 116.0625],
           [ 115.5625],
           [ 115.0625]],

          ...,

          [[-113.4375],
           [ -71.3125],
           [ -29.7188],
           ...,
           [ 598.5000],
           [ 641.0000],
           [ 682.5000]],

          [[-643.5000],
           [-580.5000],
           [-518.5000],
           ...,
           [ 415.0000],
           [ 476.2500],
           [ 540.0000]],

          [[  69.8125],
           [  66.8750],
           [  64.1250],
           ...,
           [  21.6094],
           [  18.7812],
           [  15.9375]]],


         [[[  48.0000],
           [  51.4062],
           [  54.8438],
           ...,
           [ 106.1250],
           [ 109.6250],
           [ 113.0000]],

          [[  14.0000],
           [  15.1016],
           [  16.2031],
           ...,
           [  32.7812],
           [  33.9062],
           [  35.0000]],

          [[ 125.0000],
           [ 124.4375],
           [ 123.9375],
           ...,
           [ 116.0625],
           [ 115.5000],
           [ 115.0000]],

          ...,

          [[-113.0000],
           [ -70.5000],
           [ -28.5312],
           ...,
           [ 605.5000],
           [ 648.0000],
           [ 690.0000]],

          [[-649.0000],
           [-585.5000],
           [-522.0000],
           ...,
           [ 427.7500],
           [ 490.0000],
           [ 555.0000]],

          [[  70.0000],
           [  67.1250],
           [  64.3125],
           ...,
           [  21.6875],
           [  18.8438],
           [  16.0000]]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([1, 88, 25, 20, 1]), dtype=torch.float16)
tensor([[[[[ 54.0000],
           [ 56.3750],
           [ 58.7500],
           ...,
           [ 94.2500],
           [ 96.6250],
           [ 99.0000]],

          [[ 32.0000],
           [ 31.3125],
           [ 30.6250],
           ...,
           [ 20.3750],
           [ 19.6875],
           [ 19.0000]],

          [[ 86.0000],
           [ 87.8750],
           [ 89.8125],
           ...,
           [118.1875],
           [120.1250],
           [122.0000]],

          ...,

          [[ 76.0000],
           [ 77.8125],
           [ 79.6875],
           ...,
           [107.3125],
           [109.1875],
           [111.0000]],

          [[ 59.0000],
           [ 56.6875],
           [ 54.3750],
           ...,
           [ 19.6250],
           [ 17.3125],
           [ 15.0000]],

          [[ 51.0000],
           [ 48.9062],
           [ 46.7812],
           ...,
           [ 15.2109],
           [ 13.1016],
           [ 11.0000]]],


         [[[ 53.9375],
           [ 56.3125],
           [ 58.6875],
           ...,
           [ 94.3750],
           [ 96.7500],
           [ 99.1875]],

          [[ 31.7969],
           [ 31.1250],
           [ 30.4688],
           ...,
           [ 20.5156],
           [ 19.8438],
           [ 19.1875]],

          [[ 86.4375],
           [ 88.3125],
           [ 90.1875],
           ...,
           [118.1875],
           [120.0625],
           [121.9375]],

          ...,

          [[ 75.6875],
           [ 77.5000],
           [ 79.3125],
           ...,
           [106.5625],
           [108.3750],
           [110.1875]],

          [[ 58.5938],
           [ 56.3438],
           [ 54.1250],
           ...,
           [ 20.7031],
           [ 18.4688],
           [ 16.2344]],

          [[ 51.2188],
           [ 49.0938],
           [ 47.0000],
           ...,
           [ 15.2812],
           [ 13.1719],
           [ 11.0547]]],


         [[[ 53.8750],
           [ 56.2500],
           [ 58.6562],
           ...,
           [ 94.5625],
           [ 96.9375],
           [ 99.3125]],

          [[ 31.5938],
           [ 30.9375],
           [ 30.2969],
           ...,
           [ 20.6562],
           [ 20.0156],
           [ 19.3750]],

          [[ 86.8750],
           [ 88.7500],
           [ 90.5625],
           ...,
           [118.1875],
           [120.0000],
           [121.8125]],

          ...,

          [[ 75.3125],
           [ 77.1250],
           [ 78.9375],
           ...,
           [105.8125],
           [107.6250],
           [109.4375]],

          [[ 58.1875],
           [ 56.0312],
           [ 53.8750],
           ...,
           [ 21.7656],
           [ 19.6250],
           [ 17.4844]],

          [[ 51.4375],
           [ 49.3125],
           [ 47.1875],
           ...,
           [ 15.3594],
           [ 13.2344],
           [ 11.1172]]],


         ...,


         [[[ 48.1250],
           [ 51.5312],
           [ 54.9375],
           ...,
           [105.8750],
           [109.3125],
           [112.6875]],

          [[ 14.4141],
           [ 15.4766],
           [ 16.5469],
           ...,
           [ 32.5000],
           [ 33.5625],
           [ 34.6250]],

          [[124.1250],
           [123.6250],
           [123.1875],
           ...,
           [116.1250],
           [115.6250],
           [115.1875]],

          ...,

          [[ 47.6562],
           [ 47.4375],
           [ 47.2500],
           ...,
           [ 44.0312],
           [ 43.8125],
           [ 43.5938]],

          [[ 23.8281],
           [ 28.9219],
           [ 34.0000],
           ...,
           [110.3125],
           [115.4375],
           [120.5000]],

          [[ 69.5625],
           [ 66.7500],
           [ 63.9062],
           ...,
           [ 21.5312],
           [ 18.7031],
           [ 15.8828]]],


         [[[ 48.0625],
           [ 51.4688],
           [ 54.8750],
           ...,
           [106.0000],
           [109.4375],
           [112.8125]],

          [[ 14.2031],
           [ 15.2891],
           [ 16.3750],
           ...,
           [ 32.6562],
           [ 33.7188],
           [ 34.8125]],

          [[124.5625],
           [124.0625],
           [123.5625],
           ...,
           [116.0625],
           [115.5625],
           [115.0625]],

          ...,

          [[ 47.3438],
           [ 47.0938],
           [ 46.8438],
           ...,
           [ 43.2812],
           [ 43.0312],
           [ 42.7812]],

          [[ 23.4062],
           [ 28.5938],
           [ 33.7812],
           ...,
           [111.4375],
           [116.5625],
           [121.7500]],

          [[ 69.8125],
           [ 66.9375],
           [ 64.1250],
           ...,
           [ 21.6094],
           [ 18.7812],
           [ 15.9453]]],


         [[[ 48.0000],
           [ 51.4062],
           [ 54.8438],
           ...,
           [106.1875],
           [109.5625],
           [113.0000]],

          [[ 14.0000],
           [ 15.1016],
           [ 16.2031],
           ...,
           [ 32.7812],
           [ 33.9062],
           [ 35.0000]],

          [[125.0000],
           [124.5000],
           [123.9375],
           ...,
           [116.0625],
           [115.5000],
           [115.0000]],

          ...,

          [[ 47.0000],
           [ 46.7500],
           [ 46.4688],
           ...,
           [ 42.5312],
           [ 42.2500],
           [ 42.0000]],

          [[ 23.0000],
           [ 28.2656],
           [ 33.5312],
           ...,
           [112.5000],
           [117.7500],
           [123.0000]],

          [[ 70.0000],
           [ 67.1875],
           [ 64.3125],
           ...,
           [ 21.6875],
           [ 18.8438],
           [ 16.0000]]]]], dtype=torch.float16)

2025-07-09 17:48:27.001516 GPU 7 107894 test begin: paddle.nn.functional.upsample(x=Tensor([1, 2, 2, 1073741825, 1],"float16"), size=Tensor([3],"float16"), scale_factor=None, mode="trilinear", align_corners=True, align_mode=1, data_format="NDHWC", )
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.upsample(x=Tensor([1, 2, 2, 1073741825, 1],"float16"), size=Tensor([3],"float16"), scale_factor=None, mode="trilinear", align_corners=True, align_mode=1, data_format="NDHWC", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 15816 / 24824 (63.7%)
Greatest absolute difference: 3136.0 at index (0, 106, 3, 44, 0) (up to 0.01 allowed)
Greatest relative difference: 2944.0 at index (0, 0, 0, 52, 0) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([1, 107, 4, 58, 1]), dtype=torch.float16)
tensor([[[[[ 64.0000],
           [ 60.0000],
           [104.0000],
           ...,
           [204.0000],
           [ 68.0000],
           [  2.0000]],

          [[ 84.0000],
           [ 48.0312],
           [ 98.6875],
           ...,
           [166.3750],
           [ 60.6875],
           [ 34.3438]],

          [[104.0000],
           [ 36.0000],
           [ 93.3125],
           ...,
           [128.7500],
           [ 53.3438],
           [ 66.6875]],

          [[124.0000],
           [ 24.0000],
           [ 88.0000],
           ...,
           [ 91.0000],
           [ 46.0000],
           [ 99.0000]]],


         [[[ 64.3750],
           [ 59.9062],
           [104.0000],
           ...,
           [202.3750],
           [ 68.5000],
           [  2.4434]],

          [[ 83.9375],
           [ 47.9688],
           [ 98.7500],
           ...,
           [166.1250],
           [ 61.1875],
           [ 34.6250]],

          [[103.5000],
           [ 36.0000],
           [ 93.5000],
           ...,
           [129.8750],
           [ 53.8438],
           [ 66.8125]],

          [[123.1250],
           [ 24.0312],
           [ 88.2500],
           ...,
           [ 93.5625],
           [ 46.4688],
           [ 99.0000]]],


         [[[ 64.7500],
           [ 59.7812],
           [103.9375],
           ...,
           [200.6250],
           [ 69.0000],
           [  2.8867]],

          [[ 83.8750],
           [ 47.9062],
           [ 98.8125],
           ...,
           [165.7500],
           [ 61.6875],
           [ 34.9062]],

          [[103.0000],
           [ 35.9688],
           [ 93.5625],
           ...,
           [131.0000],
           [ 54.3125],
           [ 66.9375]],

          [[122.1250],
           [ 24.0469],
           [ 88.4375],
           ...,
           [ 96.0000],
           [ 46.9375],
           [ 98.9375]]],


         ...,


         [[[102.2500],
           [ 49.2188],
           [101.0625],
           ...,
           [ 27.4375],
           [122.0000],
           [ 48.0938]],

          [[ 77.5000],
           [ 41.8125],
           [104.8750],
           ...,
           [135.8750],
           [113.0000],
           [ 63.7500]],

          [[ 52.6875],
           [ 34.3750],
           [108.6875],
           ...,
           [244.3750],
           [104.0000],
           [ 79.3750]],

          [[ 27.8594],
           [ 26.9375],
           [112.5625],
           ...,
           [353.0000],
           [ 95.0625],
           [ 95.0625]]],


         [[[102.6250],
           [ 49.0938],
           [101.0000],
           ...,
           [ 25.6719],
           [122.5000],
           [ 48.5625]],

          [[ 77.4375],
           [ 41.7500],
           [104.9375],
           ...,
           [135.6250],
           [113.5625],
           [ 64.0625]],

          [[ 52.1875],
           [ 34.3750],
           [108.8750],
           ...,
           [245.6250],
           [104.5000],
           [ 79.5000]],

          [[ 26.9219],
           [ 26.9688],
           [112.7500],
           ...,
           [355.5000],
           [ 95.5625],
           [ 95.0625]]],


         [[[103.0000],
           [ 49.0000],
           [101.0000],
           ...,
           [ 24.0000],
           [123.0000],
           [ 49.0000]],

          [[ 77.3750],
           [ 41.6875],
           [105.0000],
           ...,
           [135.3750],
           [114.0625],
           [ 64.3125]],

          [[ 51.6875],
           [ 34.3438],
           [109.0000],
           ...,
           [246.6250],
           [105.0000],
           [ 79.6250]],

          [[ 26.0000],
           [ 27.0000],
           [113.0000],
           ...,
           [358.0000],
           [ 96.0000],
           [ 95.0000]]]]], dtype=torch.float16)
DESIRED: (shape=torch.Size([1, 107, 4, 58, 1]), dtype=torch.float16)
tensor([[[[[ 64.0000],
           [ 60.0000],
           [104.0000],
           ...,
           [116.0000],
           [ 68.0000],
           [  2.0000]],

          [[ 84.0000],
           [ 48.0000],
           [ 98.6875],
           ...,
           [ 94.3125],
           [ 60.6562],
           [ 34.3438]],

          [[104.0000],
           [ 36.0000],
           [ 93.3125],
           ...,
           [ 72.6875],
           [ 53.3438],
           [ 66.6875]],

          [[124.0000],
           [ 24.0000],
           [ 88.0000],
           ...,
           [ 51.0000],
           [ 46.0000],
           [ 99.0000]]],


         [[[ 64.3750],
           [ 59.9062],
           [104.0000],
           ...,
           [115.8750],
           [ 68.5000],
           [  2.4434]],

          [[ 83.9375],
           [ 47.9375],
           [ 98.7500],
           ...,
           [ 94.3125],
           [ 61.1562],
           [ 34.6250]],

          [[103.5000],
           [ 35.9688],
           [ 93.5000],
           ...,
           [ 72.6875],
           [ 53.8125],
           [ 66.8125]],

          [[123.0625],
           [ 24.0312],
           [ 88.2500],
           ...,
           [ 51.0938],
           [ 46.4688],
           [ 98.9375]]],


         [[[ 64.7500],
           [ 59.7812],
           [103.9375],
           ...,
           [115.7500],
           [ 69.0625],
           [  2.8867]],

          [[ 83.8750],
           [ 47.8750],
           [ 98.8125],
           ...,
           [ 94.2500],
           [ 61.6875],
           [ 34.9062]],

          [[103.0000],
           [ 35.9688],
           [ 93.6250],
           ...,
           [ 72.7500],
           [ 54.3125],
           [ 66.9375]],

          [[122.1250],
           [ 24.0625],
           [ 88.5000],
           ...,
           [ 51.2188],
           [ 46.9375],
           [ 98.9375]]],


         ...,


         [[[102.2500],
           [ 49.2188],
           [101.0625],
           ...,
           [104.2500],
           [121.9375],
           [ 48.1250]],

          [[ 77.4375],
           [ 41.7812],
           [104.8750],
           ...,
           [ 90.0625],
           [113.0000],
           [ 63.7812]],

          [[ 52.6562],
           [ 34.3750],
           [108.6875],
           ...,
           [ 75.9375],
           [104.0000],
           [ 79.4375]],

          [[ 27.8438],
           [ 26.9375],
           [112.5000],
           ...,
           [ 61.7812],
           [ 95.0625],
           [ 95.0625]]],


         [[[102.6250],
           [ 49.0938],
           [101.0000],
           ...,
           [104.1250],
           [122.5000],
           [ 48.5625]],

          [[ 77.3750],
           [ 41.7188],
           [104.9375],
           ...,
           [ 90.0625],
           [113.5000],
           [ 64.0625]],

          [[ 52.1562],
           [ 34.3438],
           [108.8750],
           ...,
           [ 75.9375],
           [104.5000],
           [ 79.5625]],

          [[ 26.9219],
           [ 26.9688],
           [112.7500],
           ...,
           [ 61.9062],
           [ 95.5000],
           [ 95.0625]]],


         [[[103.0000],
           [ 49.0000],
           [101.0000],
           ...,
           [104.0000],
           [123.0000],
           [ 49.0000]],

          [[ 77.3125],
           [ 41.6562],
           [105.0000],
           ...,
           [ 90.0000],
           [114.0000],
           [ 64.3125]],

          [[ 51.6562],
           [ 34.3438],
           [109.0000],
           ...,
           [ 76.0000],
           [105.0000],
           [ 79.6875]],

          [[ 26.0000],
           [ 27.0000],
           [113.0000],
           ...,
           [ 62.0000],
           [ 96.0000],
           [ 95.0000]]]]], dtype=torch.float16)

2025-07-09 17:48:58.065421 GPU 3 108054 test begin: paddle.nn.functional.upsample(x=Tensor([1, 2, 2147483649, 1],"float16"), size=Tensor([2],"float16"), scale_factor=None, mode="bicubic", align_corners=False, align_mode=0, data_format="NHWC", )
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752054596 (unix time) try "date -d @1752054596" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a616) received by PID 108054 (TID 0x7f8b10e9d740) from PID 108054 ***]


2025-07-09 17:50:14.153535 GPU 5 109354 test begin: paddle.nn.functional.upsample(x=Tensor([2147483649, 2, 1],"float16"), size=Tensor([1],"float16"), scale_factor=None, mode="linear", align_corners=False, align_mode=0, data_format="NWC", )
/host_home/ningzhengsheng/paddle_env/lib/python3.10/site-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.upsample(x=Tensor([2147483649, 2, 1],"float16"), size=Tensor([1],"float16"), scale_factor=None, mode="linear", align_corners=False, align_mode=0, data_format="NWC", ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/dense_tensor.cc:117)


2025-07-09 17:50:14.669080 GPU 4 109441 test begin: paddle.nn.quant.weight_quantize(Tensor([128, 33554432],"float16"), algo="weight_only_int8", arch=70, group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([128, 33554432],"float16"), algo="weight_only_int8", arch=70, group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4270960506 / 4294967296 (99.4%)
Greatest absolute difference: 255 at index (2507, 70) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 90) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([33554432, 128]), dtype=torch.int8)
tensor([[-117,   27,  -42,  ...,   30,  -74,  -12],
        [  27,   56,   65,  ...,  -12,  -10,  -15],
        [ 116,   99,  -12,  ...,  115,  -44,   10],
        ...,
        [-101,   71, -127,  ..., -114,  -65,   11],
        [ -91, -124,   50,  ...,    3,   28,  -40],
        [ -75,  -13, -115,  ...,  -55,   -7,  115]], dtype=torch.int8)
DESIRED: (shape=torch.Size([33554432, 128]), dtype=torch.int8)
tensor([[  11,  -36,  113,  ...,   33,   66,  -36],
        [  85,  -54,   34,  ...,  106,   57,  -15],
        [-100,   74,  117,  ...,  -38,   54,   15],
        ...,
        [  79,  -63,  -16,  ...,  121,  -18,  121],
        [ -12,   26,   73,  ...,   22,   47,   72],
        [ -29,  114,   77,  ...,   79,  -94,  -13]], dtype=torch.int8)

2025-07-09 17:50:25.896861 GPU 7 107894 test begin: paddle.nn.quant.weight_quantize(Tensor([128, 33554432],"float16"), algo="weight_only_int8", arch=75, group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([128, 33554432],"float16"), algo="weight_only_int8", arch=75, group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4272455279 / 4294967296 (99.5%)
Greatest absolute difference: 255 at index (435, 75) (up to 0.01 allowed)
Greatest relative difference: inf at index (4, 59) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([33554432, 128]), dtype=torch.int8)
tensor([[  98,  109,   83,  ..., -107,   40,  117],
        [ -69,  -90,  -36,  ...,   43, -108,   75],
        [  72,   52,  -92,  ...,  -14,  104, -119],
        ...,
        [ 117,    7,   91,  ...,    5,  -56, -127],
        [  -8,   90,  -10,  ...,  102,  -61,   40],
        [  35,  122,   -9,  ...,  -24,  -41,  -37]], dtype=torch.int8)
DESIRED: (shape=torch.Size([33554432, 128]), dtype=torch.int8)
tensor([[ -30,  -45,   40,  ...,   20,   49,  -64],
        [ -10,   17,   99,  ...,   -6,  -84,  -52],
        [ -55,   35,  -38,  ...,   72,  113,  -33],
        ...,
        [  91,   99,    3,  ...,  117, -122,    1],
        [ 119,  117,  -75,  ..., -105,  -34,   77],
        [  74,   84,   39,  ...,   64,  104,   91]], dtype=torch.int8)

2025-07-09 17:50:42.943935 GPU 3 109674 test begin: paddle.nn.quant.weight_quantize(Tensor([128, 33554432],"float16"), algo="weight_only_int8", arch=80, group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([128, 33554432],"float16"), algo="weight_only_int8", arch=80, group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4272466977 / 4294967296 (99.5%)
Greatest absolute difference: 255 at index (156, 109) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 109) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([33554432, 128]), dtype=torch.int8)
tensor([[-116,  -66,  -72,  ...,  -77,    6,   16],
        [   4,  111,  -18,  ..., -120,   37,  -84],
        [  -7,  108,  -17,  ...,   86,   60,   75],
        ...,
        [ 123,  -53,  -89,  ...,  -65,   98,   25],
        [ -36,    8,  -67,  ...,   55,  118,    8],
        [-106,  -49,  113,  ...,   57,  -64,  -19]], dtype=torch.int8)
DESIRED: (shape=torch.Size([33554432, 128]), dtype=torch.int8)
tensor([[  11,   55,  -19,  ...,    3,  112,   88],
        [  10,  -34,  -51,  ...,   84,    8,   44],
        [ 120,  110,  125,  ...,  -92,  -76,  -81],
        ...,
        [ -71,   10,  103,  ...,   97,   62, -101],
        [  92,   61,  -24,  ...,   10,   95, -120],
        [  15,   -6,   -9,  ...,  -33,  -71,  108]], dtype=torch.int8)

2025-07-09 17:51:07.497285 GPU 6 106912 test begin: paddle.nn.quant.weight_quantize(Tensor([128, 33554432],"float16"), algo="weight_only_int8", arch=86, group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([128, 33554432],"float16"), algo="weight_only_int8", arch=86, group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4272452228 / 4294967296 (99.5%)
Greatest absolute difference: 255 at index (423, 24) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 114) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([33554432, 128]), dtype=torch.int8)
tensor([[ -87,  -43,   34,  ...,  -19,  -63,   -7],
        [-124,   14,   45,  ...,   -1,   85, -123],
        [ -90,  103,   58,  ...,   46,   89, -110],
        ...,
        [ -82,   17,   87,  ...,   -2,   -3,   99],
        [  87,   56,  127,  ...,   61,   26,   62],
        [-101,  -49,   -4,  ...,  -24,  -14,   88]], dtype=torch.int8)
DESIRED: (shape=torch.Size([33554432, 128]), dtype=torch.int8)
tensor([[  41,  -94,  -21,  ...,  -74,   96, -110],
        [ 114,  -37,  -43,  ...,  -63,  127,    5],
        [  37,  -70,  -80,  ...,   96,   42,   24],
        ...,
        [  73,  -40,   37,  ...,  -38,  126,  -29],
        [ -40,   -1,   13,  ...,   97,   76,   97],
        [ -58,   64,  -64,  ...,    4,  103,  -40]], dtype=torch.int8)

2025-07-09 17:52:03.296975 GPU 7 107894 test begin: paddle.nn.quant.weight_quantize(Tensor([128, 33554432],"float16"), algo="weight_only_int8", group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([128, 33554432],"float16"), algo="weight_only_int8", group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4272455279 / 4294967296 (99.5%)
Greatest absolute difference: 255 at index (435, 75) (up to 0.01 allowed)
Greatest relative difference: inf at index (4, 59) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([33554432, 128]), dtype=torch.int8)
tensor([[  98,  109,   83,  ..., -107,   40,  117],
        [ -69,  -90,  -36,  ...,   43, -108,   75],
        [  72,   52,  -92,  ...,  -14,  104, -119],
        ...,
        [ 117,    7,   91,  ...,    5,  -56, -127],
        [  -8,   90,  -10,  ...,  102,  -61,   40],
        [  35,  122,   -9,  ...,  -24,  -41,  -37]], dtype=torch.int8)
DESIRED: (shape=torch.Size([33554432, 128]), dtype=torch.int8)
tensor([[ -30,  -45,   40,  ...,   20,   49,  -64],
        [ -10,   17,   99,  ...,   -6,  -84,  -52],
        [ -55,   35,  -38,  ...,   72,  113,  -33],
        ...,
        [  91,   99,    3,  ...,  117, -122,    1],
        [ 119,  117,  -75,  ..., -105,  -34,   77],
        [  74,   84,   39,  ...,   64,  104,   91]], dtype=torch.int8)

2025-07-09 17:52:38.647994 GPU 2 109196 test begin: paddle.nn.quant.weight_quantize(Tensor([16777216, 256],"float16"), algo="weight_only_int4", arch=70, group_size=-1, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_weight_quantize(_object*, _object*, _object*)
1   weight_quantize_ad_func(paddle::Tensor const&, std::string, int, int)
2   paddle::experimental::weight_quantize(paddle::Tensor const&, std::string const&, int, int)
3   void phi::WeightQuantizeKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, int, int, phi::DenseTensor*, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752054784 (unix time) try "date -d @1752054784" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1aa8c) received by PID 109196 (TID 0x7efcb4cb4740) from PID 109196 ***]


2025-07-09 17:52:51.305800 GPU 6 106912 test begin: paddle.nn.quant.weight_quantize(Tensor([16777216, 256],"float16"), algo="weight_only_int4", arch=75, group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([16777216, 256],"float16"), algo="weight_only_int4", arch=75, group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([128, 16777216]) != torch.Size([256, 16777216]).
ACTUAL: (shape=torch.Size([128, 16777216]), dtype=torch.int8)
tensor([[  90,  -28,   43,  ...,   51,  -29,   73],
        [  56,  -58,   54,  ...,   23,  -43,  -77],
        [-125, -118,  114,  ...,   58,  -67, -101],
        ...,
        [  45,  -29,  -84,  ..., -117,  -23,   70],
        [  35, -102,  116,  ...,  -72,   53,   75],
        [ -26,   41,   45,  ..., -106,   75,  106]], dtype=torch.int8)
DESIRED: (shape=torch.Size([256, 16777216]), dtype=torch.int8)
tensor([[ 2,  3,  4,  ...,  5, -3,  2],
        [ 6,  6,  2,  ...,  1, -5,  3],
        [ 2,  4,  2,  ..., -5, -2,  0],
        ...,
        [ 4, -4, -6,  ..., -2, -1, -2],
        [-2,  3,  5,  ...,  4,  7,  5],
        [-1, -6, -3,  ...,  7,  1, -2]], dtype=torch.int8)

2025-07-09 17:52:54.326714 GPU 5 109354 test begin: paddle.nn.quant.weight_quantize(Tensor([16777216, 256],"float16"), algo="weight_only_int4", arch=80, group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([16777216, 256],"float16"), algo="weight_only_int4", arch=80, group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([128, 16777216]) != torch.Size([256, 16777216]).
ACTUAL: (shape=torch.Size([128, 16777216]), dtype=torch.int8)
tensor([[  66,  -89,  -74,  ...,  120,  120,  125],
        [ 114,   -2,   21,  ...,  -29,   90,   81],
        [ -75,   47,  -38,  ...,  -13,   -6,  -97],
        ...,
        [ -39, -114,   53,  ...,  117,  -92,  121],
        [ -59,  -66,   20,  ...,   78,   52,   -6],
        [  44,   18,   37,  ...,  -29,   61,   69]], dtype=torch.int8)
DESIRED: (shape=torch.Size([256, 16777216]), dtype=torch.int8)
tensor([[-6, -2, -1,  ..., -7, -3, -1],
        [ 1,  5, -1,  ..., -2, -7, -4],
        [-2, -3,  6,  ...,  5,  5, -2],
        ...,
        [-2,  0,  0,  ...,  1,  6, -6],
        [ 2, -6, -1,  ..., -1, -3,  5],
        [ 0, -4, -6,  ...,  2,  6, -4]], dtype=torch.int8)

2025-07-09 17:53:26.041593 GPU 4 109441 test begin: paddle.nn.quant.weight_quantize(Tensor([16777216, 256],"float16"), algo="weight_only_int4", arch=86, group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([16777216, 256],"float16"), algo="weight_only_int4", arch=86, group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([128, 16777216]) != torch.Size([256, 16777216]).
ACTUAL: (shape=torch.Size([128, 16777216]), dtype=torch.int8)
tensor([[ 121,  -87,  -57,  ...,  -93,  -44,   84],
        [  68,   37,  -12,  ...,  -90,  -42, -106],
        [  -1,   69,   82,  ...,  -99, -102,  -29],
        ...,
        [ -62,   56,   37,  ...,   60,  -72,   71],
        [ 119,   33,  -67,  ..., -116,  -29,   69],
        [-108,  -26,  100,  ...,   40, -124,  114]], dtype=torch.int8)
DESIRED: (shape=torch.Size([256, 16777216]), dtype=torch.int8)
tensor([[ 1, -1, -1,  ...,  7,  4,  2],
        [ 5,  6,  6,  ...,  5,  1, -4],
        [-6, -2,  6,  ..., -3, -3,  0],
        ...,
        [ 7,  2, -4,  ...,  0,  3,  7],
        [ 6,  0,  2,  ...,  0,  1,  4],
        [ 6,  5,  1,  ..., -1, -7, -1]], dtype=torch.int8)

2025-07-09 17:53:34.895639 GPU 6 106912 test begin: paddle.nn.quant.weight_quantize(Tensor([16777216, 256],"float16"), algo="weight_only_int4", group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([16777216, 256],"float16"), algo="weight_only_int4", group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([128, 16777216]) != torch.Size([256, 16777216]).
ACTUAL: (shape=torch.Size([128, 16777216]), dtype=torch.int8)
tensor([[  90,  -28,   43,  ...,   51,  -29,   73],
        [  56,  -58,   54,  ...,   23,  -43,  -77],
        [-125, -118,  114,  ...,   58,  -67, -101],
        ...,
        [  45,  -29,  -84,  ..., -117,  -23,   70],
        [  35, -102,  116,  ...,  -72,   53,   75],
        [ -26,   41,   45,  ..., -106,   75,  106]], dtype=torch.int8)
DESIRED: (shape=torch.Size([256, 16777216]), dtype=torch.int8)
tensor([[ 2,  3,  4,  ...,  5, -3,  2],
        [ 6,  6,  2,  ...,  1, -5,  3],
        [ 2,  4,  2,  ..., -5, -2,  0],
        ...,
        [ 4, -4, -6,  ..., -2, -1, -2],
        [-2,  3,  5,  ...,  4,  7,  5],
        [-1, -6, -3,  ...,  7,  1, -2]], dtype=torch.int8)

2025-07-09 17:56:19.085271 GPU 3 109674 test begin: paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int4", arch=70, group_size=-1, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_weight_quantize(_object*, _object*, _object*)
1   weight_quantize_ad_func(paddle::Tensor const&, std::string, int, int)
2   paddle::experimental::weight_quantize(paddle::Tensor const&, std::string const&, int, int)
3   void phi::WeightQuantizeKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, int, int, phi::DenseTensor*, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752054988 (unix time) try "date -d @1752054988" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ac6a) received by PID 109674 (TID 0x7f0f62224740) from PID 109674 ***]


2025-07-09 17:56:48.136061 GPU 5 109354 test begin: paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int4", arch=75, group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int4", arch=75, group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([33554432, 64]) != torch.Size([67108864, 64]).
ACTUAL: (shape=torch.Size([33554432, 64]), dtype=torch.int8)
tensor([[ -78,   85,  -41,  ...,   76,  122,   99],
        [  86,  -97, -106,  ..., -119,   57,  -13],
        [ -75,   58,  -23,  ...,  -20,  105,  -24],
        ...,
        [ 113,   58,  123,  ...,  -79,  -98,  113],
        [  70, -100,   37,  ...,   92, -116,   40],
        [  36, -103,   86,  ...,  -24, -124,   70]], dtype=torch.int8)
DESIRED: (shape=torch.Size([67108864, 64]), dtype=torch.int8)
tensor([[-6, -1, -3,  ...,  3,  1,  6],
        [ 1,  1,  0,  ..., -6, -4, -2],
        [-2, -2, -4,  ...,  7,  1, -3],
        ...,
        [ 4,  4, -1,  ..., -5, -3, -6],
        [-4, -2, -5,  ...,  3,  1,  5],
        [ 2,  4,  0,  ...,  0,  6, -4]], dtype=torch.int8)

2025-07-09 17:57:00.623272 GPU 5 109354 test begin: paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int4", arch=80, group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int4", arch=80, group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([33554432, 64]) != torch.Size([67108864, 64]).
ACTUAL: (shape=torch.Size([33554432, 64]), dtype=torch.int8)
tensor([[ -78,   85,  -41,  ...,   76,  122,   99],
        [  86,  -97, -106,  ..., -119,   57,  -13],
        [ -75,   58,  -23,  ...,  -20,  105,  -24],
        ...,
        [ 113,   58,  123,  ...,  -79,  -98,  113],
        [  70, -100,   37,  ...,   92, -116,   40],
        [  36, -103,   86,  ...,  -24, -124,   70]], dtype=torch.int8)
DESIRED: (shape=torch.Size([67108864, 64]), dtype=torch.int8)
tensor([[-6, -1, -3,  ...,  3,  1,  6],
        [ 1,  1,  0,  ..., -6, -4, -2],
        [-2, -2, -4,  ...,  7,  1, -3],
        ...,
        [ 4,  4, -1,  ..., -5, -3, -6],
        [-4, -2, -5,  ...,  3,  1,  5],
        [ 2,  4,  0,  ...,  0,  6, -4]], dtype=torch.int8)

2025-07-09 17:57:09.337555 GPU 3 110986 test begin: paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int4", arch=86, group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int4", arch=86, group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([33554432, 64]) != torch.Size([67108864, 64]).
ACTUAL: (shape=torch.Size([33554432, 64]), dtype=torch.int8)
tensor([[  -4, -127,  -65,  ...,  125, -102,  -21],
        [ -74,  -69,  -43,  ...,   46,   74,   83],
        [-107,  -17,  -43,  ...,   99,   65,   81],
        ...,
        [ -50,  -73,   50,  ...,  -56, -100,  -66],
        [  90,   37,  -76,  ..., -122,  -78,   68],
        [-120,   70,   44,  ...,  105,  -98,  -17]], dtype=torch.int8)
DESIRED: (shape=torch.Size([67108864, 64]), dtype=torch.int8)
tensor([[ 4,  7, -3,  ..., -5,  1, -2],
        [-6,  5, -6,  ...,  5, -1,  6],
        [-2, -2,  0,  ...,  2, -5,  3],
        ...,
        [ 6,  0, -7,  ...,  2,  0, -3],
        [ 0,  4, -3,  ..., -1,  2, -1],
        [-3,  3, -1,  ...,  4, -2,  6]], dtype=torch.int8)

2025-07-09 17:57:11.475814 GPU 5 109354 test begin: paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int4", group_size=-1, )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int4", group_size=-1, ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([33554432, 64]) != torch.Size([67108864, 64]).
ACTUAL: (shape=torch.Size([33554432, 64]), dtype=torch.int8)
tensor([[ -78,   85,  -41,  ...,   76,  122,   99],
        [  86,  -97, -106,  ..., -119,   57,  -13],
        [ -75,   58,  -23,  ...,  -20,  105,  -24],
        ...,
        [ 113,   58,  123,  ...,  -79,  -98,  113],
        [  70, -100,   37,  ...,   92, -116,   40],
        [  36, -103,   86,  ...,  -24, -124,   70]], dtype=torch.int8)
DESIRED: (shape=torch.Size([67108864, 64]), dtype=torch.int8)
tensor([[-6, -1, -3,  ...,  3,  1,  6],
        [ 1,  1,  0,  ..., -6, -4, -2],
        [-2, -2, -4,  ...,  7,  1, -3],
        ...,
        [ 4,  4, -1,  ..., -5, -3, -6],
        [-4, -2, -5,  ...,  3,  1,  5],
        [ 2,  4,  0,  ...,  0,  6, -4]], dtype=torch.int8)

2025-07-09 17:57:23.199425 GPU 5 109354 test begin: paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int8", arch=70, group_size=-1, )
torch_assert failed, try np_assert
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([64, 67108864],"float16"), algo="weight_only_int8", arch=70, group_size=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4270750067 / 4294967296 (99.4%)
Max absolute difference among violations: 127
Max relative difference among violations: 127.
 ACTUAL: array([[  21,   83, -113, ...,   69,   80,   42],
       [  42,  -40,   20, ...,  -89,   24,  -24],
       [ -74,  -25,    9, ...,   12,   58,   17],...
 DESIRED: array([[-107,  -26,  -53, ...,   51,   24,  115],
       [  15,   11,    3, ..., -111,  -74,  -34],
       [ -44,  -31,  -79, ...,  119,   11,  -63],...

2025-07-09 17:59:22.954761 GPU 4 109441 test begin: paddle.nn.quant.weight_quantize(Tensor([67108864, 64],"float16"), algo="weight_only_int8", )
torch_assert failed, try np_assert
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([67108864, 64],"float16"), algo="weight_only_int8", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4270365822 / 4294967296 (99.4%)
Max absolute difference among violations: 127
Max relative difference among violations: 127.
 ACTUAL: array([[-117,  112,   99, ...,   61,   48,   46],
       [  60, -124,  -44, ..., -115,  -28,  120],
       [  28,  -21,  119, ..., -108,  118,  113],...
 DESIRED: array([[  11,  -29, -102, ...,   53,   53,  114],
       [  88,   46,  -64, ...,   -7,   13,   -8],
       [-103,   -9,  -73, ..., -114,  118,  -73],...

2025-07-09 18:00:11.037430 GPU 2 110478 test begin: paddle.nn.quant.weight_quantize(Tensor([8388608, 512],"float16"), algo="weight_only_int8", )
[accuracy error] paddle.nn.quant.weight_quantize(Tensor([8388608, 512],"float16"), algo="weight_only_int8", ) 
 Not equal to tolerance rtol=0.01, atol=0.01
Tensor-likes are not close!

Mismatched elements: 4270869529 / 4294967296 (99.4%)
Greatest absolute difference: 255 at index (0, 38317) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 472) (up to 0.01 allowed)
ACTUAL: (shape=torch.Size([512, 8388608]), dtype=torch.int8)
tensor([[ -45,  101,  -61,  ...,  -67, -121,    7],
        [  79,   26, -110,  ...,   62,   78,  -26],
        [ -43,  -26, -115,  ...,  110,   43, -128],
        ...,
        [  95,  111,   13,  ...,  -68, -101, -125],
        [ -50, -108,  103,  ...,   22,   87,  -53],
        [  62,   -6,   55,  ...,  -59,   37,   77]], dtype=torch.int8)
DESIRED: (shape=torch.Size([512, 8388608]), dtype=torch.int8)
tensor([[ 83,  67,  21,  ..., -24, -85, -68],
        [-65, 118, 101,  ...,  53, -66, 102],
        [ 85,  13,  27,  ..., -81, -29,  58],
        ...,
        [ 54,  67,  40,  ...,  79,  60,   3],
        [ 78, -25, 104,  ..., 120, -36, -73],
        [ 62, -96, -82,  ...,  54,  70, -51]], dtype=torch.int8)

2025-07-09 21:33:38.008217 GPU 5 30513 test begin: paddle.vision.ops.box_coder(Tensor([30, 4],"float32"), list[0.12371375411748886,0.7415851950645447,0.40236398577690125,0.6756224632263184,], Tensor([30, 35791395, 4],"float32"), "decode_center_size", False, axis=1, )
[paddle_to_torch] Unsupported API paddle.vision.ops.box_coder: Rule for paddle.vision.ops.box_coder is not implemented

2025-07-09 21:33:40.626800 GPU 5 30513 test begin: paddle.vision.ops.box_coder(prior_box=Tensor([80, 4],"float32"), prior_box_var=Tensor([80, 4],"float32"), target_box=Tensor([13421773, 80, 4],"float32"), code_type="decode_center_size", box_normalized=False, )
[paddle_to_torch] Unsupported API paddle.vision.ops.box_coder: Rule for paddle.vision.ops.box_coder is not implemented

2025-07-09 21:33:40.791942 GPU 5 30513 test begin: paddle.vision.ops.box_coder(prior_box=Tensor([80, 4],"float32"), prior_box_var=tuple(1,2,3,4,), target_box=Tensor([13421773, 80, 4],"float32"), code_type="decode_center_size", box_normalized=False, )
[paddle_to_torch] Unsupported API paddle.vision.ops.box_coder: Rule for paddle.vision.ops.box_coder is not implemented

2025-07-09 21:33:40.910297 GPU 5 30513 test begin: paddle.vision.ops.deform_conv2d(x=Tensor([1014090, 5, 5, 5],"float32"), offset=Tensor([1014090, 90, 5, 5],"float32"), mask=Tensor([1014090, 45, 5, 5],"float32"), weight=Tensor([5, 5, 3, 3],"float32"), bias=Tensor([5],"float32"), stride=list[1,1,], padding=list[1,1,], dilation=list[1,1,], deformable_groups=5, groups=1, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DeformableConvGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::deformable_conv_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::DeformableConvGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752068196 (unix time) try "date -d @1752068196" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7731) received by PID 30513 (TID 0x7f4a4b21d740) from PID 30513 ***]


2025-07-09 21:34:28.696543 GPU 3 21715 test begin: paddle.vision.ops.deform_conv2d(x=Tensor([14084577, 2, 5, 5],"float32"), offset=Tensor([14084577, 18, 3, 3],"float32"), mask=Tensor([14084577, 9, 3, 3],"float32"), weight=Tensor([5, 2, 3, 3],"float32"), bias=None, stride=list[1,1,], padding=list[0,0,], dilation=list[1,1,], deformable_groups=1, groups=1, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DeformableConvGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::deformable_conv_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::DeformableConvGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752069095 (unix time) try "date -d @1752069095" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x54d3) received by PID 21715 (TID 0x7f44bf67d740) from PID 21715 ***]


2025-07-09 21:35:09.317934 GPU 2 22143 test begin: paddle.vision.ops.deform_conv2d(x=Tensor([2586964, 3, 5, 5],"float32"), offset=Tensor([2586964, 18, 7, 7],"float32"), mask=Tensor([2586964, 9, 7, 7],"float32"), weight=Tensor([5, 3, 3, 3],"float32"), bias=None, stride=list[1,1,], padding=list[2,2,], dilation=list[1,1,], deformable_groups=1, groups=1, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DeformableConvGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::deformable_conv_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::DeformableConvGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752068333 (unix time) try "date -d @1752068333" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x567f) received by PID 22143 (TID 0x7f6ed3acb740) from PID 22143 ***]


2025-07-09 21:35:29.552752 GPU 4 31413 test begin: paddle.vision.ops.deform_conv2d(x=Tensor([2586964, 3, 5, 5],"float32"), offset=Tensor([2586964, 18, 7, 7],"float32"), mask=Tensor([2586964, 9, 7, 7],"float32"), weight=Tensor([5, 3, 3, 3],"float32"), bias=Tensor([5],"float32"), stride=list[1,1,], padding=list[2,2,], dilation=list[1,1,], deformable_groups=1, groups=1, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DeformableConvGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::deformable_conv_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::DeformableConvGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752068427 (unix time) try "date -d @1752068427" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7ab5) received by PID 31413 (TID 0x7fe6f502f740) from PID 31413 ***]


2025-07-09 21:36:12.600616 GPU 6 1766 test begin: paddle.vision.ops.deform_conv2d(x=Tensor([30422686, 3, 5, 5],"float32"), offset=Tensor([30422686, 18, 1, 1],"float32"), mask=Tensor([30422686, 9, 1, 1],"float32"), weight=Tensor([5, 3, 3, 3],"float32"), bias=Tensor([5],"float32"), stride=list[1,1,], padding=list[1,1,], dilation=list[3,3,], deformable_groups=1, groups=1, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DeformableConvGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::deformable_conv_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::DeformableConvGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752069829 (unix time) try "date -d @1752069829" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6e6) received by PID 1766 (TID 0x7f16f43a1740) from PID 1766 ***]


2025-07-09 21:38:43.929413 GPU 5 32022 test begin: paddle.vision.ops.deform_conv2d(x=Tensor([3621749, 3, 5, 5],"float32"), offset=Tensor([3621749, 18, 5, 7],"float32"), mask=Tensor([3621749, 9, 5, 7],"float32"), weight=Tensor([5, 3, 3, 3],"float32"), bias=Tensor([5],"float32"), stride=list[1,1,], padding=list[1,2,], dilation=list[1,1,], deformable_groups=1, groups=1, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DeformableConvGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::deformable_conv_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::DeformableConvGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752068881 (unix time) try "date -d @1752068881" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7d16) received by PID 32022 (TID 0x7fbeb15e4740) from PID 32022 ***]


2025-07-09 21:39:41.407484 GPU 2 32655 test begin: paddle.vision.ops.deform_conv2d(x=Tensor([4, 22817014, 5, 5],"float32"), offset=Tensor([4, 18, 3, 3],"float32"), mask=Tensor([4, 9, 3, 3],"float32"), weight=Tensor([5, 22817014, 3, 3],"float32"), bias=None, stride=list[1,1,], padding=list[0,0,], dilation=list[1,1,], deformable_groups=1, groups=1, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_deformable_conv(_object*, _object*, _object*)
1   deformable_conv_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, int, int, int)
2   paddle::experimental::deformable_conv(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int)
3   void phi::DeformableConvKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752068578 (unix time) try "date -d @1752068578" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7f8f) received by PID 32655 (TID 0x7f34ca8d5740) from PID 32655 ***]


2025-07-09 21:42:35.031670 GPU 4 33238 test begin: paddle.vision.ops.deform_conv2d(x=Tensor([5070448, 5, 5, 5],"float32"), offset=Tensor([5070448, 18, 5, 5],"float32"), mask=Tensor([5070448, 9, 5, 5],"float32"), weight=Tensor([5, 1, 3, 3],"float32"), bias=Tensor([5],"float32"), stride=list[1,1,], padding=list[1,1,], dilation=list[1,1,], deformable_groups=1, groups=5, )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DeformableConvGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::deformable_conv_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::DeformableConvGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752069338 (unix time) try "date -d @1752069338" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x81d6) received by PID 33238 (TID 0x7fbd1c480740) from PID 33238 ***]


2025-07-09 21:45:03.982694 GPU 2 33889 test begin: paddle.vision.ops.deform_conv2d(x=Tensor([863, 64, 152, 272],"float32"), offset=Tensor([863, 18, 152, 272],"float32"), weight=Tensor([64, 64, 3, 3],"float32"), bias=Tensor([64],"float32"), stride=list[1,1,], padding=list[1,1,], dilation=list[1,1,], deformable_groups=1, groups=1, mask=Tensor([863, 9, 152, 272],"float32"), )
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /host_home/ningzhengsheng/src/Paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DeformableConvGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::deformable_conv_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::DeformableConvGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, int, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   phi::DenseTensor::~DenseTensor()
6   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1752068762 (unix time) try "date -d @1752068762" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x8461) received by PID 33889 (TID 0x7f8fd397f740) from PID 33889 ***]


2025-07-09 21:46:48.883975 GPU 2 34894 test begin: paddle.vision.ops.distribute_fpn_proposals(Tensor([410, 10475530],"float32"), 2, 5, 4, 224, rois_num=Tensor([4],"int64"), )
[accuracy error] paddle.vision.ops.distribute_fpn_proposals(Tensor([410, 10475530],"float32"), 2, 5, 4, 224, rois_num=Tensor([4],"int64"), ) 
 Not equal to tolerance rtol=0.01, atol=0.01
The values for attribute 'shape' do not match: torch.Size([409, 10475530]) != torch.Size([20, 10475530]).
ACTUAL: (shape=torch.Size([409, 10475530]), dtype=torch.float32)
tensor([[ 759.1634,  991.6230,  937.4589,  ...,    0.0000,    0.0000,    0.0000],
        [  91.9849,  611.7796,  712.3997,  ...,    0.0000,    0.0000,    0.0000],
        [  18.8937,  988.2063,  364.7075,  ...,    0.0000,    0.0000,    0.0000],
        ...,
        [ 733.7554,  886.5610, 1587.3374,  ...,    0.0000,    0.0000,    0.0000],
        [ 943.0128,   82.9882,  956.1468,  ...,    0.0000,    0.0000,    0.0000],
        [ 892.3680,  257.6677, 1098.4476,  ...,    0.0000,    0.0000,    0.0000]])
DESIRED: (shape=torch.Size([20, 10475530]), dtype=torch.float32)
tensor([[ 987.8821,  799.7585,  997.0288,  ...,    0.0000,    0.0000,    0.0000],
        [ 954.4231,  635.2732, 1048.7948,  ...,    0.0000,    0.0000,    0.0000],
        [ 204.6348,  653.9626,  218.5973,  ...,    0.0000,    0.0000,    0.0000],
        ...,
        [ 324.5803,  723.4676,  367.2345,  ...,    0.0000,    0.0000,    0.0000],
        [ 999.1685,  586.1580, 1418.5388,  ...,    0.0000,    0.0000,    0.0000],
        [ 943.0128,   82.9882,  956.1468,  ...,    0.0000,    0.0000,    0.0000]])

2025-07-09 21:51:10.258704 GPU 5 36300 test begin: paddle.vision.ops.yolo_box(Tensor([2, 14, 19173962, 8],"float32"), img_size=Tensor([2, 2],"int32"), anchors=list[10,13,16,30,], class_num=2, conf_thresh=0.01, downsample_ratio=8, clip_bbox=True, scale_x_y=1.0, )
[paddle_to_torch] Unsupported API paddle.vision.ops.yolo_box: Rule for paddle.vision.ops.yolo_box is not implemented

2025-07-09 21:51:12.652331 GPU 5 36300 test begin: paddle.vision.ops.yolo_box(Tensor([2, 14, 8, 19173962],"float32"), img_size=Tensor([2, 2],"int32"), anchors=list[10,13,16,30,], class_num=2, conf_thresh=0.01, downsample_ratio=8, clip_bbox=True, scale_x_y=1.0, )
[paddle_to_torch] Unsupported API paddle.vision.ops.yolo_box: Rule for paddle.vision.ops.yolo_box is not implemented

2025-07-09 21:51:12.782359 GPU 5 36300 test begin: paddle.vision.ops.yolo_box(Tensor([2, 16, 16777216, 8],"float32"), img_size=Tensor([2, 2],"int32"), anchors=list[10,13,16,30,], class_num=2, conf_thresh=0.01, downsample_ratio=8, clip_bbox=True, scale_x_y=1.0, iou_aware=True, iou_aware_factor=0.5, )
[paddle_to_torch] Unsupported API paddle.vision.ops.yolo_box: Rule for paddle.vision.ops.yolo_box is not implemented

2025-07-09 21:51:12.882949 GPU 5 36300 test begin: paddle.vision.ops.yolo_loss(Tensor([2863312, 30, 5, 5],"float64"), gt_box=Tensor([3, 5, 4],"float64"), gt_label=Tensor([3, 5],"int32"), anchors=list[10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326,], anchor_mask=list[0,1,2,], class_num=5, ignore_thresh=0.7, downsample_ratio=32, gt_score=None, use_label_smooth=True, scale_x_y=1.0, )
[paddle_to_torch] Unsupported API paddle.vision.ops.yolo_loss: Rule for paddle.vision.ops.yolo_loss is not implemented

2025-07-09 21:51:12.983016 GPU 5 36300 test begin: paddle.vision.ops.yolo_loss(Tensor([2863312, 30, 5, 5],"float64"), gt_box=Tensor([3, 5, 4],"float64"), gt_label=Tensor([3, 5],"int32"), anchors=list[10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326,], anchor_mask=list[0,1,2,], class_num=5, ignore_thresh=0.7, downsample_ratio=32, gt_score=Tensor([3, 5],"float64"), use_label_smooth=False, scale_x_y=1.0, )
[paddle_to_torch] Unsupported API paddle.vision.ops.yolo_loss: Rule for paddle.vision.ops.yolo_loss is not implemented

2025-07-09 21:51:13.083869 GPU 5 36300 test begin: paddle.vision.ops.yolo_loss(Tensor([2863312, 30, 5, 5],"float64"), gt_box=Tensor([3, 5, 4],"float64"), gt_label=Tensor([3, 5],"int32"), anchors=list[10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326,], anchor_mask=list[0,1,2,], class_num=5, ignore_thresh=0.7, downsample_ratio=32, gt_score=Tensor([3, 5],"float64"), use_label_smooth=True, scale_x_y=1.0, )
[paddle_to_torch] Unsupported API paddle.vision.ops.yolo_loss: Rule for paddle.vision.ops.yolo_loss is not implemented

2025-07-09 21:51:13.186925 GPU 5 36300 test begin: paddle.vision.ops.yolo_loss(Tensor([4793491, 14, 8, 8],"float32"), gt_box=Tensor([2, 10, 4],"float32"), gt_label=Tensor([2, 10],"int32"), anchors=list[10,13,16,30,], anchor_mask=list[0,1,], class_num=2, ignore_thresh=0.7, downsample_ratio=8, use_label_smooth=True, scale_x_y=1.0, )
[paddle_to_torch] Unsupported API paddle.vision.ops.yolo_loss: Rule for paddle.vision.ops.yolo_loss is not implemented

2025-07-09 21:51:13.288338 GPU 5 36300 test begin: paddle.vision.ops.yolo_loss(x=Tensor([11665, 255, 38, 38],"float32"), gt_box=Tensor([4, 1, 4],"float32"), gt_label=Tensor([4, 1],"int32"), gt_score=Tensor([4, 1],"float32"), anchors=list[10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326,], anchor_mask=list[3,4,5,], class_num=80, ignore_thresh=0.7, downsample_ratio=16, use_label_smooth=True, )
[paddle_to_torch] Unsupported API paddle.vision.ops.yolo_loss: Rule for paddle.vision.ops.yolo_loss is not implemented

2025-07-09 21:51:13.392353 GPU 5 36300 test begin: paddle.vision.ops.yolo_loss(x=Tensor([2917, 255, 76, 76],"float32"), gt_box=Tensor([4, 1, 4],"float32"), gt_label=Tensor([4, 1],"int32"), gt_score=Tensor([4, 1],"float32"), anchors=list[10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326,], anchor_mask=list[0,1,2,], class_num=80, ignore_thresh=0.7, downsample_ratio=8, use_label_smooth=True, )
[paddle_to_torch] Unsupported API paddle.vision.ops.yolo_loss: Rule for paddle.vision.ops.yolo_loss is not implemented

